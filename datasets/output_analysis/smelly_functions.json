[
    {
        "code": "def build_model(self):\r\n        '''构建模型'''\r\n        model = keras.Sequential()\r\n        model.add(Embedding(len(self.num2word) + 2, 300, input_length=self.config.max_len))\r\n        model.add(Bidirectional(GRU(128, return_sequences=True)))\r\n        model.add(Dropout(0.6))\r\n        model.add(Flatten())\r\n        model.add(Dense(len(self.words), activation='softmax'))\r\n        # 设置优化器\r\n        optimizer = Adam(lr=self.config.learning_rate)\r\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n        self.model = model",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self):\r\n        super(VGG19, self).__init__()\r\n        features = models.vgg19(pretrained=True).features\r\n        self.relu1_1 = torch.nn.Sequential()\r\n        self.relu1_2 = torch.nn.Sequential()\r\n\r\n        self.relu2_1 = torch.nn.Sequential()\r\n        self.relu2_2 = torch.nn.Sequential()\r\n\r\n        self.relu3_1 = torch.nn.Sequential()\r\n        self.relu3_2 = torch.nn.Sequential()\r\n        self.relu3_3 = torch.nn.Sequential()\r\n        self.relu3_4 = torch.nn.Sequential()\r\n\r\n        self.relu4_1 = torch.nn.Sequential()\r\n        self.relu4_2 = torch.nn.Sequential()\r\n        self.relu4_3 = torch.nn.Sequential()\r\n        self.relu4_4 = torch.nn.Sequential()\r\n\r\n        self.relu5_1 = torch.nn.Sequential()\r\n        self.relu5_2 = torch.nn.Sequential()\r\n        self.relu5_3 = torch.nn.Sequential()\r\n        self.relu5_4 = torch.nn.Sequential()\r\n\r\n        for x in range(2):\r\n            self.relu1_1.add_module(str(x), features[x])\r\n\r\n        for x in range(2, 4):\r\n            self.relu1_2.add_module(str(x), features[x])\r\n\r\n        for x in range(4, 7):\r\n            self.relu2_1.add_module(str(x), features[x])\r\n\r\n        for x in range(7, 9):\r\n            self.relu2_2.add_module(str(x), features[x])\r\n\r\n        for x in range(9, 12):\r\n            self.relu3_1.add_module(str(x), features[x])\r\n\r\n        for x in range(12, 14):\r\n            self.relu3_2.add_module(str(x), features[x])\r\n\r\n        for x in range(14, 16):\r\n            self.relu3_2.add_module(str(x), features[x])\r\n\r\n        for x in range(16, 18):\r\n            self.relu3_4.add_module(str(x), features[x])\r\n\r\n        for x in range(18, 21):\r\n            self.relu4_1.add_module(str(x), features[x])\r\n\r\n        for x in range(21, 23):\r\n            self.relu4_2.add_module(str(x), features[x])\r\n\r\n        for x in range(23, 25):\r\n            self.relu4_3.add_module(str(x), features[x])\r\n\r\n        for x in range(25, 27):\r\n            self.relu4_4.add_module(str(x), features[x])\r\n\r\n        for x in range(27, 30):\r\n            self.relu5_1.add_module(str(x), features[x])\r\n\r\n        for x in range(30, 32):\r\n            self.relu5_2.add_module(str(x), features[x])\r\n\r\n        for x in range(32, 34):\r\n            self.relu5_3.add_module(str(x), features[x])\r\n\r\n        for x in range(34, 36):\r\n            self.relu5_4.add_module(str(x), features[x])\r\n\r\n        # don't need the gradients, just want the features\r\n        for param in self.parameters():\r\n            param.requires_grad = False",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, input_channel, output_channel, layer_number):\r\n        super().__init__()\r\n        self.convs = torch.nn.Sequential()\r\n        self.convs.append(torch.nn.Conv2d(in_channels=input_channel, out_channels=output_channel, kernel_size=(3, 3), stride=(1, 1), padding=1))\r\n        for i in range(1, layer_number):\r\n            self.convs.append(torch.nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=(3, 3), stride=(1, 1), padding=1))\r\n        self.projection = torch.nn.Conv2d(in_channels=output_channel, out_channels=1, kernel_size=(1, 1), stride=(1, 1), padding=0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def run(model_path):\r\n    \"\"\"Run MonoDepthNN to compute depth maps.\r\n\r\n    Args:\r\n        model_path (str): path to saved model\r\n    \"\"\"\r\n    print(\"initialize\")\r\n\r\n    # select device\r\n\r\n    # load network\r\n    #model = MidasNet(model_path, non_negative=True)\r\n    model = MidasNet_preprocessing(model_path, non_negative=True)\r\n\r\n    model.eval()\r\n    \r\n    print(\"start processing\")\r\n\r\n    # input\r\n    img_input = np.zeros((3, 384, 384), np.float32)  \r\n\r\n    # compute\r\n    with torch.no_grad():\r\n        sample = torch.from_numpy(img_input).unsqueeze(0)\r\n        prediction = model.forward(sample)\r\n        prediction = (\r\n            torch.nn.functional.interpolate(\r\n                prediction.unsqueeze(1),\r\n                size=img_input.shape[:2],\r\n                mode=\"bicubic\",\r\n                align_corners=False,\r\n            )\r\n            .squeeze()\r\n            .cpu()\r\n            .numpy()\r\n        )\r\n\r\n    torch.onnx.export(model, sample, ntpath.basename(model_path).rsplit('.', 1)[0]+'.onnx', opset_version=9)    \r\n    \r\n    print(\"finished\")",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, requires_grad=False, pretrained=True):\r\n        super(vgg16, self).__init__()\r\n        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\r\n        self.slice1 = torch.nn.Sequential()\r\n        self.slice2 = torch.nn.Sequential()\r\n        self.slice3 = torch.nn.Sequential()\r\n        self.slice4 = torch.nn.Sequential()\r\n        self.slice5 = torch.nn.Sequential()\r\n        self.N_slices = 5\r\n        for x in range(4):\r\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\r\n        for x in range(4, 9):\r\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\r\n        for x in range(9, 16):\r\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\r\n        for x in range(16, 23):\r\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\r\n        for x in range(23, 30):\r\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\r\n        if not requires_grad:\r\n            for param in self.parameters():\r\n                param.requires_grad = False",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def generate_m3_dataset(\r\n    dataset_path: Path,\r\n    m3_freq: str,\r\n    dataset_writer: DatasetWriter,\r\n    prediction_length: Optional[int] = None,\r\n):\r\n    from gluonts.dataset.repository import default_dataset_path\r\n\r\n    m3_xls_path = default_dataset_path / \"M3C.xls\"\r\n    if not os.path.exists(m3_xls_path):\r\n        raise RuntimeError(\r\n            \"The m3 data is available at \"\r\n            \"https://forecasters.org/resources/\"\r\n            \"time-series-data/m3-competition/ \"\r\n            \"Please download the file and copy the files to this location: \"\r\n            f\"{m3_xls_path}\"\r\n        )\r\n\r\n    subsets = {\r\n        \"yearly\": M3Setting(\"M3Year\", 6, \"Y\"),\r\n        \"quarterly\": M3Setting(\"M3Quart\", 8, \"Q\"),\r\n        \"monthly\": M3Setting(\"M3Month\", 18, \"M\"),\r\n        \"other\": M3Setting(\"M3Other\", 8, \"Q\"),\r\n    }\r\n    assert (\r\n        m3_freq.lower() in subsets\r\n    ), f\"invalid m3_freq='{m3_freq}'. Allowed values: {subsets.keys()}\"\r\n\r\n    if m3_freq.lower() == \"other\":\r\n        warnings.warn(\r\n            \"Be aware: The M3-other dataset does not have a known frequency.\"\r\n            \" Since gluonts needs a known frequency, we will generate the\"\r\n            \" dataset with an artificial `quarterly` frequency.\"\r\n        )\r\n\r\n    subset = subsets[m3_freq.lower()]\r\n    df = pd.read_excel(m3_xls_path, sheet_name=subset.sheet_name)\r\n\r\n    def truncate_trailing_nan(v: np.ndarray):\r\n        last_finite_index = np.where(np.isfinite(v))[0][-1]\r\n        return v[: last_finite_index + 1]\r\n\r\n    train_data = []\r\n    test_data = []\r\n\r\n    def normalize_category(c: str):\r\n        return c.strip()\r\n\r\n    df[\"Category\"] = df[\"Category\"].apply(normalize_category)\r\n    categories = list(df[\"Category\"].unique())\r\n\r\n    cat_map = {c: i for i, c in enumerate(categories)}\r\n\r\n    for i, (_, row) in enumerate(df.iterrows()):\r\n        vals = row.values\r\n        series, n, nf, category, starting_year, starting_offset = vals[:6]\r\n        target = np.asarray(vals[6:], dtype=np.float64)\r\n        target = truncate_trailing_nan(target)\r\n        assert len(target) == n\r\n        assert nf == subset.prediction_length\r\n        mock_start = \"1750\"\r\n\r\n        if starting_year == 0:\r\n            assert starting_offset == 0\r\n            starting_year = mock_start\r\n\r\n        # fix bugs in M3C xls\r\n        if series == \"N1071\":\r\n            # bug in the m3 dataset\r\n            starting_offset = 1\r\n        if series == \"N 184\":\r\n            starting_offset = 1\r\n\r\n        offset = max(starting_offset - 1, 0)\r\n\r\n        if subset.freq == \"Q\":\r\n            assert 0 <= offset < 4\r\n            time_stamp = f\"{starting_year}-{3 * (offset + 1):02}-15\"\r\n        elif subset.freq == \"Y\":\r\n            assert offset == 0\r\n            time_stamp = f\"{starting_year}-12-15\"\r\n        elif subset.freq == \"M\":\r\n            assert 0 <= offset < 12\r\n            time_stamp = f\"{starting_year}-{offset + 1:02}-15\"\r\n\r\n        start = str(pd.Period(time_stamp, freq=subset.freq))\r\n        cat = [i, cat_map[category]]\r\n\r\n        train_data.append(\r\n            {\r\n                \"target\": target[: -subset.prediction_length],\r\n                \"start\": start,\r\n                \"feat_static_cat\": cat,\r\n                \"item_id\": series,\r\n            }\r\n        )\r\n\r\n        test_data.append(\r\n            {\r\n                \"target\": target,\r\n                \"start\": start,\r\n                \"feat_static_cat\": cat,\r\n                \"item_id\": series,\r\n            }\r\n        )\r\n\r\n    meta = MetaData(\r\n        **metadata(\r\n            cardinality=[len(train_data), len(categories)],\r\n            freq=subset.freq,\r\n            prediction_length=prediction_length or subset.prediction_length,\r\n        )\r\n    )\r\n\r\n    dataset = TrainDatasets(metadata=meta, train=train_data, test=test_data)\r\n    dataset.save(\r\n        path_str=str(dataset_path), writer=dataset_writer, overwrite=True\r\n    )\r\n\r\n    check_dataset(dataset_path, len(df), subset.sheet_name)",
        "labels": [
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def get_metrics_per_ts(\r\n        self, time_series: Union[pd.Series, pd.DataFrame], forecast: Forecast\r\n    ) -> Mapping[str, Union[float, str, None, np.ma.core.MaskedConstant]]:\r\n        if not validate_forecast(forecast, self.quantiles):\r\n            if self.allow_nan_forecast:\r\n                logging.warning(\r\n                    \"Forecast contains NaN values. Metrics may be incorrect.\"\r\n                )\r\n            else:\r\n                raise ValueError(\"Forecast contains NaN values.\")\r\n\r\n        pred_target = np.array(self.extract_pred_target(time_series, forecast))\r\n        past_data = np.array(self.extract_past_data(time_series, forecast))\r\n\r\n        if self.ignore_invalid_values:\r\n            past_data = np.ma.masked_invalid(past_data)\r\n            pred_target = np.ma.masked_invalid(pred_target)\r\n\r\n        try:\r\n            mean_fcst = getattr(forecast, \"mean\", None)\r\n        except NotImplementedError:\r\n            mean_fcst = None\r\n\r\n        median_fcst = forecast.quantile(0.5)\r\n        seasonal_error = calculate_seasonal_error(\r\n            past_data, forecast.start_date.freqstr, self.seasonality\r\n        )\r\n\r\n        metrics: Dict[str, Union[float, str, None]] = self.get_base_metrics(\r\n            forecast, pred_target, mean_fcst, median_fcst, seasonal_error\r\n        )\r\n        metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\r\n            float, metrics[\"abs_target_sum\"]\r\n        )\r\n\r\n        if self.custom_eval_fn is not None:\r\n            for k, (eval_fn, _, fcst_type) in self.custom_eval_fn.items():\r\n                if fcst_type == \"mean\":\r\n                    if mean_fcst is not None:\r\n                        target_fcst = mean_fcst\r\n                    else:\r\n                        logging.warning(\r\n                            \"mean_fcst is None, therefore median_fcst is used.\"\r\n                        )\r\n                        target_fcst = median_fcst\r\n                else:\r\n                    target_fcst = median_fcst\r\n\r\n                try:\r\n                    val = {\r\n                        k: eval_fn(\r\n                            pred_target,\r\n                            target_fcst,\r\n                        )\r\n                    }\r\n                except Exception:\r\n                    logging.warning(f\"Error occurred when evaluating {k}.\")\r\n                    val = {k: np.nan}\r\n\r\n                metrics.update(val)\r\n\r\n        try:\r\n            metrics[\"MSIS\"] = msis(\r\n                pred_target,\r\n                forecast.quantile(self.alpha / 2),\r\n                forecast.quantile(1.0 - self.alpha / 2),\r\n                seasonal_error,\r\n                self.alpha,\r\n            )\r\n        except Exception:\r\n            logging.warning(\"Could not calculate MSIS metric.\")\r\n            metrics[\"MSIS\"] = np.nan\r\n\r\n        if self.calculate_owa:\r\n            from gluonts.ext.naive_2 import naive_2\r\n\r\n            naive_median_forecast = naive_2(\r\n                past_data,\r\n                len(pred_target),\r\n                season_length=get_seasonality(forecast.start_date.freqstr),\r\n            )\r\n            metrics[\"sMAPE_naive2\"] = smape(pred_target, naive_median_forecast)\r\n            metrics[\"MASE_naive2\"] = mase(\r\n                pred_target, naive_median_forecast, seasonal_error\r\n            )\r\n\r\n        for quantile in self.quantiles:\r\n            forecast_quantile = forecast.quantile(quantile.value)\r\n\r\n            metrics[f\"QuantileLoss[{quantile}]\"] = quantile_loss(\r\n                pred_target, forecast_quantile, quantile.value\r\n            )\r\n            metrics[f\"Coverage[{quantile}]\"] = coverage(\r\n                pred_target, forecast_quantile\r\n            )\r\n\r\n        return metrics",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def generate_uber_dataset(\r\n    dataset_path: Path,\r\n    uber_freq: str,\r\n    prediction_length: int,\r\n    dataset_writer: DatasetWriter,\r\n):\r\n    subsets = {\"daily\": \"1D\", \"hourly\": \"1h\"}\r\n    assert (\r\n        uber_freq.lower() in subsets\r\n    ), f\"invalid uber_freq='{uber_freq}'. Allowed values: {subsets.keys()}\"\r\n    freq_setting = subsets[uber_freq.lower()]\r\n\r\n    # download the dataset and read the data\r\n    with tempfile.TemporaryDirectory() as dir_path:\r\n        temp_dir_path = Path(dir_path)\r\n        temp_zip_path = temp_dir_path / \"uber-dataset.zip\"\r\n        uber_url_path = (\r\n            \"http://raw.githubusercontent.com/fivethirtyeight/\"\r\n            \"uber-tlc-foil-response/master/uber-trip-data/\"\r\n            \"uber-raw-data-janjune-15.csv.zip\"\r\n        )\r\n        request.urlretrieve(uber_url_path, temp_zip_path)\r\n        with zipfile.ZipFile(temp_zip_path) as zf:\r\n            zf.extractall(path=temp_dir_path)\r\n        uber_file_path = temp_dir_path / \"uber-raw-data-janjune-15.csv\"\r\n        uber_df = pd.read_csv(\r\n            uber_file_path,\r\n            header=0,\r\n            usecols=[\"Pickup_date\", \"locationID\"],\r\n            index_col=0,\r\n        )\r\n\r\n    # We divide the raw data according to locationID. Each json line represents\r\n    # a time series of a loacationID. The targets are numbers of pickup-events\r\n    # during a day or an hour.\r\n    time_series_of_locations = list(uber_df.groupby(by=\"locationID\"))\r\n\r\n    train_data = []\r\n    test_data = []\r\n    for locationID, df in time_series_of_locations:\r\n        df.sort_index()\r\n        df.index = pd.to_datetime(df.index)\r\n\r\n        count_series = df.resample(rule=freq_setting).size()\r\n        start_time = pd.Timestamp(df.index[0]).strftime(\"%Y-%m-%d %X\")\r\n        target = count_series.values.tolist()\r\n        feat_static_cat = [locationID]\r\n\r\n        test_format_dict = {\r\n            \"start\": start_time,\r\n            \"target\": target,\r\n            \"feat_static_cat\": feat_static_cat,\r\n            \"item_id\": locationID,\r\n        }\r\n        test_data.append(test_format_dict)\r\n\r\n        train_format_dict = {\r\n            \"start\": start_time,\r\n            \"target\": target[:-prediction_length],\r\n            \"feat_static_cat\": feat_static_cat,\r\n            \"item_id\": locationID,\r\n        }\r\n        train_data.append(train_format_dict)\r\n\r\n    meta = MetaData(\r\n        **metadata(\r\n            cardinality=len(time_series_of_locations),\r\n            freq=freq_setting[1],\r\n            prediction_length=prediction_length,\r\n        )\r\n    )\r\n\r\n    dataset = TrainDatasets(metadata=meta, train=train_data, test=test_data)\r\n    dataset.save(\r\n        path_str=str(dataset_path), writer=dataset_writer, overwrite=True\r\n    )",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def generate_m5_dataset(\r\n    dataset_path: Path,\r\n    pandas_freq: str,\r\n    prediction_length: int,\r\n    m5_file_path: Path,\r\n    dataset_writer: DatasetWriter,\r\n):\r\n    cal_path = f\"{m5_file_path}/calendar.csv\"\r\n    sales_path = f\"{m5_file_path}/sales_train_validation.csv\"\r\n\r\n    if not os.path.exists(cal_path) or not os.path.exists(sales_path):\r\n        raise RuntimeError(\r\n            \"M5 data is available on Kaggle\"\r\n            \" (https://www.kaggle.com/c/m5-forecasting-accuracy/data). You\"\r\n            \" first need to agree to the terms of the competition before\"\r\n            \" being able to download the data. After you have done that,\"\r\n            f\" please supply the files at {m5_file_path}.\"\r\n        )\r\n\r\n    # Read M5 data from dataset_path\r\n    calendar = pd.read_csv(cal_path)\r\n    sales_train_validation = pd.read_csv(sales_path)\r\n    submission_prediction_length = prediction_length * 2\r\n\r\n    # Build dynamic features\r\n    cal_features = calendar.drop(\r\n        [\r\n            \"date\",\r\n            \"wm_yr_wk\",\r\n            \"weekday\",\r\n            \"wday\",\r\n            \"month\",\r\n            \"year\",\r\n            \"event_name_1\",\r\n            \"event_name_2\",\r\n            \"d\",\r\n        ],\r\n        axis=1,\r\n        errors=\"ignore\",\r\n    )\r\n    cal_features[\"event_type_1\"] = cal_features[\"event_type_1\"].apply(\r\n        lambda x: 0 if str(x) == \"nan\" else 1\r\n    )\r\n    cal_features[\"event_type_2\"] = cal_features[\"event_type_2\"].apply(\r\n        lambda x: 0 if str(x) == \"nan\" else 1\r\n    )\r\n    test_cal_features = cal_features.values.T\r\n    train_cal_features = test_cal_features[\r\n        :, : -submission_prediction_length - prediction_length\r\n    ]\r\n    test_cal_features = test_cal_features[:, :-submission_prediction_length]\r\n\r\n    test_cal_features_list = [test_cal_features] * len(sales_train_validation)\r\n    train_cal_features_list = [train_cal_features] * len(\r\n        sales_train_validation\r\n    )\r\n\r\n    # Build static features\r\n    state_ids = (\r\n        sales_train_validation[\"state_id\"].astype(\"category\").cat.codes.values\r\n    )\r\n    state_ids_un = np.unique(state_ids)\r\n    store_ids = (\r\n        sales_train_validation[\"store_id\"].astype(\"category\").cat.codes.values\r\n    )\r\n    store_ids_un = np.unique(store_ids)\r\n    cat_ids = (\r\n        sales_train_validation[\"cat_id\"].astype(\"category\").cat.codes.values\r\n    )\r\n    cat_ids_un = np.unique(cat_ids)\r\n    dept_ids = (\r\n        sales_train_validation[\"dept_id\"].astype(\"category\").cat.codes.values\r\n    )\r\n    dept_ids_un = np.unique(dept_ids)\r\n    item_ids = (\r\n        sales_train_validation[\"item_id\"].astype(\"category\").cat.codes.values\r\n    )\r\n    item_ids_un = np.unique(item_ids)\r\n    stat_cat_list = [item_ids, dept_ids, cat_ids, store_ids, state_ids]\r\n    stat_cat = np.concatenate(stat_cat_list)\r\n    stat_cat = stat_cat.reshape(len(stat_cat_list), len(item_ids)).T\r\n    cardinalities = [\r\n        len(item_ids_un),\r\n        len(dept_ids_un),\r\n        len(cat_ids_un),\r\n        len(store_ids_un),\r\n        len(state_ids_un),\r\n    ]\r\n\r\n    # Compute unique ID in case `id` column is missing\r\n    if \"id\" not in sales_train_validation.columns:\r\n        sales_train_validation[\"id\"] = (\r\n            sales_train_validation[\"item_id\"].astype(\"str\")\r\n            + \"_\"\r\n            + sales_train_validation[\"store_id\"].astype(\"str\")\r\n        )\r\n    # Build target series\r\n    train_ids = sales_train_validation[\"id\"]\r\n    train_df = sales_train_validation.drop(\r\n        [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\r\n        axis=1,\r\n    )\r\n    test_target_values = train_df.values.copy()\r\n    train_target_values = [ts[:-prediction_length] for ts in train_df.values]\r\n    dates = [\"2011-01-29 00:00:00\" for _ in range(len(sales_train_validation))]\r\n\r\n    # Build training set\r\n    train_ds = [\r\n        {\r\n            FieldName.TARGET: target.tolist(),\r\n            FieldName.START: start,\r\n            FieldName.FEAT_DYNAMIC_REAL: fdr.tolist(),\r\n            FieldName.FEAT_STATIC_CAT: fsc.tolist(),\r\n            FieldName.ITEM_ID: id,\r\n        }\r\n        for (target, start, fdr, fsc, id) in zip(\r\n            train_target_values,\r\n            dates,\r\n            train_cal_features_list,\r\n            stat_cat,\r\n            train_ids,\r\n        )\r\n    ]\r\n\r\n    # Build testing set\r\n    test_ds = [\r\n        {\r\n            FieldName.TARGET: target.tolist(),\r\n            FieldName.START: start,\r\n            FieldName.FEAT_DYNAMIC_REAL: fdr.tolist(),\r\n            FieldName.FEAT_STATIC_CAT: fsc.tolist(),\r\n            FieldName.ITEM_ID: id,\r\n        }\r\n        for (target, start, fdr, fsc, id) in zip(\r\n            test_target_values,\r\n            dates,\r\n            test_cal_features_list,\r\n            stat_cat,\r\n            train_ids,\r\n        )\r\n    ]\r\n\r\n    meta = MetaData(\r\n        **metadata(\r\n            cardinality=cardinalities,\r\n            freq=pandas_freq,\r\n            prediction_length=prediction_length,\r\n        )\r\n    )\r\n\r\n    dataset = TrainDatasets(metadata=meta, train=train_ds, test=test_ds)\r\n    dataset.save(\r\n        path_str=str(dataset_path), writer=dataset_writer, overwrite=True\r\n    )",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def fit(\r\n        self,\r\n        x_train: Union[pd.DataFrame, List],\r\n        y_train: Union[pd.Series, List],\r\n        max_sample_size: Optional[\r\n            int\r\n        ] = None,  # If not None, choose without replacement\r\n        # replacement min(max_sample_size, len(x_train)) many datapoints\r\n        # to train on.\r\n        seed: int = 1,\r\n        x_train_is_dataframe: bool = False,  # This should be False for\r\n        # XGBoost, but True if one uses lightgbm.\r\n        model_is_already_trained: bool = False,  # True if there is no need to\r\n        # train self.model\r\n        **kwargs,\r\n    ):\r\n        \"\"\"\r\n        Fits self.model and partitions R^n into cells.\r\n\r\n        More accurately, it creates two dictionaries: self.preds_to_ids whose\r\n        keys are the predictions of the training dataset and whose values are\r\n        the ids of their associated bins, and self.ids_to_bins whose keys are\r\n        the ids of the bins and whose values are associated lists of true\r\n        values.\r\n        \"\"\"\r\n        self.x_train_is_dataframe = x_train_is_dataframe\r\n        self.quantile_dicts = defaultdict(dict)\r\n        if not x_train_is_dataframe:\r\n            x_train, y_train = np.array(x_train), np.array(y_train)  # xgboost\r\n        # doens't like lists\r\n        if max_sample_size and x_train_is_dataframe:\r\n            assert max_sample_size > 0\r\n            assert isinstance(x_train, pd.DataFrame)\r\n            sample_size = min(max_sample_size, len(x_train))\r\n            x_train = x_train.sample(\r\n                n=min(sample_size, len(x_train)),\r\n                replace=False,\r\n                random_state=seed,\r\n            )\r\n            y_train = y_train[x_train.index]\r\n        elif max_sample_size:\r\n            assert max_sample_size > 0\r\n            sample_size = min(max_sample_size, len(x_train))\r\n            np.random.seed(seed)\r\n            idx = np.random.choice(\r\n                np.arange(len(x_train)), sample_size, replace=False\r\n            )\r\n            x_train = x_train[idx]\r\n            y_train = y_train[idx]\r\n        if not model_is_already_trained:\r\n            self.model.fit(x_train, y_train, **kwargs)\r\n        y_train_pred = self.model.predict(x_train)\r\n        df = pd.DataFrame(\r\n            {\r\n                \"y_true\": y_train,\r\n                \"y_pred\": y_train_pred,\r\n            }\r\n        ).reset_index(drop=True)\r\n        self.sorted_train_preds = sorted(df[\"y_pred\"].unique())\r\n        cell_values_dict = self.preprocess_df(\r\n            df, min_bin_size=self.min_bin_size\r\n        )\r\n        del df\r\n        gc.collect()\r\n        cell_values_dict_df = pd.DataFrame(\r\n            cell_values_dict.items(), columns=[\"keys\", \"values\"]\r\n        )\r\n        cell_values_dict_df[\"id\"] = cell_values_dict_df[\"values\"].apply(id)\r\n        self.id_to_bins = (\r\n            cell_values_dict_df.groupby(\"id\")[\"values\"].first().to_dict()\r\n        )\r\n        self.preds_to_id = (\r\n            cell_values_dict_df.groupby(\"keys\")[\"id\"].first().to_dict()\r\n        )\r\n        del cell_values_dict_df\r\n        del cell_values_dict\r\n        gc.collect()\r\n\r\n        df = pd.DataFrame({\"preds\": self.sorted_train_preds})\r\n        df[\"bin_ids\"] = df[\"preds\"].apply(lambda x: self.preds_to_id[x])\r\n        bin_ids = df[\"bin_ids\"].drop_duplicates().values\r\n        if len(bin_ids) > 1:\r\n            final_id, penultimate_id = bin_ids[-1], bin_ids[-2]\r\n            if len(self.id_to_bins[final_id]) < self.min_bin_size:\r\n                self.id_to_bins[final_id] += self.id_to_bins[penultimate_id]",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def generate(self) -> None:\r\n        if not self.root.exists():\r\n            self.root.mkdir(parents=True)\r\n\r\n            # Download data and move to our own managed directory\r\n            with tempfile.TemporaryDirectory() as directory:\r\n                self._materialize(Path(directory))\r\n                source = Path(directory) / self.dataset_name\r\n\r\n                # Copy and read metadata\r\n                meta_file = self.root / \"metadata.json\"\r\n                shutil.copyfile(source / \"metadata.json\", meta_file)\r\n                meta = MetaData.parse_file(meta_file)\r\n\r\n                # Copy the data and apply filters\r\n                # We filter all ts with length smaller than 3 * prediction length in the train / val set.\r\n                # Since for the train split the prediction-length-long tail is cropped\r\n                # we have that ts in train split are at least 2 * prediction length long.\r\n                # This makes it possible to predict a full prediction length\r\n                # and have a context window of size prediction length.\r\n                filters = self._filters(\r\n                    meta.prediction_length, 3 * meta.prediction_length\r\n                )\r\n                read_transform_write(\r\n                    self.root / \"train\" / \"data.json\",\r\n                    filters=filters\r\n                    + [\r\n                        EndOfSeriesCutFilter(meta.prediction_length),\r\n                        ItemIDTransform(),\r\n                    ],\r\n                    source=source / \"train\" / \"data.json\",\r\n                )\r\n                num_train = count_lines(self.root / \"train\" / \"data.json\")\r\n\r\n                read_transform_write(\r\n                    self.root / \"val\" / \"data.json\",\r\n                    filters=filters + [ItemIDTransform(num_train)],\r\n                    source=source / \"train\" / \"data.json\",\r\n                )\r\n\r\n                # Although we increase the prediction length for the filters here, this does not\r\n                # exclude any more data! The time series is only longer by the prediction length...\r\n                read_transform_write(\r\n                    self.root / \"test\" / \"data.json\",\r\n                    filters=self._filters(\r\n                        2 * meta.prediction_length, 4 * meta.prediction_length\r\n                    )\r\n                    + [\r\n                        ItemIDTransform(num_train),\r\n                    ],\r\n                    source=source / \"test\" / \"data.json\",\r\n                )\r\n        num_train = count_lines(self.root / \"train\" / \"data.json\")\r\n        num_val = count_lines(self.root / \"val\" / \"data.json\")\r\n        num_test = count_lines(self.root / \"test\" / \"data.json\")\r\n\r\n        assert num_train == num_val and (\r\n            num_test % num_val == 0\r\n        ), \"Splits do not match.\"\r\n\r\n        # compute catch22 features\r\n        file = self.root / \"catch22\" / \"features.pkl\"\r\n        val_data = (\r\n            DatasetSplits(\r\n                self.meta,\r\n                self.root,\r\n                self.dataset_name,\r\n                self.prediction_length,\r\n                _standardize=False,\r\n            )\r\n            .val()\r\n            .data()\r\n        )\r\n        if not file.exists() and (self.catch22_train or self.catch22_val_test):\r\n            file.parent.mkdir(parents=True, exist_ok=True)\r\n\r\n            ts_features = []\r\n            # running this in parallel with more workers seems to throw errors\r\n            for ts in tqdm(val_data, desc=\"generate catch22 features\"):\r\n                # only use 5000 times steps tail to compute features\r\n                start = max(len(ts) - self.catch22_tail, 0)\r\n                ts_features.append(get_features(ts[start:]))\r\n\r\n            df = pd.DataFrame(ts_features)\r\n            # remove constant features\r\n            df = df.loc[:, (df != df.iloc[0]).any()]\r\n            df.to_pickle(file)\r\n\r\n            # compute catch22 nearest neighbours\r\n        file = self.root / \"catch22\" / \"nn_100.pkl\"\r\n        if not file.exists() and (self.catch22_train or self.catch22_val_test):\r\n            df = normalize_features(self.catch22())\r\n            num_ts = len(val_data)\r\n            # for more than 5000 ts use kd_tree algo\r\n            algo = \"brute\" if num_ts <= 5000 else \"kd_tree\"\r\n            num_nn = 100\r\n            nbrs = NearestNeighbors(\r\n                n_neighbors=min(num_ts, num_nn + 1), algorithm=algo\r\n            ).fit(df)\r\n            distances, indices = nbrs.kneighbors(df)\r\n            with open(file, \"wb\") as f:\r\n                # exclude the query ts itself\r\n                pickle.dump(indices[:, 1:], f)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def __call__(\r\n        self,\r\n        net: nn.Module,\r\n        train_iter: DataLoader,\r\n        validation_iter: Optional[DataLoader] = None,\r\n    ) -> None:\r\n        optimizer = Adam(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        lr_scheduler = OneCycleLR(\r\n            optimizer,\r\n            max_lr=self.maximum_learning_rate,\r\n            steps_per_epoch=self.num_batches_per_epoch,\r\n            epochs=self.epochs,\r\n        )\r\n\r\n        for epoch_no in range(self.epochs):\r\n            # mark epoch start time\r\n            tic = time.time()\r\n            cumm_epoch_loss = 0.0\r\n            total = self.num_batches_per_epoch - 1\r\n\r\n            # training loop\r\n            with tqdm(train_iter, total=total) as it:\r\n                for batch_no, data_entry in enumerate(it, start=1):\r\n                    optimizer.zero_grad()\r\n                    inputs = []\r\n                    for v in data_entry.values():\r\n                        if v.ndim >= 4:\r\n                            v = v.reshape(v.shape[:3])\r\n                        inputs.append(v.to(self.device))\r\n                    # inputs = [v.to(self.device) for v in data_entry.values()]\r\n\r\n                    output = net(*inputs)\r\n\r\n                    if isinstance(output, (list, tuple)):\r\n                        loss = output[0]\r\n                    else:\r\n                        loss = output\r\n\r\n                    cumm_epoch_loss += loss.item()\r\n                    avg_epoch_loss = cumm_epoch_loss / batch_no\r\n                    it.set_postfix(\r\n                        {\r\n                            \"epoch\": f\"{epoch_no + 1}/{self.epochs}\",\r\n                            \"avg_loss\": avg_epoch_loss,\r\n                        },\r\n                        refresh=False,\r\n                    )\r\n\r\n                    loss.backward()\r\n                    if self.clip_gradient is not None:\r\n                        nn.utils.clip_grad_norm_(\r\n                            net.parameters(), self.clip_gradient\r\n                        )\r\n\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n\r\n                    if self.num_batches_per_epoch == batch_no:\r\n                        break\r\n                it.close()\r\n\r\n            # validation loop\r\n            if validation_iter is not None:\r\n                cumm_epoch_loss_val = 0.0\r\n                with tqdm(validation_iter, total=total, colour=\"green\") as it:\r\n                    for batch_no, data_entry in enumerate(it, start=1):\r\n                        inputs = [\r\n                            v.to(self.device) for v in data_entry.values()\r\n                        ]\r\n                        with torch.no_grad():\r\n                            output = net(*inputs)\r\n                        if isinstance(output, (list, tuple)):\r\n                            loss = output[0]\r\n                        else:\r\n                            loss = output\r\n\r\n                        cumm_epoch_loss_val += loss.item()\r\n                        avg_epoch_loss_val = cumm_epoch_loss_val / batch_no\r\n                        it.set_postfix(\r\n                            {\r\n                                \"epoch\": f\"{epoch_no + 1}/{self.epochs}\",\r\n                                \"avg_loss\": avg_epoch_loss,\r\n                                \"avg_val_loss\": avg_epoch_loss_val,\r\n                            },\r\n                            refresh=False,\r\n                        )\r\n\r\n                        if self.num_batches_per_epoch == batch_no:\r\n                            break\r\n\r\n                it.close()\r\n\r\n            # mark epoch end time and log time cost of current epoch\r\n            toc = time.time()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self,\r\n        net: nn.Module,\r\n        train_iter: DataLoader,\r\n        validation_iter: Optional[DataLoader] = None,\r\n    ) -> None:\r\n        optimizer = Adam(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n        optimizer_adv = Adam(\r\n            self.sparse_net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        lr_scheduler = OneCycleLR(\r\n            optimizer,\r\n            max_lr=self.maximum_learning_rate,\r\n            steps_per_epoch=self.num_batches_per_epoch,\r\n            epochs=self.epochs,\r\n        )\r\n        loss_function = nn.MSELoss()\r\n\r\n        for epoch_no in range(self.epochs):\r\n            # mark epoch start time\r\n            tic = time.time()\r\n            cumm_epoch_loss = 0.0\r\n            total = self.num_batches_per_epoch - 1\r\n\r\n            # training loop\r\n            with tqdm(train_iter, total=total) as it:\r\n                for batch_no, data_entry in enumerate(it, start=1):\r\n                    # train sparse net\r\n                    self.sparse_net.zero_grad()\r\n                    inputs = {\r\n                        key: data_entry[key].to(self.device)\r\n                        for key in data_entry.keys()\r\n                    }\r\n                    shapes = data_entry[\"past_target_cdf\"].shape\r\n                    delta = self.sparse_net(\r\n                        data_entry[\"past_target_cdf\"].to(self.device),\r\n                        n_sample=100,\r\n                    ).view(shapes)\r\n                    if self.clamp:\r\n                        delta = torch.clamp(\r\n                            delta,\r\n                            max=self.sparse_net.max_norm\r\n                            * data_entry[\"past_target_cdf\"].abs().max(),\r\n                        )\r\n                    inputs[\"past_target_cdf\"] += delta\r\n                    output = net(**inputs)\r\n                    mu, target = output[2], output[-1]\r\n                    loss_sparse = -loss_function(mu, target)\r\n                    loss_sparse.backward()\r\n                    optimizer_adv.step()\r\n\r\n                    # train forecasting model\r\n                    net.zero_grad()\r\n                    perturbed_inputs = {\r\n                        key: data_entry[key].to(self.device)\r\n                        for key in data_entry.keys()\r\n                    }\r\n                    shapes = data_entry[\"past_target_cdf\"].shape\r\n                    delta = self.sparse_net(\r\n                        data_entry[\"past_target_cdf\"].to(self.device),\r\n                        n_sample=100,\r\n                    ).view(shapes)\r\n                    perturbed_inputs[\"past_target_cdf\"] += delta\r\n                    output = net(**perturbed_inputs)\r\n\r\n                    if isinstance(output, (list, tuple)):\r\n                        loss = output[0]\r\n                    else:\r\n                        loss = output\r\n                    cumm_epoch_loss += loss.item()\r\n                    avg_epoch_loss = cumm_epoch_loss / batch_no\r\n                    it.set_postfix(\r\n                        {\r\n                            \"epoch\": f\"{epoch_no + 1}/{self.epochs}\",\r\n                            \"avg_loss\": avg_epoch_loss,\r\n                        },\r\n                        refresh=False,\r\n                    )\r\n\r\n                    loss.backward()\r\n                    if self.clip_gradient is not None:\r\n                        nn.utils.clip_grad_norm_(\r\n                            net.parameters(), self.clip_gradient\r\n                        )\r\n\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n\r\n                    if self.num_batches_per_epoch == batch_no:\r\n                        break\r\n\r\n                it.close()\r\n\r\n            # validation loop\r\n            if validation_iter is not None:\r\n                cumm_epoch_loss_val = 0.0\r\n                with tqdm(validation_iter, total=total, colour=\"green\") as it:\r\n                    for batch_no, data_entry in enumerate(it, start=1):\r\n                        inputs = [\r\n                            v.to(self.device) for v in data_entry.values()\r\n                        ]\r\n                        with torch.no_grad():\r\n                            output = net(*inputs)\r\n                        if isinstance(output, (list, tuple)):\r\n                            loss = output[0]\r\n                        else:\r\n                            loss = output\r\n\r\n                        cumm_epoch_loss_val += loss.item()\r\n                        avg_epoch_loss_val = cumm_epoch_loss_val / batch_no\r\n                        it.set_postfix(\r\n                            {\r\n                                \"epoch\": f\"{epoch_no + 1}/{self.epochs}\",\r\n                                \"avg_loss\": avg_epoch_loss,\r\n                                \"avg_val_loss\": avg_epoch_loss_val,\r\n                            },\r\n                            refresh=False,\r\n                        )\r\n\r\n                        if self.num_batches_per_epoch == batch_no:\r\n                            break\r\n\r\n                it.close()\r\n\r\n            # mark epoch end time and log time cost of current epoch\r\n            toc = time.time()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.Adam(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n\r\n        avg_epoch_grad = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                with timer(\"gradient oracle\"):\r\n                    data_entry = next(training_iter)\r\n                    optimizer.zero_grad()\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    loss = self.inference(net, inputs)\r\n                    loss.backward()\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1)).item()\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            validation_loss = None\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net, inputs)\r\n                        validation_loss += loss.item()\r\n                    validation_loss /= i + 1\r\n            num_iters = (\r\n                self.num_batches_per_epoch * (epoch_no + 1) * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on Adam with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.Adagrad(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n\r\n        avg_epoch_grad = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                with timer(\"gradient oracle\"):\r\n                    data_entry = next(training_iter)\r\n                    optimizer.zero_grad()\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    loss = self.inference(net, inputs)\r\n                    loss.backward()\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1)).item()\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            validation_loss = None\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net, inputs)\r\n                        validation_loss += loss.item()\r\n                    validation_loss /= i + 1\r\n            num_iters = (\r\n                self.num_batches_per_epoch * (epoch_no + 1) * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on Adagrad with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.SGD(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n            nesterov=self.nesterov,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n\r\n        avg_epoch_grad = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                with timer(\"gradient oracle\"):\r\n                    data_entry = next(training_iter)\r\n                    optimizer.zero_grad()\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    loss = self.inference(net, inputs)\r\n                    loss.backward()\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1)).item()\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            validation_loss = None\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net, inputs)\r\n                        validation_loss += loss.item()\r\n                    validation_loss /= i + 1\r\n            num_iters = (\r\n                self.num_batches_per_epoch * (epoch_no + 1) * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on SGD with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.SGD(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n            nesterov=self.nesterov,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        anchor_iter = iter(data_loaders[\"anchor_data_loader\"])\r\n\r\n        group_ratio = (\r\n            data_loaders[\"group_ratio\"] if self.weighted_batch else None\r\n        )\r\n        avg_epoch_grad = 0.0\r\n        v_0_norm = 0.0\r\n        v_t_norm = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                iter_n = epoch_no * self.num_batches_per_epoch + batch_no\r\n                if (\r\n                    iter_n == 0\r\n                    or v_t_norm <= self.gamma * v_0_norm\r\n                    or iter_n % self.freq == 0\r\n                ):\r\n                    anchor_model = copy.deepcopy(net)\r\n                    sg_model = copy.deepcopy(net)\r\n                    anchor_model.zero_grad()\r\n                    with timer(\"gradient oracle\"):\r\n                        data_entry = next(anchor_iter)\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(\r\n                            anchor_model,\r\n                            inputs,\r\n                            weighted_batch=self.weighted_batch,\r\n                            group_ratio=group_ratio,\r\n                        )\r\n                        loss.backward()\r\n                    for p in anchor_model.parameters():\r\n                        if p.grad is None:\r\n                            continue\r\n                        v_0_norm += torch.norm(p.grad.data) ** 2\r\n\r\n                v_t_norm = 0.0\r\n                data_entry = next(training_iter)\r\n                optimizer.zero_grad()\r\n                with timer(\"gradient oracle\"):\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    inputs_ = copy.deepcopy(inputs)\r\n                    net.zero_grad()\r\n                    sg_model.zero_grad()\r\n                    loss = self.inference(sg_model, inputs)\r\n                    loss.backward()\r\n\r\n                loss = self.inference(net, inputs_)\r\n                loss.backward()\r\n                with timer(\"gradient oracle\"):\r\n                    for p1, p2, p3 in zip(\r\n                        net.parameters(),\r\n                        sg_model.parameters(),\r\n                        anchor_model.parameters(),\r\n                    ):\r\n                        if (\r\n                            p1.grad is None\r\n                            or p2.grad is None\r\n                            or p3.grad is None\r\n                        ):\r\n                            continue\r\n                        v_t = torch.zeros_like(p1.grad.data, device=p1.device)\r\n                        v_t.add_(p1.grad.data - p2.grad.data + p3.grad.data)\r\n                        p1.grad.data.zero_().add_(v_t)\r\n                        v_t_norm += torch.norm(v_t) ** 2\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1))\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                net_validate = copy.deepcopy(net)\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net_validate.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net_validate, inputs)\r\n                        validation_loss += loss.item()\r\n                validation_loss /= i + 1\r\n\r\n            num_iters = (\r\n                self.num_batches_per_epoch\r\n                * (epoch_no + 1)\r\n                * 2\r\n                * self.batch_size\r\n                + self.num_batches_per_epoch\r\n                / self.freq\r\n                * self.num_strata\r\n                * (epoch_no + 1)\r\n                * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on SCott with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.Adagrad(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        anchor_iter = iter(data_loaders[\"anchor_data_loader\"])\r\n\r\n        group_ratio = (\r\n            data_loaders[\"group_ratio\"] if self.weighted_batch else None\r\n        )\r\n        avg_epoch_grad = 0.0\r\n        v_0_norm = 0.0\r\n        v_t_norm = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                iter_n = epoch_no * self.num_batches_per_epoch + batch_no\r\n                if (\r\n                    iter_n == 0\r\n                    or v_t_norm <= self.gamma * v_0_norm\r\n                    or iter_n % self.freq == 0\r\n                ):\r\n                    anchor_model = copy.deepcopy(net)\r\n                    sg_model = copy.deepcopy(net)\r\n                    anchor_model.zero_grad()\r\n                    with timer(\"gradient oracle\"):\r\n                        data_entry = next(anchor_iter)\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(\r\n                            anchor_model,\r\n                            inputs,\r\n                            weighted_batch=self.weighted_batch,\r\n                            group_ratio=group_ratio,\r\n                        )\r\n                        loss.backward()\r\n                    for p in anchor_model.parameters():\r\n                        if p.grad is None:\r\n                            continue\r\n                        v_0_norm += torch.norm(p.grad.data) ** 2\r\n\r\n                v_t_norm = 0.0\r\n                data_entry = next(training_iter)\r\n                optimizer.zero_grad()\r\n                with timer(\"gradient oracle\"):\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    inputs_ = copy.deepcopy(inputs)\r\n                    net.zero_grad()\r\n                    sg_model.zero_grad()\r\n                    loss = self.inference(sg_model, inputs)\r\n                    loss.backward()\r\n\r\n                loss = self.inference(net, inputs_)\r\n                loss.backward()\r\n                with timer(\"gradient oracle\"):\r\n                    for p1, p2, p3 in zip(\r\n                        net.parameters(),\r\n                        sg_model.parameters(),\r\n                        anchor_model.parameters(),\r\n                    ):\r\n                        if (\r\n                            p1.grad is None\r\n                            or p2.grad is None\r\n                            or p3.grad is None\r\n                        ):\r\n                            continue\r\n                        v_t = torch.zeros_like(p1.grad.data, device=p1.device)\r\n                        v_t.add_(p1.grad.data - p2.grad.data + p3.grad.data)\r\n                        p1.grad.data.zero_().add_(v_t)\r\n                        v_t_norm += torch.norm(v_t) ** 2\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1))\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                net_validate = copy.deepcopy(net)\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net_validate.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net_validate, inputs)\r\n                        validation_loss += loss.item()\r\n                validation_loss /= i + 1\r\n\r\n            num_iters = (\r\n                self.num_batches_per_epoch\r\n                * (epoch_no + 1)\r\n                * 2\r\n                * self.batch_size\r\n                + self.num_batches_per_epoch\r\n                / self.freq\r\n                * self.num_strata\r\n                * (epoch_no + 1)\r\n                * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on SAdagrad with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.SGD(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n            nesterov=self.nesterov,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        anchor_iter = iter(data_loaders[\"rand_anchor_loader\"])\r\n\r\n        avg_epoch_grad = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                iter_n = epoch_no * self.num_batches_per_epoch + batch_no\r\n                if iter_n % self.freq == 0:\r\n                    anchor_model = copy.deepcopy(net)\r\n                    sg_model = copy.deepcopy(net)\r\n                    anchor_model.zero_grad()\r\n                    with timer(\"gradient oracle\"):\r\n                        data_entry = next(anchor_iter)\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(anchor_model, inputs)\r\n                        loss.backward()\r\n\r\n                data_entry = next(training_iter)\r\n                optimizer.zero_grad()\r\n                with timer(\"gradient oracle\"):\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    inputs_ = copy.deepcopy(inputs)\r\n                    net.zero_grad()\r\n                    sg_model.zero_grad()\r\n                    loss = self.inference(sg_model, inputs)\r\n                    loss.backward()\r\n\r\n                loss = self.inference(net, inputs_)\r\n                loss.backward()\r\n                with timer(\"gradient oracle\"):\r\n                    for p1, p2, p3 in zip(\r\n                        net.parameters(),\r\n                        sg_model.parameters(),\r\n                        anchor_model.parameters(),\r\n                    ):\r\n                        if (\r\n                            p1.grad is None\r\n                            or p2.grad is None\r\n                            or p3.grad is None\r\n                        ):\r\n                            continue\r\n                        p1.grad.data.add_(-p2.grad.data + p3.grad.data)\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1))\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                net_validate = copy.deepcopy(net)\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net_validate.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net_validate, inputs)\r\n                        validation_loss += loss.item()\r\n                validation_loss /= i + 1\r\n\r\n            # log all the results to tensorboard\r\n            num_iters = (\r\n                self.num_batches_per_epoch\r\n                * (epoch_no + 1)\r\n                * 2\r\n                * self.batch_size\r\n                + self.num_batches_per_epoch\r\n                / self.freq\r\n                * self.num_strata\r\n                * (epoch_no + 1)\r\n                * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on SCSG with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __call__(\r\n        self, net: nn.Module, input_names: List[str], data_loaders\r\n    ) -> None:\r\n        optimizer = torch.optim.Adam(\r\n            net.parameters(),\r\n            lr=self.learning_rate,\r\n            weight_decay=self.weight_decay,\r\n        )\r\n\r\n        writer = SummaryWriter(self.tensorboard_path)\r\n\r\n        timer = Timer()\r\n\r\n        training_iter = iter(data_loaders[\"training_data_loader\"])\r\n        anchor_iter = iter(data_loaders[\"anchor_data_loader\"])\r\n\r\n        group_ratio = (\r\n            data_loaders[\"group_ratio\"] if self.weighted_batch else None\r\n        )\r\n        avg_epoch_grad = 0.0\r\n        v_0_norm = 0.0\r\n        v_t_norm = 0.0\r\n        for epoch_no in range(self.epochs):\r\n            if self.decreasing_step_size:\r\n                for param_group in optimizer.param_groups:\r\n                    param_group[\"lr\"] *= 1 / math.sqrt(epoch_no + 1)\r\n            for batch_no in range(self.num_batches_per_epoch):\r\n                iter_n = epoch_no * self.num_batches_per_epoch + batch_no\r\n                if (\r\n                    iter_n == 0\r\n                    or v_t_norm <= self.gamma * v_0_norm\r\n                    or iter_n % self.freq == 0\r\n                ):\r\n                    anchor_model = copy.deepcopy(net)\r\n                    sg_model = copy.deepcopy(net)\r\n                    anchor_model.zero_grad()\r\n                    with timer(\"gradient oracle\"):\r\n                        data_entry = next(anchor_iter)\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(\r\n                            anchor_model,\r\n                            inputs,\r\n                            weighted_batch=self.weighted_batch,\r\n                            group_ratio=group_ratio,\r\n                        )\r\n                        loss.backward()\r\n                    for p in anchor_model.parameters():\r\n                        if p.grad is None:\r\n                            continue\r\n                        v_0_norm += torch.norm(p.grad.data) ** 2\r\n\r\n                v_t_norm = 0.0\r\n                data_entry = next(training_iter)\r\n                optimizer.zero_grad()\r\n                with timer(\"gradient oracle\"):\r\n                    inputs = [\r\n                        data_entry[k].to(self.device) for k in input_names\r\n                    ]\r\n                    inputs_ = copy.deepcopy(inputs)\r\n                    net.zero_grad()\r\n                    sg_model.zero_grad()\r\n                    loss = self.inference(sg_model, inputs)\r\n                    loss.backward()\r\n\r\n                loss = self.inference(net, inputs_)\r\n                loss.backward()\r\n                with timer(\"gradient oracle\"):\r\n                    for p1, p2, p3 in zip(\r\n                        net.parameters(),\r\n                        sg_model.parameters(),\r\n                        anchor_model.parameters(),\r\n                    ):\r\n                        if (\r\n                            p1.grad is None\r\n                            or p2.grad is None\r\n                            or p3.grad is None\r\n                        ):\r\n                            continue\r\n                        v_t = torch.zeros_like(p1.grad.data, device=p1.device)\r\n                        v_t.add_(p1.grad.data - p2.grad.data + p3.grad.data)\r\n                        p1.grad.data.zero_().add_(v_t)\r\n                        v_t_norm += torch.norm(v_t) ** 2\r\n                    optimizer.step()\r\n\r\n            # compute the gradient norm and loss over training set\r\n            avg_epoch_loss = 0.0\r\n            full_batch_iter = iter(data_loaders[\"full_batch_loader\"])\r\n            net.zero_grad()\r\n            for i, data_entry in enumerate(full_batch_iter):\r\n                inputs = [data_entry[k].to(self.device) for k in input_names]\r\n                loss = self.inference(net, inputs)\r\n                loss.backward()\r\n                avg_epoch_loss += loss.item()\r\n            avg_epoch_loss /= i + 1\r\n            epoch_grad = 0.0\r\n            for p in net.parameters():\r\n                if p.grad is None:\r\n                    continue\r\n                epoch_grad += torch.norm(p.grad.data / (i + 1))\r\n            net.zero_grad()\r\n\r\n            # compute the validation loss\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                net_validate = copy.deepcopy(net)\r\n                validation_iter = iter(data_loaders[\"validation_data_loader\"])\r\n                validation_loss = 0.0\r\n                with torch.no_grad():\r\n                    for i, data_entry in enumerate(validation_iter):\r\n                        net_validate.zero_grad()\r\n                        inputs = [\r\n                            data_entry[k].to(self.device) for k in input_names\r\n                        ]\r\n                        loss = self.inference(net_validate, inputs)\r\n                        validation_loss += loss.item()\r\n                validation_loss /= i + 1\r\n\r\n            num_iters = (\r\n                self.num_batches_per_epoch\r\n                * (epoch_no + 1)\r\n                * 2\r\n                * self.batch_size\r\n                + self.num_batches_per_epoch\r\n                / self.freq\r\n                * self.num_strata\r\n                * (epoch_no + 1)\r\n                * self.batch_size\r\n            )\r\n            avg_epoch_grad = (avg_epoch_grad * epoch_no + epoch_grad) / (\r\n                epoch_no + 1\r\n            )\r\n            time_in_ms = timer.totals[\"gradient oracle\"] * 1000\r\n            writer.add_scalar(\r\n                \"gradnorm/iters\",\r\n                avg_epoch_grad,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"gradnorm/grads\", avg_epoch_grad, num_iters)\r\n            writer.add_scalar(\"gradnorm/time\", avg_epoch_grad, time_in_ms)\r\n            writer.add_scalar(\r\n                \"train_loss/iters\",\r\n                avg_epoch_loss,\r\n                (epoch_no + 1) * self.num_batches_per_epoch,\r\n            )\r\n            writer.add_scalar(\"train_loss/grads\", avg_epoch_loss, num_iters)\r\n            writer.add_scalar(\"train_loss/time\", avg_epoch_loss, time_in_ms)\r\n            if self.eval_model and epoch_no % self.validation_freq == 0:\r\n                writer.add_scalar(\r\n                    \"val_loss/iters\",\r\n                    validation_loss,\r\n                    (epoch_no + 1) * self.num_batches_per_epoch,\r\n                )\r\n                writer.add_scalar(\"val_loss/grads\", validation_loss, num_iters)\r\n                writer.add_scalar(\"val_loss/time\", validation_loss, time_in_ms)\r\n                print(\r\n                    \"\\nTraining Loss: {:.4f}, Test Loss: {:.4f}\\n\".format(\r\n                        avg_epoch_loss, validation_loss\r\n                    )\r\n                )\r\n            else:\r\n                print(f\"\\nTraining Loss: {avg_epoch_loss:.4f} \\n\")\r\n            print(\"Epoch \", epoch_no, \" is done!\")\r\n\r\n        writer.close()\r\n        print(\r\n            \"task: \"\r\n            + self.task_name\r\n            + \" on SAdam with lr=\"\r\n            + str(self.learning_rate)\r\n            + \" is done!\"\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def compute_catch22(dataset: Optional[str], data_path: str, output_path: str):\r\n    \"\"\"\r\n    Computes the catch22 features for each time series in a dataset.\r\n\r\n    Computations are either run for a single dataset or all datasets in the\r\n    registry.\r\n    \"\"\"\r\n    target = Path(data_path)\r\n    target.mkdir(parents=True, exist_ok=True)\r\n\r\n    if dataset is None:\r\n        dataset_names = [(k, v(target)) for k, v in DATASET_REGISTRY.items()]\r\n    else:\r\n        dataset_names = [(dataset, DATASET_REGISTRY[dataset](target))]\r\n\r\n    directory = Path(output_path)\r\n    directory.mkdir(parents=True, exist_ok=True)\r\n\r\n    for dataset_name, config in tqdm(\r\n        dataset_names, disable=len(dataset_names) == 1\r\n    ):\r\n        file = directory / f\"{dataset_name}.parquet\"\r\n        if file.exists():\r\n            continue\r\n\r\n        ts_features = process_map(\r\n            _get_features,\r\n            config.data.train(\r\n                val=False\r\n            ).gluonts(),  # Get features on train set\r\n            max_workers=os.cpu_count(),\r\n            desc=dataset_name,\r\n            chunksize=1,\r\n        )\r\n        df = pd.DataFrame(ts_features)\r\n        df.to_parquet(file)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def forward(self, *args, **kwargs):\r\n        distr_args, loc, scale = self.model.forward(*args, **kwargs)\r\n        distr = self.model.distr_output.distribution(distr_args, loc, scale)\r\n\r\n        samples = distr.sample((self.num_parallel_samples,))\r\n        if self.model.nonnegative_pred_samples:\r\n            samples = torch.relu(samples)\r\n        return samples.transpose(0, 1)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_sklearn_single_class(self):\r\n        t = self.iris[60:90]\r\n        self.assertEqual(len(np.unique(t.Y)), 1)\r\n        lr = sklearn.linear_model.LogisticRegression()\r\n        self.assertRaises(ValueError, lr.fit, t.X, t.Y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_all_models_work_after_unpickling_pca(self):\r\n        datasets = [Table('iris'), Table('titanic')]\r\n        for learner in list(all_learners()):\r\n            # calibration, threshold learners' __init__ require arguments\r\n            if learner in (ThresholdLearner, CalibratedLearner):\r\n                continue\r\n            # Skip slow tests\r\n            if issubclass(learner, _RuleLearner):\r\n                continue\r\n            # temporary exclusion of the ScoringSheet learner\r\n            if learner.__name__ == \"ScoringSheetLearner\":\r\n                continue\r\n            with self.subTest(learner.__name__):\r\n                learner = learner()\r\n                for ds in datasets:\r\n                    pca_ds = Orange.projection.PCA()(ds)(ds)\r\n                    # Skip learners that are incompatible with the dataset\r\n                    if learner.incompatibility_reason(pca_ds.domain):\r\n                        continue\r\n                    model = learner(pca_ds)\r\n                    s = pickle.dumps(model, 0)\r\n                    model2 = pickle.loads(s)\r\n\r\n                    np.testing.assert_almost_equal(\r\n                        Table.from_table(model.domain, ds).X,\r\n                        Table.from_table(model2.domain, ds).X)\r\n                    np.testing.assert_almost_equal(\r\n                        model(ds), model2(ds),\r\n                        err_msg='%s does not return same values when unpickled %s'\r\n                                % (learner.__class__.__name__, ds.name))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train(self):\r\n        model, config = self.model, self.config\r\n        raw_model = model.module if hasattr(self.model, \"module\") else model\r\n        optimizer = raw_model.configure_optimizers(config)\r\n\r\n        def run_epoch(split):\r\n            is_train = split == 'train'\r\n            model.train(is_train)\r\n            data = self.train_dataset if is_train else self.test_dataset\r\n            loader = DataLoader(data, shuffle=True, pin_memory=True,\r\n                                batch_size=config.batch_size,\r\n                                num_workers=config.num_workers)\r\n\r\n            pbar = tqdm(enumerate(loader), total=len(loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\r\n            \r\n            for it, (x, y) in pbar:\r\n                x = x.to(self.device) # place data on the correct device\r\n                y = y.to(self.device)\r\n                \r\n                with torch.set_grad_enabled(is_train):\r\n                    _, loss = model(x, y) # forward the model\r\n                    loss = loss.mean()         # collapse all losses if they are scattered on multiple gpus\r\n\r\n                if is_train: # backprop and update the parameters                    \r\n                    model.zero_grad()\r\n                    loss.backward()\r\n\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\r\n                    optimizer.step()\r\n                    \r\n                    if config.lr_decay: # decay the learning rate based on our progress\r\n                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\r\n                        lr_final_factor = config.lr_final / config.learning_rate\r\n                        if self.tokens < config.warmup_tokens:\r\n                            # linear warmup\r\n                            lr_mult = lr_final_factor + (1 - lr_final_factor) * float(self.tokens) / float(config.warmup_tokens)\r\n                            progress = 0\r\n                        else:\r\n                            # cosine learning rate decay\r\n                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\r\n                            # progress = min(progress * 1.1, 1.0) # more fine-tuning with low LR\r\n                            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor / 2) * math.cos(math.pi * progress) # better 1.0 ~ 0.1\r\n                        lr = config.learning_rate * lr_mult\r\n                        for param_group in optimizer.param_groups:\r\n                            param_group['lr'] = lr\r\n                    else:\r\n                        lr = config.learning_rate\r\n\r\n                    now_loss = loss.item() # report progress\r\n                    \r\n                    if 'wandb' in sys.modules:\r\n                        wandb.log({\"loss\": now_loss}, step = self.steps * self.config.batch_size)\r\n                    self.steps += 1\r\n\r\n                    if self.avg_loss < 0:\r\n                        self.avg_loss = now_loss\r\n                    else:\r\n                        # factor = max(1.0 / 300, 1.0 / math.sqrt(it + 1))\r\n                        factor = 1 / (it + 1)\r\n                        self.avg_loss = self.avg_loss * (1.0 - factor) + now_loss * factor\r\n                    pbar.set_description(f\"epoch {epoch+1} progress {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")\r\n\r\n        while True:\r\n            self.tokens = 0 # counter used for learning rate decay\r\n            for epoch in range(config.max_epochs):\r\n\r\n                run_epoch('train')\r\n                \r\n                if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\r\n                    raw_model = self.model.module if hasattr(self.model, \"module\") else self.model # DataParallel wrappers keep raw model object in .module\r\n                    torch.save(raw_model, self.config.epoch_save_path + str(epoch+1) + '.pth')",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_epoch(split):\r\n            is_train = split == 'train'\r\n            model.train(is_train)\r\n            data = self.train_dataset if is_train else self.test_dataset\r\n            loader = DataLoader(data, shuffle=True, pin_memory=True,\r\n                                batch_size=config.batch_size,\r\n                                num_workers=config.num_workers)\r\n\r\n            pbar = tqdm(enumerate(loader), total=len(loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\r\n            \r\n            for it, (x, y) in pbar:\r\n                x = x.to(self.device) # place data on the correct device\r\n                y = y.to(self.device)\r\n                \r\n                with torch.set_grad_enabled(is_train):\r\n                    _, loss = model(x, y) # forward the model\r\n                    loss = loss.mean()         # collapse all losses if they are scattered on multiple gpus\r\n\r\n                if is_train: # backprop and update the parameters                    \r\n                    model.zero_grad()\r\n                    loss.backward()\r\n\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\r\n                    optimizer.step()\r\n                    \r\n                    if config.lr_decay: # decay the learning rate based on our progress\r\n                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\r\n                        lr_final_factor = config.lr_final / config.learning_rate\r\n                        if self.tokens < config.warmup_tokens:\r\n                            # linear warmup\r\n                            lr_mult = lr_final_factor + (1 - lr_final_factor) * float(self.tokens) / float(config.warmup_tokens)\r\n                            progress = 0\r\n                        else:\r\n                            # cosine learning rate decay\r\n                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\r\n                            # progress = min(progress * 1.1, 1.0) # more fine-tuning with low LR\r\n                            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor / 2) * math.cos(math.pi * progress) # better 1.0 ~ 0.1\r\n                        lr = config.learning_rate * lr_mult\r\n                        for param_group in optimizer.param_groups:\r\n                            param_group['lr'] = lr\r\n                    else:\r\n                        lr = config.learning_rate\r\n\r\n                    now_loss = loss.item() # report progress\r\n                    \r\n                    if 'wandb' in sys.modules:\r\n                        wandb.log({\"loss\": now_loss}, step = self.steps * self.config.batch_size)\r\n                    self.steps += 1\r\n\r\n                    if self.avg_loss < 0:\r\n                        self.avg_loss = now_loss\r\n                    else:\r\n                        # factor = max(1.0 / 300, 1.0 / math.sqrt(it + 1))\r\n                        factor = 1 / (it + 1)\r\n                        self.avg_loss = self.avg_loss * (1.0 - factor) + now_loss * factor\r\n                    pbar.set_description(f\"epoch {epoch+1} progress {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __init__(self, load_weights=True):\r\n        super(DISTS, self).__init__()\r\n        vgg_pretrained_features = vision.models.vgg16(\r\n            weights=\"VGG16_Weights.IMAGENET1K_V1\"\r\n        ).features\r\n        self.stage1 = torch.nn.Sequential()\r\n        self.stage2 = torch.nn.Sequential()\r\n        self.stage3 = torch.nn.Sequential()\r\n        self.stage4 = torch.nn.Sequential()\r\n        self.stage5 = torch.nn.Sequential()\r\n        for x in range(0, 4):\r\n            self.stage1.add_module(str(x), vgg_pretrained_features[x])\r\n        self.stage2.add_module(str(4), L2pooling(channels=64))\r\n        for x in range(5, 9):\r\n            self.stage2.add_module(str(x), vgg_pretrained_features[x])\r\n        self.stage3.add_module(str(9), L2pooling(channels=128))\r\n        for x in range(10, 16):\r\n            self.stage3.add_module(str(x), vgg_pretrained_features[x])\r\n        self.stage4.add_module(str(16), L2pooling(channels=256))\r\n        for x in range(17, 23):\r\n            self.stage4.add_module(str(x), vgg_pretrained_features[x])\r\n        self.stage5.add_module(str(23), L2pooling(channels=512))\r\n        for x in range(24, 30):\r\n            self.stage5.add_module(str(x), vgg_pretrained_features[x])\r\n\r\n        self.register_buffer(\r\n            \"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1)\r\n        )\r\n        self.register_buffer(\r\n            \"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1)\r\n        )\r\n\r\n        self.chns = [3, 64, 128, 256, 512, 512]\r\n        self.register_buffer(\r\n            \"alpha\", nn.Parameter(torch.randn(1, sum(self.chns), 1, 1))\r\n        )\r\n        self.register_buffer(\"beta\", nn.Parameter(torch.randn(1, sum(self.chns), 1, 1)))\r\n        self.alpha.data.normal_(0.1, 0.01)\r\n        self.beta.data.normal_(0.1, 0.01)\r\n        weights = torch.load(\"test/DISTS_weights.pt\")\r\n        self.alpha.data = weights[\"alpha\"]\r\n        self.beta.data = weights[\"beta\"]\r\n\r\n        for param in self.parameters():\r\n            param.requires_grad = False",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_config=accelerator_project_config,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        if not is_wandb_available():\r\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\r\n        import wandb\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n        if args.push_to_hub:\r\n            repo_id = create_repo(\r\n                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\r\n            ).repo_id\r\n    # Load scheduler, tokenizer and models.\r\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n    tokenizer = CLIPTokenizer.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\r\n    )\r\n    text_encoder = CLIPTextModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n    # freeze parameters of models to save more memory\r\n    unet.requires_grad_(False)\r\n    vae.requires_grad_(False)\r\n    text_encoder.requires_grad_(False)\r\n\r\n    # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision\r\n    # as these weights are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move unet, vae and text_encoder to device and cast to weight_dtype\r\n    unet.to(accelerator.device, dtype=weight_dtype)\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # now we will add new LoRA weights to the attention layers\r\n    # It's important to realize here how many attention weights will be added and of which sizes\r\n    # The sizes of the attention layers consist only of two different variables:\r\n    # 1) - the \"hidden_size\", which is increased according to `unet.config.block_out_channels`.\r\n    # 2) - the \"cross attention size\", which is set to `unet.config.cross_attention_dim`.\r\n\r\n    # Let's first see how many attention processors we will have to set.\r\n    # For Stable Diffusion, it should be equal to:\r\n    # - down blocks (2x attention layers) * (2x transformer layers) * (3x down blocks) = 12\r\n    # - mid blocks (2x attention layers) * (1x transformer layers) * (1x mid blocks) = 2\r\n    # - up blocks (2x attention layers) * (3x transformer layers) * (3x down blocks) = 18\r\n    # => 32 layers\r\n\r\n    # Set correct lora layers\r\n    lora_attn_procs = {}\r\n    for name in unet.attn_processors.keys():\r\n        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\r\n        if name.startswith(\"mid_block\"):\r\n            hidden_size = unet.config.block_out_channels[-1]\r\n        elif name.startswith(\"up_blocks\"):\r\n            block_id = int(name[len(\"up_blocks.\")])\r\n            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\r\n        elif name.startswith(\"down_blocks\"):\r\n            block_id = int(name[len(\"down_blocks.\")])\r\n            hidden_size = unet.config.block_out_channels[block_id]\r\n\r\n        lora_attn_procs[name] = LoRAAttnProcessor(\r\n            hidden_size=hidden_size,\r\n            cross_attention_dim=cross_attention_dim,\r\n            rank=args.rank,\r\n        )\r\n\r\n    unet.set_attn_processor(lora_attn_procs)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            import xformers\r\n\r\n            xformers_version = version.parse(xformers.__version__)\r\n            if xformers_version == version.parse(\"0.0.16\"):\r\n                logger.warn(\r\n                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\r\n                )\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    lora_layers = AttnProcsLayers(unet.attn_processors)\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Initialize the optimizer\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\r\n            )\r\n\r\n        optimizer_cls = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_cls = torch.optim.AdamW\r\n\r\n    optimizer = optimizer_cls(\r\n        lora_layers.parameters(),\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Get the datasets: you can either provide your own training and evaluation files (see below)\r\n    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\r\n\r\n    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\r\n    # download the dataset.\r\n    if args.dataset_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        dataset = load_dataset(\r\n            args.dataset_name,\r\n            args.dataset_config_name,\r\n            cache_dir=args.cache_dir,\r\n            data_dir=args.train_data_dir,\r\n        )\r\n    else:\r\n        data_files = {}\r\n        if args.train_data_dir is not None:\r\n            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\r\n        dataset = load_dataset(\r\n            \"imagefolder\",\r\n            data_files=data_files,\r\n            cache_dir=args.cache_dir,\r\n        )\r\n        # See more about loading custom images at\r\n        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\r\n\r\n    # Preprocessing the datasets.\r\n    # We need to tokenize inputs and targets.\r\n    column_names = dataset[\"train\"].column_names\r\n\r\n    # 6. Get the column names for input/target.\r\n    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\r\n    if args.image_column is None:\r\n        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\r\n    else:\r\n        image_column = args.image_column\r\n        if image_column not in column_names:\r\n            raise ValueError(\r\n                f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\r\n            )\r\n    if args.caption_column is None:\r\n        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\r\n    else:\r\n        caption_column = args.caption_column\r\n        if caption_column not in column_names:\r\n            raise ValueError(\r\n                f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\r\n            )\r\n\r\n    # Preprocessing the datasets.\r\n    # We need to tokenize input captions and transform the images.\r\n    def tokenize_captions(examples, is_train=True):\r\n        captions = []\r\n        for caption in examples[caption_column]:\r\n            if isinstance(caption, str):\r\n                captions.append(caption)\r\n            elif isinstance(caption, (list, np.ndarray)):\r\n                # take a random caption if there are multiple\r\n                captions.append(random.choice(caption) if is_train else caption[0])\r\n            else:\r\n                raise ValueError(\r\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\r\n                )\r\n        inputs = tokenizer(\r\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\r\n        )\r\n        return inputs.input_ids\r\n\r\n    # Preprocessing the datasets.\r\n    train_transforms = transforms.Compose(\r\n        [\r\n            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\r\n            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\r\n            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize([0.5], [0.5]),\r\n        ]\r\n    )\r\n\r\n    def preprocess_train(examples):\r\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\r\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\r\n        examples[\"input_ids\"] = tokenize_captions(examples)\r\n        return examples\r\n\r\n    with accelerator.main_process_first():\r\n        if args.max_train_samples is not None:\r\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\r\n        # Set the training transforms\r\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\r\n\r\n    def collate_fn(examples):\r\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\r\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\r\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\r\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\r\n\r\n    # DataLoaders creation:\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        shuffle=True,\r\n        collate_fn=collate_fn,\r\n        batch_size=args.train_batch_size,\r\n        num_workers=args.dataloader_num_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\r\n        num_training_steps=args.max_train_steps * accelerator.num_processes,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    lora_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n        lora_layers, optimizer, train_dataloader, lr_scheduler\r\n    )\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"text2image-fine-tune\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1] if len(dirs) > 0 else None\r\n\r\n        if path is None:\r\n            accelerator.print(\r\n                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\r\n            )\r\n            args.resume_from_checkpoint = None\r\n            initial_global_step = 0\r\n        else:\r\n            accelerator.print(f\"Resuming from checkpoint {path}\")\r\n            accelerator.load_state(os.path.join(args.output_dir, path))\r\n            global_step = int(path.split(\"-\")[1])\r\n\r\n            initial_global_step = global_step\r\n            first_epoch = global_step // num_update_steps_per_epoch\r\n    else:\r\n        initial_global_step = 0\r\n\r\n    progress_bar = tqdm(\r\n        range(0, args.max_train_steps),\r\n        initial=initial_global_step,\r\n        desc=\"Steps\",\r\n        # Only show the progress bar once on each machine.\r\n        disable=not accelerator.is_local_main_process,\r\n    )\r\n\r\n    if args.sfast:\r\n        import functools\r\n        from sfast.dynamo.backends.sfast_jit import sfast_jit_trace\r\n        from sfast.compilers.diffusion_pipeline_compiler import (_build_ts_compiler,\r\n                                                                 CompilationConfig)\r\n\r\n        # torch.jit.set_fusion_strategy([('STATIC', 0), ('DYNAMIC', 0)])\r\n        config = CompilationConfig.Default()\r\n\r\n        # if args.enable_xformers_memory_efficient_attention:\r\n        #     config.enable_xformers = True\r\n        try:\r\n            import triton\r\n            config.enable_triton = True\r\n        except ImportError:\r\n            print('Triton not installed, skip')\r\n        # config.enable_cuda_graph = True\r\n\r\n        torch._dynamo.config.suppress_errors = True\r\n        if config.memory_format is not None:\r\n            unet = unet.to(memory_format=config.memory_format)\r\n        unet = torch.compile(unet, backend=functools.partial(\r\n            sfast_jit_trace,\r\n            ts_compiler=_build_ts_compiler(config)))\r\n    elif args.compile:\r\n        torch._dynamo.config.suppress_errors = True\r\n        unet = unet.to(memory_format=torch.channels_last)\r\n        unet = torch.compile(unet)\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n        train_loss = 0.0\r\n        for step, batch in enumerate(train_dataloader):\r\n            with accelerator.accumulate(unet):\r\n                # Convert images to latent space\r\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                latents = latents * vae.config.scaling_factor\r\n\r\n                # Sample noise that we'll add to the latents\r\n                noise = torch.randn_like(latents)\r\n                if args.noise_offset:\r\n                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise\r\n                    noise += args.noise_offset * torch.randn(\r\n                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\r\n                    )\r\n\r\n                bsz = latents.shape[0]\r\n                # Sample a random timestep for each image\r\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\r\n                timesteps = timesteps.long()\r\n\r\n                # Add noise to the latents according to the noise magnitude at each timestep\r\n                # (this is the forward diffusion process)\r\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                # Get the text embedding for conditioning\r\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                # Get the target for loss depending on the prediction type\r\n                if args.prediction_type is not None:\r\n                    # set prediction_type of scheduler if defined\r\n                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)\r\n\r\n                if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                    target = noise\r\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                else:\r\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                # Predict the noise residual and compute loss\r\n                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                if args.snr_gamma is None:\r\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n                else:\r\n                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\r\n                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\r\n                    # This is discussed in Section 4.2 of the same paper.\r\n                    snr = compute_snr(noise_scheduler, timesteps)\r\n                    if noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        # Velocity objective requires that we add one to SNR values before we divide by them.\r\n                        snr = snr + 1\r\n                    mse_loss_weights = (\r\n                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\r\n                    )\r\n\r\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\r\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\r\n                    loss = loss.mean()\r\n\r\n                # Gather the losses across all processes for logging (if we use distributed training).\r\n                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\r\n                train_loss += avg_loss.item() / args.gradient_accumulation_steps\r\n\r\n                # Backpropagate\r\n                accelerator.backward(loss)\r\n                if accelerator.sync_gradients:\r\n                    params_to_clip = lora_layers.parameters()\r\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n\r\n            # Checks if the accelerator has performed an optimization step behind the scenes\r\n            if accelerator.sync_gradients:\r\n                progress_bar.update(1)\r\n                global_step += 1\r\n                accelerator.log({\"train_loss\": train_loss}, step=global_step)\r\n                train_loss = 0.0\r\n\r\n                if global_step % args.checkpointing_steps == 0:\r\n                    if accelerator.is_main_process:\r\n                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\r\n                        if args.checkpoints_total_limit is not None:\r\n                            checkpoints = os.listdir(args.output_dir)\r\n                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\r\n                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\r\n\r\n                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\r\n                            if len(checkpoints) >= args.checkpoints_total_limit:\r\n                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\r\n                                removing_checkpoints = checkpoints[0:num_to_remove]\r\n\r\n                                logger.info(\r\n                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\r\n                                )\r\n                                logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\r\n\r\n                                for removing_checkpoint in removing_checkpoints:\r\n                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\r\n                                    shutil.rmtree(removing_checkpoint)\r\n\r\n                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\r\n                        accelerator.save_state(save_path)\r\n                        logger.info(f\"Saved state to {save_path}\")\r\n\r\n            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n            progress_bar.set_postfix(**logs)\r\n\r\n            if global_step >= args.max_train_steps:\r\n                break\r\n\r\n        if accelerator.is_main_process:\r\n            if args.validation_prompt is not None and epoch % args.validation_epochs == 0:\r\n                logger.info(\r\n                    f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\r\n                    f\" {args.validation_prompt}.\"\r\n                )\r\n                # create pipeline\r\n                pipeline = DiffusionPipeline.from_pretrained(\r\n                    args.pretrained_model_name_or_path,\r\n                    unet=accelerator.unwrap_model(unet),\r\n                    revision=args.revision,\r\n                    torch_dtype=weight_dtype,\r\n                )\r\n                pipeline = pipeline.to(accelerator.device)\r\n                pipeline.set_progress_bar_config(disable=True)\r\n\r\n                # run inference\r\n                generator = torch.Generator(device=accelerator.device)\r\n                if args.seed is not None:\r\n                    generator = generator.manual_seed(args.seed)\r\n                images = []\r\n                for _ in range(args.num_validation_images):\r\n                    images.append(\r\n                        pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0]\r\n                    )\r\n\r\n                for tracker in accelerator.trackers:\r\n                    if tracker.name == \"tensorboard\":\r\n                        np_images = np.stack([np.asarray(img) for img in images])\r\n                        tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                    if tracker.name == \"wandb\":\r\n                        tracker.log(\r\n                            {\r\n                                \"validation\": [\r\n                                    wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\r\n                                    for i, image in enumerate(images)\r\n                                ]\r\n                            }\r\n                        )\r\n\r\n                del pipeline\r\n                torch.cuda.empty_cache()\r\n\r\n    # Save the lora layers\r\n    accelerator.wait_for_everyone()\r\n    if accelerator.is_main_process:\r\n        unet = unet.to(torch.float32)\r\n        unet.save_attn_procs(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            save_model_card(\r\n                repo_id,\r\n                images=images,\r\n                base_model=args.pretrained_model_name_or_path,\r\n                dataset_name=args.dataset_name,\r\n                repo_folder=args.output_dir,\r\n            )\r\n            upload_folder(\r\n                repo_id=repo_id,\r\n                folder_path=args.output_dir,\r\n                commit_message=\"End of training\",\r\n                ignore_patterns=[\"step_*\", \"epoch_*\"],\r\n            )\r\n\r\n    # Final inference\r\n    # Load previous pipeline\r\n    pipeline = DiffusionPipeline.from_pretrained(\r\n        args.pretrained_model_name_or_path, revision=args.revision, torch_dtype=weight_dtype\r\n    )\r\n    pipeline = pipeline.to(accelerator.device)\r\n\r\n    # load attention processors\r\n    pipeline.unet.load_attn_procs(args.output_dir)\r\n\r\n    # run inference\r\n    generator = torch.Generator(device=accelerator.device)\r\n    if args.seed is not None:\r\n        generator = generator.manual_seed(args.seed)\r\n    images = []\r\n    for _ in range(args.num_validation_images):\r\n        images.append(pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0])\r\n\r\n    if accelerator.is_main_process:\r\n        for tracker in accelerator.trackers:\r\n            if len(images) != 0:\r\n                if tracker.name == \"tensorboard\":\r\n                    np_images = np.stack([np.asarray(img) for img in images])\r\n                    tracker.writer.add_images(\"test\", np_images, epoch, dataformats=\"NHWC\")\r\n                if tracker.name == \"wandb\":\r\n                    tracker.log(\r\n                        {\r\n                            \"test\": [\r\n                                wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\r\n                                for i, image in enumerate(images)\r\n                            ]\r\n                        }\r\n                    )\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_d_vector_file = True\r\n\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\r\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_postnet_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[0] = 30\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_embeddings = torch.rand(8, 55).to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.d_vector_dim = 55\r\n        model = Tacotron2(config).to(device)\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for i in range(5):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"d_vectors\": speaker_embeddings}\r\n            )\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.max() <= 1.0\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.min() >= 0.0\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], mel_postnet_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for name_param, param_ref in zip(model.named_parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            name, param = name_param\r\n            if name == \"gst_layer.encoder.recurrence.weight_hh_l0\":\r\n                continue\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = True\r\n        config.num_speakers = 5\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\r\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_postnet_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[0] = 30\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.d_vector_dim = 55\r\n        model = Tacotron2(config).to(device)\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(5):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.max() <= 1.0\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.min() >= 0.0\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], mel_postnet_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step(self):\r\n        # with random gst mel style\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = True\r\n        config.num_speakers = 10\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\r\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_postnet_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[0] = 30\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n        model = Tacotron2(config).to(device)\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for i in range(10):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.max() <= 1.0\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.min() >= 0.0\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], mel_postnet_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for name_param, param_ref in zip(model.named_parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            name, param = name_param\r\n            if name == \"gst_layer.encoder.recurrence.weight_hh_l0\":\r\n                # print(param.grad)\r\n                continue\r\n            assert (param != param_ref).any(), \"param {} {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                name, count, param.shape, param, param_ref\r\n            )\r\n            count += 1\r\n\r\n        # with file gst style\r\n        mel_spec = (\r\n            torch.FloatTensor(ap.melspectrogram(ap.load_wav(WAV_FILE)))[:, :30].unsqueeze(0).transpose(1, 2).to(device)\r\n        )\r\n        mel_spec = mel_spec.repeat(8, 1, 1)\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\r\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\r\n        mel_postnet_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[0] = 30\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        model = Tacotron2(config).to(device)\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for i in range(10):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.max() <= 1.0\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.min() >= 0.0\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], mel_postnet_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for name_param, param_ref in zip(model.named_parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            name, param = name_param\r\n            if name == \"gst_layer.encoder.recurrence.weight_hh_l0\":\r\n                # print(param.grad)\r\n                continue\r\n            assert (param != param_ref).any(), \"param {} {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                name, count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = Tacotron2Config(\r\n            num_chars=32,\r\n            num_speakers=10,\r\n            use_speaker_embedding=True,\r\n            out_channels=80,\r\n            decoder_output_dim=80,\r\n            use_capacitron_vae=True,\r\n            capacitron_vae=CapacitronVAEConfig(),\r\n            optimizer=\"CapacitronOptimizer\",\r\n            optimizer_params={\r\n                \"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6},\r\n                \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9},\r\n            },\r\n        )\r\n\r\n        batch = dict({})\r\n        batch[\"text_input\"] = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        batch[\"text_lengths\"] = torch.randint(100, 129, (8,)).long().to(device)\r\n        batch[\"text_lengths\"] = torch.sort(batch[\"text_lengths\"], descending=True)[0]\r\n        batch[\"text_lengths\"][0] = 128\r\n        batch[\"mel_input\"] = torch.rand(8, 120, config.audio[\"num_mels\"]).to(device)\r\n        batch[\"mel_lengths\"] = torch.randint(20, 120, (8,)).long().to(device)\r\n        batch[\"mel_lengths\"] = torch.sort(batch[\"mel_lengths\"], descending=True)[0]\r\n        batch[\"mel_lengths\"][0] = 120\r\n        batch[\"stop_targets\"] = torch.zeros(8, 120, 1).float().to(device)\r\n        batch[\"stop_target_lengths\"] = torch.randint(0, 120, (8,)).to(device)\r\n        batch[\"speaker_ids\"] = torch.randint(0, 5, (8,)).long().to(device)\r\n        batch[\"d_vectors\"] = None\r\n\r\n        for idx in batch[\"mel_lengths\"]:\r\n            batch[\"stop_targets\"][:, int(idx.item()) :, 0] = 1.0\r\n\r\n        batch[\"stop_targets\"] = batch[\"stop_targets\"].view(\r\n            batch[\"text_input\"].shape[0], batch[\"stop_targets\"].size(1) // config.r, -1\r\n        )\r\n        batch[\"stop_targets\"] = (batch[\"stop_targets\"].sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        model = Tacotron2(config).to(device)\r\n        criterion = model.get_criterion().to(device)\r\n        optimizer = model.get_optimizer()\r\n\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        for _ in range(10):\r\n            _, loss_dict = model.train_step(batch, criterion)\r\n            optimizer.zero_grad()\r\n            loss_dict[\"capacitron_vae_beta_loss\"].backward()\r\n            optimizer.first_step()\r\n            loss_dict[\"loss\"].backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = True\r\n        config.num_speakers = 10\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n        # with random gst mel style\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\r\n        input_lengths[-1] = 128\r\n        mel_spec = torch.rand(8, 120, config.audio[\"num_mels\"]).to(device)\r\n        linear_spec = torch.rand(8, 120, config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        mel_lengths = torch.randint(20, 120, (8,)).long().to(device)\r\n        mel_lengths[-1] = 120\r\n        stop_targets = torch.zeros(8, 120, 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n        model = Tacotron(config).to(device)  # FIXME: missing num_speakers parameter to Tacotron ctor\r\n        model.train()\r\n        # print(model)\r\n        print(\" > Num parameters for Tacotron GST model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(10):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], linear_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1\r\n\r\n        # with file gst style\r\n        mel_spec = (\r\n            torch.FloatTensor(ap.melspectrogram(ap.load_wav(WAV_FILE)))[:, :120].unsqueeze(0).transpose(1, 2).to(device)\r\n        )\r\n        mel_spec = mel_spec.repeat(8, 1, 1)\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\r\n        input_lengths[-1] = 128\r\n        linear_spec = torch.rand(8, mel_spec.size(1), config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        mel_lengths = torch.randint(20, mel_spec.size(1), (8,)).long().to(device)\r\n        mel_lengths[-1] = mel_spec.size(1)\r\n        stop_targets = torch.zeros(8, mel_spec.size(1), 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        model = Tacotron(config).to(device)  # FIXME: missing num_speakers parameter to Tacotron ctor\r\n        model.train()\r\n        # print(model)\r\n        print(\" > Num parameters for Tacotron GST model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(10):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], linear_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = False\r\n        config.num_speakers = 1\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\r\n        input_lengths[-1] = 128\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        linear_spec = torch.rand(8, 30, config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[-1] = mel_spec.size(1)\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        model = Tacotron(config).to(device)  # FIXME: missing num_speakers parameter to Tacotron ctor\r\n        model.train()\r\n        print(\" > Num parameters for Tacotron model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(5):\r\n            outputs = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths)\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], linear_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = True\r\n        config.num_speakers = 5\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\r\n        input_lengths[-1] = 128\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        linear_spec = torch.rand(8, 30, config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[-1] = mel_spec.size(1)\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_ids = torch.randint(0, 5, (8,)).long().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.d_vector_dim = 55\r\n        model = Tacotron(config).to(device)  # FIXME: missing num_speakers parameter to Tacotron ctor\r\n        model.train()\r\n        print(\" > Num parameters for Tacotron model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(5):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"speaker_ids\": speaker_ids}\r\n            )\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], linear_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step(self):  # pylint: disable=no-self-use\r\n        config = config_global.copy()\r\n        config.use_speaker_embedding = False\r\n        config.num_speakers = 1\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\r\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_postnet_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[0] = 30\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        model = Tacotron2(config).to(device)\r\n        model.train()\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for i in range(5):\r\n            outputs = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths)\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.max() <= 1.0\r\n            assert torch.sigmoid(outputs[\"stop_tokens\"]).data.min() >= 0.0\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], mel_postnet_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = TacotronConfig(\r\n            num_chars=32,\r\n            num_speakers=10,\r\n            use_speaker_embedding=True,\r\n            out_channels=513,\r\n            decoder_output_dim=80,\r\n            use_capacitron_vae=True,\r\n            capacitron_vae=CapacitronVAEConfig(),\r\n            optimizer=\"CapacitronOptimizer\",\r\n            optimizer_params={\r\n                \"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6},\r\n                \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9},\r\n            },\r\n        )\r\n\r\n        batch = dict({})\r\n        batch[\"text_input\"] = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        batch[\"text_lengths\"] = torch.randint(100, 129, (8,)).long().to(device)\r\n        batch[\"text_lengths\"] = torch.sort(batch[\"text_lengths\"], descending=True)[0]\r\n        batch[\"text_lengths\"][0] = 128\r\n        batch[\"linear_input\"] = torch.rand(8, 120, config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        batch[\"mel_input\"] = torch.rand(8, 120, config.audio[\"num_mels\"]).to(device)\r\n        batch[\"mel_lengths\"] = torch.randint(20, 120, (8,)).long().to(device)\r\n        batch[\"mel_lengths\"] = torch.sort(batch[\"mel_lengths\"], descending=True)[0]\r\n        batch[\"mel_lengths\"][0] = 120\r\n        batch[\"stop_targets\"] = torch.zeros(8, 120, 1).float().to(device)\r\n        batch[\"stop_target_lengths\"] = torch.randint(0, 120, (8,)).to(device)\r\n        batch[\"speaker_ids\"] = torch.randint(0, 5, (8,)).long().to(device)\r\n        batch[\"d_vectors\"] = None\r\n\r\n        for idx in batch[\"mel_lengths\"]:\r\n            batch[\"stop_targets\"][:, int(idx.item()) :, 0] = 1.0\r\n\r\n        batch[\"stop_targets\"] = batch[\"stop_targets\"].view(\r\n            batch[\"text_input\"].shape[0], batch[\"stop_targets\"].size(1) // config.r, -1\r\n        )\r\n        batch[\"stop_targets\"] = (batch[\"stop_targets\"].sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n        model = Tacotron(config).to(device)\r\n        criterion = model.get_criterion()\r\n        optimizer = model.get_optimizer()\r\n        model.train()\r\n        print(\" > Num parameters for Tacotron with Capacitron VAE model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        for _ in range(10):\r\n            _, loss_dict = model.train_step(batch, criterion)\r\n            optimizer.zero_grad()\r\n            loss_dict[\"capacitron_vae_beta_loss\"].backward()\r\n            optimizer.first_step()\r\n            loss_dict[\"loss\"].backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step():\r\n        config = config_global.copy()\r\n        config.use_d_vector_file = True\r\n\r\n        config.use_gst = True\r\n        config.gst = GSTConfig()\r\n\r\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\r\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\r\n        input_lengths[-1] = 128\r\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)\r\n        linear_spec = torch.rand(8, 30, config.audio[\"fft_size\"] // 2 + 1).to(device)\r\n        mel_lengths = torch.randint(20, 30, (8,)).long().to(device)\r\n        mel_lengths[-1] = mel_spec.size(1)\r\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\r\n        speaker_embeddings = torch.rand(8, 55).to(device)\r\n\r\n        for idx in mel_lengths:\r\n            stop_targets[:, int(idx.item()) :, 0] = 1.0\r\n\r\n        stop_targets = stop_targets.view(input_dummy.shape[0], stop_targets.size(1) // config.r, -1)\r\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\r\n\r\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\r\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\r\n        config.d_vector_dim = 55\r\n        model = Tacotron(config).to(device)  # FIXME: missing num_speakers parameter to Tacotron ctor\r\n        model.train()\r\n        print(\" > Num parameters for Tacotron model:%s\" % (count_parameters(model)))\r\n        model_ref = copy.deepcopy(model)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n        for _ in range(5):\r\n            outputs = model.forward(\r\n                input_dummy, input_lengths, mel_spec, mel_lengths, aux_input={\"d_vectors\": speaker_embeddings}\r\n            )\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs[\"decoder_outputs\"], mel_spec, mel_lengths)\r\n            stop_loss = criterion_st(outputs[\"stop_tokens\"], stop_targets)\r\n            loss = loss + criterion(outputs[\"model_outputs\"], linear_spec, mel_lengths) + stop_loss\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for name_param, param_ref in zip(model.named_parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            name, param = name_param\r\n            if name == \"gst_layer.encoder.recurrence.weight_hh_l0\":\r\n                continue\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_multispeaker_forward(self):\r\n        num_speakers = 10\r\n\r\n        config = VitsConfig(num_speakers=num_speakers, use_speaker_embedding=True)\r\n        config.model_args.spec_segment_size = 10\r\n\r\n        input_dummy, input_lengths, _, spec, spec_lengths, waveform = self._create_inputs(config)\r\n        speaker_ids = torch.randint(0, num_speakers, (8,)).long().to(device)\r\n\r\n        model = Vits(config).to(device)\r\n        output_dict = model.forward(\r\n            input_dummy, input_lengths, spec, spec_lengths, waveform, aux_input={\"speaker_ids\": speaker_ids}\r\n        )\r\n        self._check_forward_outputs(config, output_dict)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_d_vector_forward(self):\r\n        batch_size = 2\r\n        args = VitsArgs(\r\n            spec_segment_size=10,\r\n            num_chars=32,\r\n            use_d_vector_file=True,\r\n            d_vector_dim=256,\r\n            d_vector_file=[os.path.join(get_tests_data_path(), \"dummy_speakers.json\")],\r\n        )\r\n        config = VitsConfig(model_args=args)\r\n        model = Vits.init_from_config(config, verbose=False).to(device)\r\n        model.train()\r\n        input_dummy, input_lengths, _, spec, spec_lengths, waveform = self._create_inputs(config, batch_size=batch_size)\r\n        d_vectors = torch.randn(batch_size, 256).to(device)\r\n        output_dict = model.forward(\r\n            input_dummy, input_lengths, spec, spec_lengths, waveform, aux_input={\"d_vectors\": d_vectors}\r\n        )\r\n        self._check_forward_outputs(config, output_dict)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_multilingual_forward(self):\r\n        num_speakers = 10\r\n        num_langs = 3\r\n        batch_size = 2\r\n\r\n        args = VitsArgs(language_ids_file=LANG_FILE, use_language_embedding=True, spec_segment_size=10)\r\n        config = VitsConfig(num_speakers=num_speakers, use_speaker_embedding=True, model_args=args)\r\n\r\n        input_dummy, input_lengths, _, spec, spec_lengths, waveform = self._create_inputs(config, batch_size=batch_size)\r\n        speaker_ids = torch.randint(0, num_speakers, (batch_size,)).long().to(device)\r\n        lang_ids = torch.randint(0, num_langs, (batch_size,)).long().to(device)\r\n\r\n        model = Vits(config).to(device)\r\n        output_dict = model.forward(\r\n            input_dummy,\r\n            input_lengths,\r\n            spec,\r\n            spec_lengths,\r\n            waveform,\r\n            aux_input={\"speaker_ids\": speaker_ids, \"language_ids\": lang_ids},\r\n        )\r\n        self._check_forward_outputs(config, output_dict)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_forward_with_d_vector(self, batch_size):\r\n        input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids = self._create_inputs(batch_size)\r\n        d_vector = torch.rand(batch_size, 256).to(device)\r\n        # create model\r\n        config = GlowTTSConfig(\r\n            num_chars=32,\r\n            use_d_vector_file=True,\r\n            d_vector_dim=256,\r\n            d_vector_file=os.path.join(get_tests_data_path(), \"dummy_speakers.json\"),\r\n        )\r\n        model = GlowTTS.init_from_config(config, verbose=False).to(device)\r\n        model.train()\r\n        print(\" > Num parameters for GlowTTS model:%s\" % (count_parameters(model)))\r\n        # inference encoder and decoder with MAS\r\n        y = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths, {\"d_vectors\": d_vector})\r\n        self.assertEqual(y[\"z\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"logdet\"].shape, torch.Size([batch_size]))\r\n        self.assertEqual(y[\"y_mean\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"y_log_scale\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"alignments\"].shape, mel_spec.shape[:2] + (input_dummy.shape[1],))\r\n        self.assertEqual(y[\"durations_log\"].shape, input_dummy.shape + (1,))\r\n        self.assertEqual(y[\"total_durations_log\"].shape, input_dummy.shape + (1,))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_forward(self, batch_size):\r\n        input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids = self._create_inputs(batch_size)\r\n        # create model\r\n        config = GlowTTSConfig(num_chars=32)\r\n        model = GlowTTS(config).to(device)\r\n        model.train()\r\n        print(\" > Num parameters for GlowTTS model:%s\" % (count_parameters(model)))\r\n        # inference encoder and decoder with MAS\r\n        y = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths)\r\n        self.assertEqual(y[\"z\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"logdet\"].shape, torch.Size([batch_size]))\r\n        self.assertEqual(y[\"y_mean\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"y_log_scale\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"alignments\"].shape, mel_spec.shape[:2] + (input_dummy.shape[1],))\r\n        self.assertEqual(y[\"durations_log\"].shape, input_dummy.shape + (1,))\r\n        self.assertEqual(y[\"total_durations_log\"].shape, input_dummy.shape + (1,))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_secl_forward(self):\r\n        num_speakers = 10\r\n        num_langs = 3\r\n        batch_size = 2\r\n\r\n        speaker_encoder_config = load_config(SPEAKER_ENCODER_CONFIG)\r\n        speaker_encoder_config.model_params[\"use_torch_spec\"] = True\r\n        speaker_encoder = setup_encoder_model(speaker_encoder_config).to(device)\r\n        speaker_manager = SpeakerManager()\r\n        speaker_manager.encoder = speaker_encoder\r\n\r\n        args = VitsArgs(\r\n            language_ids_file=LANG_FILE,\r\n            use_language_embedding=True,\r\n            spec_segment_size=10,\r\n            use_speaker_encoder_as_loss=True,\r\n        )\r\n        config = VitsConfig(num_speakers=num_speakers, use_speaker_embedding=True, model_args=args)\r\n        config.audio.sample_rate = 16000\r\n\r\n        input_dummy, input_lengths, _, spec, spec_lengths, waveform = self._create_inputs(config, batch_size=batch_size)\r\n        speaker_ids = torch.randint(0, num_speakers, (batch_size,)).long().to(device)\r\n        lang_ids = torch.randint(0, num_langs, (batch_size,)).long().to(device)\r\n\r\n        model = Vits(config, speaker_manager=speaker_manager).to(device)\r\n        output_dict = model.forward(\r\n            input_dummy,\r\n            input_lengths,\r\n            spec,\r\n            spec_lengths,\r\n            waveform,\r\n            aux_input={\"speaker_ids\": speaker_ids, \"language_ids\": lang_ids},\r\n        )\r\n        self._check_forward_outputs(config, output_dict, speaker_encoder_config)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_train_step(self):\r\n        batch_size = BATCH_SIZE\r\n        input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids = self._create_inputs(batch_size)\r\n        criterion = GlowTTSLoss()\r\n        # model to train\r\n        config = GlowTTSConfig(num_chars=32)\r\n        model = GlowTTS(config).to(device)\r\n        # reference model to compare model weights\r\n        model_ref = GlowTTS(config).to(device)\r\n        model.train()\r\n        print(\" > Num parameters for GlowTTS model:%s\" % (count_parameters(model)))\r\n        # pass the state to ref model\r\n        model_ref.load_state_dict(copy.deepcopy(model.state_dict()))\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n        for _ in range(5):\r\n            optimizer.zero_grad()\r\n            outputs = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths, None)\r\n            loss_dict = criterion(\r\n                outputs[\"z\"],\r\n                outputs[\"y_mean\"],\r\n                outputs[\"y_log_scale\"],\r\n                outputs[\"logdet\"],\r\n                mel_lengths,\r\n                outputs[\"durations_log\"],\r\n                outputs[\"total_durations_log\"],\r\n                input_lengths,\r\n            )\r\n            loss = loss_dict[\"loss\"]\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        self._check_parameter_changes(model, model_ref)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_forward_with_speaker_id(self, batch_size):\r\n        input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids = self._create_inputs(batch_size)\r\n        speaker_ids = torch.randint(0, 24, (batch_size,)).long().to(device)\r\n        # create model\r\n        config = GlowTTSConfig(\r\n            num_chars=32,\r\n            use_speaker_embedding=True,\r\n            num_speakers=24,\r\n        )\r\n        model = GlowTTS.init_from_config(config, verbose=False).to(device)\r\n        model.train()\r\n        print(\" > Num parameters for GlowTTS model:%s\" % (count_parameters(model)))\r\n        # inference encoder and decoder with MAS\r\n        y = model.forward(input_dummy, input_lengths, mel_spec, mel_lengths, {\"speaker_ids\": speaker_ids})\r\n        self.assertEqual(y[\"z\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"logdet\"].shape, torch.Size([batch_size]))\r\n        self.assertEqual(y[\"y_mean\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"y_log_scale\"].shape, mel_spec.shape)\r\n        self.assertEqual(y[\"alignments\"].shape, mel_spec.shape[:2] + (input_dummy.shape[1],))\r\n        self.assertEqual(y[\"durations_log\"].shape, input_dummy.shape + (1,))\r\n        self.assertEqual(y[\"total_durations_log\"].shape, input_dummy.shape + (1,))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_forward(self, batch_size):\r\n        # create model\r\n        config = FreeVCConfig()\r\n        model = FreeVC(config).to(device)\r\n        model.train()\r\n        print(\" > Num parameters for FreeVC model:%s\" % (count_parameters(model)))\r\n\r\n        _, _, mel, spec, spec_lengths, waveform = self._create_inputs(config, batch_size)\r\n\r\n        wavlm_vec = model.extract_wavlm_features(waveform)\r\n        wavlm_vec_lengths = torch.ones(batch_size, dtype=torch.long)\r\n\r\n        y = model.forward(wavlm_vec, spec, None, mel, spec_lengths, wavlm_vec_lengths)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def train(model, optimizer, scheduler, criterion, data_loader, eval_data_loader, global_step):\r\n    model.train()\r\n    best_loss = {\"train_loss\": None, \"eval_loss\": float(\"inf\")}\r\n    avg_loader_time = 0\r\n    end_time = time.time()\r\n    for epoch in range(c.epochs):\r\n        tot_loss = 0\r\n        epoch_time = 0\r\n        for _, data in enumerate(data_loader):\r\n            start_time = time.time()\r\n\r\n            # setup input data\r\n            inputs, labels = data\r\n            # agroup samples of each class in the batch. perfect sampler produces [3,2,1,3,2,1] we need [3,3,2,2,1,1]\r\n            labels = torch.transpose(labels.view(c.num_utter_per_class, c.num_classes_in_batch), 0, 1).reshape(\r\n                labels.shape\r\n            )\r\n            inputs = torch.transpose(inputs.view(c.num_utter_per_class, c.num_classes_in_batch, -1), 0, 1).reshape(\r\n                inputs.shape\r\n            )\r\n            # ToDo: move it to a unit test\r\n            # labels_converted = torch.transpose(labels.view(c.num_utter_per_class, c.num_classes_in_batch), 0, 1).reshape(labels.shape)\r\n            # inputs_converted = torch.transpose(inputs.view(c.num_utter_per_class, c.num_classes_in_batch, -1), 0, 1).reshape(inputs.shape)\r\n            # idx = 0\r\n            # for j in range(0, c.num_classes_in_batch, 1):\r\n            #     for i in range(j, len(labels), c.num_classes_in_batch):\r\n            #         if not torch.all(labels[i].eq(labels_converted[idx])) or not torch.all(inputs[i].eq(inputs_converted[idx])):\r\n            #             print(\"Invalid\")\r\n            #             print(labels)\r\n            #             exit()\r\n            #         idx += 1\r\n            # labels = labels_converted\r\n            # inputs = inputs_converted\r\n\r\n            loader_time = time.time() - end_time\r\n            global_step += 1\r\n\r\n            # setup lr\r\n            if c.lr_decay:\r\n                scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n            # dispatch data to GPU\r\n            if use_cuda:\r\n                inputs = inputs.cuda(non_blocking=True)\r\n                labels = labels.cuda(non_blocking=True)\r\n\r\n            # forward pass model\r\n            outputs = model(inputs)\r\n\r\n            # loss computation\r\n            loss = criterion(\r\n                outputs.view(c.num_classes_in_batch, outputs.shape[0] // c.num_classes_in_batch, -1), labels\r\n            )\r\n            loss.backward()\r\n            grad_norm, _ = check_update(model, c.grad_clip)\r\n            optimizer.step()\r\n\r\n            step_time = time.time() - start_time\r\n            epoch_time += step_time\r\n\r\n            # acumulate the total epoch loss\r\n            tot_loss += loss.item()\r\n\r\n            # Averaged Loader Time\r\n            num_loader_workers = c.num_loader_workers if c.num_loader_workers > 0 else 1\r\n            avg_loader_time = (\r\n                1 / num_loader_workers * loader_time + (num_loader_workers - 1) / num_loader_workers * avg_loader_time\r\n                if avg_loader_time != 0\r\n                else loader_time\r\n            )\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n\r\n            if global_step % c.steps_plot_stats == 0:\r\n                # Plot Training Epoch Stats\r\n                train_stats = {\r\n                    \"loss\": loss.item(),\r\n                    \"lr\": current_lr,\r\n                    \"grad_norm\": grad_norm,\r\n                    \"step_time\": step_time,\r\n                    \"avg_loader_time\": avg_loader_time,\r\n                }\r\n                dashboard_logger.train_epoch_stats(global_step, train_stats)\r\n                figures = {\r\n                    \"UMAP Plot\": plot_embeddings(outputs.detach().cpu().numpy(), c.num_classes_in_batch),\r\n                }\r\n                dashboard_logger.train_figures(global_step, figures)\r\n\r\n            if global_step % c.print_step == 0:\r\n                print(\r\n                    \"   | > Step:{}  Loss:{:.5f}  GradNorm:{:.5f}  \"\r\n                    \"StepTime:{:.2f}  LoaderTime:{:.2f}  AvGLoaderTime:{:.2f}  LR:{:.6f}\".format(\r\n                        global_step, loss.item(), grad_norm, step_time, loader_time, avg_loader_time, current_lr\r\n                    ),\r\n                    flush=True,\r\n                )\r\n\r\n            if global_step % c.save_step == 0:\r\n                # save model\r\n                save_checkpoint(\r\n                    c, model, optimizer, None, global_step, epoch, OUT_PATH, criterion=criterion.state_dict()\r\n                )\r\n\r\n            end_time = time.time()\r\n\r\n        print(\"\")\r\n        print(\r\n            \">>> Epoch:{}  AvgLoss: {:.5f} GradNorm:{:.5f}  \"\r\n            \"EpochTime:{:.2f} AvGLoaderTime:{:.2f} \".format(\r\n                epoch, tot_loss / len(data_loader), grad_norm, epoch_time, avg_loader_time\r\n            ),\r\n            flush=True,\r\n        )\r\n        # evaluation\r\n        if c.run_eval:\r\n            model.eval()\r\n            eval_loss = evaluation(model, criterion, eval_data_loader, global_step)\r\n            print(\"\\n\\n\")\r\n            print(\"--> EVAL PERFORMANCE\")\r\n            print(\r\n                \"   | > Epoch:{}  AvgLoss: {:.5f} \".format(epoch, eval_loss),\r\n                flush=True,\r\n            )\r\n            # save the best checkpoint\r\n            best_loss = save_best_model(\r\n                {\"train_loss\": None, \"eval_loss\": eval_loss},\r\n                best_loss,\r\n                c,\r\n                model,\r\n                optimizer,\r\n                None,\r\n                global_step,\r\n                epoch,\r\n                OUT_PATH,\r\n                criterion=criterion.state_dict(),\r\n            )\r\n            model.train()\r\n\r\n    return best_loss, global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_train_step(self):  # pylint: disable=no-self-use\r\n        \"\"\"Test if all layers are updated in a basic training cycle\"\"\"\r\n        input_dummy = torch.rand(8, 1, 20 * 300).to(device)\r\n        mel_spec = torch.rand(8, 80, 20).to(device)\r\n\r\n        criterion = torch.nn.L1Loss().to(device)\r\n        args = WavegradArgs(\r\n            in_channels=80,\r\n            out_channels=1,\r\n            upsample_factors=[5, 5, 3, 2, 2],\r\n            upsample_dilations=[[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 4, 8], [1, 2, 4, 8], [1, 2, 4, 8]],\r\n        )\r\n        config = WavegradConfig(model_params=args)\r\n        model = Wavegrad(config)\r\n\r\n        model_ref = Wavegrad(config)\r\n        model.train()\r\n        model.to(device)\r\n        betas = np.linspace(1e-6, 1e-2, 1000)\r\n        model.compute_noise_level(betas)\r\n        model_ref.load_state_dict(model.state_dict())\r\n        model_ref.to(device)\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            assert (param - param_ref).sum() == 0, param\r\n            count += 1\r\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n        for i in range(5):\r\n            y_hat = model.forward(input_dummy, mel_spec, torch.rand(8).to(device))\r\n            optimizer.zero_grad()\r\n            loss = criterion(y_hat, input_dummy)\r\n            loss.backward()\r\n            optimizer.step()\r\n        # check parameter changes\r\n        count = 0\r\n        for param, param_ref in zip(model.parameters(), model_ref.parameters()):\r\n            # ignore pre-higway layer since it works conditional\r\n            # if count not in [145, 59]:\r\n            assert (param != param_ref).any(), \"param {} with shape {} not updated!! \\n{}\\n{}\".format(\r\n                count, param.shape, param, param_ref\r\n            )\r\n            count += 1",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_wavegrad_forward():\r\n    x = torch.rand(32, 1, 20 * 300)\r\n    c = torch.rand(32, 80, 20)\r\n    noise_scale = torch.rand(32)\r\n\r\n    args = WavegradArgs(\r\n        in_channels=80,\r\n        out_channels=1,\r\n        upsample_factors=[5, 5, 3, 2, 2],\r\n        upsample_dilations=[[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 4, 8], [1, 2, 4, 8], [1, 2, 4, 8]],\r\n    )\r\n    config = WavegradConfig(model_params=args)\r\n    model = Wavegrad(config)\r\n    o = model.forward(x, c, noise_scale)\r\n\r\n    assert o.shape[0] == 32\r\n    assert o.shape[1] == 1\r\n    assert o.shape[2] == 20 * 300\r\n    assert isinstance(o, torch.FloatTensor)\r\n\r\n    model.apply_weight_norm()\r\n    model.remove_weight_norm()",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def generate_voice(\r\n    audio,\r\n    model,\r\n    output_path,\r\n):\r\n    \"\"\"Generate a new voice from a given audio and text prompt.\r\n\r\n    Args:\r\n        audio (np.ndarray): The audio to use as a base for the new voice.\r\n        text (str): Transcription of the audio you are clonning.\r\n        model (BarkModel): The BarkModel to use for generating the new voice.\r\n        output_path (str): The path to save the generated voice to.\r\n    \"\"\"\r\n    if isinstance(audio, str):\r\n        audio, sr = torchaudio.load(audio)\r\n        audio = convert_audio(audio, sr, model.config.sample_rate, model.encodec.channels)\r\n        audio = audio.unsqueeze(0).to(model.device)\r\n\r\n    with torch.no_grad():\r\n        encoded_frames = model.encodec.encode(audio)\r\n    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\r\n\r\n    # move codes to cpu\r\n    codes = codes.cpu().numpy()\r\n\r\n    # generate semantic tokens\r\n    # Load the HuBERT model\r\n    hubert_manager = HubertManager()\r\n    # hubert_manager.make_sure_hubert_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"])\r\n    hubert_manager.make_sure_tokenizer_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"])\r\n\r\n    hubert_model = CustomHubert(checkpoint_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"]).to(model.device)\r\n\r\n    # Load the CustomTokenizer model\r\n    tokenizer = HubertTokenizer.load_from_checkpoint(\r\n        model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"], map_location=model.device\r\n    )\r\n    # semantic_tokens = model.text_to_semantic(\r\n    #     text, max_gen_duration_s=seconds, top_k=50, top_p=0.95, temp=0.7\r\n    # )  # not 100%\r\n    semantic_vectors = hubert_model.forward(audio[0], input_sample_hz=model.config.sample_rate)\r\n    semantic_tokens = tokenizer.get_token(semantic_vectors)\r\n    semantic_tokens = semantic_tokens.cpu().numpy()\r\n\r\n    np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(self, wav_input, flatten=True, input_sample_hz=None):\r\n        device = wav_input.device\r\n\r\n        if exists(input_sample_hz):\r\n            wav_input = resample(wav_input, input_sample_hz, self.target_sample_hz)\r\n\r\n        if exists(self.seq_len_multiple_of):\r\n            wav_input = curtail_to_multiple(wav_input, self.seq_len_multiple_of)\r\n\r\n        outputs = self.model.forward(\r\n            wav_input,\r\n            output_hidden_states=True,\r\n        )\r\n        embed = outputs[\"hidden_states\"][self.output_layer]\r\n        embed, packed_shape = pack([embed], \"* d\")\r\n        codebook_indices = torch.from_numpy(embed.cpu().detach().numpy()).to(device)\r\n        if flatten:\r\n            return codebook_indices\r\n\r\n        (codebook_indices,) = unpack(codebook_indices, packed_shape, \"*\")\r\n        return codebook_indices",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, is_train, crop_size, voc_dir):\n        self.transform = torchvision.transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        self.crop_size = crop_size\n        features, labels = read_voc_images(voc_dir, is_train=is_train)\n        self.features = [self.normalize_image(feature)\n                         for feature in self.filter(features)]\n        self.labels = self.filter(labels)\n        self.colormap2label = voc_colormap2label()\n        print('read ' + str(len(self.features)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout, max_len=1000, **kwargs):\n        super(BERTEncoder, self).__init__(**kwargs)\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n        # In BERT, positional embeddings are learnable, thus we create a\n        # parameter of positional embeddings that are long enough\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n                                                      num_hiddens))",
        "labels": [
            "Hyperparameter Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def __init__(self, dataset, num_steps, vocab=None):\n        self.num_steps = num_steps\n        all_premise_tokens = d2l.tokenize(dataset[0])\n        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n        if vocab is None:\n            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n                                   min_freq=5, reserved_tokens=['<pad>'])\n        else:\n            self.vocab = vocab\n        self.premises = self._pad(all_premise_tokens)\n        self.hypotheses = self._pad(all_hypothesis_tokens)\n        self.labels = torch.tensor(dataset[2])\n        print('read ' + str(len(self.premises)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train_gluon_ch7(trainer_name, trainer_hyperparams, features, labels,\r\n                    batch_size=10, num_epochs=2):\r\n    \"\"\"Train a linear regression model with a given Gluon trainer.\"\"\"\r\n    net = nn.Sequential()\r\n    net.add(nn.Dense(1))\r\n    net.initialize(init.Normal(sigma=0.01))\r\n    loss = gloss.L2Loss()\r\n\r\n    def eval_loss():\r\n        return loss(net(features), labels).mean().asscalar()\r\n\r\n    ls = [eval_loss()]\r\n    data_iter = gdata.DataLoader(\r\n        gdata.ArrayDataset(features, labels), batch_size, shuffle=True)\r\n    trainer = gluon.Trainer(net.collect_params(),\r\n                            trainer_name, trainer_hyperparams)\r\n    for _ in range(num_epochs):\r\n        start = time.time()\r\n        for batch_i, (X, y) in enumerate(data_iter):\r\n            with autograd.record():\r\n                l = loss(net(X), y)\r\n            l.backward()\r\n            trainer.step(batch_size)\r\n            if (batch_i + 1) * batch_size % 100 == 0:\r\n                ls.append(eval_loss())\r\n    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n    set_figsize()\r\n    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n    plt.xlabel('epoch')\r\n    plt.ylabel('loss')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, is_train, crop_size, voc_dir):\r\n        self.transform = paddle.vision.transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        self.crop_size = crop_size\r\n        features, labels = read_voc_images(voc_dir, is_train=is_train)\r\n        self.features = [self.normalize_image(feature)\r\n                         for feature in self.filter(features)]\r\n        self.labels = self.filter(labels)\r\n        self.colormap2label = voc_colormap2label()\r\n        print('read ' + str(len(self.features)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\r\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\r\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\r\n                 **kwargs):\r\n        super(BERTEncoder, self).__init__(**kwargs)\r\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\r\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\r\n        self.blks = nn.Sequential()\r\n        for i in range(num_layers):\r\n            self.blks.add_sublayer(f\"{i}\", d2l.EncoderBlock(\r\n                key_size, query_size, value_size, num_hiddens, norm_shape,\r\n                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\r\n        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\r\n        x = paddle.randn([1, max_len, num_hiddens])\r\n        self.pos_embedding = paddle.create_parameter(shape=x.shape, dtype=str(x.numpy().dtype),\r\n                                                     default_initializer=paddle.nn.initializer.Assign(x))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, dataset, num_steps, vocab=None):\r\n        self.num_steps = num_steps\r\n        all_premise_tokens = d2l.tokenize(dataset[0])\r\n        all_hypothesis_tokens = d2l.tokenize(dataset[1])\r\n        if vocab is None:\r\n            self.vocab = d2l.Vocab(all_premise_tokens + \\\r\n                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\r\n        else:\r\n            self.vocab = vocab\r\n        self.premises = self._pad(all_premise_tokens)\r\n        self.hypotheses = self._pad(all_hypothesis_tokens)\r\n        self.labels = paddle.to_tensor(dataset[2])\r\n        print('read ' + str(len(self.premises)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit_epoch(self):\n        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n        self.model.train()\n        for batch in self.train_dataloader:\n            loss = self.model.training_step(self.prepare_batch(batch))\n            self.optim.zero_grad()\n            with torch.no_grad():\n                loss.backward()\n                if self.gradient_clip_val > 0:  # To be discussed later\n                    self.clip_gradients(self.gradient_clip_val, self.model)\n                self.optim.step()\n            self.train_batch_idx += 1\n        if self.val_dataloader is None:\n            return\n        self.model.eval()\n        for batch in self.val_dataloader:\n            with torch.no_grad():\n                self.model.validation_step(self.prepare_batch(batch))\n            self.val_batch_idx += 1",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    \"\"\"Train a model with a GPU (defined in Chapter 6).\n\n    Defined in :numref:`sec_utils`\"\"\"\n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            nn.init.xavier_uniform_(m.weight)\n    net.apply(init_weights)\n    print('training on', device)\n    net.to(device)\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no. of examples\n        metric = d2l.Accumulator(3)\n        net.train()\n        for i, (X, y) in enumerate(train_iter):\n            timer.start()\n            optimizer.zero_grad()\n            X, y = X.to(device), y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_l = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (train_l, train_acc, None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(device)}')",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n    \"\"\"Train a model for sequence to sequence.\n\n    Defined in :numref:`sec_utils`\"\"\"\n    def xavier_init_weights(m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n        if type(m) == nn.GRU:\n            for param in m._flat_weights_names:\n                if \"weight\" in param:\n                    nn.init.xavier_uniform_(m._parameters[param])\n    net.apply(xavier_init_weights)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    loss = MaskedSoftmaxCELoss()\n    net.train()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[10, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n        for batch in data_iter:\n            optimizer.zero_grad()\n            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n                               device=device).reshape(-1, 1)\n            dec_input = d2l.concat([bos, Y[:, :-1]], 1)  # Teacher forcing\n            Y_hat, _ = net(X, dec_input, X_valid_len)\n            l = loss(Y_hat, Y, Y_valid_len)\n            l.sum().backward()  # Make the loss scalar for `backward`\n            d2l.grad_clipping(net, 1)\n            num_tokens = Y_valid_len.sum()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l.sum(), num_tokens)\n        if (epoch + 1) % 10 == 0:\n            animator.add(epoch + 1, (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n          f'tokens/sec on {str(device)}')",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def __init__(self, batch_size=64, resize=(28, 28)):\n        super().__init__()\n        self.save_hyperparameters()\n        trans = transforms.Compose([transforms.Resize(resize),\n                                    transforms.ToTensor()])\n        self.train = torchvision.datasets.FashionMNIST(\n            root=self.root, train=True, transform=trans, download=True)\n        self.val = torchvision.datasets.FashionMNIST(\n            root=self.root, train=False, transform=trans, download=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\r\n    \"\"\"用GPU训练模型(在第六章定义)\r\n\r\n    Defined in :numref:`sec_lenet`\"\"\"\r\n    def init_weights(m):\r\n        if type(m) == nn.Linear or type(m) == nn.Conv2D:\r\n            nn.initializer.XavierUniform(m.weight)\r\n    net.apply(init_weights)\r\n    print('training on', device)\r\n    net.to(device)\r\n    optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters())\r\n    loss = nn.CrossEntropyLoss()\r\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\r\n                            legend=['train loss', 'train acc', 'test acc'])\r\n    timer, num_batches = d2l.Timer(), len(train_iter)\r\n    for epoch in range(num_epochs):\r\n        # 训练损失之和，训练准确率之和，样本数\r\n        metric = d2l.Accumulator(3)\r\n        net.train()\r\n        for i, (X, y) in enumerate(train_iter):\r\n            timer.start()\r\n            optimizer.clear_grad()\r\n            X, y = paddle.to_tensor(X, place=device), paddle.to_tensor(y, place=device)\r\n            y_hat = net(X)\r\n            l = loss(y_hat, y)\r\n            l.backward()\r\n            optimizer.step()\r\n            with paddle.no_grad():\r\n                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\r\n            timer.stop()\r\n            train_l = metric[0] / metric[2]\r\n            train_acc = metric[1] / metric[2]\r\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\r\n                animator.add(epoch + (i + 1) / num_batches,\r\n                             (train_l, train_acc, None))\r\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\r\n        animator.add(epoch + 1, (None, None, test_acc))\r\n    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\r\n          f'test acc {test_acc:.3f}')\r\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\r\n          f'on {str(device)}')",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n               feature_dim, num_epochs=2):\n    \"\"\"Defined in :numref:`sec_minibatches`\"\"\"\n    # Initialization\n    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n                     requires_grad=True)\n    b = torch.zeros((1), requires_grad=True)\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    # Train\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            l = loss(net(X), y).mean()\n            l.backward()\n            trainer_fn([w, b], states, hyperparams)\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                animator.add(n/X.shape[0]/len(data_iter),\n                             (d2l.evaluate_loss(net, data_iter, loss),))\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n    return timer.cumsum(), animator.Y[0]",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\r\n    \"\"\"用GPU训练模型(在第六章定义)\r\n\r\n    Defined in :numref:`sec_lenet`\"\"\"\r\n    def init_weights(m):\r\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\r\n            nn.init.xavier_uniform_(m.weight)\r\n    net.apply(init_weights)\r\n    print('training on', device)\r\n    net.to(device)\r\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\r\n    loss = nn.CrossEntropyLoss()\r\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\r\n                            legend=['train loss', 'train acc', 'test acc'])\r\n    timer, num_batches = d2l.Timer(), len(train_iter)\r\n    for epoch in range(num_epochs):\r\n        # 训练损失之和，训练准确率之和，样本数\r\n        metric = d2l.Accumulator(3)\r\n        net.train()\r\n        for i, (X, y) in enumerate(train_iter):\r\n            timer.start()\r\n            optimizer.zero_grad()\r\n            X, y = X.to(device), y.to(device)\r\n            y_hat = net(X)\r\n            l = loss(y_hat, y)\r\n            l.backward()\r\n            optimizer.step()\r\n            with torch.no_grad():\r\n                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\r\n            timer.stop()\r\n            train_l = metric[0] / metric[2]\r\n            train_acc = metric[1] / metric[2]\r\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\r\n                animator.add(epoch + (i + 1) / num_batches,\r\n                             (train_l, train_acc, None))\r\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\r\n        animator.add(epoch + 1, (None, None, test_acc))\r\n    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\r\n          f'test acc {test_acc:.3f}')\r\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\r\n          f'on {str(device)}')",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\r\n    \"\"\"训练网络一个迭代周期（定义见第8章）\r\n\r\n    Defined in :numref:`sec_rnn_scratch`\"\"\"\r\n    state, timer = None, d2l.Timer()\r\n    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\r\n    for X, Y in train_iter:\r\n        if state is None or use_random_iter:\r\n            # 在第一次迭代或使用随机抽样时初始化state\r\n            state = net.begin_state(batch_size=X.shape[0], device=device)\r\n        else:\r\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\r\n                # state对于nn.GRU是个张量\r\n                state.detach_()\r\n            else:\r\n                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\r\n                for s in state:\r\n                    s.detach_()\r\n        y = Y.T.reshape(-1)\r\n        X, y = X.to(device), y.to(device)\r\n        y_hat, state = net(X, state)\r\n        l = loss(y_hat, y.long()).mean()\r\n        if isinstance(updater, torch.optim.Optimizer):\r\n            updater.zero_grad()\r\n            l.backward()\r\n            grad_clipping(net, 1)\r\n            updater.step()\r\n        else:\r\n            l.backward()\r\n            grad_clipping(net, 1)\r\n            # 因为已经调用了mean函数\r\n            updater(batch_size=1)\r\n        metric.add(l * d2l.size(y), d2l.size(y))\r\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_to_orc_delayed(tmp_path):\r\n    # See: https://github.com/dask/dask/issues/8022\r\n    df = pd.DataFrame(np.random.randn(100, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\r\n    ddf = dd.from_pandas(df, npartitions=4)\r\n\r\n    eager_path = os.path.join(tmp_path, \"eager_orc_dataset\")\r\n    ddf.to_orc(eager_path)\r\n    assert len(glob.glob(os.path.join(eager_path, \"*\"))) == 4\r\n\r\n    delayed_path = os.path.join(tmp_path, \"delayed_orc_dataset\")\r\n    dataset = ddf.to_orc(delayed_path, compute=False)\r\n    dataset.compute()\r\n    assert len(glob.glob(os.path.join(delayed_path, \"*\"))) == 4",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\r\n    \"\"\"训练序列到序列模型\r\n\r\n    Defined in :numref:`sec_seq2seq_decoder`\"\"\"\r\n    def xavier_init_weights(m):\r\n        if type(m) == nn.Linear:\r\n            nn.init.xavier_uniform_(m.weight)\r\n        if type(m) == nn.GRU:\r\n            for param in m._flat_weights_names:\r\n                if \"weight\" in param:\r\n                    nn.init.xavier_uniform_(m._parameters[param])\r\n\r\n    net.apply(xavier_init_weights)\r\n    net.to(device)\r\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\r\n    loss = MaskedSoftmaxCELoss()\r\n    net.train()\r\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\r\n                     xlim=[10, num_epochs])\r\n    for epoch in range(num_epochs):\r\n        timer = d2l.Timer()\r\n        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\r\n        for batch in data_iter:\r\n            optimizer.zero_grad()\r\n            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\r\n            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\r\n                          device=device).reshape(-1, 1)\r\n            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\r\n            Y_hat, _ = net(X, dec_input, X_valid_len)\r\n            l = loss(Y_hat, Y, Y_valid_len)\r\n            l.sum().backward()\t# 损失函数的标量进行“反向传播”\r\n            d2l.grad_clipping(net, 1)\r\n            num_tokens = Y_valid_len.sum()\r\n            optimizer.step()\r\n            with torch.no_grad():\r\n                metric.add(l.sum(), num_tokens)\r\n        if (epoch + 1) % 10 == 0:\r\n            animator.add(epoch + 1, (metric[0] / metric[1],))\r\n    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\r\n        f'tokens/sec on {str(device)}')",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def __init__(self, dataset, num_steps, vocab=None):\r\n        self.num_steps = num_steps\r\n        all_premise_tokens = d2l.tokenize(dataset[0])\r\n        all_hypothesis_tokens = d2l.tokenize(dataset[1])\r\n        if vocab is None:\r\n            self.vocab = d2l.Vocab(all_premise_tokens + \\\r\n                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\r\n        else:\r\n            self.vocab = vocab\r\n        self.premises = self._pad(all_premise_tokens)\r\n        self.hypotheses = self._pad(all_hypothesis_tokens)\r\n        self.labels = torch.tensor(dataset[2])\r\n        print('read ' + str(len(self.premises)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train_ch11(trainer_fn, states, hyperparams, data_iter,\r\n               feature_dim, num_epochs=2):\r\n    \"\"\"Defined in :numref:`sec_minibatches`\"\"\"\r\n    # 初始化模型\r\n    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\r\n                     requires_grad=True)\r\n    b = torch.zeros((1), requires_grad=True)\r\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\r\n    # 训练模型\r\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\r\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\r\n    n, timer = 0, d2l.Timer()\r\n    for _ in range(num_epochs):\r\n        for X, y in data_iter:\r\n            l = loss(net(X), y).mean()\r\n            l.backward()\r\n            trainer_fn([w, b], states, hyperparams)\r\n            n += X.shape[0]\r\n            if n % 200 == 0:\r\n                timer.stop()\r\n                animator.add(n/X.shape[0]/len(data_iter),\r\n                             (d2l.evaluate_loss(net, data_iter, loss),))\r\n                timer.start()\r\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')\r\n    return timer.cumsum(), animator.Y[0]",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __init__(self, is_train, crop_size, voc_dir):\r\n        self.transform = torchvision.transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        self.crop_size = crop_size\r\n        features, labels = read_voc_images(voc_dir, is_train=is_train)\r\n        self.features = [self.normalize_image(feature)\r\n                         for feature in self.filter(features)]\r\n        self.labels = self.filter(labels)\r\n        self.colormap2label = voc_colormap2label()\r\n        print('read ' + str(len(self.features)) + ' examples')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\r\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\r\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\r\n                 **kwargs):\r\n        super(BERTEncoder, self).__init__(**kwargs)\r\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\r\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\r\n        self.blks = nn.Sequential()\r\n        for i in range(num_layers):\r\n            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\r\n                key_size, query_size, value_size, num_hiddens, norm_shape,\r\n                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\r\n        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\r\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\r\n                                                      num_hiddens))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def check_model_precision(self,\r\n                              model: tf.keras.models.Model,\r\n                              state: \"State\") -> tf.keras.models.Model:\r\n        \"\"\" Check the model's precision.\r\n\r\n        If this is a new model, then\r\n        Rewrite an existing model's training precsion mode from mixed-float16 to float32 or\r\n        vice versa.\r\n\r\n        This is not easy to do in keras, so we edit the model's config to change the dtype policy\r\n        for compatible layers. Create a new model from this config, then port the weights from the\r\n        old model to the new model.\r\n\r\n        Parameters\r\n        ----------\r\n        model: :class:`keras.models.Model`\r\n            The original saved keras model to rewrite the dtype\r\n        state: ~:class:`plugins.train.model._base.model.State`\r\n            The State information for the model\r\n\r\n        Returns\r\n        -------\r\n        :class:`keras.models.Model`\r\n            The original model with the datatype updated\r\n        \"\"\"\r\n        if self.use_mixed_precision and not state.mixed_precision_layers:\r\n            # Switching to mixed precision on a model which was started in FP32 prior to the\r\n            # ability to switch between precisions on a saved model is not supported as we\r\n            # do not have the compatible layer names\r\n            logger.warning(\"Switching from Full Precision to Mixed Precision is not supported on \"\r\n                           \"older model files. Reverting to Full Precision.\")\r\n            return model\r\n\r\n        config = model.get_config()\r\n\r\n        if not self.use_mixed_precision and not state.mixed_precision_layers:\r\n            # Switched to Full Precision, get compatible layers from model if not already stored\r\n            state.add_mixed_precision_layers(self._get_mixed_precision_layers(config[\"layers\"]))\r\n\r\n        self._switch_precision(config[\"layers\"], state.mixed_precision_layers)\r\n\r\n        new_model = keras.models.Model().from_config(config)\r\n        new_model.set_weights(model.get_weights())\r\n        logger.info(\"Mixed precision has been updated from '%s' to '%s'\",\r\n                    not self.use_mixed_precision, self.use_mixed_precision)\r\n        del model\r\n        return new_model",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_groupby_transform_ufunc_partitioning(npartitions, indexed):\r\n    pdf = pd.DataFrame({\"group\": [1, 2, 3, 4, 5] * 20, \"value\": np.random.randn(100)})\r\n\r\n    if indexed:\r\n        pdf = pdf.set_index(\"group\")\r\n\r\n    ddf = dd.from_pandas(pdf, npartitions)\r\n\r\n    with pytest.warns(UserWarning):\r\n        # DataFrame\r\n        assert_eq(\r\n            pdf.groupby(\"group\").transform(lambda series: series - series.mean()),\r\n            ddf.groupby(\"group\").transform(lambda series: series - series.mean()),\r\n        )\r\n\r\n        # Series\r\n        assert_eq(\r\n            pdf.groupby(\"group\")[\"value\"].transform(\r\n                lambda series: series - series.mean()\r\n            ),\r\n            ddf.groupby(\"group\")[\"value\"].transform(\r\n                lambda series: series - series.mean()\r\n            ),\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def _compile_metrics(metrics_output_directory: str) -> Dict:\r\n    \"\"\"\r\n    Compiles metrics from given directory and returns results as dict.\r\n\r\n    Parameters:\r\n        metrics_output_directory (str):\r\n            Directory to get metrics from.\r\n\r\n    Returns:\r\n        Dict:\r\n            Compiled metrics as dict.\r\n    \"\"\"\r\n    import numpy as np\r\n    import pandas as pd  # type: ignore\r\n\r\n    songs = glob(join(metrics_output_directory, \"test/*.json\"))\r\n    index = pd.MultiIndex.from_tuples(\r\n        product(EVALUATION_INSTRUMENTS, EVALUATION_METRICS),\r\n        names=[\"instrument\", \"metric\"],\r\n    )\r\n    pd.DataFrame([], index=[\"config1\", \"config2\"], columns=index)\r\n    metrics: Dict = {\r\n        instrument: {k: [] for k in EVALUATION_METRICS}\r\n        for instrument in EVALUATION_INSTRUMENTS\r\n    }\r\n    for song in songs:\r\n        with open(song, \"r\") as stream:\r\n            data = json.load(stream)\r\n        for target in data[\"targets\"]:\r\n            instrument = target[\"name\"]\r\n            for metric in EVALUATION_METRICS:\r\n                sdr_med = np.median(\r\n                    [\r\n                        frame[\"metrics\"][metric]\r\n                        for frame in target[\"frames\"]\r\n                        if not np.isnan(frame[\"metrics\"][metric])\r\n                    ]\r\n                )\r\n                metrics[instrument][metric].append(sdr_med)\r\n    return metrics",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _get_coord_data(\r\n    neox_args,\r\n    timers,\r\n    lr_scheduler,\r\n    models,\r\n    dataloader,\r\n    optcls,\r\n    nsteps=3,\r\n    dict_in_out=False,\r\n    flatten_input=False,\r\n    flatten_output=False,\r\n    output_name=\"loss\",\r\n    lossfn=\"xent\",\r\n    filter_module_by_name=None,\r\n    fix_data=True,\r\n    cuda=True,\r\n    nseeds=1,\r\n    output_fdict=None,\r\n    input_fdict=None,\r\n    param_fdict=None,\r\n    show_progress=True,\r\n    one_hot_target=False,\r\n):\r\n    df = []\r\n\r\n    for i in range(nseeds):\r\n        torch.manual_seed(i)\r\n        for width, model in models.items():\r\n            model = model()\r\n            model.train()\r\n            optimizer = optcls(model)\r\n            for step in range(nsteps + 1):\r\n                remove_hooks = []\r\n                # add hooks\r\n                for name, module in model.named_modules():\r\n                    if filter_module_by_name and not filter_module_by_name(name):\r\n                        continue\r\n                    remove_hooks.append(\r\n                        module.register_forward_hook(\r\n                            mup_coord_check._record_coords(\r\n                                df,\r\n                                width,\r\n                                name,\r\n                                step + 1,\r\n                                output_fdict=output_fdict,\r\n                                input_fdict=input_fdict,\r\n                                param_fdict=param_fdict,\r\n                            )\r\n                        )\r\n                    )\r\n\r\n                # train for a step\r\n                loss_dict, skipped_iter = train_step(\r\n                    neox_args=neox_args,\r\n                    timers=timers,\r\n                    data_iterator=dataloader,\r\n                    model=model,\r\n                    optimizer=optimizer,\r\n                    lr_scheduler=lr_scheduler,\r\n                )\r\n\r\n                # remove hooks\r\n                for handle in remove_hooks:\r\n                    handle.remove()\r\n\r\n            import gc\r\n\r\n            del model\r\n            gc.collect()\r\n\r\n    return pd.DataFrame(df)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_groupby_transform_funcs(transformation):\r\n    pdf = pd.DataFrame(\r\n        {\r\n            \"A\": [1, 2, 3, 4] * 5,\r\n            \"B\": np.random.randn(20),\r\n            \"C\": np.random.randn(20),\r\n            \"D\": np.random.randn(20),\r\n        }\r\n    )\r\n    ddf = dd.from_pandas(pdf, 3)\r\n\r\n    with pytest.warns(UserWarning):\r\n        # DataFrame\r\n        assert_eq(\r\n            pdf.groupby(\"A\").transform(transformation),\r\n            ddf.groupby(\"A\").transform(transformation),\r\n        )\r\n\r\n        # Series\r\n        assert_eq(\r\n            pdf.groupby(\"A\")[\"B\"].transform(transformation),\r\n            ddf.groupby(\"A\")[\"B\"].transform(transformation),\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def run_experiment(model, platform, mcu):\r\n\r\n    results_file = os.path.join(here, f'trees-feature-quantization-{platform}+{mcu}.csv')\r\n    # check if AVR build tools are present. If not, just load results from a file\r\n    missing_tools = check_build_tools(platform)\r\n\r\n    if missing_tools:\r\n        print(f\"WARNING: Compiler toolchain for platform '{platform}' not found. Loading cached results\")\r\n        results = pandas.read_csv(results_file)\r\n    else:\r\n        experiments = pandas.DataFrame({\r\n            'dtype': ('no-model', 'loadable', 'float', 'int32_t', 'int16_t', 'int8_t', 'uint8_t'),\r\n        })\r\n        results = experiments['dtype'].apply(check_program_size, model=model, platform=platform, mcu=mcu)\r\n        results = pandas.merge(experiments, results, left_index=True, right_index=True)\r\n        results = results.set_index('dtype')\r\n        # subtract overall program size to get only model size\r\n        results = (results - results.loc['no-model'])\r\n        results = results.drop(index='no-model')\r\n\r\n        # add identifying information\r\n        results['platform'] = platform\r\n        results['cpu'] = mcu\r\n        results = results.reset_index().set_index(['platform', 'cpu', 'dtype'])\r\n\r\n        results.to_csv(results_file)\r\n        print(\"Ran experiments. Results written to\", results_file)\r\n\r\n    return results",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_trees_evaluate_scoring(model, data):\r\n    \"\"\"\r\n    Test that the emlearn.evaluate.tree functions for cost metrics can be used with scikit-learn\r\n    \"\"\"\r\n    estimator = CLASSIFICATION_MODELS[model]\r\n    X, y = CLASSIFICATION_DATASETS[data]\r\n\r\n    from emlearn.evaluate.trees import \\\r\n        model_size_nodes, model_size_bytes, \\\r\n        tree_depth_average, tree_depth_difference, \\\r\n        count_trees, compute_cost_estimate \\\r\n\r\n    hyperparameters = {\r\n        #'max_depth': scipy.stats.randint(1, 10),\r\n        'min_samples_leaf': scipy.stats.loguniform(0.01, 0.33),\r\n    }\r\n    if 'DTC' not in model:\r\n        hyperparameters.update({\r\n            'n_estimators': scipy.stats.randint(5, 100),\r\n        })\r\n\r\n    # custom emlearn metrics for the model costs\r\n    custom_metrics = {\r\n        'bytes': model_size_bytes,\r\n        'nodes': model_size_nodes,\r\n        'compute': compute_cost_estimate,\r\n        'depth_avg': model_size_nodes,\r\n        'depth_diff': tree_depth_difference,\r\n        'trees': count_trees,\r\n    }\r\n    # standard metrics\r\n    metrics = {\r\n        'accuracy': sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score),\r\n    }\r\n    metrics.update(custom_metrics)\r\n\r\n    search = sklearn.model_selection.RandomizedSearchCV(\r\n        estimator,\r\n        param_distributions=hyperparameters,\r\n        scoring=metrics,\r\n        refit='accuracy',\r\n        n_iter=4,\r\n        cv=2,\r\n        return_train_score=True,\r\n        n_jobs=4,\r\n    )\r\n    model = search.fit(X, y)\r\n    results = pandas.DataFrame(model.cv_results_)\r\n\r\n    result_keys = [ f'mean_test_{m}' for m in custom_metrics ]\r\n    missing_columns = set(result_keys) - set(results.columns)\r\n    assert missing_columns == set(), missing_columns\r\n\r\n    values = results[result_keys]\r\n    rows_with_nan = values[values.isna().any(axis=1)]\r\n    assert len(rows_with_nan) == 0, rows_with_nan",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_trees_single_leaf_tree():\r\n    \"\"\"\r\n    Test ensemble that includes tree with a single node/leaf (edge case)\r\n    \"\"\"\r\n    estimator = CLASSIFICATION_MODELS['RFC']\r\n    # FIXME: fails more often for 5way dataset. Maybe to do with how tree predictions are combined\r\n    X, y = CLASSIFICATION_DATASETS['binary']\r\n\r\n    # force there to be a chance of getting single leaf in a tree\r\n    estimator.set_params(min_samples_leaf=0.33, n_estimators=3)\r\n\r\n    model = estimator.fit(X, y)\r\n\r\n    # Check that we were able to trigger the edge case\r\n    depths = [ e.tree_.max_depth for e in estimator.estimators_ ]\r\n    leaf_only_trees = [ e.tree_ for e in estimator.estimators_ if e.tree_.max_depth == 0 ]\r\n    assert leaf_only_trees, depths\r\n\r\n    for t in leaf_only_trees:\r\n        assert t.children_right == [-1]\r\n        assert t.children_left == [-1]\r\n\r\n    # Check that model can be converted\r\n    cmodel = emlearn.convert(estimator, method='inline')\r\n    nodes = pandas.DataFrame(cmodel.forest_[0], columns=['feature', 'treshold', 'left', 'right'])\r\n    roots = pandas.DataFrame(cmodel.forest_[1], columns=['index'])\r\n    leaves = pandas.DataFrame(cmodel.forest_[2], columns=['data'])\r\n\r\n    pred_original = estimator.predict(X)\r\n    pred_c = cmodel.predict(X)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def get_forecast(lib, time):\r\n\r\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\r\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")\r\n    start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\r\n    df = data[(data['date'] > str(start_date))]\r\n\r\n    df1 = df[['date','downloads']]\r\n    df1.columns = ['ds','y']\r\n\r\n    m = Prophet()\r\n    m.fit(df1)\r\n    future = m.make_future_dataframe(periods=90)\r\n    forecast = m.predict(future)\r\n    fig1 = m.plot(forecast)\r\n    return fig1",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def evaluate(self, y_hat, y, sigmas=None, bins=None,\r\n                 confidence=0.95, y_min=-np.inf, y_max=np.inf, metrics=None):\r\n        \"\"\"\r\n        Evaluate conformal regressor.\r\n\r\n        Parameters\r\n        ----------\r\n        y_hat : array-like of shape (n_values,)\r\n            predicted values\r\n        y : array-like of shape (n_values,)\r\n            correct target values\r\n        sigmas : array-like of shape (n_values,), default=None\r\n            difficulty estimates\r\n        bins : array-like of shape (n_values,), default=None\r\n            Mondrian categories\r\n        confidence : float in range (0,1), default=0.95\r\n            confidence level\r\n        y_min : float or int, default=-numpy.inf\r\n            minimum value to include in prediction intervals\r\n        y_max : float or int, default=numpy.inf\r\n            maximum value to include in prediction intervals\r\n        metrics : a string or a list of strings, \r\n                  default=list of all metrics, i.e., \r\n                  [\"error\", \"eff_mean\", \"eff_med\", \"time_fit\", \"time_evaluate\"]\r\n        \r\n        Returns\r\n        -------\r\n        results : dictionary with a key for each selected metric \r\n            estimated performance using the metrics\r\n\r\n        Examples\r\n        --------\r\n        Assuming that ``y_hat_test`` and ``y_test`` are vectors with predicted\r\n        and true targets for a test set, ``sigmas_test`` and ``bins_test`` are\r\n        vectors with difficulty estimates and Mondrian categories (bin labels) \r\n        for the test set, and ``cr_norm_mond`` is a fitted normalized Mondrian\r\n        conformal regressor, then the latter can be evaluated at the default\r\n        confidence level with respect to error and mean efficiency (interval \r\n        size) by:\r\n\r\n        .. code-block:: python\r\n\r\n           results = cr_norm_mond.evaluate(y_hat_test, y_test, \r\n                                           sigmas=sigmas_test, bins=bins_test,\r\n                                           metrics=[\"error\", \"eff_mean\"])\r\n        \"\"\"\r\n        tic = time.time()\r\n        if metrics is None:\r\n            metrics = [\"error\",\"eff_mean\",\"eff_med\",\"time_fit\",\"time_evaluate\"]\r\n        test_results = {}\r\n        intervals = self.predict(y_hat, sigmas, bins, confidence, y_min, y_max)\r\n        if \"error\" in metrics:\r\n            test_results[\"error\"] = 1-np.mean(\r\n                np.logical_and(intervals[:,0]<=y, y<=intervals[:,1]))\r\n        if \"eff_mean\" in metrics:            \r\n            test_results[\"eff_mean\"] = np.mean(intervals[:,1]-intervals[:,0])\r\n        if \"eff_med\" in metrics:            \r\n            test_results[\"eff_med\"] = np.median(intervals[:,1]-intervals[:,0])\r\n        if \"time_fit\" in metrics:\r\n            test_results[\"time_fit\"] = self.time_fit\r\n        toc = time.time()\r\n        self.time_evaluate = toc-tic\r\n        if \"time_evaluate\" in metrics:\r\n            test_results[\"time_evaluate\"] = self.time_evaluate\r\n        return test_results",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def evaluate(self, X, y, confidence=0.95, smoothing=True,\r\n                 metrics=None, seed=None):\r\n        \"\"\"\r\n        Evaluate the conformal classifier.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n           set of objects\r\n        y : array-like of shape (n_samples,)\r\n            correct target values\r\n        confidence : float in range (0,1), default=0.95\r\n            confidence level\r\n        smoothing : bool, default=True\r\n           use smoothed p-values\r\n        metrics : a string or a list of strings, \r\n                  default=list of all metrics, i.e., [\"error\", \"avg_c\", \"one_c\",\r\n                  \"empty\", \"time_fit\", \"time_evaluate\"]\r\n        seed : int, default=None\r\n           set random seed\r\n        \r\n        Returns\r\n        -------\r\n        results : dictionary with a key for each selected metric \r\n            estimated performance using the metrics, where \"error\" is the \r\n            fraction of prediction sets not containing the true class label,\r\n            \"avg_c\" is the average no. of predicted class labels, \"one_c\" is\r\n            the fraction of singleton prediction sets, \"empty\" is the fraction\r\n            of empty prediction sets, \"time_fit\" is the time taken to fit the \r\n            conformal classifier, and \"time_evaluate\" is the time taken for the\r\n            evaluation \r\n\r\n        Examples\r\n        --------\r\n        Assuming that ``X_test`` is a set of test objects, ``y_test`` is a \r\n        vector with true targets, and ``w`` is a calibrated \r\n        :class:`.WrapClassifier` object, then the latter can be evaluated at \r\n        the 90% confidence level with respect to error, average prediction set\r\n        size and fraction of singleton predictions by:\r\n\r\n        .. code-block:: python\r\n\r\n           results = w.evaluate(X_test, y_test, confidence=0.9,\r\n                                metrics=[\"error\", \"avg_c\", \"one_c\"])\r\n\r\n        Note\r\n        ----\r\n        The reported result for ``time_fit`` only considers fitting the\r\n        conformal regressor or predictive system; not for fitting the\r\n        learner.\r\n\r\n        Note\r\n        ----\r\n        The use of smoothed p-values increases computation time and typically\r\n        has a minor effect on the results, except for small calibration sets.\r\n\r\n        Note\r\n        ----\r\n        If a value for ``seed`` is given, it will take precedence over any ``seed``\r\n        value given when calling ``calibrate``.        \r\n        \"\"\"\r\n        if isinstance(y, pd.Series):\r\n            y = y.values\r\n        if not self.calibrated:\r\n            raise RuntimeError((\"evaluate requires that calibrate has been\"\r\n                                \"called first\"))\r\n        else:\r\n            if metrics is None:\r\n                metrics = [\"error\", \"avg_c\", \"one_c\", \"empty\", \"time_fit\",\r\n                           \"time_evaluate\"]\r\n            tic = time.time()\r\n            if seed is None:\r\n                seed = self.seed\r\n            if seed is not None:\r\n                random_state = np.random.get_state()\r\n                np.random.seed(seed)\r\n            prediction_sets = self.predict_set(X, confidence, smoothing)\r\n            test_results = get_test_results(prediction_sets,\r\n                                            self.learner.classes_, y, metrics)\r\n            if seed is not None:\r\n                np.random.set_state(random_state)\r\n            toc = time.time()\r\n            self.time_evaluate = toc-tic\r\n            if \"time_fit\" in metrics:\r\n                test_results[\"time_fit\"] = self.cc.time_fit\r\n            if \"time_evaluate\" in metrics:\r\n                test_results[\"time_evaluate\"] = self.time_evaluate\r\n            return test_results",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def evaluate(self, alphas, classes, y, bins=None, confidence=0.95,\r\n                 smoothing=True, metrics=None, seed=None):\r\n        \"\"\"\r\n        Evaluate conformal classifier.\r\n\r\n        Parameters\r\n        ----------\r\n        alphas : array-like of shape (n_samples, n_classes)\r\n            non-conformity scores\r\n        classes : array-like of shape (n_classes,)\r\n            class names\r\n        y : array-like of shape (n_samples,)\r\n            correct class labels\r\n        bins : array-like of shape (n_samples,), default=None\r\n            Mondrian categories\r\n        confidence : float in range (0,1), default=0.95\r\n            confidence level\r\n        smoothing : bool, default=True\r\n           use smoothed p-values\r\n        metrics : a string or a list of strings, \r\n                  default = list of all metrics, i.e., [\"error\", \"avg_c\", \r\n                  \"one_c\", \"empty\", \"time_fit\", \"time_evaluate\"]\r\n        seed : int, default=None\r\n           set random seed\r\n        \r\n        Returns\r\n        -------\r\n        results : dictionary with a key for each selected metric \r\n            estimated performance using the metrics, where \"error\" is the \r\n            fraction of prediction sets not containing the true class label,\r\n            \"avg_c\" is the average no. of predicted class labels, \"one_c\" is\r\n            the fraction of singleton prediction sets, \"empty\" is the fraction\r\n            of empty prediction sets, \"time_fit\" is the time taken to fit the \r\n            conformal classifier, and \"time_evaluate\" is the time taken for the\r\n            evaluation \r\n\r\n        Examples\r\n        --------\r\n        Assuming that ``alphas`` is an array containing non-conformity scores \r\n        for all classes for the test objects, ``classes`` and ``y_test`` are \r\n        vectors with the class names and true class labels for the test set, \r\n        respectively, and ``cc`` is a fitted standard conformal classifier, \r\n        then the latter can be evaluated at the default confidence level with \r\n        respect to error and average number of labels in the prediction sets by:\r\n\r\n        .. code-block:: python\r\n\r\n           results = cc.evaluate(alphas, y_test, metrics=[\"error\", \"avg_c\"])\r\n\r\n        Note\r\n        ----\r\n        The use of smoothed p-values increases computation time and typically\r\n        has a minor effect on the results, except for small calibration sets.\r\n\r\n        Note\r\n        ----\r\n        If a value for ``seed`` is given, it will take precedence over any ``seed``\r\n        value given when calling ``fit``.        \r\n        \"\"\"\r\n        if metrics is None:\r\n            metrics = [\"error\", \"avg_c\", \"one_c\", \"empty\", \"time_fit\",\r\n                       \"time_evaluate\"]\r\n        tic = time.time()\r\n        if seed is None:\r\n            seed = self.seed\r\n        if seed is not None:\r\n            random_state = np.random.get_state()\r\n            np.random.seed(seed)\r\n        prediction_sets = self.predict_set(alphas, bins, confidence, smoothing)\r\n        test_results = get_test_results(prediction_sets, classes, y, metrics)\r\n        if seed is not None:\r\n            np.random.set_state(random_state)\r\n        toc = time.time()\r\n        self.time_evaluate = toc-tic\r\n        if \"time_fit\" in metrics:\r\n            test_results[\"time_fit\"] = self.time_fit\r\n        if \"time_evaluate\" in metrics:\r\n            test_results[\"time_evaluate\"] = self.time_evaluate\r\n        return test_results",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def evaluate(self, y_hat, y, sigmas=None, bins=None,\r\n                 confidence=0.95, y_min=-np.inf, y_max=np.inf,\r\n                 metrics=None, seed=None):\r\n        \"\"\"\r\n        Evaluate conformal predictive system.\r\n\r\n        Parameters\r\n        ----------\r\n        y_hat : array-like of shape (n_values,)\r\n            predicted values\r\n        y : array-like of shape (n_values,)\r\n            correct target values\r\n        sigmas : array-like of shape (n_values,), default=None,\r\n            difficulty estimates\r\n        bins : array-like of shape (n_values,), default=None,\r\n            Mondrian categories\r\n        confidence : float in range (0,1), default=0.95\r\n            confidence level\r\n        y_min : float or int, default=-numpy.inf\r\n            minimum value to include in prediction intervals\r\n        y_max : float or int, default=numpy.inf\r\n            maximum value to include in prediction intervals\r\n        metrics : a string or a list of strings, default=list of all \r\n            metrics; [\"error\", \"eff_mean\",\"eff_med\", \"CRPS\", \"time_fit\",\r\n                      \"time_evaluate\"]\r\n        seed : int, default=None\r\n           set random seed\r\n        \r\n        Returns\r\n        -------\r\n        results : dictionary with a key for each selected metric \r\n            estimated performance using the metrics\r\n\r\n        Examples\r\n        --------\r\n        Assuming that ``y_hat_test`` and ``y_test`` are vectors with predicted\r\n        and true targets for a test set, ``sigmas_test`` and ``bins_test`` are\r\n        vectors with difficulty estimates and Mondrian categories (bin labels) \r\n        for the test set, and ``cps_norm_mond`` is a fitted normalized Mondrian\r\n        conformal predictive system, then the latter can be evaluated at the \r\n        default confidence level with respect to error, mean and median \r\n        efficiency (interval size, given the default confidence level) and \r\n        continuous-ranked probability score (CRPS) by:\r\n\r\n        .. code-block:: python\r\n\r\n           results = cps_norm_mond.evaluate(y_hat_test, y_test, \r\n                                            sigmas=sigmas_test, bins=bins_test,\r\n                                            metrics=[\"error\", \"eff_mean\", \r\n                                                     \"eff_med\", \"CRPS\"])\r\n\r\n        Note\r\n        ----\r\n        The use of the metric ``CRPS`` may consume a lot of memory, as a matrix\r\n        is generated for which the number of elements is the product of the \r\n        number of calibration and test objects, unless a Mondrian approach is \r\n        employed; for the latter, this number is reduced by increasing the number \r\n        of bins.\r\n\r\n        Note\r\n        ----\r\n        If a value for ``seed`` is given, it will take precedence over any ``seed``\r\n        value given when calling ``fit``.        \r\n        \"\"\"\r\n        tic = time.time()\r\n        if seed is None:\r\n            seed = self.seed\r\n        if seed is not None:\r\n            random_state = np.random.get_state()\r\n            np.random.seed(seed)\r\n        if isinstance(y, pd.Series):\r\n            y = y.values\r\n        if isinstance(y_hat, pd.Series):\r\n            y_hat = y_hat.values\r\n        test_results = {}\r\n        lower_percentile = (1-confidence)/2*100\r\n        higher_percentile = (confidence+(1-confidence)/2)*100\r\n        if metrics is None:\r\n            metrics = [\"error\",\"eff_mean\",\"eff_med\",\"CRPS\",\"time_fit\",\r\n                       \"time_evaluate\"]\r\n        if \"CRPS\" in metrics:\r\n            results, cpds = self.predict(y_hat, sigmas=sigmas, bins=bins, y=y,\r\n                                         lower_percentiles=lower_percentile,\r\n                                         higher_percentiles=higher_percentile,\r\n                                         y_min=y_min, y_max=y_max,\r\n                                         return_cpds=True, cpds_by_bins=True)\r\n            intervals = results[:,[1,2]]\r\n            if not self.mondrian:\r\n                if self.normalized:\r\n                    crps = calculate_crps(cpds, self.alphas, sigmas, y)\r\n                else:\r\n                    crps = calculate_crps(cpds, self.alphas,\r\n                                          np.ones(len(y_hat)), y)\r\n            else:\r\n                bin_values, bin_alphas = self.alphas\r\n                bin_indexes = [np.argwhere(bins == b).T[0]\r\n                               for b in bin_values]\r\n                if self.normalized:\r\n                    crps = np.sum([calculate_crps(cpds[b],\r\n                                                  bin_alphas[b],\r\n                                                  sigmas[bin_indexes[b]],\r\n                                                  y[bin_indexes[b]]) \\\r\n                                   * len(bin_indexes[b])\r\n                                   for b in range(len(bin_values))])/len(y)\r\n                else:\r\n                    crps = np.sum([calculate_crps(cpds[b],\r\n                                                  bin_alphas[b],\r\n                                                  np.ones(len(bin_indexes[b])),\r\n                                                  y[bin_indexes[b]]) \\\r\n                                   * len(bin_indexes[b])\r\n                                   for b in range(len(bin_values))])/len(y)\r\n        else:\r\n            intervals = self.predict(y_hat, sigmas=sigmas, bins=bins,\r\n                                     lower_percentiles=lower_percentile,\r\n                                     higher_percentiles=higher_percentile,\r\n                                     y_min=y_min, y_max=y_max,\r\n                                     return_cpds=False)\r\n        if \"error\" in metrics:\r\n            test_results[\"error\"] = 1-np.mean(np.logical_and(\r\n                intervals[:,0]<=y, y<=intervals[:,1]))\r\n        if \"eff_mean\" in metrics:            \r\n            test_results[\"eff_mean\"] = np.mean(intervals[:,1]-intervals[:,0])\r\n        if \"eff_med\" in metrics:            \r\n            test_results[\"eff_med\"] = np.median(intervals[:,1]-intervals[:,0])\r\n        if \"CRPS\" in metrics:\r\n            test_results[\"CRPS\"] = crps\r\n        if \"time_fit\" in metrics:\r\n            test_results[\"time_fit\"] = self.time_fit\r\n            toc = time.time()\r\n        if seed is not None:\r\n            np.random.set_state(random_state)\r\n        self.time_evaluate = toc-tic\r\n        if \"time_evaluate\" in metrics:\r\n            test_results[\"time_evaluate\"] = self.time_evaluate\r\n        return test_results",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def concatenated_forward(\r\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"]\r\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\r\n        target_logits, target_logps, target_logps_avg = self.forward(model, batch)\r\n        with torch.no_grad():\r\n            _, kl_logps, _ = self.forward(model, batch, prefix=\"kl_\")\r\n\r\n        if len(target_logps) != len(batch[\"kto_tags\"]):\r\n            raise ValueError(\"Mismatched shape of inputs and labels.\")\r\n\r\n        chosen_logits = target_logits[batch[\"kto_tags\"]]\r\n        chosen_logps = target_logps[batch[\"kto_tags\"]]\r\n        rejected_logits = target_logits[~batch[\"kto_tags\"]]\r\n        rejected_logps = target_logps[~batch[\"kto_tags\"]]\r\n        chosen_logps_avg = target_logps_avg[batch[\"kto_tags\"]]\r\n        return chosen_logps, rejected_logps, chosen_logits, rejected_logits, kl_logps, chosen_logps_avg",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_repartition_partition_size(use_index, n, partition_size, transform):\r\n    df = pd.DataFrame(\r\n        {\"x\": [1, 2, 3, 4, 5, 6] * 10, \"y\": list(\"abdabd\") * 10},\r\n        index=pd.Series([10, 20, 30, 40, 50, 60] * 10),\r\n    )\r\n    df = transform(df)\r\n    a = dd.from_pandas(df, npartitions=n, sort=use_index)\r\n    b = a.repartition(partition_size=partition_size)\r\n    assert_eq(a, b, check_divisions=False)\r\n    assert np.all(b.map_partitions(total_mem_usage, deep=True).compute() <= 1024)\r\n    parts = dask.get(b.dask, b.__dask_keys__())\r\n    assert all(map(len, parts))",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train(epoch):\r\n    model.train()\r\n    train_sampler.set_epoch(epoch)\r\n    train_loss = Metric('train_loss')\r\n    train_accuracy = Metric('train_accuracy')\r\n\r\n    with tqdm(total=len(train_loader),\r\n              desc='Train Epoch     #{}'.format(epoch + 1),\r\n              disable=not verbose) as t:\r\n        for batch_idx, (data, target) in enumerate(train_loader):\r\n            adjust_learning_rate(epoch, batch_idx)\r\n\r\n            if args.cuda:\r\n                data, target = data.cuda(), target.cuda()\r\n            optimizer.zero_grad()\r\n            # Split data into sub-batches of size batch_size\r\n            for i in range(0, len(data), args.batch_size):\r\n                data_batch = data[i:i + args.batch_size]\r\n                target_batch = target[i:i + args.batch_size]\r\n                output = model(data_batch)\r\n                train_accuracy.update(accuracy(output, target_batch))\r\n                loss = F.cross_entropy(output, target_batch)\r\n                train_loss.update(loss)\r\n                # Average gradients among sub-batches\r\n                loss.div_(math.ceil(float(len(data)) / args.batch_size))\r\n                loss.backward()\r\n            # Gradient is applied across all ranks\r\n            optimizer.step()\r\n            t.set_postfix({'loss': train_loss.avg.item(),\r\n                           'accuracy': 100. * train_accuracy.avg.item()})\r\n            t.update(1)\r\n\r\n    if log_writer:\r\n        log_writer.add_scalar('train/loss', train_loss.avg, epoch)\r\n        log_writer.add_scalar('train/accuracy', train_accuracy.avg, epoch)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(state):\r\n    model.train()\r\n    epoch = state.epoch\r\n    train_loss = Metric('train_loss')\r\n    train_accuracy = Metric('train_accuracy')\r\n\r\n    batch_offset = state.batch\r\n    with tqdm(total=len(train_loader),\r\n              desc='Train Epoch     #{}'.format(epoch + 1),\r\n              disable=not verbose) as t:\r\n        for idx, (data, target) in enumerate(train_loader):\r\n            # Elastic Horovod: update the current batch index this epoch\r\n            # and commit / check for host updates. Do not check hosts when\r\n            # we commit as it would be redundant.\r\n            state.batch = batch_idx = batch_offset + idx\r\n            if args.batches_per_commit > 0 and \\\r\n                    state.batch % args.batches_per_commit == 0:\r\n                state.commit()\r\n            elif args.batches_per_host_check > 0 and \\\r\n                    state.batch % args.batches_per_host_check == 0:\r\n                state.check_host_updates()\r\n\r\n            adjust_learning_rate(epoch, batch_idx)\r\n\r\n            if args.cuda:\r\n                data, target = data.cuda(), target.cuda()\r\n            optimizer.zero_grad()\r\n            # Split data into sub-batches of size batch_size\r\n            for i in range(0, len(data), args.batch_size):\r\n                data_batch = data[i:i + args.batch_size]\r\n                target_batch = target[i:i + args.batch_size]\r\n                output = model(data_batch)\r\n                train_accuracy.update(accuracy(output, target_batch))\r\n                loss = F.cross_entropy(output, target_batch)\r\n                train_loss.update(loss)\r\n                # Average gradients among sub-batches\r\n                loss.div_(math.ceil(float(len(data)) / args.batch_size))\r\n                loss.backward()\r\n\r\n            # Elastic Horovod: record which samples were processed this batch\r\n            # so we do not reprocess them if a reset event occurs\r\n            state.train_sampler.record_batch(idx, allreduce_batch_size)\r\n\r\n            # Gradient is applied across all ranks\r\n            optimizer.step()\r\n            t.set_postfix({'loss': train_loss.avg.item(),\r\n                           'accuracy': 100. * train_accuracy.avg.item()})\r\n\r\n            t.update(1)\r\n\r\n    if log_writer:\r\n        log_writer.add_scalar('train/loss', train_loss.avg, epoch)\r\n        log_writer.add_scalar('train/accuracy', train_accuracy.avg, epoch)\r\n\r\n    state.commit()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(state, dir):\r\n        state.rendezvous += 1\r\n        logging.info('rank %s: rendezvous %s', hvd.rank(), state.rendezvous)\r\n\r\n        for state.epoch in range(state.epoch, epochs):\r\n            logging.info('rank %s: start epoch %s at batch %s', hvd.rank(), state.epoch, state.batch)\r\n\r\n            for state.batch in range(state.batch, batches_per_epoch):\r\n                check_fail(dir, hvd.rank(), state.epoch, state.batch)\r\n\r\n                optimizer.zero_grad()\r\n                output = model(data)\r\n                loss = F.cross_entropy(output, target)\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n                # TODO: this sleep makes the fault tolerant test fail\r\n                #       torch all gather throws an RuntimeError which should be a HorovodInternalError\r\n                #import time\r\n                #time.sleep(0.2)\r\n\r\n                if state.batch % batches_per_commit == 0:\r\n                    logging.info('rank %s: allgather', hvd.rank())\r\n                    hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n                    logging.info('rank %s: commit epoch %s batch %s', hvd.rank(), state.epoch, state.batch)\r\n                    state.commits += 1\r\n                    state.commit()\r\n\r\n            logging.info('rank %s: allgather', hvd.rank())\r\n            hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n            logging.info('rank %s: commit epoch %s', hvd.rank(), state.epoch)\r\n            state.commits += 1\r\n            state.commit()\r\n            state.batch = 0\r\n\r\n        res = hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n        logging.info('rank %s: returning', hvd.rank())\r\n        return res, hvd.rank()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(state, dir):\r\n        state.rendezvous += 1\r\n        logging.info('rank %s: rendezvous %s', hvd.rank(), state.rendezvous)\r\n\r\n        for state.epoch in range(state.epoch, epochs):\r\n            logging.info('rank %s: start epoch %s at batch %s', hvd.rank(), state.epoch, state.batch)\r\n\r\n            for state.batch in range(state.batch, batches_per_epoch):\r\n                check_fail(dir, hvd.rank(), state.epoch, state.batch)\r\n\r\n                optimizer.zero_grad()\r\n                output = model(data)\r\n                loss = F.cross_entropy(output, target)\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n                # TODO: this sleep makes the fault tolerant test fail\r\n                #       torch all gather throws an RuntimeError which should be a HorovodInternalError\r\n                #import time\r\n                #time.sleep(0.2)\r\n\r\n                if state.batch % batches_per_commit == 0:\r\n                    logging.info('rank %s: allgather', hvd.rank())\r\n                    hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n                    logging.info('rank %s: commit epoch %s batch %s', hvd.rank(), state.epoch, state.batch)\r\n                    state.commits += 1\r\n                    state.commit()\r\n\r\n            logging.info('rank %s: allgather', hvd.rank())\r\n            hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n            logging.info('rank %s: commit epoch %s', hvd.rank(), state.epoch)\r\n            state.commits += 1\r\n            state.commit()\r\n            state.batch = 0\r\n\r\n        res = hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()\r\n        logging.info('rank %s: returning', hvd.rank())\r\n        return res, hvd.rank()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(state):\r\n    state.rendezvous += 1\r\n    while state.epoch < args.epochs:\r\n        print('epoch {} batch {}'.format(state.epoch, state.batch))\r\n\r\n        while state.batch < args.batches_per_epoch:\r\n            check_exit(state.epoch, state.batch)\r\n\r\n            optimizer.zero_grad()\r\n            output = model(data)\r\n            loss = F.cross_entropy(output, target)\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            state.batch += 1\r\n            if state.batch % args.batches_per_commit == 0:\r\n                state.commits += 1\r\n                state.commit()\r\n\r\n        if hvd.rank() == 0:\r\n            log_state(state)\r\n\r\n            current_hosts = epoch_to_hosts.get(state.epoch, default_hosts)\r\n            next_hosts = epoch_to_hosts.get(state.epoch + 1, default_hosts)\r\n            if args.discovery_wait > 0 and current_hosts != next_hosts:\r\n                print('host changes: {} -> {}'.format(current_hosts, next_hosts))\r\n                start = int(time.time())\r\n\r\n                # Reset the last updated timestamp to ensure we still raise HostsUpdatedInterrupt even\r\n                # if the timestamp from the driver isn't greater than the last (due to time loops).\r\n                state._last_updated_timestamp = 0\r\n                while state._host_messages.empty():\r\n                    if int(time.time()) - start > args.discovery_wait:\r\n                        raise TimeoutError('Timed out waiting for notifications from driver.')\r\n                    time.sleep(0.1)\r\n\r\n        if args.epoch_wait > 0:\r\n            time.sleep(args.epoch_wait)\r\n\r\n        state.epoch += 1\r\n        state.batch = 0\r\n        state.commits += 1\r\n        state.commit()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_load_model_custom_objects(self):\r\n        class TestOptimizer(keras.optimizers.RMSprop):\r\n            def __init__(self, **kwargs):\r\n                super().__init__(**kwargs)\r\n\r\n        with self.test_session(config=self.config) as sess:\r\n            K.set_session(sess)\r\n\r\n            opt = TestOptimizer(lr=0.0001)\r\n            opt = hvd.DistributedOptimizer(opt)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n            model.add(keras.layers.RepeatVector(3))\r\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt,\r\n                          metrics=[keras.metrics.categorical_accuracy],\r\n                          sample_weight_mode='temporal')\r\n\r\n            x = np.random.random((1, 3))\r\n            y = np.random.random((1, 3, 3))\r\n            model.train_on_batch(x, y)\r\n\r\n            with temppath() as fname:\r\n                model.save(fname)\r\n\r\n                custom_objects = {\r\n                    'TestOptimizer': lambda **kwargs: hvd.DistributedOptimizer(\r\n                        TestOptimizer(**kwargs))\r\n                }\r\n                new_model = hvd.load_model(fname, custom_objects=custom_objects)\r\n                new_opt = new_model.optimizer\r\n\r\n            self.assertEqual(type(new_opt).__module__, 'horovod._keras')\r\n            self.assertEqual(type(new_opt).__name__, 'TestOptimizer')\r\n            self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))\r\n            self._check_optimizer_weights(opt, new_opt)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_load_model(self):\r\n        with self.test_session(config=self.config) as sess:\r\n            K.set_session(sess)\r\n\r\n            opt = keras.optimizers.RMSprop(lr=0.0001)\r\n            opt = hvd.DistributedOptimizer(opt)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n            model.add(keras.layers.RepeatVector(3))\r\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt,\r\n                          metrics=[keras.metrics.categorical_accuracy],\r\n                          sample_weight_mode='temporal')\r\n\r\n            x = np.random.random((1, 3))\r\n            y = np.random.random((1, 3, 3))\r\n            model.train_on_batch(x, y)\r\n\r\n            with temppath() as fname:\r\n                model.save(fname)\r\n\r\n                new_model = hvd.load_model(fname)\r\n                new_opt = new_model.optimizer\r\n\r\n            self.assertEqual(type(new_opt).__module__, 'horovod._keras')\r\n            self.assertEqual(type(new_opt).__name__, 'RMSprop')\r\n            self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))\r\n            self._check_optimizer_weights(opt, new_opt)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_load_model_custom_optimizers(self):\r\n        class TestOptimizer(keras.optimizers.RMSprop):\r\n            def __init__(self, **kwargs):\r\n                super().__init__(**kwargs)\r\n\r\n        with self.test_session(config=self.config) as sess:\r\n            K.set_session(sess)\r\n\r\n            opt = TestOptimizer(lr=0.0001)\r\n            opt = hvd.DistributedOptimizer(opt)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n            model.add(keras.layers.RepeatVector(3))\r\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt,\r\n                          metrics=[keras.metrics.categorical_accuracy],\r\n                          sample_weight_mode='temporal')\r\n\r\n            x = np.random.random((1, 3))\r\n            y = np.random.random((1, 3, 3))\r\n            model.train_on_batch(x, y)\r\n\r\n            with temppath() as fname:\r\n                model.save(fname)\r\n\r\n                custom_optimizers = [TestOptimizer]\r\n                new_model = hvd.load_model(fname, custom_optimizers=custom_optimizers)\r\n                new_opt = new_model.optimizer\r\n\r\n            self.assertEqual(type(new_opt).__module__, 'horovod._keras')\r\n            self.assertEqual(type(new_opt).__name__, 'TestOptimizer')\r\n            self._check_optimizer_weights(opt, new_opt)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_load_model_broadcast(self):\r\n        def create_model():\r\n            opt = keras.optimizers.SGD(lr=0.01 * hvd.size(), momentum=0.9)\r\n            opt = hvd.DistributedOptimizer(opt)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n            model.add(keras.layers.RepeatVector(3))\r\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt,\r\n                          metrics=[keras.metrics.categorical_accuracy],\r\n                          sample_weight_mode='temporal')\r\n\r\n            return model\r\n\r\n        with temppath() as fname:\r\n            with self.test_session(config=self.config) as sess:\r\n                K.set_session(sess)\r\n\r\n                model = create_model()\r\n\r\n                x = np.random.random((1, 3))\r\n                y = np.random.random((1, 3, 3))\r\n                model.train_on_batch(x, y)\r\n\r\n                if hvd.rank() == 0:\r\n                    model.save(fname)\r\n\r\n            K.clear_session()\r\n            with self.test_session(config=self.config) as sess:\r\n                K.set_session(sess)\r\n\r\n                if hvd.rank() == 0:\r\n                    model = hvd.load_model(fname)\r\n                else:\r\n                    model = create_model()\r\n\r\n                def generator():\r\n                    while 1:\r\n                        yield (x, y)\r\n\r\n                if hvd.rank() == 0:\r\n                    self.assertEqual(len(model.optimizer.weights), 5)\r\n                else:\r\n                    self.assertEqual(len(model.optimizer.weights), 0)\r\n\r\n                # No assertions, we just need to verify that it doesn't hang\r\n                callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\r\n                model.fit_generator(generator(),\r\n                                    steps_per_epoch=10,\r\n                                    callbacks=callbacks,\r\n                                    epochs=0,\r\n                                    verbose=0,\r\n                                    workers=4,\r\n                                    initial_epoch=1)\r\n\r\n                self.assertEqual(len(model.optimizer.weights), 5)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparse_as_dense(self):\r\n        with self.test_session(config=self.config) as sess:\r\n            K.set_session(sess)\r\n\r\n            opt = keras.optimizers.RMSprop(lr=0.0001)\r\n            opt = hvd.DistributedOptimizer(opt, sparse_as_dense=True)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Embedding(1000, 64, input_length=10))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt)\r\n\r\n            x = np.random.randint(1000, size=(32, 10))\r\n            y = np.random.random((32, 10, 64))\r\n            # No assertions, we just need to verify that it doesn't hang\r\n            model.train_on_batch(x, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def create_model():\r\n            opt = keras.optimizers.SGD(lr=0.01 * hvd.size(), momentum=0.9)\r\n            opt = hvd.DistributedOptimizer(opt)\r\n\r\n            model = keras.models.Sequential()\r\n            model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n            model.add(keras.layers.RepeatVector(3))\r\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\r\n            model.compile(loss=keras.losses.MSE,\r\n                          optimizer=opt,\r\n                          metrics=[keras.metrics.categorical_accuracy],\r\n                          sample_weight_mode='temporal')\r\n\r\n            return model",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_broadcast_state_options(self):\r\n        hvd.init()\r\n\r\n        N, D_in, H, D_out = 64, 100, 10, 10\r\n        x = torch.randn(N, D_in).requires_grad_()\r\n        y = torch.randn(N, D_out).requires_grad_()\r\n\r\n        params_0 = dict(lr=0.1, momentum=0.8, weight_decay=0.2, nesterov=True,\r\n                        betas=(0.9, 0.999), etas=(0.8, 2.4), step_sizes=(1e-5, 100))\r\n        params_1 = dict(lr=0.2, momentum=0.9, weight_decay=0.1, nesterov=False,\r\n                        betas=(0.8, 0.9), etas=(0.25, 1.75), step_sizes=(1e-7, 5))\r\n\r\n        def create_model(opt_class):\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(D_in, H),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(H, D_out),\r\n            )\r\n\r\n            params = params_0 if hvd.rank() == 0 else params_1\r\n            p = {\r\n                k: v for k, v in params.items()\r\n                if k in inspect.signature(opt_class.__init__).parameters\r\n            }\r\n            opt = opt_class(model.parameters(), **p)\r\n            opt = hvd.DistributedOptimizer(opt, named_parameters=model.named_parameters())\r\n\r\n            return model, opt\r\n\r\n        # Include subclass name so we can sort them lexicographically, otherwise different\r\n        # ranks will have different optimizer orderings\r\n        optimizers = [\r\n            (subclass.__name__, subclass)\r\n            for subclass in torch.optim.Optimizer.__subclasses__()\r\n            if subclass.__module__.startswith('torch.optim') and\r\n               subclass != torch.optim.LBFGS and\r\n               subclass != torch.optim.SparseAdam\r\n        ]\r\n        optimizers.sort(key=lambda tup: tup[0])\r\n\r\n        for _, opt_class in optimizers:\r\n            model, optimizer = create_model(opt_class)\r\n            y_pred = model(x)\r\n            loss = F.mse_loss(y_pred, y, size_average=False)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            hvd.broadcast_optimizer_state(optimizer, root_rank=0)\r\n            p0 = {\r\n                k: v for k, v in params_0.items()\r\n                if k in inspect.signature(opt_class.__init__).parameters\r\n            }\r\n            for k, p in p0.items():\r\n                p_actual = optimizer.param_groups[0][k]\r\n                if not isinstance(p, Iterable):\r\n                    p_actual = [p_actual]\r\n                    p = [p]\r\n                for i in range(len(p)):\r\n                    self.assertEqual(type(p_actual[i]), type(p[i]))\r\n                    self.assertAlmostEqual(p_actual[i], p[i], delta=1e-5)\r\n\r\n            # Ensure that the parameter option types are compatible with ops\r\n            y_pred = model(x)\r\n            loss = F.mse_loss(y_pred, y, size_average=False)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args) -> None:\r\n    # ==============================\r\n    # Initialize Distributed Training\r\n    # ==============================\r\n    colossalai.launch_from_torch()\r\n    accelerator = get_accelerator()\r\n    coordinator = DistCoordinator()\r\n\r\n    # ==============================\r\n    # Initialize Tensorboard and Save Config\r\n    # ==============================\r\n    if coordinator.is_master():\r\n        os.makedirs(args.tensorboard_dir, exist_ok=True)\r\n        writer = SummaryWriter(args.tensorboard_dir)\r\n\r\n        with open(args.config_file, \"w\") as f:\r\n            json.dump(args.__dict__, f, indent=4)\r\n\r\n    # ==============================\r\n    # Initialize Booster\r\n    # ==============================\r\n    if args.plugin == \"ddp\":\r\n        plugin = TorchDDPPlugin(find_unused_parameters=True if args.use_grad_checkpoint is False else False)\r\n    elif args.plugin == \"gemini\":\r\n        plugin = GeminiPlugin(\r\n            precision=args.mixed_precision,\r\n            initial_scale=2**16,\r\n            max_norm=args.grad_clip,\r\n            enable_gradient_accumulation=(args.accumulation_steps > 1),\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_flash_attention=args.use_flash_attn,\r\n        )\r\n    elif args.plugin == \"gemini_auto\":\r\n        plugin = GeminiPlugin(\r\n            precision=args.mixed_precision,\r\n            placement_policy=\"auto\",\r\n            initial_scale=2**16,\r\n            max_norm=args.grad_clip,\r\n            enable_gradient_accumulation=(args.accumulation_steps > 1),\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_flash_attention=args.use_flash_attn,\r\n        )\r\n    elif args.plugin == \"zero2\":\r\n        plugin = LowLevelZeroPlugin(\r\n            stage=2,\r\n            precision=args.mixed_precision,\r\n            initial_scale=2**16,\r\n            max_norm=args.grad_clip,\r\n        )\r\n    elif args.plugin == \"zero2_cpu\":\r\n        plugin = LowLevelZeroPlugin(\r\n            stage=2,\r\n            precision=args.mixed_precision,\r\n            initial_scale=2**16,\r\n            cpu_offload=True,\r\n            max_norm=args.grad_clip,\r\n        )\r\n    elif args.plugin == \"3d\":\r\n        plugin = HybridParallelPlugin(\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            sp_size=args.sp,\r\n            sequence_parallelism_mode=args.sp_mode,\r\n            zero_stage=args.zero_stage,\r\n            enable_flash_attention=args.use_flash_attn,\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_sequence_parallelism=args.enable_sequence_parallelism,\r\n            cpu_offload=True if args.zero_stage >= 1 and args.zero_cpu_offload else False,\r\n            max_norm=args.grad_clip,\r\n            precision=args.mixed_precision,\r\n            microbatch_size=args.microbatch_size,\r\n        )\r\n    else:\r\n        raise ValueError(f\"Unknown plugin {args.plugin}\")\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    # ======================================================\r\n    # Initialize Tokenizer, Dataset, Collator and Dataloader\r\n    # ======================================================\r\n    tokenizer = AutoTokenizer.from_pretrained(args.pretrained, trust_remote_code=True)\r\n    if args.pad_token == \"eos\":\r\n        try:\r\n            tokenizer.pad_token = tokenizer.eos_token\r\n        except AttributeError:\r\n            coordinator.print_on_master(f\"pad_token can't be set\")\r\n    elif args.pad_token == \"unk\":\r\n        try:\r\n            tokenizer.pad_token = tokenizer.unk_token\r\n        except AttributeError:\r\n            coordinator.print_on_master(f\"pad_token can't be set\")\r\n    tokenizer.add_bos_token = False\r\n    tokenizer.add_eos_token = False\r\n\r\n    coordinator.print_on_master(\r\n        f\"Training Info:\\nConfig file: {args.config_file} \\nTensorboard logs: {args.tensorboard_dir} \\nModel checkpoint: {args.save_dir}\"\r\n    )\r\n\r\n    if args.benchmark:\r\n        coordinator.print_on_master(f\"Run benchmark with {args.num_samples} random samples.\")\r\n        dataset = RandomDataset(\r\n            num_samples=args.num_samples, max_length=args.max_length, vocab_size=tokenizer.vocab_size\r\n        )\r\n        dataloader = plugin.prepare_dataloader(\r\n            dataset,\r\n            batch_size=args.batch_size,\r\n            shuffle=True,\r\n            drop_last=True,\r\n            seed=42,\r\n            distributed_sampler_cls=StatefulDistributedSampler,\r\n        )\r\n    else:\r\n        coordinator.print_on_master(f\"Load dataset: {args.dataset}\")\r\n        dataset = load_tokenized_dataset(dataset_paths=args.dataset, mode=\"train\")\r\n        data_collator = DataCollatorForSupervisedDataset(\r\n            tokenizer=tokenizer, max_length=args.max_length, padding=args.padding_mode\r\n        )\r\n        dataloader = plugin.prepare_dataloader(\r\n            dataset=dataset,\r\n            batch_size=args.batch_size,\r\n            shuffle=True,\r\n            drop_last=True,\r\n            collate_fn=data_collator,\r\n            distributed_sampler_cls=StatefulDistributedSampler,\r\n        )\r\n\r\n    coordinator.print_on_master(\r\n        f\"Max device memory after data loader: {accelerator.max_memory_allocated() / 1024 ** 2:.2f} MB\"\r\n    )\r\n\r\n    # ======================================================\r\n    # Initialize Model, Objective, Optimizer and LR Scheduler\r\n    # ======================================================\r\n    # When training the ChatGLM model, LoRA and gradient checkpointing are incompatible.\r\n    init_ctx = (\r\n        LazyInitContext(default_device=get_current_device())\r\n        if isinstance(plugin, (GeminiPlugin, HybridParallelPlugin)) and args.lora_rank == 0\r\n        else nullcontext()\r\n    )\r\n    with init_ctx:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            args.pretrained,\r\n            torch_dtype=torch.bfloat16 if args.mixed_precision == \"bf16\" else torch.float16,\r\n            trust_remote_code=True,\r\n        )\r\n        # Freeze part of parameters.\r\n        if args.freeze_non_embeds_params:\r\n            freeze_non_embeds_parameters(model=model)\r\n\r\n    if args.lora_rank > 0:\r\n        lora_config = LoraConfig(task_type=\"CAUSAL_LM\", r=args.lora_rank, lora_alpha=32, lora_dropout=0.1)\r\n        model = booster.enable_lora(model, lora_config=lora_config)\r\n\r\n    # this is essential, otherwise the grad checkpoint will not work.\r\n    model.train()\r\n\r\n    if args.use_grad_checkpoint:\r\n        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\r\n        coordinator.print_on_master(msg=\"Gradient checkpointing enabled successfully\")\r\n\r\n    model_numel = get_model_numel(model)\r\n    coordinator.print_on_master(f\"Model params: {format_numel_str(model_numel)}\")\r\n\r\n    optimizer = HybridAdam(\r\n        model_params=(\r\n            filter(lambda p: p.requires_grad, model.parameters())\r\n            if args.freeze_non_embeds_params\r\n            else model.parameters()\r\n        ),\r\n        lr=args.lr,\r\n        betas=(0.9, 0.95),\r\n        weight_decay=args.weight_decay,\r\n        adamw_mode=True,\r\n    )\r\n\r\n    if args.warmup_steps is None:\r\n        args.warmup_steps = int(args.num_epochs * 0.025 * (len(dataloader) // args.accumulation_steps))\r\n        coordinator.print_on_master(f\"Warmup steps is set to {args.warmup_steps}\")\r\n\r\n    lr_scheduler = CosineAnnealingWarmupLR(\r\n        optimizer=optimizer,\r\n        total_steps=args.num_epochs * (len(dataloader) // args.accumulation_steps),\r\n        warmup_steps=args.warmup_steps,\r\n        eta_min=0.1 * args.lr,\r\n    )\r\n\r\n    # Flash attention will be disabled because it does NOT support fp32.\r\n    default_dtype = torch.float16 if args.mixed_precision == \"fp16\" else torch.bfloat16\r\n    torch.set_default_dtype(default_dtype)\r\n    model, optimizer, _, dataloader, lr_scheduler = booster.boost(\r\n        model=model,\r\n        optimizer=optimizer,\r\n        lr_scheduler=lr_scheduler,\r\n        dataloader=dataloader,\r\n    )\r\n\r\n    torch.set_default_dtype(torch.float)\r\n\r\n    coordinator.print_on_master(\r\n        f\"Booster init max device memory: {accelerator.max_memory_allocated() / 1024 ** 2:.2f} MB\"\r\n    )\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024:.2f} MB\"\r\n    )\r\n\r\n    start_epoch = 0\r\n    start_step = 0\r\n    sampler_start_idx = 0\r\n    if args.load_checkpoint is not None:\r\n        if \"modeling\" in args.load_checkpoint:\r\n            coordinator.print_on_master(f\"Continued pretrain from checkpoint {args.load_checkpoint}\")\r\n            booster.load_model(model, args.load_checkpoint)\r\n        else:\r\n            coordinator.print_on_master(f\"Load model checkpoint from {args.load_checkpoint}\")\r\n            start_epoch, start_step, sampler_start_idx = load_checkpoint(\r\n                load_dir=args.load_checkpoint,\r\n                booster=booster,\r\n                model=model,\r\n                optimizer=optimizer,\r\n                lr_scheduler=lr_scheduler,\r\n            )\r\n            coordinator.print_on_master(\r\n                f\"Loaded checkpoint {args.load_checkpoint} at epoch {start_epoch} step {start_step}\"\r\n            )\r\n            coordinator.print_on_master(f\"Loaded sample at index {sampler_start_idx}\")\r\n\r\n        coordinator.print_on_master(\r\n            f\"Checkpoint loaded max device memory: {accelerator.max_memory_allocated() / 1024 ** 2:.2f} MB\"\r\n        )\r\n        coordinator.print_on_master(\r\n            f\"Checkpoint loaded device memory: {accelerator.memory_allocated() / 1024 ** 2:.2f} MB\"\r\n        )\r\n        coordinator.print_on_master(\r\n            f\"Checkpoint loaded max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024:.2f} MB\"\r\n        )\r\n\r\n    if args.use_neft:\r\n        coordinator.print_on_master(\"Activate NEFTune.\")\r\n        model, handle = activate_neftune(model)\r\n\r\n    num_steps_per_epoch = len(dataloader) // args.accumulation_steps\r\n    # If resume training, set the sampler start index to the correct value\r\n    assert isinstance(dataloader.sampler, StatefulDistributedSampler)\r\n    dataloader.sampler.set_start_index(start_index=sampler_start_idx)\r\n\r\n    for epoch in range(start_epoch, args.num_epochs):\r\n        dataloader.sampler.set_epoch(epoch=epoch)\r\n        if isinstance(plugin, HybridParallelPlugin) and plugin.pp_size > 1:\r\n            data_iter = iter(dataloader)\r\n            step_bar = tqdm(\r\n                range(len(dataloader)),\r\n                desc=\"Step\",\r\n                disable=not (coordinator._local_rank == coordinator._world_size - 1),\r\n            )\r\n            for step in step_bar:\r\n                outputs = booster.execute_pipeline(\r\n                    data_iter,\r\n                    model,\r\n                    criterion=lambda outputs, inputs: outputs[0],\r\n                    optimizer=optimizer,\r\n                    return_loss=True,\r\n                )\r\n                loss = outputs[\"loss\"]\r\n                if booster.plugin.stage_manager.is_last_stage():\r\n                    global_loss = all_reduce_mean(loss, plugin)\r\n                    if coordinator._local_rank == coordinator._world_size - 1:\r\n                        step_bar.set_postfix({\"train/loss\": global_loss.item()})\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                # Save modeling.\r\n                save_model_condition = args.save_interval > 0 and (step + 1) % args.save_interval == 0\r\n\r\n                if not args.skip_save_each_epoch:\r\n                    save_model_condition = save_model_condition or (step + 1) == len(dataloader)\r\n\r\n                if save_model_condition and not args.benchmark:\r\n                    coordinator.print_on_master(\"\\nStart saving model checkpoint with running states\")\r\n\r\n                    if args.use_neft:\r\n                        coordinator.print_on_master(\"Deactivate NEFTune before saving model.\")\r\n                        deactivate_neftune(model, handle)\r\n\r\n                    accelerator.empty_cache()\r\n                    save_checkpoint(\r\n                        save_dir=args.save_dir,\r\n                        booster=booster,\r\n                        model=model,\r\n                        optimizer=optimizer,\r\n                        lr_scheduler=lr_scheduler,\r\n                        epoch=epoch,\r\n                        step=step + 1,\r\n                        batch_size=args.batch_size,\r\n                        coordinator=coordinator,\r\n                        use_lora=(args.lora_rank > 0),\r\n                    )\r\n                    coordinator.print_on_master(\r\n                        f\"Saved checkpoint at epoch {epoch} step {step + 1} at folder {args.save_dir}\"\r\n                    )\r\n\r\n                    if args.use_neft:\r\n                        coordinator.print_on_master(\"Activate NEFTune.\")\r\n                        model, handle = activate_neftune(model)\r\n        else:\r\n            pbar = tqdm(\r\n                desc=f\"Epoch {epoch}\",\r\n                disable=not coordinator.is_master(),\r\n                total=num_steps_per_epoch,\r\n                initial=start_step // args.accumulation_steps,\r\n            )\r\n            total_loss = torch.tensor(0.0, device=get_current_device())\r\n            for step, batch in enumerate(dataloader, start=start_step):\r\n                batch = {k: v.to(get_current_device()) for k, v in batch.items() if isinstance(v, torch.Tensor)}\r\n\r\n                batch_output = model(**batch)\r\n\r\n                loss = batch_output.loss / args.accumulation_steps\r\n                total_loss.add_(loss.data)\r\n\r\n                booster.backward(loss=loss, optimizer=optimizer)\r\n\r\n                if (step + 1) % args.accumulation_steps == 0:\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                    all_reduce_mean(tensor=total_loss)\r\n                    pbar.set_postfix({\"Loss\": f\"{total_loss.item():.4f}\"})\r\n                    if coordinator.is_master():\r\n                        global_step = (epoch * num_steps_per_epoch) + (step + 1) // args.accumulation_steps\r\n                        writer.add_scalar(tag=\"Loss\", scalar_value=total_loss.item(), global_step=global_step)\r\n                        writer.add_scalar(\r\n                            tag=\"Learning Rate\",\r\n                            scalar_value=lr_scheduler.get_last_lr()[0],\r\n                            global_step=global_step,\r\n                        )\r\n                    total_loss.fill_(0.0)\r\n                    pbar.update()\r\n\r\n                # Save modeling.\r\n                save_model_condition = (\r\n                    args.save_interval > 0 and (step + 1) % (args.save_interval * args.accumulation_steps) == 0\r\n                )\r\n\r\n                if not args.skip_save_each_epoch:\r\n                    save_model_condition = save_model_condition or (step + 1) == len(dataloader)\r\n\r\n                if save_model_condition and not args.benchmark:\r\n                    coordinator.print_on_master(\"\\nStart saving model checkpoint with running states\")\r\n\r\n                    if args.use_neft:\r\n                        coordinator.print_on_master(\"Deactivate NEFTune before saving model.\")\r\n                        deactivate_neftune(model, handle)\r\n\r\n                    accelerator.empty_cache()\r\n                    save_checkpoint(\r\n                        save_dir=args.save_dir,\r\n                        booster=booster,\r\n                        model=model,\r\n                        optimizer=optimizer,\r\n                        lr_scheduler=lr_scheduler,\r\n                        epoch=epoch,\r\n                        step=step + 1,\r\n                        batch_size=args.batch_size,\r\n                        coordinator=coordinator,\r\n                        use_lora=(args.lora_rank > 0),\r\n                    )\r\n                    coordinator.print_on_master(\r\n                        f\"Saved checkpoint at epoch {epoch} step {step + 1} at folder {args.save_dir}\"\r\n                    )\r\n\r\n                    if args.use_neft:\r\n                        coordinator.print_on_master(\"Activate NEFTune.\")\r\n                        model, handle = activate_neftune(model)\r\n\r\n        # Delete cache.\r\n        # del batch, batch_labels, batch_output, loss\r\n        accelerator.empty_cache()\r\n\r\n        # the continue epochs are not resumed, so we need to reset the sampler start index and start step\r\n        dataloader.sampler.set_start_index(start_index=0)\r\n        start_step = 0\r\n\r\n    if args.use_neft:\r\n        coordinator.print_on_master(\"Deactivate NEFTune.\")\r\n        deactivate_neftune(model, handle)\r\n\r\n    # Final save.\r\n    if not args.benchmark:\r\n        coordinator.print_on_master(\"Start saving final model checkpoint\")\r\n        booster.save_model(model, os.path.join(args.save_dir, \"modeling\"), shard=True)\r\n        coordinator.print_on_master(f\"Saved final model checkpoint at epoch {epoch} at folder {args.save_dir}\")\r\n\r\n    coordinator.print_on_master(f\"Max device memory usage: {accelerator.max_memory_allocated()/1024**2:.2f} MB\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_force_allreduce(self):\r\n        \"\"\"Test that allreduce is forced on all gradients during opt.step().\"\"\"\r\n        hvd.init()\r\n        rank = hvd.rank()\r\n        size = hvd.size()\r\n\r\n        # This test does not apply if there is only one worker.\r\n        if size == 1:\r\n            self.skipTest(\"Only one worker available\")\r\n\r\n        N, D_in, H, D_out = 64, 100, 10, 10\r\n        x = torch.randn(N, D_in).requires_grad_()\r\n        y = torch.randn(N, D_out).requires_grad_()\r\n\r\n        def new_optimizer(cls, opt_params, model):\r\n            p = {\r\n                k: v for k, v in opt_params.items()\r\n                if k in inspect.signature(cls.__init__).parameters\r\n            }\r\n            return cls(model.parameters(), **p)\r\n\r\n        class Net(torch.nn.Module):\r\n            def __init__(self):\r\n                super(Net, self).__init__()\r\n                self.fc1 = torch.nn.Linear(D_in, H)\r\n                self.fc2 = torch.nn.Linear(H, D_out)\r\n                self.fc3 = torch.nn.Linear(D_out, D_out)\r\n\r\n            def forward(self, x_):\r\n                x_ = F.relu(self.fc1(x_))\r\n                x1_ = self.fc2(x_)\r\n                x2_ = self.fc3(F.relu(x1_))\r\n                return x1_, x2_\r\n\r\n        def create_model(opt_class, opt_params):\r\n            model = Net()\r\n            hvd.broadcast_parameters(model.state_dict(), root_rank=0)\r\n            opt = new_optimizer(opt_class, opt_params, model)\r\n            opt = hvd.DistributedOptimizer(\r\n                opt, named_parameters=model.named_parameters())\r\n            return model, opt\r\n\r\n        # L-BFGS is currently unsupported, as are sparse tensors, which are\r\n        # required by SparseAdam optimizer\r\n        optimizers = [\r\n            (subclass.__name__, subclass)\r\n            for subclass in torch.optim.Optimizer.__subclasses__()\r\n            if subclass.__module__.startswith('torch.optim') and\r\n               subclass != torch.optim.LBFGS and\r\n               subclass != torch.optim.SparseAdam\r\n        ]\r\n        optimizers.sort(key=lambda tup: tup[0])\r\n\r\n        opt_params_list = [\r\n            dict(lr=0.2, momentum=0.9, weight_decay=0.1, centered=True),\r\n            dict(lr=0.2)\r\n        ]\r\n\r\n        for (opt_name, opt_class), opt_params in itertools.product(optimizers, opt_params_list):\r\n            model, optimizer = create_model(opt_class, opt_params)\r\n            y_pred1, y_pred2 = model(x)\r\n            if rank == 0:\r\n                loss = F.mse_loss(y_pred1, y, size_average=False)\r\n            else:\r\n                loss = F.mse_loss(y_pred2, y, size_average=False)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    master_addr = str(get_local_ip())\r\n    # trainer_env_info\r\n    trainer_port = str(get_free_port())\r\n    env_info_trainers = [\r\n        {\r\n            \"local_rank\": \"0\",\r\n            \"rank\": str(rank),\r\n            \"world_size\": str(args.num_trainers),\r\n            \"master_port\": trainer_port,\r\n            \"master_addr\": master_addr,\r\n        }\r\n        for rank in range(args.num_trainers)\r\n    ]\r\n\r\n    # maker_env_info\r\n    maker_port = str(get_free_port())\r\n    env_info_maker = {\r\n        \"local_rank\": \"0\",\r\n        \"rank\": \"0\",\r\n        \"world_size\": \"1\",\r\n        \"master_port\": maker_port,\r\n        \"master_addr\": master_addr,\r\n    }\r\n\r\n    # configure tokenizer\r\n    tokenizer = get_tokenizer_from_args(args.model)\r\n\r\n    def trainer_model_fn():\r\n        actor = get_actor_from_args(args.model, args.pretrain).half().cuda()\r\n        critic = get_critic_from_args(args.model, args.critic_pretrain).half().cuda()\r\n        return actor, critic\r\n\r\n    # configure Trainer\r\n    trainer_refs = [\r\n        DetachedPPOTrainer.options(name=f\"trainer{i}\", num_gpus=1, max_concurrency=2).remote(\r\n            experience_maker_holder_name_list=[\"maker1\"],\r\n            strategy_fn=partial(get_strategy_from_args, args.trainer_strategy),\r\n            model_fn=trainer_model_fn,\r\n            env_info=env_info_trainer,\r\n            train_batch_size=args.train_batch_size,\r\n            buffer_limit=16,\r\n            eval_performance=True,\r\n            debug=args.debug,\r\n            update_lora_weights=not (args.lora_rank == 0),\r\n        )\r\n        for i, env_info_trainer in enumerate(env_info_trainers)\r\n    ]\r\n\r\n    def model_fn():\r\n        actor = get_actor_from_args(args.model, args.pretrain).requires_grad_(False).half().cuda()\r\n        critic = get_critic_from_args(args.model, args.critic_pretrain).requires_grad_(False).half().cuda()\r\n        reward_model = get_reward_model_from_args(args.model, args.critic_pretrain).requires_grad_(False).half().cuda()\r\n        if args.initial_model_quant_ckpt is not None and args.model == \"llama\":\r\n            # quantize initial model\r\n            actor_cfg = AutoConfig.from_pretrained(args.pretrain)\r\n            with low_resource_init(), no_init_weights():\r\n                initial_model = get_actor_from_args(args.model, config=actor_cfg)\r\n            initial_model.model = (\r\n                llama_load_quant(\r\n                    initial_model.model, args.initial_model_quant_ckpt, args.quant_bits, args.quant_group_size\r\n                )\r\n                .cuda()\r\n                .requires_grad_(False)\r\n            )\r\n        else:\r\n            initial_model = get_actor_from_args(args.model, args.pretrain).requires_grad_(False).half().cuda()\r\n        return actor, critic, reward_model, initial_model\r\n\r\n    # configure Experience Maker\r\n    experience_holder_ref = ExperienceMakerHolder.options(name=\"maker1\", num_gpus=1, max_concurrency=2).remote(\r\n        detached_trainer_name_list=[f\"trainer{i}\" for i in range(args.num_trainers)],\r\n        strategy_fn=partial(get_strategy_from_args, args.maker_strategy),\r\n        model_fn=model_fn,\r\n        env_info=env_info_maker,\r\n        experience_batch_size=args.experience_batch_size,\r\n        kl_coef=0.1,\r\n        debug=args.debug,\r\n        update_lora_weights=not (args.lora_rank == 0),\r\n        # sync_models_from_trainers=True,\r\n        # generation kwargs:\r\n        max_length=512,\r\n        do_sample=True,\r\n        temperature=1.0,\r\n        top_k=50,\r\n        pad_token_id=tokenizer.pad_token_id,\r\n        eos_token_id=tokenizer.eos_token_id,\r\n        eval_performance=True,\r\n        use_cache=True,\r\n    )\r\n\r\n    # uncomment this function if sync_models_from_trainers is True\r\n    # ray.get([\r\n    #     trainer_ref.sync_models_to_remote_makers.remote()\r\n    #     for trainer_ref in trainer_refs\r\n    # ])\r\n\r\n    wait_tasks = []\r\n\r\n    total_steps = args.experience_batch_size * args.experience_steps // (args.num_trainers * args.train_batch_size)\r\n    for trainer_ref in trainer_refs:\r\n        wait_tasks.append(trainer_ref.fit.remote(total_steps, args.update_steps, args.train_epochs))\r\n\r\n    dataset_size = args.experience_batch_size * 4\r\n\r\n    def build_dataloader():\r\n        def tokenize_fn(texts):\r\n            batch = tokenizer(texts, return_tensors=\"pt\", max_length=96, padding=\"max_length\", truncation=True)\r\n            return {k: v.cuda() for k, v in batch.items()}\r\n\r\n        dataset = pd.read_csv(args.prompt_path)[\"prompt\"]\r\n        dataloader = DataLoader(dataset=dataset, batch_size=dataset_size, shuffle=True, collate_fn=tokenize_fn)\r\n        return dataloader\r\n\r\n    wait_tasks.append(experience_holder_ref.workingloop.remote(build_dataloader, num_steps=args.experience_steps))\r\n\r\n    ray.get(wait_tasks)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def main(args):\r\n    master_addr = str(get_local_ip())\r\n    # trainer_env_info\r\n    trainer_port = str(get_free_port())\r\n    env_info_trainers = [\r\n        {\r\n            \"local_rank\": \"0\",\r\n            \"rank\": str(rank),\r\n            \"world_size\": str(args.num_trainers),\r\n            \"master_port\": trainer_port,\r\n            \"master_addr\": master_addr,\r\n        }\r\n        for rank in range(args.num_trainers)\r\n    ]\r\n\r\n    # maker_env_info\r\n    maker_port = str(get_free_port())\r\n    env_info_makers = [\r\n        {\r\n            \"local_rank\": \"0\",\r\n            \"rank\": str(rank),\r\n            \"world_size\": str(args.num_makers),\r\n            \"master_port\": maker_port,\r\n            \"master_addr\": master_addr,\r\n        }\r\n        for rank in range(args.num_makers)\r\n    ]\r\n\r\n    # configure tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\r\n    tokenizer.pad_token = tokenizer.eos_token\r\n\r\n    def model_fn():\r\n        actor = get_actor_from_args(args.model, args.pretrain).requires_grad_(False).half().cuda()\r\n        critic = get_critic_from_args(args.model, args.critic_pretrain).requires_grad_(False).half().cuda()\r\n        reward_model = get_reward_model_from_args(args.model, args.critic_pretrain).requires_grad_(False).half().cuda()\r\n        if args.initial_model_quant_ckpt is not None and args.model == \"llama\":\r\n            # quantize initial model\r\n            actor_cfg = AutoConfig.from_pretrained(args.pretrain)\r\n            with low_resource_init(), no_init_weights():\r\n                initial_model = get_actor_from_args(args.model, config=actor_cfg)\r\n            initial_model.model = (\r\n                llama_load_quant(\r\n                    initial_model.model, args.initial_model_quant_ckpt, args.quant_bits, args.quant_group_size\r\n                )\r\n                .cuda()\r\n                .requires_grad_(False)\r\n            )\r\n        else:\r\n            initial_model = get_actor_from_args(args.model, args.pretrain).requires_grad_(False).half().cuda()\r\n        return actor, critic, reward_model, initial_model\r\n\r\n    # configure Experience Maker\r\n    experience_holder_refs = [\r\n        ExperienceMakerHolder.options(name=f\"maker{i}\", num_gpus=1, max_concurrency=2).remote(\r\n            detached_trainer_name_list=[\r\n                f\"trainer{x}\"\r\n                for x in get_receivers_per_sender(i, args.num_makers, args.num_trainers, allow_idle_sender=False)\r\n            ],\r\n            strategy_fn=partial(get_strategy_from_args, args.maker_strategy),\r\n            model_fn=model_fn,\r\n            env_info=env_info_maker,\r\n            kl_coef=0.1,\r\n            debug=args.debug,\r\n            update_lora_weights=not (args.lora_rank == 0),\r\n            # sync_models_from_trainers=True,\r\n            # generation kwargs:\r\n            max_length=512,\r\n            do_sample=True,\r\n            temperature=1.0,\r\n            top_k=50,\r\n            pad_token_id=tokenizer.pad_token_id,\r\n            eos_token_id=tokenizer.eos_token_id,\r\n            eval_performance=True,\r\n            use_cache=True,\r\n        )\r\n        for i, env_info_maker in enumerate(env_info_makers)\r\n    ]\r\n\r\n    def trainer_model_fn():\r\n        actor = get_actor_from_args(args.model, args.pretrain, lora_rank=args.lora_rank).half().cuda()\r\n        critic = get_critic_from_args(args.model, args.critic_pretrain, lora_rank=args.lora_rank).half().cuda()\r\n        return actor, critic\r\n\r\n    # configure Trainer\r\n    trainer_refs = [\r\n        DetachedPPOTrainer.options(name=f\"trainer{i}\", num_gpus=1, max_concurrency=2).remote(\r\n            experience_maker_holder_name_list=[\r\n                f\"maker{x}\"\r\n                for x in get_receivers_per_sender(i, args.num_trainers, args.num_makers, allow_idle_sender=True)\r\n            ],\r\n            strategy_fn=partial(get_strategy_from_args, args.trainer_strategy),\r\n            model_fn=trainer_model_fn,\r\n            env_info=env_info_trainer,\r\n            train_batch_size=args.train_batch_size,\r\n            buffer_limit=16,\r\n            eval_performance=True,\r\n            debug=args.debug,\r\n            update_lora_weights=not (args.lora_rank == 0),\r\n        )\r\n        for i, env_info_trainer in enumerate(env_info_trainers)\r\n    ]\r\n\r\n    dataset_size = args.experience_batch_size * 4\r\n\r\n    def build_dataloader():\r\n        def tokenize_fn(texts):\r\n            batch = tokenizer(texts, return_tensors=\"pt\", max_length=96, padding=\"max_length\", truncation=True)\r\n            return {k: v.cuda() for k, v in batch.items()}\r\n\r\n        dataset = pd.read_csv(args.prompt_path)[\"prompt\"]\r\n        dataloader = DataLoader(dataset=dataset, batch_size=dataset_size, shuffle=True, collate_fn=tokenize_fn)\r\n        return dataloader\r\n\r\n    # uncomment this function if sync_models_from_trainers is True\r\n    # ray.get([\r\n    #     trainer_ref.sync_models_to_remote_makers.remote()\r\n    #     for trainer_ref in trainer_refs\r\n    # ])\r\n\r\n    wait_tasks = []\r\n\r\n    for experience_holder_ref in experience_holder_refs:\r\n        wait_tasks.append(experience_holder_ref.workingloop.remote(build_dataloader, num_steps=args.experience_steps))\r\n\r\n    total_steps = (\r\n        args.experience_batch_size\r\n        * args.experience_steps\r\n        * args.num_makers\r\n        // (args.num_trainers * args.train_batch_size)\r\n    )\r\n    for trainer_ref in trainer_refs:\r\n        wait_tasks.append(trainer_ref.fit.remote(total_steps, args.update_steps, args.train_epochs))\r\n\r\n    ray.get(wait_tasks)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def get_gpt_evaluation_without_logprobs(\r\n    prompt: Dict[str, Any],\r\n    inst: Dict[str, Any],\r\n    metrics: List[str],\r\n    language: str,\r\n    reference: Dict[str, Any] = None,\r\n    model: str = \"gpt-3.5-turbo\",\r\n    max_tokens: int = 2048,\r\n) -> Dict[str, Any]:\r\n    \"\"\"\r\n    Use chat models(gpt-3.5-turbo or gpt-4) to evaluate one model answer.\r\n\r\n    Temprature is set to 0 to make the model more deterministic.\r\n\r\n    Args:\r\n        prompt: a dictionary including prompt template, CoT and metrics.\r\n        inst: the instruction that is needed to be evaluated.\r\n        metrics: the metrics for evaluation.\r\n        language: language used to change the CoT(add one more step about comparing the given answer and reference) if reference is not None.\r\n        reference: the reference answer.\r\n        model: the model used to evaluate answers.\r\n        max_tokens: the maximum number of tokens to generate in the chat completion.\r\n\r\n    Returns:\r\n        An evaluation of one answer.\r\n    \"\"\"\r\n\r\n    MAX_API_RETRY = 3\r\n\r\n    question = inst[\"instruction\"] if inst[\"input\"] == \"\" else inst[\"instruction\"] + \"\\n\" + inst[\"input\"]\r\n    answer = inst[\"output\"]\r\n    inst[\"evaluation\"] = {}\r\n\r\n    for metric in metrics:\r\n        if prompt[\"metrics\"].get(metric, None) is None:\r\n            raise Exception(\r\n                f\"Unsupported metric {metric} for category {inst['category']}! You should add this metric in the prompt file!\"\r\n            )\r\n        for i in range(MAX_API_RETRY):\r\n            try:\r\n                prompt_reference = \"\" if reference is None else reference_template(metric, language, reference)\r\n\r\n                prompt_1st_round = prompt[\"prompt\"].format(\r\n                    question=question,\r\n                    answer=answer,\r\n                    metric=prompt[\"metrics\"][metric],\r\n                    steps=prompt[\"CoT\"][metric],\r\n                )\r\n\r\n                if prompt_reference and (reference[\"target\"] or reference[\"output\"]):\r\n                    # Do a 2-round conversation\r\n                    response = multiturn_chat_completion(\r\n                        [prompt_1st_round, prompt_reference], model, max_tokens=max_tokens, turns=2\r\n                    )\r\n                else:\r\n                    response = multiturn_chat_completion([prompt_1st_round], model, max_tokens=max_tokens, turns=1)\r\n\r\n                inst[\"evaluation\"][metric] = {\r\n                    \"response\": response[\"choices\"][0][\"message\"][\"content\"],\r\n                    \"logprobs\": None,\r\n                }\r\n\r\n                # Prevent exceeding rate limits because we have multiple workers.\r\n                # But this will slow down the evaluation process.\r\n                # You can comment this line if your request doesn't contain many tokens.\r\n                time.sleep(len(metrics) * 0.5)\r\n\r\n                break\r\n            except Exception as e:\r\n                print(e)\r\n                time.sleep(1)\r\n        if metric not in inst[\"evaluation\"]:\r\n            print(f\"Evaluation {inst['id']} for metric {metric} failed after {MAX_API_RETRY} retries.\")\r\n            inst[\"evaluation\"][metric] = {}\r\n    return inst",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def get_gpt_evaluation_with_logprobs(\r\n    prompt: Dict[str, Any], inst: Dict[str, Any], metrics: List[str], max_tokens: int = 2048\r\n) -> Dict[str, Any]:\r\n    \"\"\"\r\n    Use completion model(text-davinci-003) to evaluate one model answer.\r\n    Only completion models can return log probabilities.\r\n\r\n    Temprature is set to 0 to make the model more deterministic.\r\n\r\n    Args:\r\n        prompt: a dictionary including prompt template, CoT and metrics.\r\n        inst: the instruction that is needed to be evaluated.\r\n        metrics: the metrics for evaluation.\r\n        max_tokens: the maximum number of tokens to generate in the completion.\r\n\r\n    Returns:\r\n        An evaluation of one answer.\r\n    \"\"\"\r\n\r\n    MAX_API_RETRY = 3\r\n\r\n    question = inst[\"instruction\"] if inst[\"input\"] == \"\" else inst[\"instruction\"] + \"\\n\" + inst[\"input\"]\r\n    answer = inst[\"output\"]\r\n    inst[\"evaluation\"] = {}\r\n\r\n    for metric in metrics:\r\n        if prompt[\"metrics\"].get(metric, None) is None:\r\n            raise Exception(\r\n                f\"Unsupported metric {metric} for category {inst['category']}! You should add this metric in the prompt file!\"\r\n            )\r\n        for i in range(MAX_API_RETRY):\r\n            try:\r\n                response = openai.Completion.create(\r\n                    model=\"text-davinci-003\",\r\n                    prompt=prompt[\"prompt\"].format(\r\n                        question=question,\r\n                        answer=answer,\r\n                        metric=prompt[\"metrics\"][metric],\r\n                        steps=prompt[\"CoT\"][metric],\r\n                    ),\r\n                    logprobs=5,\r\n                    temperature=0,\r\n                    max_tokens=max_tokens,\r\n                )\r\n                inst[\"evaluation\"][metric] = {\r\n                    \"response\": response[\"choices\"][0][\"text\"],\r\n                    \"logprobs\": response[\"choices\"][0][\"logprobs\"][\"top_logprobs\"],\r\n                }\r\n\r\n                # Prevent exceeding rate limits because we have multiple workers.\r\n                # But this will slow down the evaluation process.\r\n                # You can comment this line if your request doesn't contain many tokens.\r\n                time.sleep(len(metrics) * 0.5)\r\n\r\n                break\r\n            except Exception as e:\r\n                print(e)\r\n                time.sleep(1)\r\n        if metric not in inst[\"evaluation\"]:\r\n            print(f\"Evaluation {inst['id']} for metric {metric} failed after {MAX_API_RETRY} retries.\")\r\n            inst[\"evaluation\"][metric] = {}\r\n    return inst",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def forward(self, batch_id, is_prefill):\r\n        \"\"\"\r\n        Forward is used in Dynamic Batching Manager\r\n        \"\"\"\r\n        batch = self.cache.pop(batch_id)\r\n        if is_prefill:\r\n            input_ = torch.tensor(batch.all_input_ids).cuda()\r\n        else:\r\n            input_ = batch.input_ids.reshape(len(batch), 1)\r\n\r\n        batch_args = {\r\n            \"batch_size\": len(batch),\r\n            \"max_len_in_batch\": batch.nopad_max_len_in_batch,\r\n            \"block_loc\": batch.nopad_b_loc,\r\n            \"start_loc\": batch.nopad_b_start_loc,\r\n            \"seq_len\": batch.nopad_b_seq_len,\r\n            \"cache_manager\": batch.cache_manager,\r\n            \"is_context_stage\": is_prefill,\r\n        }\r\n\r\n        infer_state = BatchInferState(**batch_args)\r\n        model = self.model\r\n        if isinstance(model, LlamaForCausalLM):\r\n            model = self.model.model\r\n        elif isinstance(model, BloomForCausalLM):\r\n            model = self.model.transformer\r\n\r\n        setattr(model, \"infer_state\", infer_state)\r\n        output = self.model.forward(input_ids=input_)\r\n        logits = output.logits\r\n        # bsz, seq_len, vocab_size\r\n        prob_out = torch.softmax(\r\n            logits[\r\n                :,\r\n                -1,\r\n            ],\r\n            dim=-1,\r\n        ).squeeze(1)\r\n        # prob_out: bsz, vocab_size\r\n        predict_ids = torch.argmax(prob_out, dim=-1, keepdim=True)\r\n        prob_out = torch.log(prob_out).detach().cpu().numpy()\r\n        predict_ids = predict_ids.detach().cpu().numpy()\r\n        # [ batch_size, 1 ]\r\n\r\n        output_dict = {}\r\n        new_input_ids = []\r\n        for i, (r, all_input_ids, next_token_id, next_token_logprob) in enumerate(\r\n            zip(batch.requests, batch.all_input_ids, predict_ids, prob_out)\r\n        ):\r\n            next_token_id = int(next_token_id)\r\n            next_token_logprob = next_token_logprob[next_token_id]\r\n            # all_input_ids_tensor = torch.tensor(all_input_ids, dtype=torch.long, device=\"cuda\")\r\n            all_input_ids.append(next_token_id)\r\n            # all_input_ids_tensor = None\r\n            new_input_ids.append(next_token_id)\r\n            batch.all_input_ids[i] = all_input_ids\r\n            batch.input_lengths[i] += 1\r\n            batch.out_token_id_counts[i][next_token_id] += 1\r\n            metadata = {\r\n                \"id\": int(next_token_id),\r\n                \"logprob\": float(next_token_logprob),\r\n            }\r\n            output_dict[r[\"request_id\"]] = (int(next_token_id), metadata)\r\n\r\n        batch.input_ids = torch.tensor(new_input_ids, dtype=torch.long).cuda()\r\n        batch.nopad_total_token_num += len(batch)\r\n        batch.nopad_max_len_in_batch += 1  # NOTE: we may repalce this\r\n        self.cache[batch.batch_id] = batch\r\n        return output_dict",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def fit(\r\n    model: nn.Module,\r\n    optimizer: Optimizer,\r\n    scheduler,\r\n    train_dataloader,\r\n    max_epochs,\r\n    accumulation_steps,\r\n    batch_size,\r\n    coordinator,\r\n):\r\n    step_bar = tqdm(\r\n        range(len(train_dataloader) // accumulation_steps * max_epochs),\r\n        desc=f\"steps\",\r\n        disable=not coordinator.is_master(),\r\n    )\r\n    total_loss = 0\r\n    for epoch in range(max_epochs):\r\n        model.train()\r\n        for batch_id, batch in enumerate(train_dataloader):\r\n            batch = to_device(batch, torch.cuda.current_device())\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            loss = loss / accumulation_steps\r\n            loss.backward()\r\n            total_loss += loss.item()\r\n            if (batch_id + 1) % accumulation_steps == 0:\r\n                optimizer.step()\r\n                scheduler.step()\r\n                optimizer.zero_grad()\r\n                step_bar.set_postfix(\r\n                    {\"epoch\": epoch, \"loss\": total_loss / batch_size, \"lr\": scheduler.get_last_lr()[0]}\r\n                )\r\n                total_loss = 0\r\n                step_bar.update()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n    launch_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\r\n\r\n    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\n\r\n    logger = Logger(os.path.join(args.log_path, launch_time), cuda=torch.cuda.is_available(), debug=args.vscode_debug)\r\n\r\n    if args.vscode_debug:\r\n        colossalai.launch(\r\n            rank=args.rank, world_size=args.world_size, host=args.host, port=args.port, backend=args.backend\r\n        )\r\n        args.local_rank = -1\r\n        args.log_interval = 1\r\n    else:\r\n        colossalai.launch_from_torch()  # args.colossal_config\r\n        args.local_rank = int(os.environ[\"LOCAL_RANK\"])\r\n        logger.info(\r\n            f\"launch_from_torch, world size: {torch.distributed.get_world_size()} | \"\r\n            + f\"ParallelMode.MODEL: {ParallelMode.MODEL} | ParallelMode.DATA: {ParallelMode.DATA} | ParallelMode.TENSOR: {ParallelMode.TENSOR}\"\r\n        )\r\n\r\n    log_args(logger, args)\r\n    args.tokenizer = tokenizer\r\n    args.logger = logger\r\n    set_global_variables(launch_time, args.tensorboard_path)\r\n\r\n    world_size = torch.distributed.get_world_size()\r\n    get_accelerator().get_current_device()\r\n\r\n    # build model, optimizer and criterion\r\n    if args.distplan.startswith(\"CAI\"):\r\n        # all param must use the same process group.\r\n        world_size = torch.distributed.get_world_size()\r\n        shard_pg = ProcessGroup(tp_degree=world_size) if args.shardinit else None\r\n        default_dist_spec = ShardSpec([-1], [world_size]) if args.shardinit else None\r\n\r\n        if args.shardinit and args.distplan != \"CAI_Gemini\":\r\n            raise RuntimeError(\"You can only use shardinit with CAI_Gemini\")\r\n\r\n        # build GPT model\r\n        with ColoInitContext(\r\n            device=get_accelerator().get_current_device(),\r\n            dtype=torch.half,\r\n            default_dist_spec=default_dist_spec,\r\n            default_pg=shard_pg,\r\n        ):\r\n            config, model, numel = get_model(args, logger)\r\n\r\n        # assign running configurations\r\n        gemini_config = None\r\n        if args.distplan.startswith(\"CAI_ZeRO\"):\r\n            optim_config = dict(reduce_bucket_size=12 * 1024 * 1024, overlap_communication=True, verbose=True)\r\n        elif args.distplan == \"CAI_Gemini\":\r\n            gemini_config = dict(\r\n                strict_ddp_mode=args.tp_degree == 1,\r\n                device=get_accelerator().get_current_device(),\r\n                placement_policy=args.placement,\r\n                pin_memory=True,\r\n                hidden_dim=model.config.hidden_size,\r\n                search_range_m=128,\r\n            )\r\n            optim_config = dict(gpu_margin_mem_ratio=0.0)\r\n        else:\r\n            raise RuntimeError\r\n\r\n        # build a highly optimized gpu/cpu optimizer\r\n        optimizer = get_optimizer(model, lr=args.lr)\r\n\r\n        if args.distplan == \"CAI_ZeRO1\":\r\n            zero_stage = 1\r\n        elif args.distplan == \"CAI_ZeRO2\":\r\n            zero_stage = 2\r\n        elif args.distplan == \"CAI_Gemini\":\r\n            zero_stage = 3\r\n        else:\r\n            raise RuntimeError\r\n\r\n        # wrap your model and optimizer\r\n        model = zero_model_wrapper(model, zero_stage, gemini_config)\r\n        optimizer = zero_optim_wrapper(model, optimizer, optim_config=optim_config)\r\n\r\n        logger.info(get_mem_info(prefix=\"After init optim, \"))\r\n\r\n    else:\r\n        config, model, numel = get_model(args, logger)\r\n        logger.info(\"no_zero\")\r\n\r\n    if torch.distributed.get_rank() == 0:\r\n        os.mkdir(os.path.join(args.ckpt_path, launch_time))\r\n\r\n    logger.info(f\"Model numel: {numel}\")\r\n\r\n    get_tflops_func = partial(get_tflops, numel, args.train_micro_batch_size_per_gpu, args.max_seq_length)\r\n\r\n    # 144003367 is is the length of the entire dataset\r\n    # len(dataloader)\r\n    steps_per_epoch = (\r\n        144003367\r\n        // world_size\r\n        // args.train_micro_batch_size_per_gpu\r\n        // args.gradient_accumulation_steps\r\n        // args.refresh_bucket_size\r\n    )\r\n    total_steps = steps_per_epoch * args.epoch\r\n\r\n    lr_scheduler = get_lr_scheduler(optimizer, total_steps=total_steps, last_epoch=-1)\r\n\r\n    start_epoch = 0\r\n    start_shard = 0\r\n    global_step = 0\r\n    if args.resume_train:\r\n        assert os.path.exists(args.load_optimizer_lr)\r\n        o_l_state_dict = torch.load(args.load_optimizer_lr, map_location=\"cpu\")\r\n        o_l_state_dict[\"lr_scheduler\"][\"last_epoch\"] = o_l_state_dict[\"lr_scheduler\"][\"last_epoch\"] - 1\r\n        optimizer.load_state_dict(o_l_state_dict[\"optimizer\"])\r\n        # o_l_state_dict['lr_scheduler']['last_epoch']\r\n        lr_scheduler = get_lr_scheduler(\r\n            optimizer, total_steps=total_steps, last_epoch=o_l_state_dict[\"lr_scheduler\"][\"last_epoch\"]\r\n        )\r\n        for state in optimizer.state.values():\r\n            for k, v in state.items():\r\n                if isinstance(v, torch.Tensor):\r\n                    state[k] = v.cuda(f\"cuda:{torch.cuda.current_device()}\")\r\n        # if you want delete the above three code, must move the model to gpu. Because in optimizer.step()\r\n        lr_scheduler.load_state_dict(o_l_state_dict[\"lr_scheduler\"])\r\n\r\n        start_epoch = o_l_state_dict[\"epoch\"]\r\n        start_shard = o_l_state_dict[\"shard\"] + 1\r\n        # global_step = o_l_state_dict['global_step'] + 1\r\n        logger.info(\r\n            f\"resume from epoch {start_epoch} shard {start_shard} step {lr_scheduler.last_epoch} lr {lr_scheduler.get_last_lr()[0]}\"\r\n        )\r\n\r\n    criterion = LossForPretraining(config.vocab_size)\r\n\r\n    # build dataloader\r\n    pretrain_dataset_provider = NvidiaBertDatasetProvider(args)\r\n\r\n    logger.info(get_mem_info(prefix=\"After init model, \"))\r\n\r\n    eval_loss = 0\r\n    train_loss = 0\r\n    timers = get_timers()\r\n    timers(\"interval_time\").start()\r\n    timers(\"epoch_time\").start()\r\n    timers(\"shard_time\").start()\r\n\r\n    for epoch in range(start_epoch, args.epoch):\r\n        for shard in range(start_shard, len(os.listdir(args.data_path_prefix))):\r\n            dataset_iterator, total_length = pretrain_dataset_provider.get_shard(shard)\r\n            # pretrain_dataset_provider.prefetch_shard(shard + 1) # may cause cpu memory overload\r\n            if torch.distributed.get_rank() == 0:\r\n                iterator_data = tqdm(\r\n                    enumerate(dataset_iterator),\r\n                    total=(total_length // args.train_micro_batch_size_per_gpu // world_size),\r\n                    colour=\"cyan\",\r\n                    smoothing=1,\r\n                )\r\n            else:\r\n                iterator_data = enumerate(dataset_iterator)\r\n\r\n            model.train()\r\n\r\n            for step, batch_data in iterator_data:\r\n                # batch_data = pretrain_dataset_provider.get_batch(batch_index)\r\n                input_ids = batch_data[0].cuda(f\"cuda:{torch.cuda.current_device()}\")\r\n                attention_mask = batch_data[1].cuda(f\"cuda:{torch.cuda.current_device()}\")\r\n                token_type_ids = batch_data[2].cuda(f\"cuda:{torch.cuda.current_device()}\")\r\n                mlm_label = batch_data[3].cuda(f\"cuda:{torch.cuda.current_device()}\")\r\n                # nsp_label = batch_data[5].cuda()\r\n\r\n                output = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n\r\n                loss = criterion(output.logits, mlm_label)\r\n                pretrain_dataset_provider.prefetch_batch()\r\n\r\n                optimizer.backward(loss)\r\n                train_loss += loss.float().item()\r\n                # if  (step + 1) % args.accumulation_step == 0:\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n\r\n                global_step += 1\r\n\r\n                if global_step % args.log_interval == 0 and global_step != 0 and torch.distributed.get_rank() == 0:\r\n                    elapsed_time = timers(\"interval_time\").elapsed(reset=False)\r\n                    elapsed_time_per_iteration = elapsed_time / global_step\r\n                    samples_per_sec, tflops, approx_parameters_in_billions = throughput_calculator(\r\n                        numel, args, config, elapsed_time, global_step, world_size\r\n                    )\r\n\r\n                    cur_loss = train_loss / args.log_interval\r\n                    current_lr = lr_scheduler.get_last_lr()[0]\r\n                    log_str = (\r\n                        f\"| epoch: {epoch} | shard: {shard} | step: {global_step} | lr {current_lr:.7f} | elapsed_time: {elapsed_time / 60 :.3f} minutes \"\r\n                        + f\"| mins/batch: {elapsed_time_per_iteration :.3f} seconds | loss: {cur_loss:.7f} | ppl: {math.exp(cur_loss):.3f} | TFLOPS: {get_tflops_func(elapsed_time_per_iteration):.3f} or {tflops:.3f}\"\r\n                    )\r\n                    logger.info(log_str, print_=False)\r\n\r\n                    if args.wandb:\r\n                        tensorboard_log = get_tensorboard_writer()\r\n                        tensorboard_log.log_train(\r\n                            {\r\n                                \"lr\": current_lr,\r\n                                \"loss\": cur_loss,\r\n                                \"ppl\": math.exp(cur_loss),\r\n                                \"mins_batch\": elapsed_time_per_iteration,\r\n                            },\r\n                            global_step,\r\n                        )\r\n\r\n                    train_loss = 0\r\n\r\n            logger.info(f'epoch {epoch} shard {shard} has cost {timers(\"shard_time\").elapsed() / 60 :.3f} mins')\r\n            logger.info(\"*\" * 100)\r\n\r\n            eval_loss += evaluate(model, args, logger, global_step, criterion)\r\n            save_ckpt(\r\n                model,\r\n                optimizer,\r\n                lr_scheduler,\r\n                os.path.join(args.ckpt_path, launch_time, f\"epoch-{epoch}_shard-{shard}_\" + launch_time),\r\n                epoch,\r\n                shard,\r\n                global_step,\r\n            )\r\n\r\n        eval_loss /= len(os.listdir(args.data_path_prefix))\r\n        logger.info(\r\n            f'epoch {epoch} | shard_length {len(os.listdir(args.data_path_prefix))} | elapsed_time: {timers(\"epoch_time\").elapsed() / 60 :.3f} mins'\r\n            + f\"eval_loss: {eval_loss} | ppl: {math.exp(eval_loss)}\"\r\n        )\r\n        logger.info(\"-\" * 100)\r\n        if args.wandb and torch.distributed.get_rank() == 0:\r\n            tensorboard_log = get_tensorboard_writer()\r\n            tensorboard_log.log_eval(\r\n                {\r\n                    \"all_eval_shard_loss\": eval_loss,\r\n                },\r\n                epoch,\r\n            )\r\n        start_shard = 0\r\n        eval_loss = 0\r\n\r\n    pretrain_dataset_provider.release_shard()\r\n\r\n    logger.info(\"Congratulation, training has finished!!!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=\"tensorboard\",\r\n        logging_dir=logging_dir,\r\n    )\r\n\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            if args.hub_model_id is None:\r\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\r\n            else:\r\n                repo_name = args.hub_model_id\r\n            repo = Repository(args.output_dir, clone_from=repo_name)\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.tokenizer_name,\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path)\r\n\r\n    # Load models and create wrapper for stable diffusion\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path,\r\n        subfolder=\"text_encoder\",\r\n        revision=args.revision,\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(\r\n        args.pretrained_model_name_or_path,\r\n        subfolder=\"vae\",\r\n        revision=args.revision,\r\n    )\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path,\r\n        subfolder=\"unet\",\r\n        revision=args.revision,\r\n    )\r\n\r\n    vae.requires_grad_(False)\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        if args.train_text_encoder:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\")\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    params_to_optimize = (\r\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\r\n    )\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    def collate_fn(examples):\r\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\r\n        pixel_values = [example[\"instance_images\"] for example in examples]\r\n\r\n        # Concat class and instance examples for prior preservation.\r\n        # We do this to avoid doing two forward passes.\r\n        if args.with_prior_preservation:\r\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\r\n            pixel_values += [example[\"class_images\"] for example in examples]\r\n\r\n        pixel_values = torch.stack(pixel_values)\r\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\r\n\r\n        input_ids = tokenizer.pad(\r\n            {\"input_ids\": input_ids},\r\n            padding=\"max_length\",\r\n            max_length=tokenizer.model_max_length,\r\n            return_tensors=\"pt\",\r\n        ).input_ids\r\n\r\n        batch = {\r\n            \"input_ids\": input_ids,\r\n            \"pixel_values\": pixel_values,\r\n        }\r\n        return batch\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=1\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n    )\r\n\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move text_encode and vae to gpu.\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n    global_step = 0\r\n\r\n    for epoch in range(args.num_train_epochs):\r\n        unet.train()\r\n        if args.train_text_encoder:\r\n            text_encoder.train()\r\n        for step, batch in enumerate(train_dataloader):\r\n            with accelerator.accumulate(unet):\r\n                # Convert images to latent space\r\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                latents = latents * 0.18215\r\n\r\n                # Sample noise that we'll add to the latents\r\n                noise = torch.randn_like(latents)\r\n                bsz = latents.shape[0]\r\n                # Sample a random timestep for each image\r\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\r\n                timesteps = timesteps.long()\r\n\r\n                # Add noise to the latents according to the noise magnitude at each timestep\r\n                # (this is the forward diffusion process)\r\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                # Get the text embedding for conditioning\r\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                # Predict the noise residual\r\n                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                # Get the target for loss depending on the prediction type\r\n                if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                    target = noise\r\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                else:\r\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                if args.with_prior_preservation:\r\n                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                    target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                    # Compute instance loss\r\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\r\n\r\n                    # Compute prior loss\r\n                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                    # Add the prior loss to the instance loss.\r\n                    loss = loss + args.prior_loss_weight * prior_loss\r\n                else:\r\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                accelerator.backward(loss)\r\n                if accelerator.sync_gradients:\r\n                    params_to_clip = (\r\n                        itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                        if args.train_text_encoder\r\n                        else unet.parameters()\r\n                    )\r\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n\r\n            # Checks if the accelerator has performed an optimization step behind the scenes\r\n            if accelerator.sync_gradients:\r\n                progress_bar.update(1)\r\n                global_step += 1\r\n\r\n                if global_step % args.save_steps == 0:\r\n                    if accelerator.is_main_process:\r\n                        pipeline = DiffusionPipeline.from_pretrained(\r\n                            args.pretrained_model_name_or_path,\r\n                            unet=accelerator.unwrap_model(unet),\r\n                            text_encoder=accelerator.unwrap_model(text_encoder),\r\n                            revision=args.revision,\r\n                        )\r\n                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\r\n                        pipeline.save_pretrained(save_path)\r\n\r\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n            progress_bar.set_postfix(**logs)\r\n            accelerator.log(logs, step=global_step)\r\n\r\n            if global_step >= args.max_train_steps:\r\n                break\r\n\r\n        accelerator.wait_for_everyone()\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    if accelerator.is_main_process:\r\n        pipeline = DiffusionPipeline.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            unet=accelerator.unwrap_model(unet),\r\n            text_encoder=accelerator.unwrap_model(text_encoder),\r\n            revision=args.revision,\r\n        )\r\n        pipeline.save_pretrained(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=\"tensorboard\",\r\n        logging_dir=logging_dir,\r\n    )\r\n\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            pipeline = StableDiffusionInpaintPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path, torch_dtype=torch_dtype, safety_checker=None\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(\r\n                sample_dataset, batch_size=args.sample_batch_size, num_workers=1\r\n            )\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n            transform_to_pil = transforms.ToPILImage()\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                bsz = len(example[\"prompt\"])\r\n                fake_images = torch.rand((3, args.resolution, args.resolution))\r\n                transform_to_pil = transforms.ToPILImage()\r\n                fake_pil_images = transform_to_pil(fake_images)\r\n\r\n                fake_mask = random_mask((args.resolution, args.resolution), ratio=1, mask_full_image=True)\r\n\r\n                images = pipeline(prompt=example[\"prompt\"], mask_image=fake_mask, image=fake_pil_images).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            if args.hub_model_id is None:\r\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\r\n            else:\r\n                repo_name = args.hub_model_id\r\n            repo = Repository(args.output_dir, clone_from=repo_name)\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\r\n\r\n    # Load models and create wrapper for stable diffusion\r\n    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\r\n    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\r\n\r\n    vae.requires_grad_(False)\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        if args.train_text_encoder:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\")\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    params_to_optimize = (\r\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\r\n    )\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    def collate_fn(examples):\r\n        image_transforms = transforms.Compose(\r\n            [\r\n                transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\r\n                transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\r\n            ]\r\n        )\r\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\r\n        pixel_values = [example[\"instance_images\"] for example in examples]\r\n\r\n        # Concat class and instance examples for prior preservation.\r\n        # We do this to avoid doing two forward passes.\r\n        if args.with_prior_preservation:\r\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\r\n            pixel_values += [example[\"class_images\"] for example in examples]\r\n            pior_pil = [example[\"class_PIL_images\"] for example in examples]\r\n\r\n        masks = []\r\n        masked_images = []\r\n        for example in examples:\r\n            pil_image = example[\"PIL_images\"]\r\n            # generate a random mask\r\n            mask = random_mask(pil_image.size, 1, False)\r\n            # apply transforms\r\n            mask = image_transforms(mask)\r\n            pil_image = image_transforms(pil_image)\r\n            # prepare mask and masked image\r\n            mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\r\n\r\n            masks.append(mask)\r\n            masked_images.append(masked_image)\r\n\r\n        if args.with_prior_preservation:\r\n            for pil_image in pior_pil:\r\n                # generate a random mask\r\n                mask = random_mask(pil_image.size, 1, False)\r\n                # apply transforms\r\n                mask = image_transforms(mask)\r\n                pil_image = image_transforms(pil_image)\r\n                # prepare mask and masked image\r\n                mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\r\n\r\n                masks.append(mask)\r\n                masked_images.append(masked_image)\r\n\r\n        pixel_values = torch.stack(pixel_values)\r\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\r\n\r\n        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\r\n        masks = torch.stack(masks)\r\n        masked_images = torch.stack(masked_images)\r\n        batch = {\"input_ids\": input_ids, \"pixel_values\": pixel_values, \"masks\": masks, \"masked_images\": masked_images}\r\n        return batch\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n    )\r\n\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    weight_dtype = torch.float32\r\n    if args.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif args.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move text_encode and vae to gpu.\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n    global_step = 0\r\n\r\n    for epoch in range(args.num_train_epochs):\r\n        unet.train()\r\n        for step, batch in enumerate(train_dataloader):\r\n            with accelerator.accumulate(unet):\r\n                # Convert images to latent space\r\n\r\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                latents = latents * 0.18215\r\n\r\n                # Convert masked images to latent space\r\n                masked_latents = vae.encode(\r\n                    batch[\"masked_images\"].reshape(batch[\"pixel_values\"].shape).to(dtype=weight_dtype)\r\n                ).latent_dist.sample()\r\n                masked_latents = masked_latents * 0.18215\r\n\r\n                masks = batch[\"masks\"]\r\n                # resize the mask to latents shape as we concatenate the mask to the latents\r\n                mask = torch.stack(\r\n                    [\r\n                        torch.nn.functional.interpolate(mask, size=(args.resolution // 8, args.resolution // 8))\r\n                        for mask in masks\r\n                    ]\r\n                )\r\n                mask = mask.reshape(-1, 1, args.resolution // 8, args.resolution // 8)\r\n\r\n                # Sample noise that we'll add to the latents\r\n                noise = torch.randn_like(latents)\r\n                bsz = latents.shape[0]\r\n                # Sample a random timestep for each image\r\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\r\n                timesteps = timesteps.long()\r\n\r\n                # Add noise to the latents according to the noise magnitude at each timestep\r\n                # (this is the forward diffusion process)\r\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                # concatenate the noised latents with the mask and the masked latents\r\n                latent_model_input = torch.cat([noisy_latents, mask, masked_latents], dim=1)\r\n\r\n                # Get the text embedding for conditioning\r\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                # Predict the noise residual\r\n                noise_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\r\n\r\n                # Get the target for loss depending on the prediction type\r\n                if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                    target = noise\r\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                else:\r\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                if args.with_prior_preservation:\r\n                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\r\n                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\r\n                    target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                    # Compute instance loss\r\n                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\r\n\r\n                    # Compute prior loss\r\n                    prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                    # Add the prior loss to the instance loss.\r\n                    loss = loss + args.prior_loss_weight * prior_loss\r\n                else:\r\n                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                accelerator.backward(loss)\r\n                if accelerator.sync_gradients:\r\n                    params_to_clip = (\r\n                        itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                        if args.train_text_encoder\r\n                        else unet.parameters()\r\n                    )\r\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n\r\n            # Checks if the accelerator has performed an optimization step behind the scenes\r\n            if accelerator.sync_gradients:\r\n                progress_bar.update(1)\r\n                global_step += 1\r\n\r\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n            progress_bar.set_postfix(**logs)\r\n            accelerator.log(logs, step=global_step)\r\n\r\n            if global_step >= args.max_train_steps:\r\n                break\r\n\r\n        accelerator.wait_for_everyone()\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    if accelerator.is_main_process:\r\n        pipeline = StableDiffusionPipeline.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            unet=accelerator.unwrap_model(unet),\r\n            text_encoder=accelerator.unwrap_model(text_encoder),\r\n        )\r\n        pipeline.save_pretrained(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    # ==============================\r\n    # Parse Arguments\r\n    # ==============================\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-c\", \"--config\", type=str, default=\"100m\", help=\"Model configuration\")\r\n    parser.add_argument(\r\n        \"-p\",\r\n        \"--plugin\",\r\n        choices=[\"3d\"],\r\n        default=\"3d\",\r\n        help=\"Choose which plugin to use\",\r\n    )\r\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1, help=\"Batch size\")\r\n    parser.add_argument(\"-s\", \"--num_steps\", type=int, default=5, help=\"Number of steps to run\")\r\n    parser.add_argument(\"-i\", \"--ignore_steps\", type=int, default=2, help=\"Number of steps to ignore\")\r\n    parser.add_argument(\"-g\", \"--grad_checkpoint\", action=\"store_true\", help=\"Use gradient checkpointing\")\r\n    parser.add_argument(\"-l\", \"--max_length\", type=int, default=4096, help=\"Max sequence length\")\r\n    parser.add_argument(\r\n        \"-w\", \"--warmup_ratio\", type=float, default=0.8, help=\"warm up ratio of non-model data. Only for gemini-auto\"\r\n    )\r\n    parser.add_argument(\"-m\", \"--memory_limit\", type=int, help=\"Gemini memory limit in mb\")\r\n    parser.add_argument(\"-x\", \"--xformers\", action=\"store_true\", help=\"Use xformers\")\r\n    parser.add_argument(\"--shard_param_frac\", type=float, default=1.0, help=\"Shard param fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_optim_frac\", type=float, default=0.0, help=\"Offload optim fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_param_frac\", type=float, default=0.0, help=\"Offload param fraction. Only for gemini\")\r\n    parser.add_argument(\"--tp\", type=int, default=1, help=\"Tensor parallel size\")\r\n    parser.add_argument(\"--ep\", type=int, default=1, help=\"Expert parallel size\")\r\n    parser.add_argument(\"--sp\", type=int, default=1, help=\"Sequence parallel size\")\r\n    parser.add_argument(\"--extra_dp\", type=int, default=1, help=\"Extra data parallel size, used for Gemini\")\r\n    parser.add_argument(\"--pp\", type=int, default=1, help=\"Pipeline parallel size\")\r\n    parser.add_argument(\"--mbs\", type=int, default=1, help=\"Micro batch size of pipeline parallel\")\r\n    parser.add_argument(\"--zero\", type=int, default=1, help=\"Zero Stage when hybrid plugin is enabled\")\r\n    parser.add_argument(\"--custom-ckpt\", action=\"store_true\", help=\"Customize checkpoint\", default=False)\r\n\r\n    parser.add_argument(\"--pp_style\", default=\"1f1b\", choices=[\"1f1b\", \"interleaved\"])\r\n    parser.add_argument(\"--n_chunks\", default=1, help=\"number of model chunks\", type=eval)\r\n    parser.add_argument(\"--profile\", action=\"store_true\", help=\"Profile the code\")\r\n    parser.add_argument(\r\n        \"--nsys\",\r\n        action=\"store_true\",\r\n        help=\"Use nsys for profiling. \\\r\n        You should put something like this before colossalai launch: \\\r\n        nsys profile -w true -t cuda,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --capture-range-end=stop --cudabacktrace=true -x true --python-backtrace=cuda -o prof_out\",\r\n    )\r\n    parser.add_argument(\"--disable-async-reduce\", action=\"store_true\", help=\"Disable the asynchronous reduce operation\")\r\n    parser.add_argument(\"--prefetch_num\", type=int, default=0, help=\"chunk prefetch max number\")\r\n    parser.add_argument(\"--no_cache\", action=\"store_true\")\r\n    parser.add_argument(\"--use_fp8_comm\", action=\"store_true\", default=False, help=\"for using fp8 during communication\")\r\n    parser.add_argument(\"--use_fp8\", action=\"store_true\", default=False, help=\"for using fp8 linear\")\r\n    parser.add_argument(\"--overlap_allgather\", action=\"store_true\")\r\n    parser.add_argument(\r\n        \"--sp_mode\",\r\n        default=\"all_to_all\",\r\n        choices=[\"all_to_all\"],\r\n        help=\"Sequence parallelism mode\",\r\n    )\r\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\r\n    args = parser.parse_args()\r\n\r\n    colossalai.launch_from_torch()\r\n    coordinator = DistCoordinator()\r\n\r\n    # ckpt config for LLaMA3-70B on 64 H100 GPUs\r\n    hybrid_kwargs = (\r\n        {\r\n            \"gradient_checkpoint_config\": PipelineGradientCheckpointConfig(\r\n                num_ckpt_layers_per_stage=[19, 19, 19, 13],\r\n            ),\r\n            \"num_layers_per_stage\": [19, 20, 20, 21],\r\n            \"pp_style\": \"interleaved\",\r\n        }\r\n        if args.custom_ckpt\r\n        else {}\r\n    )\r\n\r\n    # ==============================\r\n    # Initialize Booster\r\n    # ==============================\r\n    if args.plugin == \"3d\":\r\n        plugin = MoeHybridParallelPlugin(\r\n            ep_size=args.ep,\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            pp_style=args.pp_style,\r\n            num_model_chunks=args.n_chunks,\r\n            zero_stage=args.zero,\r\n            sp_size=args.sp,\r\n            sequence_parallelism_mode=args.sp_mode,\r\n            enable_sequence_parallelism=args.sp > 1,\r\n            enable_fused_normalization=torch.cuda.is_available(),\r\n            enable_flash_attention=args.xformers,\r\n            microbatch_size=args.mbs,\r\n            precision=\"bf16\",\r\n            enable_metadata_cache=not args.no_cache,\r\n            overlap_allgather=args.overlap_allgather,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n            **hybrid_kwargs,\r\n        )\r\n    else:\r\n        raise ValueError(f\"Unknown plugin {args.plugin}\")\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    # ==============================\r\n    # Initialize Dataset and Dataloader\r\n    # ==============================\r\n    dp_size = getattr(plugin, \"dp_size\", coordinator.world_size)\r\n\r\n    config = MODEL_CONFIGS[args.config]()\r\n\r\n    torch.cuda.manual_seed(42)\r\n\r\n    dataset = RandomDataset(\r\n        num_samples=args.batch_size * args.num_steps * dp_size, max_length=args.max_length, vocab_size=config.vocab_size\r\n    )\r\n    dataloader = plugin.prepare_dataloader(dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, seed=42)\r\n\r\n    # ==============================\r\n    # Initialize Model and Optimizer\r\n    # ==============================\r\n    init_ctx = (\r\n        LazyInitContext(default_device=get_accelerator().get_current_device())\r\n        if isinstance(plugin, MoeHybridParallelPlugin)\r\n        else nullcontext()\r\n    )\r\n\r\n    with init_ctx:\r\n        model = AutoModelForCausalLM.from_config(config, trust_remote_code=True).to(torch.bfloat16)\r\n\r\n    if args.grad_checkpoint:\r\n        model.gradient_checkpointing_enable()\r\n\r\n    model_numel = get_model_numel(model)\r\n    coordinator.print_on_master(f\"Model params: {format_numel_str(model_numel)}\")\r\n    performance_evaluator = PerformanceEvaluator(\r\n        model_numel,\r\n        model.config.num_hidden_layers,\r\n        model.config.hidden_size,\r\n        model.config.vocab_size,\r\n        args.grad_checkpoint,\r\n        args.ignore_steps,\r\n        dp_world_size=dp_size,\r\n    )\r\n\r\n    optimizer = HybridAdam(model.parameters())\r\n    torch.set_default_dtype(torch.bfloat16)\r\n    model, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\r\n\r\n    torch.set_default_dtype(torch.float)\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CUDA memory: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\"\r\n    )\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024:.2f} MB\"\r\n    )\r\n\r\n    with get_profile_context(\r\n        args.profile,\r\n        args.ignore_steps,\r\n        1,  # avoid creating massive log files\r\n        save_dir=f\"profile/{time.strftime('%H:%M', time.localtime())}-{args.plugin}-llama-{args.config}\",\r\n        nsys=args.nsys,\r\n    ) as prof:  # , distributed_debug_mode(10, enable=True):\r\n        if isinstance(plugin, MoeHybridParallelPlugin) and args.pp > 1:\r\n            data_iter = iter(dataloader)\r\n            for step in tqdm(range(len(dataloader)), desc=\"Step\", disable=not coordinator.is_master()):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = booster.execute_pipeline(\r\n                    data_iter,\r\n                    model,\r\n                    criterion=lambda outputs, inputs: outputs[0],\r\n                    optimizer=optimizer,\r\n                    return_loss=True,\r\n                )\r\n                loss = outputs[\"loss\"]\r\n                if dist.get_rank() == dist.get_world_size() - 1:\r\n                    print(f\"Step {step} loss: {loss}\")\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(input_ids=torch.empty(args.batch_size, args.max_length))\r\n                prof.step()\r\n                print(f\"rank {dist.get_rank()} step {step} passed\")\r\n        else:\r\n            for step, batch in enumerate(tqdm(dataloader, desc=\"Step\", disable=not coordinator.is_master())):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = model(**batch)\r\n                loss = outputs[0]\r\n                del outputs  # free memory\r\n\r\n                if dist.get_rank() == dist.get_world_size() - 1:\r\n                    print(f\"Step {step} loss: {loss}\")\r\n\r\n                booster.backward(loss, optimizer)\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(**batch)\r\n                prof.step()\r\n\r\n    performance_evaluator.on_fit_end()\r\n    coordinator.print_on_master(f\"Max CUDA memory usage: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    # ==============================\r\n    # Parse Arguments\r\n    # ==============================\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-c\", \"--config\", type=str, default=\"6.21B\", help=\"Model configuration\")\r\n    parser.add_argument(\r\n        \"-p\",\r\n        \"--plugin\",\r\n        choices=[\"gemini\", \"gemini_auto\", \"fsdp\", \"fsdp_cpu\", \"3d\", \"3d_cpu\"],\r\n        default=\"gemini\",\r\n        help=\"Choose which plugin to use\",\r\n    )\r\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=2, help=\"Batch size\")\r\n    parser.add_argument(\"-s\", \"--num_steps\", type=int, default=200, help=\"Number of steps to run\")\r\n    parser.add_argument(\"-i\", \"--ignore_steps\", type=int, default=3, help=\"Number of steps to ignore\")\r\n    parser.add_argument(\"-g\", \"--grad_checkpoint\", action=\"store_true\", help=\"Use gradient checkpointing\")\r\n    parser.add_argument(\"-l\", \"--max_length\", type=int, default=4096, help=\"Max sequence length\")\r\n    parser.add_argument(\r\n        \"-w\", \"--warmup_ratio\", type=float, default=0.8, help=\"warm up ratio of non-model data. Only for gemini-auto\"\r\n    )\r\n    parser.add_argument(\"-m\", \"--memory_limit\", type=int, help=\"Gemini memory limit in mb\")\r\n    parser.add_argument(\"--shard_param_frac\", type=float, default=1.0, help=\"Shard param fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_optim_frac\", type=float, default=0.0, help=\"Offload optim fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_param_frac\", type=float, default=0.0, help=\"Offload param fraction. Only for gemini\")\r\n    parser.add_argument(\"--tp\", type=int, default=1, help=\"Tensor parallel size\")\r\n    parser.add_argument(\"--extra_dp\", type=int, default=1, help=\"Extra data parallel size, used for Gemini\")\r\n    parser.add_argument(\"--pp\", type=int, default=1, help=\"Pipeline parallel size\")\r\n    parser.add_argument(\"--sp\", type=int, default=1, help=\"Sequence parallel size\")\r\n    parser.add_argument(\"--sp_mode\", type=str, default=\"ring_attn\", help=\"Sequence parallel mode\")\r\n    parser.add_argument(\"--mbs\", type=int, default=1)\r\n    parser.add_argument(\"--zero\", type=int, default=0)\r\n    parser.add_argument(\"--pp_style\", type=str, default=\"1f1b\")\r\n    parser.add_argument(\"--num_model_chunks\", type=int, default=2)\r\n    parser.add_argument(\"--cpu_offload\", action=\"store_true\", help=\"Use gradient checkpointing\")\r\n    args = parser.parse_args()\r\n\r\n    colossalai.launch_from_torch()\r\n    coordinator = DistCoordinator()\r\n\r\n    def empty_init():\r\n        pass\r\n\r\n    # ==============================\r\n    # Initialize Booster\r\n    # ==============================\r\n    use_empty_init = True\r\n    if args.plugin == \"gemini\":\r\n        plugin = GeminiPlugin(\r\n            precision=\"bf16\",\r\n            shard_param_frac=args.shard_param_frac,\r\n            offload_optim_frac=args.offload_optim_frac,\r\n            offload_param_frac=args.offload_param_frac,\r\n            tp_size=args.tp,\r\n            extra_dp_size=args.extra_dp,\r\n        )\r\n    elif args.plugin == \"gemini_auto\":\r\n        plugin = GeminiPlugin(\r\n            placement_policy=\"auto\",\r\n            precision=\"bf16\",\r\n            warmup_non_model_data_ratio=args.warmup_ratio,\r\n            tp_size=args.tp,\r\n            extra_dp_size=args.extra_dp,\r\n        )\r\n    elif args.plugin == \"fsdp\":\r\n        if use_empty_init:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16, reduce_dtype=torch.float16, buffer_dtype=torch.float16\r\n                ),\r\n                param_init_fn=empty_init(),\r\n            )\r\n        else:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16, reduce_dtype=torch.float16, buffer_dtype=torch.float16\r\n                )\r\n            )\r\n    elif args.plugin == \"fsdp_cpu\":\r\n        if use_empty_init:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16, reduce_dtype=torch.float16, buffer_dtype=torch.float16\r\n                ),\r\n                cpu_offload=CPUOffload(offload_params=True),\r\n                param_init_fn=empty_init(),\r\n            )\r\n        else:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16, reduce_dtype=torch.float16, buffer_dtype=torch.float16\r\n                ),\r\n                cpu_offload=CPUOffload(offload_params=True),\r\n            )\r\n    elif args.plugin == \"3d\":\r\n        plugin = HybridParallelPlugin(\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            pp_style=args.pp_style,\r\n            sp_size=args.sp,\r\n            sequence_parallelism_mode=args.sp_mode,\r\n            enable_sequence_parallelism=True,\r\n            zero_stage=args.zero,\r\n            num_model_chunks=args.num_model_chunks,\r\n            enable_all_optimization=True,\r\n            num_microbatches=args.mbs,\r\n            cpu_offload=args.cpu_offload,\r\n            precision=\"bf16\",\r\n        )\r\n    elif args.plugin == \"3d_cpu\":\r\n        plugin = HybridParallelPlugin(\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            zero_stage=args.zero,\r\n            cpu_offload=True,\r\n            enable_fused_normalization=torch.cuda.is_available(),\r\n            num_microbatches=args.mbs,\r\n            initial_scale=2**8,\r\n            precision=\"bf16\",\r\n        )\r\n    else:\r\n        raise ValueError(f\"Unknown plugin {args.plugin}\")\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    # ==============================\r\n    # Initialize Dataset and Dataloader\r\n    # ==============================\r\n    dp_size = plugin.dp_size if isinstance(plugin, HybridParallelPlugin) else coordinator.world_size\r\n\r\n    config = MODEL_CONFIGS[args.config]\r\n    dataset = RandomDataset(\r\n        num_samples=args.batch_size * args.num_steps * dp_size, max_length=args.max_length, vocab_size=config.vocab_size\r\n    )\r\n    dataloader = plugin.prepare_dataloader(dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\r\n\r\n    # ==============================\r\n    # Initialize Model and Optimizer\r\n    # ==============================\r\n    init_ctx = (\r\n        LazyInitContext(default_device=get_current_device())\r\n        if isinstance(plugin, (GeminiPlugin, HybridParallelPlugin))\r\n        else nullcontext()\r\n    )\r\n\r\n    with init_ctx:\r\n        model = GPT2LMHeadModel(config)\r\n\r\n    if args.grad_checkpoint:\r\n        model.gradient_checkpointing_enable()\r\n\r\n    model_numel = get_model_numel(model)\r\n    coordinator.print_on_master(f\"Model params: {format_numel_str(model_numel)}\")\r\n    performance_evaluator = PerformanceEvaluator(\r\n        model_numel,\r\n        model.config.n_layer,\r\n        model.config.n_embd,\r\n        model.config.vocab_size,\r\n        args.grad_checkpoint,\r\n        args.ignore_steps,\r\n        dp_world_size=dp_size,\r\n    )\r\n\r\n    optimizer = Adam(model.parameters())\r\n    torch.set_default_dtype(torch.bfloat16)\r\n    model, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\r\n    torch.set_default_dtype(torch.float)\r\n    coordinator.print_on_master(f\"Booster init max CUDA memory: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024:.2f} MB\"\r\n    )\r\n\r\n    if isinstance(plugin, HybridParallelPlugin) and args.pp > 1:\r\n        data_iter = iter(dataloader)\r\n        for step in tqdm(range(len(dataloader)), desc=\"Step\", disable=not coordinator.is_master()):\r\n            performance_evaluator.on_step_start(step)\r\n            booster.execute_pipeline(\r\n                data_iter, model, criterion=lambda outputs, inputs: outputs[0], optimizer=optimizer, return_loss=False\r\n            )\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n            performance_evaluator.on_step_end(input_ids=torch.empty(args.batch_size, args.max_length))\r\n    else:\r\n        for step, batch in enumerate(tqdm(dataloader, desc=\"Step\", disable=not coordinator.is_master())):\r\n            performance_evaluator.on_step_start(step)\r\n            outputs = model(**batch)\r\n            loss = outputs[0]\r\n            del outputs\r\n\r\n            booster.backward(loss, optimizer)\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n            performance_evaluator.on_step_end(**batch)\r\n        coordinator.print_on_master(f\"Max CUDA memory usage: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\r\n\r\n    performance_evaluator.on_fit_end()\r\n    coordinator.print_on_master(f\"Max CUDA memory usage: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    # ==============================\r\n    # Parse Arguments\r\n    # ==============================\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-c\", \"--config\", type=str, default=\"7b\", help=\"Model configuration\")\r\n    parser.add_argument(\r\n        \"-p\",\r\n        \"--plugin\",\r\n        choices=[\"gemini\", \"gemini_auto\", \"fsdp\", \"fsdp_cpu\", \"3d\", \"3d_cpu\"],\r\n        default=\"gemini\",\r\n        help=\"Choose which plugin to use\",\r\n    )\r\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=2, help=\"Batch size\")\r\n    parser.add_argument(\"-s\", \"--num_steps\", type=int, default=5, help=\"Number of steps to run\")\r\n    parser.add_argument(\"-i\", \"--ignore_steps\", type=int, default=2, help=\"Number of steps to ignore\")\r\n    parser.add_argument(\"-g\", \"--grad_checkpoint\", action=\"store_true\", help=\"Use gradient checkpointing\")\r\n    parser.add_argument(\"-l\", \"--max_length\", type=int, default=4096, help=\"Max sequence length\")\r\n    parser.add_argument(\r\n        \"-w\", \"--warmup_ratio\", type=float, default=0.8, help=\"warm up ratio of non-model data. Only for gemini-auto\"\r\n    )\r\n    parser.add_argument(\"-m\", \"--memory_limit\", type=int, help=\"Gemini memory limit in mb\")\r\n    parser.add_argument(\"-x\", \"--xformers\", action=\"store_true\", help=\"Use xformers\")\r\n    parser.add_argument(\"--shard_param_frac\", type=float, default=1.0, help=\"Shard param fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_optim_frac\", type=float, default=0.0, help=\"Offload optim fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_param_frac\", type=float, default=0.0, help=\"Offload param fraction. Only for gemini\")\r\n    parser.add_argument(\"--tp\", type=int, default=1, help=\"Tensor parallel size\")\r\n    parser.add_argument(\"--sp\", type=int, default=1, help=\"Sequence parallel size\")\r\n    parser.add_argument(\"--extra_dp\", type=int, default=1, help=\"Extra data parallel size, used for Gemini\")\r\n    parser.add_argument(\"--pp\", type=int, default=1, help=\"Pipeline parallel size\")\r\n    parser.add_argument(\"--mbs\", type=int, default=1, help=\"Micro batch size of pipeline parallel\")\r\n    parser.add_argument(\"--zero\", type=int, default=0, help=\"Zero Stage when hybrid plugin is enabled\")\r\n    parser.add_argument(\"--custom-ckpt\", action=\"store_true\", help=\"Customize checkpoint\", default=False)\r\n\r\n    parser.add_argument(\"--pp_style\", default=\"1f1b\", choices=[\"1f1b\", \"interleaved\", \"zbv\"])\r\n    parser.add_argument(\"--n_chunks\", default=1, help=\"number of model chunks\", type=eval)\r\n    parser.add_argument(\"--profile\", action=\"store_true\", help=\"Profile the code\")\r\n    parser.add_argument(\r\n        \"--nsys\",\r\n        action=\"store_true\",\r\n        help=\"Use nsys for profiling. \\\r\n        You should put something like this before colossalai launch: \\\r\n        nsys profile -w true -t cuda,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --capture-range-end=stop --cudabacktrace=true -x true --python-backtrace=cuda -o prof_out\",\r\n    )\r\n    parser.add_argument(\"--disable-async-reduce\", action=\"store_true\", help=\"Disable the asynchronous reduce operation\")\r\n    parser.add_argument(\"--prefetch_num\", type=int, default=0, help=\"chunk prefetch max number\")\r\n    parser.add_argument(\"--no_cache\", action=\"store_true\")\r\n    parser.add_argument(\"--use_fp8_comm\", action=\"store_true\", default=False, help=\"for using fp8 during communication\")\r\n    parser.add_argument(\"--use_fp8\", action=\"store_true\", default=False, help=\"for using fp8 linear\")\r\n    parser.add_argument(\"--overlap_p2p\", action=\"store_true\", default=True, help=\"for using overlap p2p\")\r\n    parser.add_argument(\"--overlap_allgather\", action=\"store_true\")\r\n    parser.add_argument(\r\n        \"--sp_mode\",\r\n        default=\"all_to_all\",\r\n        choices=[\"all_to_all\", \"ring_attn\", \"ring\", \"split_gather\"],\r\n        help=\"Sequence parallelism mode\",\r\n    )\r\n    args = parser.parse_args()\r\n\r\n    colossalai.launch_from_torch()\r\n    coordinator = DistCoordinator()\r\n\r\n    def empty_init():\r\n        pass\r\n\r\n    # ckpt config for LLaMA3-70B on 64 H100 GPUs\r\n    hybrid_kwargs = (\r\n        {\r\n            \"gradient_checkpoint_config\": PipelineGradientCheckpointConfig(\r\n                num_ckpt_layers_per_stage=[19, 19, 19, 13],\r\n            ),\r\n            \"num_layers_per_stage\": [19, 20, 20, 21],\r\n            \"pp_style\": \"interleaved\",\r\n        }\r\n        if args.custom_ckpt\r\n        else {}\r\n    )\r\n\r\n    # ==============================\r\n    # Initialize Booster\r\n    # ==============================\r\n    if args.config in MODEL_CONFIGS:\r\n        config = MODEL_CONFIGS[args.config]\r\n    else:\r\n        config = AutoConfig.from_pretrained(args.config, trust_remote_code=True)\r\n\r\n    use_empty_init = True\r\n    if args.plugin == \"gemini\":\r\n        plugin = GeminiPlugin(\r\n            precision=\"bf16\",\r\n            shard_param_frac=args.shard_param_frac,\r\n            offload_optim_frac=args.offload_optim_frac,\r\n            offload_param_frac=args.offload_param_frac,\r\n            tp_size=args.tp,\r\n            extra_dp_size=args.extra_dp,\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_flash_attention=args.xformers,\r\n            max_prefetch=args.prefetch_num,\r\n            enable_async_reduce=not args.disable_async_reduce,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n        )\r\n    elif args.plugin == \"gemini_auto\":\r\n        plugin = GeminiPlugin(\r\n            placement_policy=\"auto\",\r\n            precision=\"bf16\",\r\n            warmup_non_model_data_ratio=args.warmup_ratio,\r\n            tp_size=args.tp,\r\n            extra_dp_size=args.extra_dp,\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            max_prefetch=args.prefetch_num,\r\n            enable_async_reduce=not args.disable_async_reduce,\r\n            enable_flash_attention=args.xformers,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n        )\r\n    elif args.plugin == \"fsdp\":\r\n        if use_empty_init:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16,\r\n                    reduce_dtype=torch.float16,\r\n                    buffer_dtype=torch.float16,\r\n                ),\r\n                param_init_fn=empty_init(),\r\n                fp8_communication=args.use_fp8_comm,\r\n            )\r\n        else:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16,\r\n                    reduce_dtype=torch.float16,\r\n                    buffer_dtype=torch.float16,\r\n                ),\r\n                fp8_communication=args.use_fp8_comm,\r\n            )\r\n    elif args.plugin == \"fsdp_cpu\":\r\n        if use_empty_init:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16,\r\n                    reduce_dtype=torch.float16,\r\n                    buffer_dtype=torch.float16,\r\n                ),\r\n                cpu_offload=CPUOffload(offload_params=True),\r\n                param_init_fn=empty_init(),\r\n                fp8_communication=args.use_fp8_comm,\r\n            )\r\n        else:\r\n            plugin = TorchFSDPPlugin(\r\n                mixed_precision=MixedPrecision(\r\n                    param_dtype=torch.float16,\r\n                    reduce_dtype=torch.float16,\r\n                    buffer_dtype=torch.float16,\r\n                ),\r\n                cpu_offload=CPUOffload(offload_params=True),\r\n                fp8_communication=args.use_fp8_comm,\r\n            )\r\n    elif args.plugin == \"3d\":\r\n        if args.pp_style == \"zbv\":\r\n            mem_f = 34 * config.hidden_size + 5 * config.num_attention_heads * args.max_length\r\n            mem_w = -32 * config.hidden_size\r\n            mem_b = -mem_w - mem_f\r\n            scheduler_nodes = PipelineGraph(\r\n                n_stage=args.pp,\r\n                n_micro=args.batch_size // args.mbs,\r\n                f_cost=1000,\r\n                b_cost=1000,\r\n                w_cost=1000,\r\n                c_cost=1,\r\n                f_mem=mem_f * 1.5,\r\n                b_mem=mem_b * 1.5,\r\n                w_mem=mem_w * 1.5,\r\n            ).get_v_schedule()\r\n        else:\r\n            scheduler_nodes = None\r\n\r\n        plugin = HybridParallelPlugin(\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            pp_style=args.pp_style,\r\n            num_model_chunks=args.n_chunks,\r\n            zero_stage=args.zero,\r\n            sp_size=args.sp,\r\n            sequence_parallelism_mode=args.sp_mode,\r\n            enable_sequence_parallelism=args.sp > 1,\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_flash_attention=args.xformers,\r\n            microbatch_size=args.mbs,\r\n            precision=\"bf16\",\r\n            enable_metadata_cache=not args.no_cache,\r\n            overlap_allgather=args.overlap_allgather,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n            scheduler_nodes=scheduler_nodes,\r\n            **hybrid_kwargs,\r\n        )\r\n    elif args.plugin == \"3d_cpu\":\r\n        plugin = HybridParallelPlugin(\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            pp_style=args.pp_style,\r\n            num_model_chunks=args.n_chunks,\r\n            zero_stage=args.zero,\r\n            cpu_offload=True,\r\n            enable_fused_normalization=get_accelerator().is_available(),\r\n            enable_flash_attention=args.xformers,\r\n            microbatch_size=args.mbs,\r\n            initial_scale=2**8,\r\n            precision=\"bf16\",\r\n            overlap_p2p=args.overlap_p2p,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n        )\r\n    else:\r\n        raise ValueError(f\"Unknown plugin {args.plugin}\")\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    # ==============================\r\n    # Initialize Dataset and Dataloader\r\n    # ==============================\r\n    dp_size = getattr(plugin, \"dp_size\", coordinator.world_size)\r\n\r\n    if args.config in MODEL_CONFIGS:\r\n        config = MODEL_CONFIGS[args.config]\r\n    else:\r\n        config = AutoConfig.from_pretrained(args.config, trust_remote_code=True)\r\n    get_accelerator().manual_seed(42)\r\n\r\n    dataset = RandomDataset(\r\n        num_samples=args.batch_size * args.num_steps * dp_size, max_length=args.max_length, vocab_size=config.vocab_size\r\n    )\r\n    dataloader = plugin.prepare_dataloader(dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, seed=42)\r\n\r\n    # ==============================\r\n    # Initialize Model and Optimizer\r\n    # ==============================\r\n    init_ctx = (\r\n        LazyInitContext(default_device=get_accelerator().get_current_device())\r\n        if isinstance(plugin, (GeminiPlugin, HybridParallelPlugin))\r\n        else nullcontext()\r\n    )\r\n    init_kwargs = {}\r\n    if config.model_type == \"chatglm\":\r\n        init_kwargs[\"empty_init\"] = False\r\n\r\n    with init_ctx:\r\n        model = AutoModelForCausalLM.from_config(\r\n            config,\r\n            trust_remote_code=True,\r\n            **init_kwargs,\r\n            torch_dtype=torch.bfloat16,\r\n        )\r\n    if args.grad_checkpoint:\r\n        model.gradient_checkpointing_enable()\r\n        if config.model_type == \"chatglm\":\r\n            model.transformer.encoder.gradient_checkpointing = True\r\n\r\n    model_numel = get_model_numel(model)\r\n    coordinator.print_on_master(f\"Model params: {format_numel_str(model_numel)}\")\r\n    if config.model_type == \"chatglm\":\r\n        num_layers = model.config.num_layers\r\n    else:\r\n        num_layers = model.config.num_hidden_layers\r\n    performance_evaluator = PerformanceEvaluator(\r\n        model_numel,\r\n        num_layers,\r\n        model.config.hidden_size,\r\n        model.config.vocab_size,\r\n        args.grad_checkpoint,\r\n        args.ignore_steps,\r\n        dp_world_size=dp_size,\r\n    )\r\n\r\n    optimizer = HybridAdam(model.parameters())\r\n    torch.set_default_dtype(torch.bfloat16)\r\n    model, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\r\n\r\n    torch.set_default_dtype(torch.float)\r\n    coordinator.print_on_master(\r\n        f\"Booster init max device memory: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\"\r\n    )\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024:.2f} MB\"\r\n    )\r\n\r\n    with get_profile_context(\r\n        args.profile,\r\n        args.ignore_steps,\r\n        1,  # avoid creating massive log files\r\n        save_dir=f\"./profile/{time.strftime('%H:%M', time.localtime())}-{args.plugin}-llama-{args.config}\",\r\n        nsys=args.nsys,\r\n    ) as prof:\r\n        if isinstance(plugin, HybridParallelPlugin) and args.pp > 1:\r\n            data_iter = iter(dataloader)\r\n            for step in tqdm(range(len(dataloader)), desc=\"Step\", disable=not coordinator.is_master()):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = booster.execute_pipeline(\r\n                    data_iter,\r\n                    model,\r\n                    criterion=lambda outputs, inputs: outputs[0],\r\n                    optimizer=optimizer,\r\n                    return_loss=True,\r\n                )\r\n                loss = outputs[\"loss\"]\r\n                if args.pp_style == \"zbv\":\r\n                    if coordinator.is_master():\r\n                        print(f\"Step {step} loss: {loss}\")\r\n                else:\r\n                    if coordinator.is_last_process():\r\n                        print(f\"Step {step} loss: {loss}\")\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(input_ids=torch.empty(args.batch_size, args.max_length))\r\n                prof.step()\r\n        else:\r\n            for step, batch in enumerate(tqdm(dataloader, desc=\"Step\", disable=not coordinator.is_master())):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = model(**batch)\r\n                loss = outputs[0]\r\n                del outputs  # free memory\r\n\r\n                if dist.get_rank() == dist.get_world_size() - 1:\r\n                    print(f\"Step {step} loss: {loss}\")\r\n                booster.backward(loss, optimizer)\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(**batch)\r\n                prof.step()\r\n    performance_evaluator.on_fit_end()\r\n    coordinator.print_on_master(f\"Max device memory usage: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def bench(\r\n    gm: torch.fx.GraphModule, criterion: torch.nn.Module, data_gen: Callable, num_steps: int = 5\r\n) -> Tuple[int, int]:\r\n    \"\"\"Benchmarking a given graph module\r\n    Args:\r\n        gm (torch.fx.GraphModule): The graph module to benchmark.\r\n        criterion (torch.nn.Module): Loss function.\r\n        data_gen (Callable): Data generator.\r\n        num_steps (int, optional): Number of test steps. Defaults to 5.\r\n    Returns:\r\n        Tuple[int, int]: peak memory in MB and step time in MS.\r\n    \"\"\"\r\n    gm.train()\r\n    gm.cuda()\r\n    step_time = float(\"inf\")\r\n    torch.cuda.synchronize()\r\n    torch.cuda.empty_cache()\r\n    torch.cuda.reset_peak_memory_stats()\r\n    cached = torch.cuda.max_memory_allocated(device=\"cuda\")\r\n    try:\r\n        for _ in range(num_steps):\r\n            args, label = data_gen()\r\n            output, loss = None, None\r\n\r\n            torch.cuda.synchronize(device=\"cuda\")\r\n            start = time.time()\r\n            output = gm(*args)\r\n            loss = criterion(output, label)\r\n            loss.backward()\r\n            torch.cuda.synchronize(device=\"cuda\")\r\n            step_time = min(step_time, time.time() - start)\r\n\r\n            for child in gm.children():\r\n                for param in child.parameters():\r\n                    param.grad = None\r\n            del args, label, output, loss\r\n    except:\r\n        del args, label, output, loss\r\n    gm.to(\"cpu\")\r\n    torch.cuda.empty_cache()\r\n    peak_mem = (torch.cuda.max_memory_allocated(device=\"cuda\") - cached) / 1024**2\r\n    return peak_mem, step_time * 1.0e3",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    # ==============================\r\n    # Parse Arguments\r\n    # ==============================\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-c\", \"--config\", type=str, default=\"100m\", help=\"Model configuration\")\r\n    parser.add_argument(\r\n        \"-p\",\r\n        \"--plugin\",\r\n        choices=[\"3d\"],\r\n        default=\"3d\",\r\n        help=\"Choose which plugin to use\",\r\n    )\r\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1, help=\"Batch size\")\r\n    parser.add_argument(\"-s\", \"--num_steps\", type=int, default=5, help=\"Number of steps to run\")\r\n    parser.add_argument(\"-i\", \"--ignore_steps\", type=int, default=2, help=\"Number of steps to ignore\")\r\n    parser.add_argument(\"-g\", \"--grad_checkpoint\", action=\"store_true\", help=\"Use gradient checkpointing\")\r\n    parser.add_argument(\"-l\", \"--max_length\", type=int, default=4096, help=\"Max sequence length\")\r\n    parser.add_argument(\r\n        \"-w\", \"--warmup_ratio\", type=float, default=0.8, help=\"warm up ratio of non-model data. Only for gemini-auto\"\r\n    )\r\n    parser.add_argument(\"-m\", \"--memory_limit\", type=int, help=\"Gemini memory limit in mb\")\r\n    parser.add_argument(\"-x\", \"--xformers\", action=\"store_true\", help=\"Use xformers\")\r\n    parser.add_argument(\"--shard_param_frac\", type=float, default=1.0, help=\"Shard param fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_optim_frac\", type=float, default=0.0, help=\"Offload optim fraction. Only for gemini\")\r\n    parser.add_argument(\"--offload_param_frac\", type=float, default=0.0, help=\"Offload param fraction. Only for gemini\")\r\n    parser.add_argument(\"--tp\", type=int, default=1, help=\"Tensor parallel size\")\r\n    parser.add_argument(\"--ep\", type=int, default=1, help=\"Expert parallel size\")\r\n    parser.add_argument(\"--sp\", type=int, default=1, help=\"Sequence parallel size\")\r\n    parser.add_argument(\"--extra_dp\", type=int, default=1, help=\"Extra data parallel size, used for Gemini\")\r\n    parser.add_argument(\"--pp\", type=int, default=1, help=\"Pipeline parallel size\")\r\n    parser.add_argument(\"--mbs\", type=int, default=1, help=\"Micro batch size of pipeline parallel\")\r\n    parser.add_argument(\"--zero\", type=int, default=1, help=\"Zero Stage when hybrid plugin is enabled\")\r\n    parser.add_argument(\"--custom-ckpt\", action=\"store_true\", help=\"Customize checkpoint\", default=False)\r\n\r\n    parser.add_argument(\"--pp_style\", default=\"1f1b\", choices=[\"1f1b\", \"interleaved\", \"zbv\"])\r\n    parser.add_argument(\"--n_chunks\", default=1, help=\"number of model chunks\", type=eval)\r\n    parser.add_argument(\"--profile\", action=\"store_true\", help=\"Profile the code\")\r\n    parser.add_argument(\r\n        \"--nsys\",\r\n        action=\"store_true\",\r\n        help=\"Use nsys for profiling. \\\r\n        You should put something like this before colossalai launch: \\\r\n        nsys profile -w true -t cuda,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --capture-range-end=stop --cudabacktrace=true -x true --python-backtrace=cuda -o prof_out\",\r\n    )\r\n    parser.add_argument(\"--disable-async-reduce\", action=\"store_true\", help=\"Disable the asynchronous reduce operation\")\r\n    parser.add_argument(\"--prefetch_num\", type=int, default=0, help=\"chunk prefetch max number\")\r\n    parser.add_argument(\"--no_cache\", action=\"store_true\")\r\n    parser.add_argument(\"--use_fp8_comm\", action=\"store_true\", default=False, help=\"for using fp8 during communication\")\r\n    parser.add_argument(\"--use_fp8\", action=\"store_true\", default=False, help=\"for using fp8 linear\")\r\n    parser.add_argument(\"--overlap_allgather\", action=\"store_true\")\r\n    parser.add_argument(\r\n        \"--sp_mode\",\r\n        default=\"all_to_all\",\r\n        choices=[\"all_to_all\"],\r\n        help=\"Sequence parallelism mode\",\r\n    )\r\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\r\n    args = parser.parse_args()\r\n\r\n    colossalai.launch_from_torch()\r\n    coordinator = DistCoordinator()\r\n\r\n    # ckpt config for LLaMA3-70B on 64 H100 GPUs\r\n    hybrid_kwargs = (\r\n        {\r\n            \"gradient_checkpoint_config\": PipelineGradientCheckpointConfig(\r\n                num_ckpt_layers_per_stage=[19, 19, 19, 13],\r\n            ),\r\n            \"num_layers_per_stage\": [19, 20, 20, 21],\r\n            \"pp_style\": \"interleaved\",\r\n        }\r\n        if args.custom_ckpt\r\n        else {}\r\n    )\r\n\r\n    # ==============================\r\n    # Initialize Booster\r\n    # ==============================\r\n    if args.config in MODEL_CONFIGS:\r\n        config = MODEL_CONFIGS[args.config]\r\n    else:\r\n        config = AutoConfig.from_pretrained(args.config, trust_remote_code=True)\r\n\r\n    if args.plugin == \"3d\":\r\n        if args.pp_style == \"zbv\":\r\n            mem_f = 34 * config.hidden_size + 5 * config.num_attention_heads * args.max_length\r\n            mem_w = -32 * config.hidden_size\r\n            mem_b = -mem_w - mem_f\r\n            scheduler_nodes = PipelineGraph(\r\n                n_stage=args.pp,\r\n                n_micro=args.batch_size // args.mbs,\r\n                f_cost=1000,\r\n                b_cost=1000,\r\n                w_cost=1000,\r\n                c_cost=1,\r\n                f_mem=mem_f,\r\n                b_mem=mem_b,\r\n                w_mem=mem_w,\r\n            ).get_v_schedule()\r\n        else:\r\n            scheduler_nodes = None\r\n        plugin = MoeHybridParallelPlugin(\r\n            ep_size=args.ep,\r\n            tp_size=args.tp,\r\n            pp_size=args.pp,\r\n            pp_style=args.pp_style,\r\n            num_model_chunks=args.n_chunks,\r\n            zero_stage=args.zero,\r\n            sp_size=args.sp,\r\n            sequence_parallelism_mode=args.sp_mode,\r\n            enable_sequence_parallelism=args.sp > 1,\r\n            enable_fused_normalization=torch.cuda.is_available(),\r\n            enable_flash_attention=args.xformers,\r\n            microbatch_size=args.mbs,\r\n            num_microbatches=args.batch_size // args.mbs,\r\n            precision=\"bf16\",\r\n            enable_metadata_cache=not args.no_cache,\r\n            overlap_allgather=args.overlap_allgather,\r\n            use_fp8=args.use_fp8,\r\n            fp8_communication=args.use_fp8_comm,\r\n            scheduler_nodes=scheduler_nodes,\r\n            **hybrid_kwargs,\r\n        )\r\n    else:\r\n        raise ValueError(f\"Unknown plugin {args.plugin}\")\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    # ==============================\r\n    # Initialize Dataset and Dataloader\r\n    # ==============================\r\n    dp_size = getattr(plugin, \"dp_size\", coordinator.world_size)\r\n\r\n    if args.config in MODEL_CONFIGS:\r\n        config = MODEL_CONFIGS[args.config]\r\n    else:\r\n        config = MixtralConfig.from_pretrained(args.config, trust_remote_code=True)\r\n    torch.cuda.manual_seed(42)\r\n\r\n    dataset = RandomDataset(\r\n        num_samples=args.batch_size * args.num_steps * dp_size, max_length=args.max_length, vocab_size=config.vocab_size\r\n    )\r\n    dataloader = plugin.prepare_dataloader(dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, seed=42)\r\n\r\n    # ==============================\r\n    # Initialize Model and Optimizer\r\n    # ==============================\r\n    init_ctx = (\r\n        LazyInitContext(default_device=get_accelerator().get_current_device())\r\n        if isinstance(plugin, MoeHybridParallelPlugin)\r\n        else nullcontext()\r\n    )\r\n\r\n    with init_ctx:\r\n        model = MixtralForCausalLM(config=config).to(torch.bfloat16)\r\n\r\n    # if args.grad_checkpoint:\r\n    #     model.gradient_checkpointing_enable()\r\n    if args.grad_checkpoint:\r\n        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\r\n\r\n    model_numel = get_model_numel(model)\r\n    coordinator.print_on_master(f\"Model params: {format_numel_str(model_numel)}\")\r\n    performance_evaluator = PerformanceEvaluator(\r\n        model_numel,\r\n        model.config.num_hidden_layers,\r\n        model.config.hidden_size,\r\n        model.config.vocab_size,\r\n        args.grad_checkpoint,\r\n        args.ignore_steps,\r\n        dp_world_size=dp_size,\r\n    )\r\n\r\n    optimizer = HybridAdam(model.parameters())\r\n    torch.set_default_dtype(torch.bfloat16)\r\n    model, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\r\n\r\n    torch.set_default_dtype(torch.float)\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CUDA memory: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\"\r\n    )\r\n    coordinator.print_on_master(\r\n        f\"Booster init max CPU memory: {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024:.2f} MB\"\r\n    )\r\n\r\n    with get_profile_context(\r\n        args.profile,\r\n        args.ignore_steps,\r\n        1,  # avoid creating massive log files\r\n        save_dir=f\"profile/{time.strftime('%H:%M', time.localtime())}-{args.plugin}-llama-{args.config}\",\r\n        nsys=args.nsys,\r\n    ) as prof:\r\n        if isinstance(plugin, MoeHybridParallelPlugin) and args.pp > 1:\r\n            data_iter = iter(dataloader)\r\n            for step in tqdm(range(len(dataloader)), desc=\"Step\", disable=not coordinator.is_master()):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = booster.execute_pipeline(\r\n                    data_iter,\r\n                    model,\r\n                    criterion=lambda outputs, inputs: outputs[0],\r\n                    optimizer=optimizer,\r\n                    return_loss=True,\r\n                )\r\n                loss = outputs[\"loss\"]\r\n                if args.pp_style == \"zbv\":\r\n                    if dist.get_rank() == 0:\r\n                        print(f\"Step {step} loss: {loss}\")\r\n                else:\r\n                    if dist.get_rank() == dist.get_world_size() - 1:\r\n                        print(f\"Step {step} loss: {loss}\")\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(input_ids=torch.empty(args.batch_size, args.max_length))\r\n                prof.step()\r\n        else:\r\n            for step, batch in enumerate(tqdm(dataloader, desc=\"Step\", disable=not coordinator.is_master())):\r\n                performance_evaluator.on_step_start(step)\r\n                outputs = model(**batch)\r\n                loss = outputs[0]\r\n                del outputs  # free memory\r\n\r\n                if dist.get_rank() == dist.get_world_size() - 1:\r\n                    print(f\"Step {step} loss: {loss}\")\r\n                booster.backward(loss, optimizer)\r\n                optimizer.step()\r\n                optimizer.zero_grad()\r\n\r\n                performance_evaluator.on_step_end(**batch)\r\n                prof.step()\r\n    performance_evaluator.on_fit_end()\r\n    coordinator.print_on_master(f\"Max CUDA memory usage: {get_accelerator().max_memory_allocated()/1024**2:.2f} MB\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n    disable_existing_loggers()\r\n    colossalai.legacy.launch_from_torch()\r\n    logger = get_dist_logger()\r\n    is_main_process = dist.get_rank() == 0\r\n\r\n    if is_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        logging.set_verbosity_error()\r\n\r\n    if args.mem_cap > 0:\r\n        colo_memory_cap(args.mem_cap)\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n        logger.info(f\"Rank {dist.get_rank()}: random seed is set to {args.seed}\")\r\n\r\n    # Handle the repository creation\r\n    with barrier_context():\r\n        if args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\r\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\r\n    # (the dataset will be downloaded automatically from the datasets Hub).\r\n    #\r\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\r\n    # 'text' is found. You can easily tweak this behavior (see below).\r\n    #\r\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\r\n    # download the dataset.\r\n    logger.info(\"Start preparing dataset\", ranks=[0])\r\n    if not args.synthetic:\r\n        if args.dataset_name is not None:\r\n            # Downloading and loading a dataset from the hub.\r\n            raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\r\n            if \"validation\" not in raw_datasets.keys():\r\n                raw_datasets[\"validation\"] = load_dataset(\r\n                    args.dataset_name,\r\n                    args.dataset_config_name,\r\n                    split=f\"train[:{args.validation_split_percentage}%]\",\r\n                )\r\n                raw_datasets[\"train\"] = load_dataset(\r\n                    args.dataset_name,\r\n                    args.dataset_config_name,\r\n                    split=f\"train[{args.validation_split_percentage}%:]\",\r\n                )\r\n        else:\r\n            data_files = {}\r\n            dataset_args = {}\r\n            if args.train_file is not None:\r\n                data_files[\"train\"] = args.train_file\r\n            if args.validation_file is not None:\r\n                data_files[\"validation\"] = args.validation_file\r\n            extension = args.train_file.split(\".\")[-1]\r\n            if extension == \"txt\":\r\n                extension = \"text\"\r\n                dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\r\n            raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\r\n            # If no validation data is there, validation_split_percentage will be used to divide the dataset.\r\n            if \"validation\" not in raw_datasets.keys():\r\n                raw_datasets[\"validation\"] = load_dataset(\r\n                    extension,\r\n                    data_files=data_files,\r\n                    split=f\"train[:{args.validation_split_percentage}%]\",\r\n                    **dataset_args,\r\n                )\r\n                raw_datasets[\"train\"] = load_dataset(\r\n                    extension,\r\n                    data_files=data_files,\r\n                    split=f\"train[{args.validation_split_percentage}%:]\",\r\n                    **dataset_args,\r\n                )\r\n    logger.info(\"Dataset is prepared\", ranks=[0])\r\n\r\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\r\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\r\n\r\n    # Load pretrained model and tokenizer\r\n    #\r\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\r\n    # download model & vocab.\r\n    if args.config_name:\r\n        config = AutoConfig.from_pretrained(args.config_name)\r\n    elif args.model_name_or_path:\r\n        config = AutoConfig.from_pretrained(args.model_name_or_path)\r\n    else:\r\n        config = CONFIG_MAPPING[args.model_type]()\r\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\r\n    logger.info(\"Model config has been created\", ranks=[0])\r\n\r\n    if args.model_name_or_path == \"facebook/opt-13b\":\r\n        tokenizer = GPT2Tokenizer.from_pretrained(args.model_name_or_path)\r\n    else:\r\n        print(f\"load model from {args.model_name_or_path}\")\r\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\r\n    logger.info(f\"{tokenizer.__class__.__name__} has been created\", ranks=[0])\r\n\r\n    if args.init_in_cpu:\r\n        init_dev = torch.device(\"cpu\")\r\n    else:\r\n        init_dev = get_accelerator().get_current_device()\r\n\r\n    cai_version = colossalai.__version__\r\n    logger.info(f\"using Colossal-AI version {cai_version}\")\r\n    # build model\r\n    if version.parse(cai_version) >= version.parse(\"0.3.1\"):\r\n        from contextlib import nullcontext\r\n\r\n        from colossalai.lazy import LazyInitContext\r\n\r\n        ctx = (\r\n            LazyInitContext(default_device=init_dev)\r\n            if args.model_name_or_path is None or args.model_name_or_path == \"facebook/opt-13b\"\r\n            else nullcontext()\r\n        )\r\n    else:\r\n        from colossalai.zero import ColoInitContext\r\n\r\n        ctx = ColoInitContext(device=init_dev)\r\n    if args.model_name_or_path is None or args.model_name_or_path == \"facebook/opt-13b\":\r\n        # currently, there has a bug in pretrained opt-13b\r\n        # we can not import it until huggingface fix it\r\n        logger.info(\"Train a new model from scratch\", ranks=[0])\r\n        with ctx:\r\n            model = OPTForCausalLM(config)\r\n    else:\r\n        logger.info(\"Finetune a pre-trained model\", ranks=[0])\r\n        with ctx:\r\n            model = OPTForCausalLM.from_pretrained(\r\n                args.model_name_or_path,\r\n                from_tf=bool(\".ckpt\" in args.model_name_or_path),\r\n                config=config,\r\n                local_files_only=False,\r\n            )\r\n\r\n    # enable graident checkpointing\r\n    model.gradient_checkpointing_enable()\r\n\r\n    PLACEMENT_POLICY = \"auto\"\r\n    if version.parse(cai_version) >= version.parse(\"0.3.1\"):\r\n        from colossalai.zero import GeminiDDP\r\n\r\n        model = GeminiDDP(model, offload_optim_frac=1.0, pin_memory=True)\r\n    elif version.parse(cai_version) > version.parse(\"0.1.10\"):\r\n        try:\r\n            from colossalai.nn.parallel import GeminiDDP\r\n        except ImportError:\r\n            # this works for unreleased main branch, and this may be released on 0.2.9\r\n            from colossalai.zero import GeminiDDP\r\n        model = GeminiDDP(\r\n            model, device=get_accelerator().get_current_device(), placement_policy=PLACEMENT_POLICY, pin_memory=True\r\n        )\r\n    elif version.parse(cai_version) <= version.parse(\"0.1.10\") and version.parse(cai_version) >= version.parse(\"0.1.9\"):\r\n        from colossalai.gemini import ChunkManager, GeminiManager\r\n\r\n        pg = ProcessGroup()\r\n        chunk_size = ChunkManager.search_chunk_size(model, 64 * 1024**2, 32)\r\n        chunk_manager = ChunkManager(\r\n            chunk_size,\r\n            pg,\r\n            enable_distributed_storage=True,\r\n            init_device=GeminiManager.get_default_device(PLACEMENT_POLICY),\r\n        )\r\n        gemini_manager = GeminiManager(PLACEMENT_POLICY, chunk_manager)\r\n        model = ZeroDDP(model, gemini_manager)\r\n\r\n    logger.info(f\"{model.__class__.__name__} has been created\", ranks=[0])\r\n\r\n    if not args.synthetic:\r\n        # Preprocessing the datasets.\r\n        # First we tokenize all the texts.\r\n        column_names = raw_datasets[\"train\"].column_names\r\n        text_column_name = \"text\" if \"text\" in column_names else column_names[0]\r\n\r\n        def tokenize_function(examples):\r\n            return tokenizer(examples[text_column_name])\r\n\r\n        with barrier_context(executor_rank=0, parallel_mode=ParallelMode.DATA):\r\n            tokenized_datasets = raw_datasets.map(\r\n                tokenize_function,\r\n                batched=True,\r\n                num_proc=args.preprocessing_num_workers,\r\n                remove_columns=column_names,\r\n                load_from_cache_file=not args.overwrite_cache,\r\n                desc=\"Running tokenizer on dataset\",\r\n            )\r\n\r\n    if args.block_size is None:\r\n        block_size = tokenizer.model_max_length\r\n        if block_size > 1024:\r\n            logger.warning(\r\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\r\n                \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\r\n            )\r\n        block_size = 1024\r\n    else:\r\n        if args.block_size > tokenizer.model_max_length:\r\n            logger.warning(\r\n                f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\r\n                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\r\n            )\r\n        block_size = min(args.block_size, tokenizer.model_max_length)\r\n\r\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\r\n    def group_texts(examples):\r\n        # Concatenate all texts.\r\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\r\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\r\n        # customize this part to your needs.\r\n        if total_length >= block_size:\r\n            total_length = (total_length // block_size) * block_size\r\n        # Split by chunks of max_len.\r\n        result = {\r\n            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\r\n            for k, t in concatenated_examples.items()\r\n        }\r\n        result[\"labels\"] = result[\"input_ids\"].copy()\r\n        return result\r\n\r\n    if not args.synthetic:\r\n        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\r\n        # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\r\n        # to preprocess.\r\n        #\r\n        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\r\n        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\r\n\r\n        with barrier_context(executor_rank=0, parallel_mode=ParallelMode.DATA):\r\n            lm_datasets = tokenized_datasets.map(\r\n                group_texts,\r\n                batched=True,\r\n                num_proc=args.preprocessing_num_workers,\r\n                load_from_cache_file=not args.overwrite_cache,\r\n                desc=f\"Grouping texts in chunks of {block_size}\",\r\n            )\r\n\r\n        train_dataset = lm_datasets[\"train\"]\r\n        eval_dataset = lm_datasets[\"validation\"]\r\n\r\n        # Log a few random samples from the training set:\r\n        # for index in random.sample(range(len(train_dataset)), 3):\r\n        #     logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\r\n\r\n        # DataLoaders creation:\r\n        train_dataloader = get_dataloader(\r\n            train_dataset,\r\n            shuffle=True,\r\n            add_sampler=True,\r\n            collate_fn=default_data_collator,\r\n            batch_size=args.per_device_train_batch_size,\r\n        )\r\n        eval_dataloader = DataLoader(\r\n            eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\r\n        )\r\n    else:\r\n        train_dataloader = DummyDataloader(\r\n            30, args.per_device_train_batch_size, config.max_position_embeddings, config.vocab_size\r\n        )\r\n        eval_dataloader = DummyDataloader(\r\n            10, args.per_device_train_batch_size, config.max_position_embeddings, config.vocab_size\r\n        )\r\n    logger.info(\"Dataloaders have been created\", ranks=[0])\r\n\r\n    # Optimizer\r\n    # Split weights in two groups, one with weight decay and the other not.\r\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n    optimizer_grouped_parameters = [\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n            \"weight_decay\": args.weight_decay,\r\n        },\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n            \"weight_decay\": 0.0,\r\n        },\r\n    ]\r\n\r\n    optimizer = HybridAdam(optimizer_grouped_parameters, lr=args.learning_rate)\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        name=args.lr_scheduler_type,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.num_warmup_steps,\r\n        num_training_steps=args.max_train_steps,\r\n    )\r\n    optimizer = GeminiOptimizer(optimizer, model, initial_scale=2**14)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # Train!\r\n    total_batch_size = args.per_device_train_batch_size * gpc.get_world_size(ParallelMode.DATA)\r\n    num_train_samples = len(train_dataset) if not args.synthetic else 30 * total_batch_size\r\n    num_eval_samples = len(eval_dataset) if not args.synthetic else 10 * total_batch_size\r\n\r\n    logger.info(\"***** Running training *****\", ranks=[0])\r\n    logger.info(f\"  Num examples = {num_train_samples}\", ranks=[0])\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\", ranks=[0])\r\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\", ranks=[0])\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\", ranks=[0])\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\", ranks=[0])\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\", ranks=[0])\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not is_main_process)\r\n    completed_steps = 0\r\n    starting_epoch = 0\r\n    global_step = 0\r\n\r\n    for epoch in range(starting_epoch, args.num_train_epochs):\r\n        if completed_steps >= args.max_train_steps:\r\n            break\r\n\r\n        model.train()\r\n        for step, batch in enumerate(train_dataloader):\r\n            batch = {k: v.cuda() for k, v in batch.items()}\r\n            outputs = model(use_cache=False, **batch)\r\n            loss = outputs[\"loss\"]\r\n            optimizer.backward(loss)\r\n\r\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n                progress_bar.update(1)\r\n                completed_steps += 1\r\n\r\n            global_step += 1\r\n            logger.info(\"Global step {} finished\".format(global_step + 1), ranks=[0])\r\n\r\n            if completed_steps >= args.max_train_steps:\r\n                break\r\n\r\n        model.eval()\r\n        losses = []\r\n        for step, batch in enumerate(eval_dataloader):\r\n            with torch.no_grad():\r\n                batch = {k: v.cuda() for k, v in batch.items()}\r\n                outputs = model(**batch)\r\n\r\n        loss = outputs[\"loss\"].unsqueeze(0)\r\n        losses.append(loss)\r\n\r\n        losses = torch.cat(losses)\r\n        losses = losses[:num_eval_samples]\r\n        try:\r\n            eval_loss = torch.mean(losses)\r\n            perplexity = math.exp(eval_loss)\r\n        except OverflowError:\r\n            perplexity = float(\"inf\")\r\n\r\n        logger.info(f\"Epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\", ranks=[0])\r\n\r\n    if args.output_dir is not None:\r\n        model_state = model.state_dict()\r\n        if is_main_process:\r\n            torch.save(model_state, args.output_dir + \"/epoch_{}_model.pth\".format(completed_steps))\r\n        dist.barrier()\r\n        # load_state = torch.load(args.output_dir + '/epoch_{}_model.pth'.format(completed_steps))\r\n        # model.load_state_dict(load_state, strict=False)\r\n\r\n    logger.info(\"Training finished\", ranks=[0])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_grad_acc_test(test_args):\r\n    model_fn, *_ = next(iter(model_zoo.get_sub_registry(\"transformers_gpt_lm\").values()))\r\n    model = model_fn()\r\n    optimizer = HybridAdam(model.parameters())\r\n    origin_model = copy.deepcopy(model).cuda()\r\n    origin_optimizer = HybridAdam(origin_model.parameters())\r\n\r\n    plugin = HybridParallelPlugin(\r\n        tp_size=test_args[\"tp\"],\r\n        pp_size=test_args[\"pp\"],\r\n        pp_style=test_args[\"pp_style\"],\r\n        zero_stage=test_args[\"zero\"],\r\n        num_model_chunks=test_args[\"num_model_chunks\"],\r\n        enable_fused_normalization=True,\r\n        num_microbatches=test_args[\"num_microbatches\"],\r\n        precision=test_args[\"precision\"],\r\n    )\r\n    booster = Booster(plugin=plugin)\r\n\r\n    dataset = RandomDataset(\r\n        num_samples=test_args[\"batch_size\"] * test_args[\"num_steps\"] * plugin.dp_size,\r\n        max_length=test_args[\"max_length\"],\r\n        vocab_size=model.config.vocab_size,\r\n    )\r\n    dataloader = plugin.prepare_dataloader(dataset, batch_size=test_args[\"batch_size\"], shuffle=True, drop_last=True)\r\n\r\n    model, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\r\n\r\n    grad_accu_step = test_args[\"gradient_accumulation_step\"]\r\n    for step, batch in enumerate(dataloader):\r\n        batch = move_to_cuda(batch)\r\n        # train origin model\r\n        origin_output = origin_model(**batch)\r\n        origin_loss = origin_output[0] / grad_accu_step\r\n        origin_loss.backward()\r\n\r\n        if (step + 1) % grad_accu_step != 0 and test_args[\"zero\"] != 2:\r\n            ctx = booster.no_sync(model, optimizer)\r\n        else:\r\n            ctx = nullcontext()\r\n\r\n        with ctx:\r\n            if plugin.stage_manager is not None:\r\n                batch = iter([batch])\r\n                booster.execute_pipeline(\r\n                    batch,\r\n                    model,\r\n                    criterion=lambda outputs, inputs: outputs[0] / grad_accu_step,\r\n                    optimizer=optimizer,\r\n                    return_loss=False,\r\n                )\r\n            else:\r\n                outputs = model(**batch)\r\n                loss = outputs[0] / grad_accu_step\r\n                booster.backward(loss, optimizer)\r\n\r\n        if (step + 1) % grad_accu_step == 0:\r\n            # update origin model weight\r\n            origin_optimizer.step()\r\n            origin_optimizer.zero_grad()\r\n\r\n            # update sharded model\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n    # tricky code here, shard the origin model inorder to check the parameters in the same stage.\r\n    origin_model, origin_optimizer, _, dataloader, _ = booster.boost(\r\n        origin_model, origin_optimizer, dataloader=dataloader\r\n    )\r\n    for p1, p2 in zip(model.unwrap().parameters(), origin_model.unwrap().parameters()):\r\n        assert_close(p1.to(p2.dtype), p2, atol=1e-2, rtol=1e-2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_torch_amp(rank, world_size, port):\r\n    # init dist env\r\n    colossalai.launch(rank=rank, world_size=world_size, port=port, host=\"localhost\")\r\n    sub_model_zoo = model_zoo.get_sub_registry(\"timm\")\r\n    for name, (model_fn, data_gen_fn, output_transform_fn, _, _) in sub_model_zoo.items():\r\n        # dlrm_interactionarch has not parameters, so skip\r\n        if name == \"dlrm_interactionarch\":\r\n            continue\r\n\r\n        model = model_fn().cuda()\r\n        optimizer = Adam(model.parameters(), lr=1e-3)\r\n        criterion = lambda x: x.mean()\r\n        data = data_gen_fn()\r\n        data = {\r\n            k: v.to(\"cuda\") if torch.is_tensor(v) or \"Tensor\" in v.__class__.__name__ else v for k, v in data.items()\r\n        }\r\n        mixed_precision = FP16TorchMixedPrecision()\r\n        model, optimizer, criterion = mixed_precision.configure(model, optimizer, criterion)\r\n        output = model(**data)\r\n        output = output_transform_fn(output)\r\n        output_key = list(output.keys())[0]\r\n        loss = criterion(output[output_key])\r\n        optimizer.backward(loss)\r\n        optimizer.clip_grad_by_norm(1.0)\r\n        optimizer.step()\r\n        del model, optimizer, criterion, data, output, mixed_precision",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def numerical_test_for_node_strategy(\r\n    model: torch.nn.Module,\r\n    device_mesh: DeviceMesh,\r\n    node_index: int,\r\n    strategy_number: int,\r\n    input_args: List[torch.Tensor],\r\n    meta_arg_names: List[str],\r\n    input_kwargs: Dict[str, torch.Tensor] = {},\r\n    node_type: str = \"normal\",\r\n):\r\n    for strategy_index in range(strategy_number):\r\n        print(f\"#strategy_index: {strategy_index}\")\r\n        # We need to copy the model to avoid do backward more than once in same graph\r\n        grad_to_compare_dict = {}\r\n        grad_to_shard_dict = {}\r\n        model_to_compare, args_to_compare, kwargs_to_compare = _build_model_to_compare(\r\n            model, input_args, input_kwargs, grad_to_compare_dict\r\n        )\r\n        model_to_shard, args_to_shard, kwargs_to_shard = _build_model_to_compare(\r\n            model, input_args, input_kwargs, grad_to_shard_dict\r\n        )\r\n\r\n        tracer = ColoTracer(bias_addition_split=True)\r\n        input_sample = {}\r\n        for input_arg, meta_arg_name in zip(input_args, meta_arg_names):\r\n            input_sample[meta_arg_name] = torch.empty(input_arg.shape, dtype=input_arg.dtype).to(\"meta\")\r\n        for meta_kwarg_name, input_kwarg in input_kwargs.items():\r\n            input_sample[meta_kwarg_name] = torch.empty(input_kwarg.shape, dtype=input_kwarg.dtype).to(\"meta\")\r\n        graph = tracer.trace(root=model_to_shard, meta_args=input_sample)\r\n        gm = ColoGraphModule(model_to_shard, graph, model_to_shard.__class__.__name__)\r\n        shape_prop_pass(gm, *input_sample.values())\r\n\r\n        solver_options = SolverOptions()\r\n        strategies_constructor = StrategiesConstructor(graph, device_mesh, solver_options)\r\n        strategies_constructor.build_strategies_and_cost()\r\n        target_node = [strategies_vector.node for strategies_vector in strategies_constructor.leaf_strategies][\r\n            node_index\r\n        ]\r\n        if node_type == \"normal\":\r\n            solution_len = len(strategies_constructor.leaf_strategies)\r\n            solution = [0] * solution_len\r\n            solution[node_index] = strategy_index\r\n        elif node_type == \"following\":\r\n            solution_len = len(strategies_constructor.leaf_strategies)\r\n            solution = [0] * solution_len\r\n            solution[node_index] = strategy_index\r\n            solution[node_index + 1] = strategy_index\r\n        else:\r\n            node_vector = strategies_constructor.leaf_strategies[node_index]\r\n            strategy_to_keep = node_vector[strategy_index]\r\n            node_vector = [strategy_to_keep]\r\n            # solution construction\r\n            cost_graph = CostGraph(strategies_constructor.leaf_strategies)\r\n            cost_graph.simplify_graph()\r\n            solver = Solver(gm.graph, strategies_constructor, cost_graph, verbose=False)\r\n            ret = solver.call_solver_serialized_args()\r\n            solution = list(ret[0])\r\n        gm, sharding_spec_dict, origin_spec_dict, comm_actions_dict = runtime_preparation_pass(\r\n            gm, solution, device_mesh, strategies_constructor\r\n        )\r\n        gm = runtime_apply_pass(gm)\r\n        gm.recompile()\r\n\r\n        # forward result compare\r\n        output = gm(\r\n            *args_to_shard,\r\n            sharding_spec_convert_dict=sharding_spec_dict,\r\n            origin_node_sharding_spec_dict=origin_spec_dict,\r\n            comm_actions_dict=comm_actions_dict,\r\n            **kwargs_to_shard,\r\n        )\r\n        output_to_compare = model_to_compare(*args_to_compare, **kwargs_to_compare)\r\n        assert_close_helper(output, output_to_compare, strategy_index=strategy_index, type=\"forward output\")\r\n\r\n        # backward result compare\r\n        if isinstance(output, (tuple, list)):\r\n            loss = output[0].sum()\r\n            loss_to_compare = output_to_compare[0].sum()\r\n        else:\r\n            loss = output.sum()\r\n            loss_to_compare = output_to_compare.sum()\r\n\r\n        loss_to_compare.backward()\r\n        loss.backward()\r\n        for key in grad_to_shard_dict.keys():\r\n            grad_to_shard = grad_to_shard_dict[key]\r\n            grad_to_compare = grad_to_compare_dict[key]\r\n            assert_close_helper(grad_to_shard, grad_to_compare, strategy_index=strategy_index, type=\"input grad\")\r\n        # extract the strategy used in this iter\r\n        strategy_in_use = target_node.strategies_vector[strategy_index]\r\n        param_to_shard_dict = dict(gm.named_parameters())\r\n        param_to_compare_dict = dict(model_to_compare.named_parameters())\r\n        for name in param_to_shard_dict.keys():\r\n            param_name = name.split(\".\")[-1]\r\n            if node_type == \"normal\":\r\n                param_sharding_spec = strategy_in_use.get_sharding_spec_by_name(param_name)\r\n            else:\r\n                if \"weight\" in name:\r\n                    param_sharding_spec = None\r\n\r\n                    for node in list(graph.nodes):\r\n                        if \"weight\" in node.name:\r\n                            param_sharding_spec = node.sharding_spec\r\n\r\n                elif \"bias\" in name:\r\n                    param_sharding_spec = None\r\n\r\n                    for node in list(graph.nodes):\r\n                        if \"bias\" in node.name:\r\n                            param_sharding_spec = node.sharding_spec\r\n\r\n            assert param_sharding_spec is not None\r\n            grad_sharded = param_to_shard_dict[name].grad\r\n            grad_to_compare = param_to_compare_dict[name].grad\r\n            global_grad = to_global(grad_sharded, param_sharding_spec)\r\n            assert_close_helper(global_grad, grad_to_compare, strategy_index=strategy_index, type=\"param grad\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_sharded_optimizer_checkpoint(use_async: bool):\r\n    # create a model and optimizer\r\n    model = resnet18()\r\n    optimizer = Adam(model.parameters(), lr=0.001)\r\n\r\n    # create test data sample\r\n    x = torch.randn(1, 3, 224, 224)\r\n\r\n    # run fwd and bwd\r\n    y = model(x)\r\n    loss = y.sum()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    # create temp directories for checkpoint\r\n    model_ckpt_dir = tempfile.TemporaryDirectory()\r\n    optimizer_ckpt_dir = tempfile.TemporaryDirectory()\r\n\r\n    # save the model and optimizer\r\n    ckpt_io = GeneralCheckpointIO()\r\n\r\n    ckpt_io.save_model(model, model_ckpt_dir.name, True, True, \"\", 10, use_safetensors=False)\r\n    ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)\r\n\r\n    ckpt_io._sync_d2h()\r\n    ckpt_io._sync_io()\r\n\r\n    # create new model\r\n    new_model = resnet18()\r\n    new_optimizer = Adam(new_model.parameters(), lr=0.001)\r\n\r\n    ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)\r\n    ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))\r\n\r\n    # check for model and optimizer state dict recursively\r\n    check_state_dict_equal(model.state_dict(), new_model.state_dict())\r\n    check_state_dict_equal(optimizer.state_dict(), new_optimizer.state_dict())\r\n\r\n    # continue running fwd and bwd\r\n    for _ in range(5):\r\n        y = new_model(x)\r\n        loss = y.sum()\r\n        loss.backward()\r\n        new_optimizer.step()\r\n\r\n    # create temp directories for checkpoint\r\n    model_ckpt_dir = tempfile.TemporaryDirectory()\r\n    optimizer_ckpt_dir = tempfile.TemporaryDirectory()\r\n\r\n    # save the newly got optimizer\r\n    ckpt_io.save_model(new_model, model_ckpt_dir.name, True, True, \"\", 10, use_safetensors=False)\r\n    ckpt_io.save_optimizer(new_optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)\r\n\r\n    ckpt_io._sync_d2h()\r\n    ckpt_io._sync_io()\r\n\r\n    # create another new model\r\n    new_new_model = resnet18()\r\n    new_new_optimizer = Adam(new_new_model.parameters(), lr=0.001)\r\n\r\n    ckpt_io.load_model(new_new_model, str(model_ckpt_dir.name), strict=True)\r\n    ckpt_io.load_optimizer(new_new_optimizer, str(optimizer_ckpt_dir.name))\r\n\r\n    # check for model and optimizer state dict recursively\r\n    check_state_dict_equal(new_model.state_dict(), new_new_model.state_dict())\r\n    check_state_dict_equal(new_optimizer.state_dict(), new_new_optimizer.state_dict())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_torch_amp():\r\n    \"\"\"\r\n    In this test, we compare the torch amp and apex amp implemented in colossalai\r\n    \"\"\"\r\n\r\n    torch.backends.cudnn.benchmark = False\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n    # create layer\r\n    test_models = [\"torchvision_resnet18\", \"custom_simple_net\"]\r\n    for test_name in test_models:\r\n        model_builder, data_gen_fn, *_ = next(iter(model_zoo.get_sub_registry(test_name).values()))\r\n\r\n        # create model\r\n        torch_amp_model = model_builder().cuda()\r\n        apex_amp_model = copy.deepcopy(torch_amp_model)\r\n\r\n        # create optimizer\r\n        # we use SGD here, since the correctness of gradient clipping can't be tested with Adam\r\n        torch_amp_optimizer = torch.optim.SGD(torch_amp_model.parameters(), lr=1e-3)\r\n        apex_amp_optimizer = torch.optim.SGD(apex_amp_model.parameters(), lr=1e-3)\r\n\r\n        # inject torch and apex amp\r\n        torch_amp_config = dict(init_scale=128, enabled=True)\r\n        torch_amp_model, torch_amp_optimizer, _ = convert_to_torch_amp(\r\n            torch_amp_model, torch_amp_optimizer, amp_config=torch_amp_config\r\n        )\r\n        apex_amp_config = dict(opt_level=\"O1\", loss_scale=128)\r\n        apex_amp_model, apex_amp_optimizer = convert_to_apex_amp(apex_amp_model, apex_amp_optimizer, apex_amp_config)\r\n\r\n        # create data\r\n        data = data_gen_fn()\r\n        data = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in data.items()}\r\n\r\n        # forward pass\r\n        torch_amp_output = torch_amp_model(**data)\r\n        apex_amp_output = apex_amp_model(**data)\r\n        assert_close_loose(torch_amp_output, apex_amp_output)\r\n\r\n        for torch_amp_param, apex_amp_param in zip(torch_amp_model.parameters(), apex_amp_model.parameters()):\r\n            assert_close_loose(torch_amp_param, apex_amp_param)\r\n\r\n        # backward\r\n        # use sum() to get big gradient\r\n        torch_amp_optimizer.backward(torch_amp_output.sum())\r\n        apex_amp_optimizer.backward(apex_amp_output.sum())\r\n\r\n        # check grad\r\n        # In apex amp, grad is not scaled before backward, but torch amp does\r\n        for torch_amp_param, apex_amp_param in zip(torch_amp_model.parameters(), apex_amp_model.parameters()):\r\n            assert_close_loose(torch_amp_param.grad, apex_amp_param.grad * apex_amp_config[\"loss_scale\"])\r\n\r\n        # clip gradient\r\n        apex_amp_optimizer.clip_grad_norm(model=apex_amp_model, max_norm=1.0)\r\n        torch_amp_optimizer.clip_grad_norm(model=torch_amp_model, max_norm=1.0)\r\n\r\n        # step\r\n        torch_amp_optimizer.step()\r\n        apex_amp_optimizer.step()\r\n\r\n        # check updated param and grad\r\n        for torch_amp_param, apex_amp_param in zip(torch_amp_model.parameters(), apex_amp_model.parameters()):\r\n            assert_close_loose(torch_amp_param.grad, apex_amp_param.grad)\r\n            assert_close_loose(torch_amp_param, apex_amp_param)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_naive_amp():\r\n    \"\"\"\r\n    In this test, we compare the naive fp16 optimizer implemented in colossalai\r\n    and fp32 torch optimizer\r\n    \"\"\"\r\n    torch.backends.cudnn.benchmark = False\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n    # create layer\r\n    test_models = [\"custom_repeated_computed_layers\", \"custom_nested_model\", \"torchvision_resnet18\"]\r\n    for test_name in test_models:\r\n        model_builder, data_gen_fn, *_ = next(iter(model_zoo.get_sub_registry(test_name).values()))\r\n\r\n        # create model\r\n        naive_amp_model = model_builder().cuda()\r\n        apex_amp_model = copy.deepcopy(naive_amp_model)\r\n\r\n        # create optimizer\r\n        # we use SGD here, since the correctness of gradient clipping can't be tested with Adam\r\n        naive_amp_optimizer = torch.optim.SGD(naive_amp_model.parameters(), lr=1e-3)\r\n        apex_amp_optimizer = torch.optim.SGD(apex_amp_model.parameters(), lr=1e-3)\r\n\r\n        # inject naive and apex amp\r\n        naive_amp_config = dict(initial_scale=128, clip_grad_norm=1.0)\r\n        naive_amp_model, naive_amp_optimizer = convert_to_naive_amp(\r\n            naive_amp_model, naive_amp_optimizer, naive_amp_config\r\n        )\r\n        apex_amp_config = dict(opt_level=\"O2\", loss_scale=128, keep_batchnorm_fp32=False)\r\n        apex_amp_model, apex_amp_optimizer = convert_to_apex_amp(apex_amp_model, apex_amp_optimizer, apex_amp_config)\r\n\r\n        # create data\r\n        data = data_gen_fn()\r\n        data = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in data.items()}\r\n\r\n        # forward pass\r\n        naive_amp_output = naive_amp_model(**data)\r\n        apex_amp_output = apex_amp_model(**data)\r\n        assert_close_loose(naive_amp_output, apex_amp_output)\r\n\r\n        # backward\r\n        # use sum() to get big gradient\r\n        naive_amp_optimizer.backward(naive_amp_output.sum())\r\n        apex_amp_optimizer.backward(apex_amp_output.sum())\r\n\r\n        # check grad\r\n        for naive_amp_param, apex_amp_param in zip(naive_amp_model.parameters(), apex_amp_model.parameters()):\r\n            assert_close_loose(naive_amp_param.grad, apex_amp_param.grad)\r\n\r\n        # clip gradient\r\n        apex_amp_optimizer.clip_grad_norm(model=apex_amp_model, max_norm=1.0)\r\n\r\n        # step\r\n        naive_amp_optimizer.step()\r\n        apex_amp_optimizer.step()\r\n\r\n        # check updated param\r\n        for naive_amp_param, apex_amp_param in zip(naive_amp_model.parameters(), apex_amp_model.parameters()):\r\n            assert_close_loose(naive_amp_param, apex_amp_param)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_freq_aware_embed(use_LFU: bool):\r\n    device = torch.device(\"cuda\", 0)\r\n    evict_strategy = EvictionStrategy.LFU if use_LFU else EvictionStrategy.DATASET\r\n    model = CachedEmbeddingBag(\r\n        NUM_EMBED,\r\n        EMBED_DIM,\r\n        mode=\"mean\",\r\n        include_last_offset=True,\r\n        cache_ratio=min(BATCH_SIZE * 2 / NUM_EMBED, 1.0),\r\n        ids_freq_mapping=None,\r\n        evict_strategy=evict_strategy,\r\n    ).to(device)\r\n\r\n    assert model.weight.shape[0] == NUM_EMBED\r\n    ref_model = torch.nn.EmbeddingBag.from_pretrained(\r\n        model.weight.detach().to(device), mode=\"mean\", include_last_offset=True, freeze=False\r\n    )\r\n\r\n    assert torch.allclose(ref_model.weight.detach(), model.weight.detach().to(device))\r\n\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n    ref_optimizer = torch.optim.SGD(ref_model.parameters(), lr=1e-3)\r\n\r\n    for i in range(5):\r\n        indices, offsets = synthesize_1d_sparse_feature(BATCH_SIZE, NUM_EMBED, device)\r\n        res = model(indices, offsets)\r\n        ref_res = ref_model(indices, offsets)\r\n        assert torch.allclose(res, ref_res), f\"model result: {res}, reference: {ref_res}\"\r\n\r\n        grad = torch.rand_like(res)\r\n        # comparing gradient here is nontrivial\r\n        res.backward(grad)\r\n        ref_res.backward(grad)\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        ref_optimizer.step()\r\n        ref_optimizer.zero_grad()\r\n\r\n    model.cache_weight_mgr.flush()\r\n    model_weight = model.weight.detach().to(device)\r\n    ref_weight = ref_model.weight.detach()\r\n    assert torch.allclose(\r\n        model_weight, ref_weight\r\n    ), f\"model weight: {model_weight[10:18, :8]}, reference: {ref_weight[10:18, :8]}\"",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_parallel_freq_aware_embed_columnwise(rank, world_size):\r\n    device = torch.device(\"cuda\", torch.cuda.current_device())\r\n\r\n    num_embed = 100\r\n    embed_dim = 16\r\n    batch_size = 4\r\n\r\n    set_seed(4321)\r\n    weight = torch.rand(num_embed, embed_dim)\r\n    coloweight = ColoTensor(weight.clone().detach().cpu(), spec=None)\r\n\r\n    # initialize the tensor spec for the embedding weight parameter,\r\n    # which is an ColoParameter.\r\n    coloweight.set_process_group(ProcessGroup(tp_degree=world_size))\r\n    coloweight.set_tensor_spec(ShardSpec(dims=[-1], num_partitions=[world_size]), ComputeSpec(ComputePattern.TP1D))\r\n\r\n    model = ParallelCachedEmbeddingBag.from_pretrained(\r\n        coloweight,\r\n        include_last_offset=True,\r\n        freeze=False,\r\n        cache_ratio=batch_size * 2 / num_embed,\r\n    )\r\n\r\n    assert model.cache_weight_mgr.weight.device.type == \"cpu\"\r\n    assert model.cache_weight_mgr.cuda_cached_weight.requires_grad\r\n    weight_in_rank = torch.tensor_split(weight, world_size, -1)[rank]\r\n    print(f\"model weight: {model.cache_weight_mgr.weight.shape}, ref weight: {weight_in_rank.shape}\")\r\n    assert torch.allclose(\r\n        weight_in_rank, model.cache_weight_mgr.weight.detach()\r\n    ), f\"{weight_in_rank - model.cache_weight_mgr.weight}\"\r\n\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n\r\n    if rank == 0:\r\n        ref_model = torch.nn.EmbeddingBag.from_pretrained(\r\n            weight.detach().clone(), include_last_offset=True, freeze=False\r\n        ).to(device)\r\n        ref_optimizer = torch.optim.SGD(ref_model.parameters(), lr=1e-3)\r\n\r\n    set_seed(4321)\r\n    for i in range(5):\r\n        indices, offsets = synthesize_1d_sparse_feature(batch_size, num_embed, device)\r\n        res = model(indices, offsets)\r\n\r\n        grad = torch.rand(batch_size * 2, embed_dim, dtype=res.dtype, device=res.device)\r\n        grad_in_rank = torch.tensor_split(grad, world_size, 0)[rank]\r\n        res.backward(grad_in_rank)\r\n\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        res_list = gather_tensor(res.detach(), rank, world_size)\r\n\r\n        if rank == 0:\r\n            ref_res = ref_model(indices, offsets)\r\n            recover_res = torch.cat(res_list, dim=0)\r\n\r\n            assert torch.allclose(ref_res, recover_res)\r\n\r\n            ref_res.backward(grad)\r\n            ref_optimizer.step()\r\n            ref_optimizer.zero_grad()\r\n\r\n    model.cache_weight_mgr.flush()\r\n    weight_list = gather_tensor(model.cache_weight_mgr.weight.detach().cuda(), rank, world_size)\r\n    if rank == 0:\r\n        recover_weight = torch.cat(weight_list, dim=1)\r\n        assert torch.allclose(recover_weight, ref_model.weight.detach()), f\"{recover_weight - ref_model.weight}\"",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_zero_with_original_model(stage: int, ep_size: int):\r\n    dtype = torch.bfloat16\r\n\r\n    rank = torch.distributed.get_rank()\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    plugin = MoeHybridParallelPlugin(\r\n        pp_size=1, tp_size=1, ep_size=ep_size, zero_stage=stage, overlap_communication=False, initial_scale=1\r\n    )\r\n    booster = Booster(plugin=plugin)\r\n\r\n    seed_all(10086)\r\n\r\n    config = MixtralConfig(\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=2,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        num_local_experts=NUM_EXPERTS,\r\n        num_experts_per_tok=TOP_K,\r\n    )\r\n\r\n    torch_model = MixtralModel(config).to(dtype).cuda()\r\n\r\n    zero_model = deepcopy(torch_model).to(dtype)\r\n    zero_optimizer = torch.optim.SGD(zero_model.parameters(), lr=1)\r\n\r\n    zero_model, zero_optimizer, _, _, _ = booster.boost(zero_model, zero_optimizer)\r\n\r\n    ddp_model = DDP(\r\n        torch_model.cuda(),\r\n        process_group=plugin.dp_group,\r\n        find_unused_parameters=True,  # important for torch ddp, not all experts are routed\r\n    ).cuda()\r\n    ddp_optimizer = torch.optim.SGD(ddp_model.parameters(), lr=1)\r\n\r\n    # create different input\r\n    seed_all(1453 + rank)\r\n\r\n    ddp_model.train()\r\n    zero_model.train()\r\n    for _ in range(2):\r\n        # zero-dp forward\r\n        input_data = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        zero_output = zero_model(inputs_embeds=input_data.to(dtype)).last_hidden_state.mean()\r\n        # zero-dp backward\r\n        zero_optimizer.backward(zero_output)\r\n\r\n        # torch-ddp forward\r\n        ddp_output = ddp_model(inputs_embeds=input_data.to(dtype)).last_hidden_state.mean()\r\n        assert_loose_close(zero_output, ddp_output, dtype=dtype)\r\n        # torch-ddp backward\r\n        ddp_output.backward()\r\n\r\n        # check grad\r\n        name_to_p = {n: p for n, p in ddp_model.named_parameters()}\r\n        for n, p in zero_model.named_parameters():\r\n            zero_grad = zero_optimizer.get_param_grad(p)\r\n            if name_to_p[n].grad is None:\r\n                name_to_p[n].grad = torch.zeros_like(name_to_p[n].data)\r\n                continue\r\n            assert_loose_close(zero_grad, name_to_p[n].grad, dtype=dtype, name=n)\r\n\r\n        # zero-dp step\r\n        zero_optimizer.step()\r\n\r\n        # original model step\r\n        ddp_optimizer.step()\r\n\r\n        # check updated param\r\n        for n, p in zero_model.named_parameters():\r\n            assert_loose_close(p.data, name_to_p[n].data, dtype=dtype, name=n)\r\n\r\n    print(f\"{dist.get_rank()} test passed\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type):\r\n    model = model_fn()\r\n    lora_config = LoraConfig(task_type=task_type, r=8, lora_alpha=32, lora_dropout=0.1)\r\n\r\n    test_plugins = [TorchDDPPlugin(), LowLevelZeroPlugin(), HybridParallelPlugin(tp_size=1, pp_size=1)]\r\n    test_configs = [\r\n        {\r\n            \"lora_config\": lora_config,\r\n            \"quantize\": False,\r\n        },\r\n        {\r\n            \"lora_config\": lora_config,\r\n            \"quantize\": True,\r\n        },\r\n    ]\r\n    for plugin, test_config in product(test_plugins, test_configs):\r\n        # checkpoint loaded model\r\n        model_save = model_fn()\r\n        model_load = copy.deepcopy(model_save)\r\n\r\n        optimizer = AdamW(model.parameters(), lr=0.001)\r\n        criterion = loss_fn\r\n\r\n        booster = Booster(plugin=plugin)\r\n        model_save = booster.enable_lora(model_save, **test_config)\r\n        model_save, optimizer, criterion, _, _ = booster.boost(model_save, optimizer, criterion)\r\n\r\n        with shared_tempdir() as tempdir:\r\n            lora_ckpt_path = os.path.join(tempdir, \"ckpt\")\r\n            booster.save_lora_as_pretrained(model_save, lora_ckpt_path)\r\n            dist.barrier()\r\n\r\n            # The Lora checkpoint should be small in size\r\n            checkpoint_size_mb = os.path.getsize(os.path.join(lora_ckpt_path, \"adapter_model.bin\")) / (1024 * 1024)\r\n            assert checkpoint_size_mb < 1\r\n\r\n            model_load = booster.enable_lora(model_load, pretrained_dir=lora_ckpt_path, **test_config)\r\n            model_load, _, _, _, _ = booster.boost(model_load)\r\n\r\n            check_state_dict_equal(model_save.state_dict(), model_load.state_dict())\r\n\r\n        # test fwd bwd correctness\r\n        test_model = model_load\r\n        if isinstance(model_load, HybridParallelModule):\r\n            model_load = model_load.module.module\r\n        model_copy = copy.deepcopy(model_load)\r\n\r\n        data = data_gen_fn()\r\n        data = {\r\n            k: v.to(\"cuda\") if torch.is_tensor(v) or \"Tensor\" in v.__class__.__name__ else v for k, v in data.items()\r\n        }\r\n\r\n        output = test_model(**data)\r\n        output = output_transform_fn(output)\r\n        loss = criterion(output)\r\n\r\n        booster.backward(loss, optimizer)\r\n        optimizer.clip_grad_by_norm(1.0)\r\n        optimizer.step()\r\n\r\n        for (n1, p1), (n2, p2) in zip(test_model.named_parameters(), model_copy.named_parameters()):\r\n            if \"lora_\" in n1:\r\n                # lora modules require gradients, thus updated\r\n                assert p1.requires_grad\r\n                assert not torch.testing.assert_close(p1.to(p2.device).to(p2.dtype), p2, atol=5e-3, rtol=5e-3)\r\n            else:\r\n                if not p1.requires_grad:\r\n                    torch.testing.assert_close(p1.to(p2.device).to(p2.dtype), p2, atol=5e-3, rtol=5e-3)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_zero_with_original_model(stage: int, ep_size: int):\r\n    tp_size = dist.get_world_size() // ep_size\r\n    dtype = torch.bfloat16\r\n\r\n    rank = torch.distributed.get_rank()\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    seed_all(10086)\r\n\r\n    config = MixtralConfig(\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=2,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        num_local_experts=NUM_EXPERTS,\r\n        num_experts_per_tok=TOP_K,\r\n    )\r\n    torch_model = MixtralModel(config).to(dtype).cuda()\r\n\r\n    zero_model = deepcopy(torch_model).to(dtype)\r\n    zero_optimizer = torch.optim.SGD(zero_model.parameters(), lr=1)\r\n    moe_booster = Booster(\r\n        plugin=MoeHybridParallelPlugin(\r\n            tp_size=tp_size,\r\n            moe_tp_size=tp_size,\r\n            pp_size=1,\r\n            ep_size=ep_size,\r\n            zero_stage=stage,\r\n            overlap_communication=False,\r\n            initial_scale=1,\r\n        )\r\n    )\r\n    zero_model, zero_optimizer, _, _, _ = moe_booster.boost(zero_model, zero_optimizer)\r\n\r\n    hybird_booster = Booster(\r\n        plugin=HybridParallelPlugin(\r\n            tp_size=tp_size,\r\n            pp_size=1,\r\n            zero_stage=stage,\r\n            overlap_communication=False,\r\n            initial_scale=1,\r\n        )\r\n    )\r\n    hybrid_model, hybrid_optimizer, _, _, _ = hybird_booster.boost(\r\n        torch_model, torch.optim.SGD(torch_model.parameters(), lr=1)\r\n    )\r\n    # create different input\r\n    seed_all(1453 + rank)\r\n\r\n    hybrid_model.train()\r\n    zero_model.train()\r\n    for _ in range(2):\r\n        # zero-dp forward\r\n        input_data = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        zero_output = zero_model(inputs_embeds=input_data.to(dtype)).last_hidden_state.mean()\r\n        # zero-dp backward\r\n        zero_optimizer.backward(zero_output)\r\n        # torch-ddp forward\r\n        hybrid_output = hybrid_model(inputs_embeds=input_data.to(dtype)).last_hidden_state.mean()\r\n        assert_loose_close(zero_output, hybrid_output, dtype=dtype)\r\n        # torch-ddp backward\r\n        hybrid_optimizer.backward(hybrid_output)\r\n\r\n        # check grad\r\n        name_to_p = {n: p for n, p in hybrid_model.named_parameters()}\r\n        for n, p in zero_model.named_parameters():\r\n            zero_grad = zero_optimizer.get_param_grad(p)\r\n            if name_to_p[n].grad is None:\r\n                name_to_p[n].grad = torch.zeros_like(name_to_p[n])\r\n                continue\r\n            if zero_grad.shape != name_to_p[n].grad.shape:  # TODO check sharded and sliced moe\r\n                continue\r\n            assert_loose_close(zero_grad, name_to_p[n].grad, dtype=dtype, name=n)\r\n\r\n        # zero-dp step\r\n        zero_optimizer.step()\r\n\r\n        # original model step\r\n        hybrid_optimizer.step()\r\n\r\n        # check updated param\r\n        for n, p in zero_model.named_parameters():\r\n            if p.data.shape != name_to_p[n].data.shape:  # TODO check sharded and sliced moe\r\n                continue\r\n            assert_loose_close(p.data, name_to_p[n].data, dtype=dtype, name=n)\r\n\r\n    print(f\"{dist.get_rank()} test passed\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check_shardformer_with_ddp(lazy_init: bool):\r\n    sub_model_zoo = model_zoo.get_sub_registry(\"transformers_gpt\", exclude=\"transformers_gptj\")\r\n\r\n    # create shardformer\r\n    # ranks: [0, 1, 2, 3]\r\n    # tp ranks = [0, 1], [2, 3]\r\n    # dp ranks = [0, 2], [1, 3]\r\n    dp_process_group_1 = dist.new_group([0, 2])\r\n    dp_process_group_2 = dist.new_group([1, 3])\r\n    tp_process_group_1 = dist.new_group([0, 1])\r\n    tp_process_group_2 = dist.new_group([2, 3])\r\n\r\n    coordinator = DistCoordinator()\r\n\r\n    if coordinator.rank in [0, 1]:\r\n        tp_process_group = tp_process_group_1\r\n    else:\r\n        tp_process_group = tp_process_group_2\r\n\r\n    if coordinator.rank in [0, 2]:\r\n        dp_process_group = dp_process_group_1\r\n    else:\r\n        dp_process_group = dp_process_group_2\r\n\r\n    shard_config = ShardConfig(tensor_parallel_process_group=tp_process_group, enable_fused_normalization=True)\r\n    shardformer = ShardFormer(shard_config=shard_config)\r\n\r\n    ctx = LazyInitContext() if lazy_init else nullcontext()\r\n\r\n    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():\r\n        # create and shard model\r\n        with ctx:\r\n            model = model_fn().cuda()\r\n        sharded_model, _ = shardformer.optimize(model)\r\n\r\n        # add ddp\r\n        sharded_ddp_model = DDP(sharded_model, process_group=dp_process_group)\r\n\r\n        # prepare input\r\n        data = data_gen_fn()\r\n        data = {k: v.cuda() for k, v in data.items()}\r\n\r\n        # switch to train mode\r\n        sharded_ddp_model.train()\r\n\r\n        # run forward\r\n        output = sharded_ddp_model(**data)\r\n        loss = loss_fn(output)\r\n\r\n        # backward\r\n        loss.backward()\r\n        torch.cuda.empty_cache()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_with_booster_hybridplugin(config: Tuple[int, ...]):\r\n    stage, pp_size, tp_size, sp_size = config\r\n    num_microbatches = pp_size\r\n    dist.get_world_size()\r\n    rank = dist.get_rank()\r\n    dtype, precision = torch.float16, \"fp16\"\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    ########\r\n    # init base model\r\n    ########\r\n    assert pp_size <= NUM_LAYERS, \"pp_size should be less than or equal to NUM_LAYERS\"\r\n    config = LlamaConfig(\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=NUM_LAYERS,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        attn_implementation=\"flash_attention_2\",\r\n    )\r\n\r\n    # init model with the same seed\r\n    seed_all(10086)\r\n\r\n    torch_model = LlamaModel(config).to(dtype).cuda()\r\n    # TODO: Support MixtralForCausalLM\r\n    # torch_model = MixtralForCausalLM(config).to(dtype).cuda()\r\n    torch_optimizer = torch.optim.SGD(torch_model.parameters(), lr=1)\r\n    # init schedule\r\n    h, a, s = config.hidden_size, config.num_attention_heads, 1024\r\n    mem_f = 34 * h + 5 * a * s\r\n    mem_w = -32 * h\r\n    mem_b = -mem_w - mem_f\r\n    graph = PipelineGraph(\r\n        n_stage=pp_size,\r\n        n_micro=num_microbatches,\r\n        f_cost=1,\r\n        b_cost=1,\r\n        w_cost=1,\r\n        c_cost=1,\r\n        f_mem=mem_f,\r\n        b_mem=mem_b,\r\n        w_mem=mem_w,\r\n    )\r\n\r\n    zbv_schedule = graph.get_v_schedule()\r\n\r\n    # init HybridParallelPlugin\r\n    plugin = HybridParallelPlugin(\r\n        pp_size=pp_size,\r\n        num_microbatches=pp_size,\r\n        tp_size=tp_size,\r\n        sp_size=sp_size,\r\n        zero_stage=stage,\r\n        enable_sequence_parallelism=sp_size > 1,\r\n        sequence_parallelism_mode=\"all_to_all\" if sp_size > 1 else None,\r\n        overlap_communication=False,\r\n        initial_scale=1,\r\n        precision=precision,\r\n        find_unused_parameters=True,\r\n        pp_style=\"zbv\",\r\n        scheduler_nodes=zbv_schedule,\r\n        num_model_chunks=2,\r\n    )\r\n\r\n    dp_size = plugin.dp_size\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    ########\r\n    # init pp model\r\n    ########\r\n\r\n    parallel_model = deepcopy(torch_model)\r\n    parallel_optimizer = torch.optim.SGD(parallel_model.parameters(), lr=1)\r\n    parallel_model, parallel_optimizer, _, _, _ = booster.boost(parallel_model, parallel_optimizer)\r\n    # create different input along dp axis\r\n    seed_all(1453 + rank)\r\n\r\n    torch_model.train()\r\n    parallel_model.train()\r\n    for _ in range(2):\r\n        # gen random input\r\n        input_embeddings = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        dist.all_reduce(\r\n            input_embeddings, group=plugin.pp_group\r\n        )  # pp inputs except the first stage doesn't matter, but need to be replicate for torch model check\r\n\r\n        dist.all_reduce(input_embeddings, group=plugin.tp_group)  # tp group duplicate input\r\n        dist.all_reduce(input_embeddings, group=plugin.sp_group)  # sp group duplicate input\r\n\r\n        # run the model with hybrid parallel\r\n        if booster.plugin.stage_manager is not None:\r\n            # for test with pp\r\n            data_iter = iter([{\"inputs_embeds\": input_embeddings}])\r\n            sharded_output = booster.execute_pipeline(\r\n                data_iter,\r\n                parallel_model,\r\n                lambda x, y: x.last_hidden_state.mean(),\r\n                parallel_optimizer,\r\n                return_loss=True,\r\n                return_outputs=True,\r\n            )\r\n            # stage 0 chunk 0\r\n            if (\r\n                booster.plugin.stage_manager.is_first_stage(ignore_chunk=True)\r\n                and rank == dist.get_process_group_ranks(plugin.pp_group)[0]\r\n            ):\r\n                parallel_output = sharded_output[\"loss\"]\r\n            else:\r\n                parallel_output = torch.tensor(12345.0, device=\"cuda\")\r\n            # broadcast along pp axis\r\n            dist.broadcast(parallel_output, src=dist.get_process_group_ranks(plugin.pp_group)[0], group=plugin.pp_group)\r\n\r\n        else:\r\n            # for test without pp\r\n            parallel_output = parallel_model(inputs_embeds=input_embeddings.to(dtype)).last_hidden_state.mean()\r\n            parallel_optimizer.backward(parallel_output)\r\n        parallel_optimizer.step()\r\n        parallel_optimizer.zero_grad()\r\n        dist.all_reduce(parallel_output, group=plugin.dp_group)\r\n\r\n        # ===================================================================================\r\n        # run normal model with all dp(different) inputs\r\n        all_inputs = [input_embeddings.clone() for _ in range(dp_size)]\r\n        dist.all_gather(all_inputs, input_embeddings, group=plugin.dp_group)\r\n        torch_output_sum = 0\r\n        for input_data_ in all_inputs:\r\n            torch_output = torch_model(inputs_embeds=input_data_.to(dtype)).last_hidden_state.mean()\r\n            torch_output.backward()\r\n            torch_output_sum += torch_output.detach()\r\n        # avg dp grads follows zero optimizer\r\n        for p in torch_model.parameters():\r\n            if p.grad is not None:\r\n                p.grad /= dp_size\r\n        torch_optimizer.step()\r\n        torch_optimizer.zero_grad()\r\n        assert_loose_close(parallel_output, torch_output_sum, dtype=dtype)\r\n\r\n    clear_layout_converter()\r\n    Randomizer.reset_index()\r\n    torch.cuda.empty_cache()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_with_booster_moehybridplugin(config: Tuple[int, ...]):\r\n    stage, ep_size, pp_size, tp_size, sp_size = config\r\n    num_microbatches = pp_size\r\n    dist.get_world_size()\r\n    rank = dist.get_rank()\r\n    dtype, precision = torch.float16, \"fp16\"\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    ########\r\n    # init base model\r\n    ########\r\n    assert pp_size <= NUM_LAYERS, \"pp_size should be less than or equal to NUM_LAYERS\"\r\n    config = MixtralConfig(\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=NUM_LAYERS,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        num_local_experts=NUM_EXPERTS,\r\n        num_experts_per_tok=TOP_K,\r\n        attn_implementation=\"flash_attention_2\",\r\n    )\r\n\r\n    # init model with the same seed\r\n    seed_all(10086)\r\n\r\n    torch_model = MixtralModel(config).to(dtype).cuda()\r\n    # TODO: Support MixtralForCausalLM\r\n    # torch_model = MixtralForCausalLM(config).to(dtype).cuda()\r\n    torch_optimizer = torch.optim.SGD(torch_model.parameters(), lr=1)\r\n    # init schedule\r\n    h, a, s = config.hidden_size, config.num_attention_heads, 1024\r\n    mem_f = 34 * h + 5 * a * s\r\n    mem_w = -32 * h\r\n    mem_b = -mem_w - mem_f\r\n    graph = PipelineGraph(\r\n        n_stage=pp_size,\r\n        n_micro=num_microbatches,\r\n        f_cost=1,\r\n        b_cost=1,\r\n        w_cost=1,\r\n        c_cost=1,\r\n        f_mem=mem_f,\r\n        b_mem=mem_b,\r\n        w_mem=mem_w,\r\n        # max_mem=mem_f * (p * 2 + m_offset),\r\n    )\r\n\r\n    zbv_schedule = graph.get_v_schedule()\r\n\r\n    # init MoeHybridPlugin\r\n    plugin = MoeHybridParallelPlugin(\r\n        pp_size=pp_size,\r\n        num_microbatches=pp_size,\r\n        tp_size=tp_size,\r\n        sp_size=sp_size,\r\n        ep_size=ep_size,\r\n        zero_stage=stage,\r\n        enable_sequence_parallelism=sp_size > 1,\r\n        sequence_parallelism_mode=\"all_to_all\" if sp_size > 1 else None,\r\n        overlap_communication=False,\r\n        initial_scale=1,\r\n        precision=precision,\r\n        find_unused_parameters=True,\r\n        pp_style=\"zbv\",\r\n        scheduler_nodes=zbv_schedule,\r\n        num_model_chunks=2,\r\n    )\r\n\r\n    dp_size = plugin.dp_size\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    ########\r\n    # init pp model\r\n    ########\r\n\r\n    parallel_model = deepcopy(torch_model)\r\n    parallel_optimizer = torch.optim.SGD(parallel_model.parameters(), lr=1)\r\n    parallel_model, parallel_optimizer, _, _, _ = booster.boost(parallel_model, parallel_optimizer)\r\n    # create different input along dp axis\r\n    seed_all(1453 + rank)\r\n\r\n    torch_model.train()\r\n    parallel_model.train()\r\n    for _ in range(2):\r\n        # gen random input\r\n        input_embeddings = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        dist.all_reduce(\r\n            input_embeddings, group=plugin.pp_group\r\n        )  # pp inputs except the first stage doesn't matter, but need to be replicate for torch model check\r\n\r\n        dist.all_reduce(input_embeddings, group=plugin.tp_group)  # tp group duplicate input\r\n        dist.all_reduce(input_embeddings, group=plugin.sp_group)  # sp group duplicate input\r\n\r\n        # run the model with hybrid parallel\r\n        if booster.plugin.stage_manager is not None:\r\n            # for test with pp\r\n            data_iter = iter([{\"inputs_embeds\": input_embeddings}])\r\n            sharded_output = booster.execute_pipeline(\r\n                data_iter,\r\n                parallel_model,\r\n                lambda x, y: x.last_hidden_state.mean(),\r\n                parallel_optimizer,\r\n                return_loss=True,\r\n                return_outputs=True,\r\n            )\r\n            # stage 0 chunk 0\r\n            if (\r\n                booster.plugin.stage_manager.is_first_stage(ignore_chunk=True)\r\n                and rank == dist.get_process_group_ranks(plugin.pp_group)[0]\r\n            ):\r\n                parallel_output = sharded_output[\"loss\"]\r\n            else:\r\n                parallel_output = torch.tensor(12345.0, device=\"cuda\")\r\n            # broadcast along pp axis\r\n            dist.broadcast(parallel_output, src=dist.get_process_group_ranks(plugin.pp_group)[0], group=plugin.pp_group)\r\n\r\n        else:\r\n            # for test without pp\r\n            parallel_output = parallel_model(inputs_embeds=input_embeddings.to(dtype)).last_hidden_state.mean()\r\n            parallel_optimizer.backward(parallel_output)\r\n        parallel_optimizer.step()\r\n        parallel_optimizer.zero_grad()\r\n        dist.all_reduce(parallel_output, group=plugin.dp_group)\r\n\r\n        # ===================================================================================\r\n        # run normal model with all dp(different) inputs\r\n        all_inputs = [input_embeddings.clone() for _ in range(dp_size)]\r\n        dist.all_gather(all_inputs, input_embeddings, group=plugin.dp_group)\r\n        torch_output_sum = 0\r\n        for input_data_ in all_inputs:\r\n            torch_output = torch_model(inputs_embeds=input_data_.to(dtype)).last_hidden_state.mean()\r\n            torch_output.backward()\r\n            torch_output_sum += torch_output.detach()\r\n        # avg dp grads follows zero optimizer\r\n        for p in torch_model.parameters():\r\n            if p.grad is not None:\r\n                p.grad /= dp_size\r\n        torch_optimizer.step()\r\n        torch_optimizer.zero_grad()\r\n        assert_loose_close(parallel_output, torch_output_sum, dtype=dtype)\r\n    clear_layout_converter()\r\n    Randomizer.reset_index()\r\n    torch.cuda.empty_cache()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def exam_gemini_grad_acc(\r\n    placement_config,\r\n    keep_gathered: bool,\r\n    model_name: str,\r\n    master_weights: bool,\r\n    use_grad_checkpoint: bool,\r\n    max_prefetch: int,\r\n    enable_async_reduce: bool,\r\n):\r\n    init_device = get_accelerator().get_current_device()\r\n    model_builder, data_gen_fn, output_transform_fn, loss_fn, *_ = next(\r\n        iter(model_zoo.get_sub_registry(model_name).values())\r\n    )\r\n\r\n    set_seed(42)\r\n    gemini_model = model_builder()\r\n\r\n    set_seed(42)\r\n    torch_model = model_builder().cuda()\r\n    for torch_p, p in zip(torch_model.parameters(), gemini_model.parameters()):\r\n        torch_p.data.copy_(p.data)\r\n\r\n    if use_grad_checkpoint:\r\n        gemini_model.gradient_checkpointing_enable()\r\n        torch_model.gradient_checkpointing_enable()\r\n\r\n    world_size = torch.distributed.get_world_size()\r\n    config_dict, *_ = search_chunk_configuration(gemini_model, search_range_m=1, search_interval=100)\r\n    config_dict[world_size][\"chunk_size\"] = 5000\r\n    config_dict[world_size][\"keep_gathered\"] = keep_gathered\r\n    gemini_model = GeminiDDP(\r\n        gemini_model,\r\n        config_dict,\r\n        init_device,\r\n        pin_memory=True,\r\n        enable_gradient_accumulation=True,\r\n        master_weights=master_weights,\r\n        max_prefetch=max_prefetch,\r\n        enable_async_reduce=enable_async_reduce,\r\n        **placement_config,\r\n    )\r\n    optimizer = HybridAdam(gemini_model.parameters(), lr=1e-3)\r\n    gemini_optim = GeminiOptimizer(\r\n        optimizer, gemini_model, initial_scale=1, max_norm=1.0, enable_async_reduce=enable_async_reduce\r\n    )\r\n\r\n    rank = dist.get_rank()\r\n\r\n    # setting master_weights to False will cause overflow after optimizer.step()\r\n    amp_config = dict(\r\n        opt_level=\"O2\", keep_batchnorm_fp32=False, loss_scale=1, min_loss_scale=1, max_loss_scale=1, master_weights=True\r\n    )\r\n    torch_optim = torch.optim.Adam(torch_model.parameters(), lr=1e-3)\r\n    torch_model, torch_optim = amp.initialize(torch_model, torch_optim, **amp_config)\r\n    torch_model = DDP(torch_model, device_ids=[rank])\r\n\r\n    set_seed(rank)\r\n    accum_iter = 2\r\n    train_dataloader = DummyDataloader(data_gen_fn)\r\n    for i, data in enumerate(train_dataloader):\r\n        delay_unscale = False if (i + 1) % accum_iter == 0 else True\r\n        data = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in data.items()}\r\n\r\n        set_seed(42 + rank)\r\n        torch_loss = run_fwd(torch_model, data, output_transform_fn, loss_fn)\r\n        torch_loss = torch_loss / accum_iter\r\n        with amp.scale_loss(torch_loss, torch_optim, delay_unscale=delay_unscale) as scaled_loss:\r\n            scaled_loss.backward()\r\n\r\n        set_seed(42 + rank)\r\n        gemini_loss = run_fwd(gemini_model, data, output_transform_fn, loss_fn)\r\n        gemini_loss = gemini_loss / accum_iter\r\n        gemini_optim.backward(gemini_loss)\r\n\r\n        assert torch.allclose(torch_loss.float(), gemini_loss.float(), rtol=1e-3, atol=1e-5)\r\n\r\n        check_grad(gemini_model, torch_model)\r\n\r\n        if (i + 1) % accum_iter == 0:\r\n            torch.nn.utils.clip_grad_norm_(amp.master_params(torch_optim), 1.0)\r\n            torch_optim.step()\r\n            gemini_optim.step()\r\n            torch_optim.zero_grad()\r\n\r\n            # check updated param\r\n            torch_dict = torch_model.state_dict()\r\n            gemini_dict = gemini_model.state_dict(only_rank_0=False)\r\n\r\n            for key, value in gemini_dict.items():\r\n                torch_key = \"module.\" + key\r\n                torch_value = torch_dict[torch_key].to(value.device).to(value.dtype)\r\n                assert_close(value, torch_value, rtol=1e-3, atol=2e-3)\r\n\r\n        if i == accum_iter:\r\n            break",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_deepseek_commom(parallel_config: Tuple[int, ...]):\r\n    Randomizer.reset_index()\r\n    print(f\"rank {dist.get_rank()} testing {parallel_config}\")\r\n    stage, ep_size, pp_size, tp_size, sp_size = parallel_config\r\n    world_size = dist.get_world_size()\r\n    rank = dist.get_rank()\r\n    dtype, precision = torch.bfloat16, \"bf16\"\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    plugin = MoeHybridParallelPlugin(\r\n        pp_size=pp_size,\r\n        num_microbatches=pp_size,\r\n        tp_size=tp_size,\r\n        sp_size=sp_size,\r\n        ep_size=ep_size,\r\n        zero_stage=stage,\r\n        enable_sequence_parallelism=sp_size > 1,\r\n        sequence_parallelism_mode=\"all_to_all\" if sp_size > 1 else None,\r\n        overlap_communication=False,\r\n        initial_scale=1,\r\n        precision=precision,\r\n        find_unused_parameters=True,\r\n        enable_flash_attention=True,\r\n    )\r\n    dp_size = plugin.dp_size\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    assert pp_size <= NUM_LAYERS, \"pp_size should be less than or equal to NUM_LAYERS\"\r\n    config = AutoConfig.from_pretrained(\r\n        \"deepseek-ai/deepseek-moe-16b-base\",\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        moe_intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=4,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        first_k_dense_replace=1,\r\n        attn_implementation=\"flash_attention_2\",\r\n        torch_dtype=\"float16\",\r\n        n_routed_experts=NUM_EXPERTS,\r\n        n_shared_experts=2,\r\n        num_experts_per_tok=TOP_K,\r\n        trust_remote_code=True,\r\n    )\r\n\r\n    # init model with the same seed\r\n    seed_all(10086)\r\n\r\n    torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)\r\n    torch_optimizer = torch.optim.SGD(torch_model.parameters(), lr=1)\r\n\r\n    parallel_model = deepcopy(torch_model)\r\n    parallel_optimizer = torch.optim.SGD(parallel_model.parameters(), lr=1)\r\n    parallel_model, parallel_optimizer, _, _, _ = booster.boost(parallel_model, parallel_optimizer)\r\n\r\n    # create different input along dp axis\r\n    seed_all(1453 + rank)\r\n\r\n    torch_model.train()\r\n    parallel_model.train()\r\n    for _ in range(2):\r\n        # gen random input\r\n        input_embeddings = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        dist.all_reduce(\r\n            input_embeddings, group=plugin.pp_group\r\n        )  # pp inputs except the first stage doesn't matter, but need to be replicate for torch model check\r\n\r\n        dist.all_reduce(input_embeddings, group=plugin.tp_group)  # tp group duplicate input\r\n        dist.all_reduce(input_embeddings, group=plugin.sp_group)  # sp group duplicate input\r\n\r\n        # run the model with hybrid parallel\r\n        if booster.plugin.stage_manager is not None:\r\n            # for test with pp\r\n            data_iter = iter([{\"inputs_embeds\": input_embeddings}])\r\n            sharded_output = booster.execute_pipeline(\r\n                data_iter,\r\n                parallel_model,\r\n                lambda x, y: x[0].mean(),\r\n                parallel_optimizer,\r\n                return_loss=True,\r\n                return_outputs=True,\r\n            )\r\n            if booster.plugin.stage_manager.is_last_stage():\r\n                parallel_output = sharded_output[\"loss\"]\r\n            else:\r\n                parallel_output = torch.tensor(12345.0, device=\"cuda\")\r\n\r\n            # broadcast along pp axis\r\n            dist.broadcast(\r\n                parallel_output, src=dist.get_process_group_ranks(plugin.pp_group)[-1], group=plugin.pp_group\r\n            )\r\n        else:\r\n            # for test without pp\r\n            parallel_output = parallel_model(inputs_embeds=input_embeddings.to(dtype)).last_hidden_state.mean()\r\n            parallel_optimizer.backward(parallel_output)\r\n        parallel_optimizer.step()\r\n        parallel_optimizer.zero_grad()\r\n        dist.all_reduce(parallel_output, group=plugin.dp_group)\r\n\r\n        # ===================================================================================\r\n        # run normal model with all dp(different) inputs\r\n        all_inputs = [torch.empty_like(input_embeddings) for _ in range(dp_size)]\r\n        dist.all_gather(all_inputs, input_embeddings, group=plugin.dp_group)\r\n        torch_output_sum = 0\r\n        for input_data_ in all_inputs:\r\n            torch_output = torch_model(inputs_embeds=input_data_.to(dtype)).last_hidden_state.mean()\r\n            torch_output.backward()\r\n            torch_output_sum += torch_output.detach()\r\n        # avg dp grads follows zero optimizer\r\n        for p in torch_model.parameters():\r\n            if p.grad is not None:\r\n                p.grad /= dp_size\r\n        torch_optimizer.step()\r\n        torch_optimizer.zero_grad()\r\n\r\n        assert_loose_close(parallel_output, torch_output_sum, dtype=dtype)\r\n\r\n    # use checkpoint to load sharded zero model\r\n    model_dir = \"./test_deepseek\"\r\n    if rank == world_size - 1:\r\n        os.makedirs(model_dir, exist_ok=True)\r\n\r\n    dist.barrier()\r\n    booster.save_model(parallel_model, model_dir, shard=True)\r\n    dist.barrier()\r\n\r\n    saved_model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).cuda()\r\n    check_model_equal(torch_model, saved_model, dtype=dtype)\r\n    dist.barrier()\r\n\r\n    if rank == world_size - 1:\r\n        shutil.rmtree(model_dir)\r\n\r\n    print(f\"rank {dist.get_rank()} passed {parallel_config}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_fwd_bwd_vschedule_with_optim(test_config):\r\n    # init dist\r\n    rank = dist.get_rank()\r\n    pp_size = test_config[\"pp_size\"]\r\n    pg_mesh = ProcessGroupMesh(pp_size)\r\n    num_microbatch = test_config[\"num_microbatches\"]\r\n    num_model_chunk = test_config[\"num_model_chunk\"]\r\n    # stage_manager\r\n    stage_manager = PipelineStageManager(\r\n        pg_mesh, pipeline_axis=0, enable_interleave=True, num_model_chunks=num_model_chunk, use_zbv=True\r\n    )\r\n\r\n    h, a, s = 4096, 32, 1024\r\n    mem_f = 34 * h + 5 * a * s\r\n    mem_w = -32 * h\r\n    mem_b = -mem_w - mem_f\r\n    graph = PipelineGraph(\r\n        n_stage=pp_size,\r\n        n_micro=num_microbatch,\r\n        f_cost=1,\r\n        b_cost=1,\r\n        w_cost=1,\r\n        c_cost=1,\r\n        f_mem=mem_f,\r\n        b_mem=mem_b,\r\n        w_mem=mem_w,\r\n        # max_mem=mem_f * (p * 2 + m_offset),\r\n    )\r\n\r\n    zbv_schedule = graph.get_v_schedule()\r\n\r\n    scheduler = ZeroBubbleVPipeScheduler(\r\n        schedule=zbv_schedule,  # hint: send whole schedule or local schedule only ?\r\n        stage_manager=stage_manager,\r\n        num_model_chunks=num_model_chunk,\r\n        num_microbatch=num_microbatch,\r\n        overlap_p2p=False,\r\n    )\r\n\r\n    # init loss func\r\n    def criterion(x, *args, **kwargs):\r\n        x = x[\"hidden_states\"]\r\n        return (x * x).mean()\r\n\r\n    def criterion_base(x, *args, **kwargs):\r\n        return (x * x).mean()\r\n\r\n    # init model and input\r\n    batch_size = test_config[\"batch_size\"]\r\n    num_layers = 8\r\n    assert num_layers % num_model_chunk == 0, f\"Model with {num_layers} layer can not dist on {num_model_chunk} chunk\"\r\n    in_dim = out_dim = 1024\r\n    before_init_memory = torch.cuda.memory_allocated() / 1024**3\r\n    print(f\"Before init Model: {before_init_memory :.3f} GB on device {stage_manager.get_rank()};\")\r\n    model = MlpModel(in_dim=in_dim, out_dim=out_dim, num_layers=num_layers).to(rank)\r\n    data_iter = {\"data\": torch.rand(batch_size, in_dim, out_dim, requires_grad=True).to(rank)}\r\n    input_base = {k: v.clone() for k, v in data_iter.items()}\r\n    model_base = deepcopy(model)\r\n    model_pp = deepcopy(model)\r\n    layers_per_stage = stage_manager.distribute_layers(len(model.layers))\r\n    stage_manager.stage_indices = stage_manager.get_stage_index(layers_per_stage)\r\n\r\n    model_pp._forward = model_pp.forward\r\n\r\n    model_pp.forward = partial(model_pp._forward, stage_mgr=stage_manager)\r\n\r\n    # init optimizer\r\n    optimizer_base = torch.optim.SGD(model_base.parameters(), momentum=0.1, lr=1e-5)\r\n    optimizer_pp = OptimizerWrapper(torch.optim.SGD(model_pp.parameters(), momentum=0.1, lr=1e-5))\r\n\r\n    after_init_memory = torch.cuda.memory_allocated() / 1024**3\r\n    print(f\"After init Model & input: {after_init_memory :.5f} GB on device {stage_manager.get_rank()};\")\r\n\r\n    torch.cuda.synchronize()\r\n    result = scheduler.forward_backward_step(\r\n        model_chunk=model_pp,\r\n        data_iter=iter([data_iter]),\r\n        criterion=criterion,\r\n        optimizer=optimizer_pp,\r\n        return_loss=True,\r\n        return_outputs=True,\r\n    )\r\n\r\n    optimizer_pp.step()\r\n\r\n    after_pp_step_memory = torch.cuda.memory_allocated() / 1024**3\r\n\r\n    # assert memory\r\n    if rank != 0:\r\n        # w.grad: hid_dim * hid_dim * microbatch * 4(fp32) * 2 (2 layer in each stage) / 1024**3\r\n        # output: hid_dim * hid_dim * microbatch * 4(fp32) / 1024**3\r\n        # optim: state hid_dim * hid_dim * 4(fp32) * 2 (2 layer in each stage) / 1024**3\r\n        print(\r\n            f\" num_microbatch {num_microbatch} rank {rank}: {(after_pp_step_memory - after_init_memory)} <= {(in_dim * in_dim * 4 * 5 * batch_size / 1024**3)}\"\r\n        )\r\n        assert (after_pp_step_memory - after_init_memory) <= (in_dim * in_dim * 4 * 5 * batch_size / 1024**3)\r\n    else:\r\n        # rank0 will also hold output;\r\n        print(\r\n            f\" num_microbatch {num_microbatch} rank {rank}: {round((after_pp_step_memory - after_init_memory), 5)} <= {round((in_dim * in_dim * 4 * 5 * batch_size / 1024**3 + batch_size * in_dim * in_dim * 4 / 1024**3), 5)}\"\r\n        )\r\n        assert round((after_pp_step_memory - after_init_memory), 5) <= round(\r\n            (in_dim * in_dim * 4 * 5 * batch_size / 1024**3 + batch_size * in_dim * in_dim * 4 / 1024**3), 5\r\n        )\r\n\r\n    ##########################\r\n    # Fwd bwd for base\r\n    ##########################\r\n    # fwd & bwd\r\n    # output_base = model_base(input_base[\"data\"])\r\n    output_base = model_base.forward(data=input_base[\"data\"])\r\n    loss_base = criterion_base(output_base)\r\n    loss_base.backward()\r\n    optimizer_base.step()\r\n\r\n    ##########################\r\n    # assert loss & output\r\n    ##########################\r\n    # only chunk 1 stage 0 hold loss and output\r\n    if rank == 0:\r\n        assert_close(result[\"loss\"], loss_base)\r\n        assert_close(result[\"outputs\"][\"hidden_states\"], output_base)\r\n\r\n    # ##########################\r\n    # # assert weight & optim state\r\n    # ##########################\r\n    optim_base_state = optimizer_base.state_dict()[\"state\"]\r\n    optim_pp_state = optimizer_pp.state_dict()[\"state\"]\r\n    optim_base_param_groups = optimizer_base.state_dict()[\"param_groups\"][0]\r\n    optim_pp_param_groups = optimizer_pp.state_dict()[\"param_groups\"][0]\r\n\r\n    if rank == 0:\r\n        # layer 0\r\n        assert_close(model_pp.layers[0].weight, model_base.layers[0].weight)\r\n        assert_close(model_pp.layers[0].weight.grad, model_base.layers[0].weight.grad)\r\n        assert_close(optim_pp_state[0][\"momentum_buffer\"], optim_base_state[0][\"momentum_buffer\"])\r\n        # layer 7\r\n        assert_close(model_pp.layers[7].weight, model_base.layers[7].weight)\r\n        assert_close(model_pp.layers[7].weight.grad, model_base.layers[7].weight.grad)\r\n        assert_close(optim_pp_state[7][\"momentum_buffer\"], optim_base_state[7][\"momentum_buffer\"])\r\n    if rank == 1:\r\n        # layer 1\r\n        assert_close(model_pp.layers[1].weight, model_base.layers[1].weight)\r\n        assert_close(model_pp.layers[1].weight.grad, model_base.layers[1].weight.grad)\r\n        assert_close(optim_pp_state[1][\"momentum_buffer\"], optim_base_state[1][\"momentum_buffer\"])\r\n        # layer 6\r\n        assert_close(model_pp.layers[6].weight, model_base.layers[6].weight)\r\n        assert_close(model_pp.layers[6].weight.grad, model_base.layers[6].weight.grad)\r\n        assert_close(optim_pp_state[6][\"momentum_buffer\"], optim_base_state[6][\"momentum_buffer\"])\r\n    if rank == 2:\r\n        # layer 2\r\n        assert_close(model_pp.layers[2].weight, model_base.layers[2].weight)\r\n        assert_close(model_pp.layers[2].weight.grad, model_base.layers[2].weight.grad)\r\n        assert_close(optim_pp_state[2][\"momentum_buffer\"], optim_base_state[2][\"momentum_buffer\"])\r\n        # layer 5\r\n        assert_close(model_pp.layers[5].weight, model_base.layers[5].weight)\r\n        assert_close(model_pp.layers[5].weight.grad, model_base.layers[5].weight.grad)\r\n        assert_close(optim_pp_state[5][\"momentum_buffer\"], optim_base_state[5][\"momentum_buffer\"])\r\n    if rank == 3:\r\n        # layer 3\r\n        assert_close(model_pp.layers[3].weight, model_base.layers[3].weight)\r\n        assert_close(model_pp.layers[3].weight.grad, model_base.layers[3].weight.grad)\r\n        assert_close(optim_pp_state[3][\"momentum_buffer\"], optim_base_state[3][\"momentum_buffer\"])\r\n        # layer 4\r\n        assert_close(model_pp.layers[4].weight, model_base.layers[4].weight)\r\n        assert_close(model_pp.layers[4].weight.grad, model_base.layers[4].weight.grad)\r\n        assert_close(optim_pp_state[4][\"momentum_buffer\"], optim_base_state[4][\"momentum_buffer\"])\r\n\r\n    # assert optim param_groups\r\n    assert_optim_param_groups(optim_base_param_groups, optim_pp_param_groups)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def run_mixtral_commom(config: Tuple[int, ...]):\r\n    Randomizer.reset_index()\r\n    stage, ep_size, pp_size, tp_size, sp_size = config\r\n    world_size = dist.get_world_size()\r\n    rank = dist.get_rank()\r\n    dtype, precision = torch.bfloat16, \"bf16\"\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    plugin = MoeHybridParallelPlugin(\r\n        pp_size=pp_size,\r\n        num_microbatches=pp_size,\r\n        tp_size=tp_size,\r\n        sp_size=sp_size,\r\n        ep_size=ep_size,\r\n        zero_stage=stage,\r\n        enable_sequence_parallelism=sp_size > 1,\r\n        sequence_parallelism_mode=\"all_to_all\" if sp_size > 1 else None,\r\n        overlap_communication=False,\r\n        initial_scale=1,\r\n        precision=precision,\r\n        find_unused_parameters=True,\r\n    )\r\n    dp_size = plugin.dp_size\r\n\r\n    booster = Booster(plugin=plugin)\r\n\r\n    assert pp_size <= NUM_LAYERS, \"pp_size should be less than or equal to NUM_LAYERS\"\r\n    config = MixtralConfig(\r\n        hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\r\n        intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\r\n        num_hidden_layers=NUM_LAYERS,\r\n        num_attention_heads=NUM_HEADS,\r\n        num_key_value_heads=NUM_HEADS,\r\n        num_local_experts=NUM_EXPERTS,\r\n        num_experts_per_tok=TOP_K,\r\n        attn_implementation=\"flash_attention_2\",\r\n    )\r\n\r\n    # init model with the same seed\r\n    seed_all(10086)\r\n\r\n    torch_model = MixtralModel(config).to(dtype).cuda()\r\n    torch_optimizer = torch.optim.SGD(torch_model.parameters(), lr=1)\r\n\r\n    parallel_model = deepcopy(torch_model)\r\n    parallel_optimizer = torch.optim.SGD(parallel_model.parameters(), lr=1)\r\n    parallel_model, parallel_optimizer, _, _, _ = booster.boost(parallel_model, parallel_optimizer)\r\n\r\n    # create different input along dp axis\r\n    seed_all(1453 + rank)\r\n\r\n    torch_model.train()\r\n    parallel_model.train()\r\n    for _ in range(2):\r\n        # gen random input\r\n        input_embeddings = torch.rand(\r\n            NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\r\n        ).cuda()\r\n        dist.all_reduce(\r\n            input_embeddings, group=plugin.pp_group\r\n        )  # pp inputs except the first stage doesn't matter, but need to be replicate for torch model check\r\n\r\n        dist.all_reduce(input_embeddings, group=plugin.tp_group)  # tp group duplicate input\r\n        dist.all_reduce(input_embeddings, group=plugin.sp_group)  # sp group duplicate input\r\n\r\n        # run the model with hybrid parallel\r\n        if booster.plugin.stage_manager is not None:\r\n            # for test with pp\r\n            data_iter = iter([{\"inputs_embeds\": input_embeddings}])\r\n            sharded_output = booster.execute_pipeline(\r\n                data_iter,\r\n                parallel_model,\r\n                lambda x, y: x.last_hidden_state.mean(),\r\n                parallel_optimizer,\r\n                return_loss=True,\r\n                return_outputs=True,\r\n            )\r\n            if booster.plugin.stage_manager.is_last_stage():\r\n                parallel_output = sharded_output[\"loss\"]\r\n            else:\r\n                parallel_output = torch.tensor(12345.0, device=\"cuda\")\r\n\r\n            # broadcast along pp axis\r\n            dist.broadcast(\r\n                parallel_output, src=dist.get_process_group_ranks(plugin.pp_group)[-1], group=plugin.pp_group\r\n            )\r\n        else:\r\n            # for test without pp\r\n            parallel_output = parallel_model(inputs_embeds=input_embeddings.to(dtype)).last_hidden_state.mean()\r\n            parallel_optimizer.backward(parallel_output)\r\n        parallel_optimizer.step()\r\n        parallel_optimizer.zero_grad()\r\n        dist.all_reduce(parallel_output, group=plugin.dp_group)\r\n\r\n        # ===================================================================================\r\n        # run normal model with all dp(different) inputs\r\n        all_inputs = [torch.empty_like(input_embeddings) for _ in range(dp_size)]\r\n        dist.all_gather(all_inputs, input_embeddings, group=plugin.dp_group)\r\n        torch_output_sum = 0\r\n        for input_data_ in all_inputs:\r\n            torch_output = torch_model(inputs_embeds=input_data_.to(dtype)).last_hidden_state.mean()\r\n            torch_output.backward()\r\n            torch_output_sum += torch_output.detach()\r\n        # avg dp grads follows zero optimizer\r\n        for p in torch_model.parameters():\r\n            if p.grad is not None:\r\n                p.grad /= dp_size\r\n        torch_optimizer.step()\r\n        torch_optimizer.zero_grad()\r\n\r\n        assert_loose_close(parallel_output, torch_output_sum, dtype=dtype)\r\n\r\n    # use checkpoint to load sharded zero model\r\n    model_dir = \"./test_mixtral\"\r\n    if rank == world_size - 1:\r\n        os.makedirs(model_dir, exist_ok=True)\r\n\r\n    dist.barrier()\r\n    booster.save_model(parallel_model, model_dir, shard=True)\r\n    dist.barrier()\r\n\r\n    saved_model = MixtralModel.from_pretrained(model_dir).cuda().to(dtype)\r\n    check_model_equal(torch_model, saved_model, dtype=dtype)\r\n    dist.barrier()\r\n\r\n    if rank == world_size - 1:\r\n        shutil.rmtree(model_dir)\r\n\r\n    print(f\"rank {dist.get_rank()} test passed\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def exam_zero_1_torch_ddp(dtype: torch.dtype, master_weights: bool, extra_dp_size: int):\r\n    \"\"\"\r\n    In this test, two pairs of model and optimizers are created.\r\n    1. zero: use sharded optimizer and fp16 parameters\r\n    2. torch: use torch DDP and fp32 parameters\r\n\r\n    We feed these two sets of models with the same input and check if the\r\n    differences in model output and updated parameters are within tolerance.\r\n    \"\"\"\r\n    if extra_dp_size > 1 and dtype != torch.bfloat16:\r\n        return\r\n    if extra_dp_size > 1:\r\n        pg_mesh = ProcessGroupMesh(extra_dp_size, dist.get_world_size() // extra_dp_size)\r\n        extra_dp_group = pg_mesh.get_group_along_axis(0)\r\n        dp_group = pg_mesh.get_group_along_axis(1)\r\n    else:\r\n        extra_dp_group = None\r\n        dp_group = None\r\n    local_rank = torch.distributed.get_rank()\r\n    seed_all(1453)\r\n\r\n    # create models\r\n    torch_model = MlpModel().cuda().to(dtype)\r\n    zero_model = copy.deepcopy(torch_model).to(dtype)\r\n\r\n    torch_model = DDP(torch_model.cuda(), static_graph=True).cuda()\r\n\r\n    # create optimizer\r\n    zero_optimizer = torch.optim.SGD(zero_model.parameters(), lr=1)\r\n\r\n    # we only test stage 1 here\r\n    # in `check_sharded_param_consistency.py`, we will test whether\r\n    # level 1 and 2 will produce exactly the same results\r\n    zero_optimizer = LowLevelZeroOptimizer(\r\n        zero_optimizer,\r\n        overlap_communication=True,\r\n        initial_scale=1,\r\n        reduce_bucket_size=1024 * 1024,\r\n        master_weights=master_weights,\r\n        dp_process_group=dp_group,\r\n        extra_dp_group=extra_dp_group,\r\n    )\r\n\r\n    torch_optimizer = torch.optim.SGD(torch_model.parameters(), lr=1)\r\n\r\n    seed_all(1453 + local_rank)\r\n\r\n    for _ in range(2):\r\n        # create\r\n        input_data = torch.rand(32, 123).cuda().to(dtype)\r\n\r\n        # zero-dp forward\r\n        zero_output = zero_model(input_data)\r\n\r\n        # torch-ddp forward\r\n        torch_output = torch_model(input_data)\r\n        loose_close(zero_output, torch_output, dtype=dtype)\r\n\r\n        # zero-dp backward\r\n        zero_optimizer.backward(zero_output.mean())\r\n\r\n        # torch-ddp backward\r\n        torch_output.mean().backward()\r\n\r\n        # check grad\r\n        for (n, p), z1p in zip(torch_model.named_parameters(), zero_model.parameters()):\r\n            zero_grad = zero_optimizer.get_param_grad(z1p)\r\n            if p.grad is None:\r\n                assert zero_grad is None\r\n                continue\r\n            loose_close(p.grad, zero_grad, dtype=dtype)\r\n\r\n        # zero-dp step\r\n        zero_optimizer.step()\r\n\r\n        # torch ddp step\r\n        torch_optimizer.step()\r\n\r\n        zero_optimizer._force_wait_all_gather()\r\n\r\n        # check updated param\r\n        for (n, p), z1p in zip(torch_model.named_parameters(), zero_model.parameters()):\r\n            loose_close(p, z1p, dtype=dtype)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def rename_columns(self, column_mapping: Dict[str, str], new_fingerprint: Optional[str] = None) -> \"Dataset\":\r\n        \"\"\"\r\n        Rename several columns in the dataset, and move the features associated to the original columns under\r\n        the new column names.\r\n\r\n        Args:\r\n            column_mapping (`Dict[str, str]`):\r\n                A mapping of columns to rename to their new names\r\n            new_fingerprint (`str`, *optional*):\r\n                The new fingerprint of the dataset after transform.\r\n                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\r\n\r\n        Returns:\r\n            [`Dataset`]: A copy of the dataset with renamed columns\r\n\r\n        Example:\r\n\r\n        ```py\r\n        >>> from datasets import load_dataset\r\n        >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\r\n        >>> ds = ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\r\n        Dataset({\r\n            features: ['text_new', 'label_new'],\r\n            num_rows: 1066\r\n        })\r\n        ```\r\n        \"\"\"\r\n        dataset = copy.deepcopy(self)\r\n\r\n        extra_columns = set(column_mapping.keys()) - set(dataset.column_names)\r\n        if extra_columns:\r\n            raise ValueError(\r\n                f\"Original column names {extra_columns} not in the dataset. \"\r\n                f\"Current columns in the dataset: {dataset._data.column_names}\"\r\n            )\r\n\r\n        number_of_duplicates_in_new_columns = len(column_mapping.values()) - len(set(column_mapping.values()))\r\n        if number_of_duplicates_in_new_columns != 0:\r\n            raise ValueError(\r\n                \"New column names must all be different, but this column mapping \"\r\n                f\"has {number_of_duplicates_in_new_columns} duplicates\"\r\n            )\r\n\r\n        empty_new_columns = [new_col for new_col in column_mapping.values() if not new_col]\r\n        if empty_new_columns:\r\n            raise ValueError(f\"New column names {empty_new_columns} are empty.\")\r\n\r\n        def rename(columns):\r\n            return [column_mapping[col] if col in column_mapping else col for col in columns]\r\n\r\n        new_column_names = rename(self._data.column_names)\r\n        if self._format_columns is not None:\r\n            dataset._format_columns = rename(self._format_columns)\r\n\r\n        dataset._info.features = Features(\r\n            {\r\n                column_mapping[col] if col in column_mapping else col: feature\r\n                for col, feature in (self._info.features or {}).items()\r\n            }\r\n        )\r\n\r\n        dataset._data = dataset._data.rename_columns(new_column_names)\r\n        dataset._data = update_metadata_with_features(dataset._data, dataset.features)\r\n        dataset._fingerprint = new_fingerprint\r\n        return dataset",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def map(\r\n        self,\r\n        function: Optional[Callable] = None,\r\n        with_indices: bool = False,\r\n        with_rank: bool = False,\r\n        input_columns: Optional[Union[str, List[str]]] = None,\r\n        batched: bool = False,\r\n        batch_size: Optional[int] = 1000,\r\n        drop_last_batch: bool = False,\r\n        remove_columns: Optional[Union[str, List[str]]] = None,\r\n        keep_in_memory: bool = False,\r\n        load_from_cache_file: Optional[bool] = None,\r\n        cache_file_name: Optional[str] = None,\r\n        writer_batch_size: Optional[int] = 1000,\r\n        features: Optional[Features] = None,\r\n        disable_nullable: bool = False,\r\n        fn_kwargs: Optional[dict] = None,\r\n        num_proc: Optional[int] = None,\r\n        suffix_template: str = \"_{rank:05d}_of_{num_proc:05d}\",\r\n        new_fingerprint: Optional[str] = None,\r\n        desc: Optional[str] = None,\r\n    ) -> \"Dataset\":\r\n        \"\"\"\r\n        Apply a function to all the examples in the table (individually or in batches) and update the table.\r\n        If your function returns a column that already exists, then it overwrites it.\r\n\r\n        You can specify whether the function should be batched or not with the `batched` parameter:\r\n\r\n        - If batched is `False`, then the function takes 1 example in and should return 1 example.\r\n          An example is a dictionary, e.g. `{\"text\": \"Hello there !\"}`.\r\n        - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\r\n          A batch is a dictionary, e.g. a batch of 1 example is `{\"text\": [\"Hello there !\"]}`.\r\n        - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\r\n          Note that the last batch may have less than `n` examples.\r\n          A batch is a dictionary, e.g. a batch of `n` examples is `{\"text\": [\"Hello there !\"] * n}`.\r\n\r\n        Args:\r\n            function (`Callable`): Function with one of the following signatures:\r\n\r\n                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\r\n                - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\r\n                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\r\n                - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\r\n\r\n                For advanced usage, the function can also return a `pyarrow.Table`.\r\n                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\r\n                If no function is provided, default to identity function: `lambda x: x`.\r\n            with_indices (`bool`, defaults to `False`):\r\n                Provide example indices to `function`. Note that in this case the\r\n                signature of `function` should be `def function(example, idx[, rank]): ...`.\r\n            with_rank (`bool`, defaults to `False`):\r\n                Provide process rank to `function`. Note that in this case the\r\n                signature of `function` should be `def function(example[, idx], rank): ...`.\r\n            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\r\n                The columns to be passed into `function`\r\n                as positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\r\n            batched (`bool`, defaults to `False`):\r\n                Provide batch of examples to `function`.\r\n            batch_size (`int`, *optional*, defaults to `1000`):\r\n                Number of examples per batch provided to `function` if `batched=True`.\r\n                If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\r\n            drop_last_batch (`bool`, defaults to `False`):\r\n                Whether a last batch smaller than the batch_size should be\r\n                dropped instead of being processed by the function.\r\n            remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\r\n                Remove a selection of columns while doing the mapping.\r\n                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\r\n                columns with names in `remove_columns`, these columns will be kept.\r\n            keep_in_memory (`bool`, defaults to `False`):\r\n                Keep the dataset in memory instead of writing it to a cache file.\r\n            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\r\n                If a cache file storing the current computation from `function`\r\n                can be identified, use it instead of recomputing.\r\n            cache_file_name (`str`, *optional*, defaults to `None`):\r\n                Provide the name of a path for the cache file. It is used to store the\r\n                results of the computation instead of the automatically generated cache file name.\r\n            writer_batch_size (`int`, defaults to `1000`):\r\n                Number of rows per write operation for the cache file writer.\r\n                This value is a good trade-off between memory usage during the processing, and processing speed.\r\n                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\r\n            features (`Optional[datasets.Features]`, defaults to `None`):\r\n                Use a specific Features to store the cache file\r\n                instead of the automatically generated one.\r\n            disable_nullable (`bool`, defaults to `False`):\r\n                Disallow null values in the table.\r\n            fn_kwargs (`Dict`, *optional*, defaults to `None`):\r\n                Keyword arguments to be passed to `function`.\r\n            num_proc (`int`, *optional*, defaults to `None`):\r\n                Max number of processes when generating cache. Already cached shards are loaded sequentially.\r\n            suffix_template (`str`):\r\n                If `cache_file_name` is specified, then this suffix\r\n                will be added at the end of the base name of each. Defaults to `\"_{rank:05d}_of_{num_proc:05d}\"`. For example, if `cache_file_name` is \"processed.arrow\", then for\r\n                `rank=1` and `num_proc=4`, the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix.\r\n            new_fingerprint (`str`, *optional*, defaults to `None`):\r\n                The new fingerprint of the dataset after transform.\r\n                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\r\n            desc (`str`, *optional*, defaults to `None`):\r\n                Meaningful description to be displayed alongside with the progress bar while mapping examples.\r\n\r\n        Example:\r\n\r\n        ```py\r\n        >>> from datasets import load_dataset\r\n        >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\r\n        >>> def add_prefix(example):\r\n        ...     example[\"text\"] = \"Review: \" + example[\"text\"]\r\n        ...     return example\r\n        >>> ds = ds.map(add_prefix)\r\n        >>> ds[0:3][\"text\"]\r\n        ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\r\n         'Review: the soundtrack alone is worth the price of admission .',\r\n         'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\r\n\r\n        # process a batch of examples\r\n        >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\r\n        # set number of processors\r\n        >>> ds = ds.map(add_prefix, num_proc=4)\r\n        ```\r\n        \"\"\"\r\n        if keep_in_memory and cache_file_name is not None:\r\n            raise ValueError(\"Please use either `keep_in_memory` or `cache_file_name` but not both.\")\r\n\r\n        if num_proc is not None and num_proc <= 0:\r\n            raise ValueError(\"num_proc must be an integer > 0.\")\r\n\r\n        # If the array is empty we do nothing (but we make sure to handle an empty indices mapping and remove the requested columns anyway)\r\n        if len(self) == 0:\r\n            if self._indices is not None:  # empty indices mapping\r\n                self = Dataset(\r\n                    self.data.slice(0, 0),\r\n                    info=self.info.copy(),\r\n                    split=self.split,\r\n                    fingerprint=new_fingerprint,\r\n                )\r\n            if remove_columns:\r\n                return self.remove_columns(remove_columns)\r\n            else:\r\n                return self\r\n\r\n        if function is None:\r\n            function = lambda x: x  # noqa: E731\r\n\r\n        if isinstance(input_columns, str):\r\n            input_columns = [input_columns]\r\n\r\n        if input_columns is not None:\r\n            missing_columns = set(input_columns) - set(self._data.column_names)\r\n            if missing_columns:\r\n                raise ValueError(\r\n                    f\"Input column {list(missing_columns)} not in the dataset. Current columns in the dataset: {self._data.column_names}\"\r\n                )\r\n\r\n        if isinstance(remove_columns, str):\r\n            remove_columns = [remove_columns]\r\n\r\n        if remove_columns is not None:\r\n            missing_columns = set(remove_columns) - set(self._data.column_names)\r\n            if missing_columns:\r\n                raise ValueError(\r\n                    f\"Column to remove {list(missing_columns)} not in the dataset. Current columns in the dataset: {self._data.column_names}\"\r\n                )\r\n\r\n        load_from_cache_file = load_from_cache_file if load_from_cache_file is not None else is_caching_enabled()\r\n\r\n        if fn_kwargs is None:\r\n            fn_kwargs = {}\r\n\r\n        if num_proc is not None and num_proc > len(self):\r\n            num_proc = len(self)\r\n            logger.warning(\r\n                f\"num_proc must be <= {len(self)}. Reducing num_proc to {num_proc} for dataset of size {len(self)}.\"\r\n            )\r\n\r\n        dataset_kwargs = {\r\n            \"shard\": self,\r\n            \"function\": function,\r\n            \"with_indices\": with_indices,\r\n            \"with_rank\": with_rank,\r\n            \"input_columns\": input_columns,\r\n            \"batched\": batched,\r\n            \"batch_size\": batch_size,\r\n            \"drop_last_batch\": drop_last_batch,\r\n            \"remove_columns\": remove_columns,\r\n            \"keep_in_memory\": keep_in_memory,\r\n            \"writer_batch_size\": writer_batch_size,\r\n            \"features\": features,\r\n            \"disable_nullable\": disable_nullable,\r\n            \"fn_kwargs\": fn_kwargs,\r\n        }\r\n\r\n        if new_fingerprint is None:\r\n            # we create a unique hash from the function,\r\n            # current dataset file and the mapping args\r\n            transform = format_transform_for_fingerprint(Dataset._map_single)\r\n            kwargs_for_fingerprint = format_kwargs_for_fingerprint(Dataset._map_single, (), dataset_kwargs)\r\n            kwargs_for_fingerprint[\"fingerprint_name\"] = \"new_fingerprint\"\r\n            new_fingerprint = update_fingerprint(self._fingerprint, transform, kwargs_for_fingerprint)\r\n        else:\r\n            validate_fingerprint(new_fingerprint)\r\n        dataset_kwargs[\"new_fingerprint\"] = new_fingerprint\r\n\r\n        if self.cache_files:\r\n            if cache_file_name is None:\r\n                cache_file_name = self._get_cache_file_path(new_fingerprint)\r\n        dataset_kwargs[\"cache_file_name\"] = cache_file_name\r\n\r\n        def load_processed_shard_from_cache(shard_kwargs):\r\n            \"\"\"Load a processed shard from cache if it exists, otherwise throw an error.\"\"\"\r\n            shard = shard_kwargs[\"shard\"]\r\n            # Check if we've already cached this computation (indexed by a hash)\r\n            if shard_kwargs[\"cache_file_name\"] is not None:\r\n                if os.path.exists(shard_kwargs[\"cache_file_name\"]) and load_from_cache_file:\r\n                    info = shard.info.copy()\r\n                    info.features = features\r\n                    return Dataset.from_file(shard_kwargs[\"cache_file_name\"], info=info, split=shard.split)\r\n            raise NonExistentDatasetError\r\n\r\n        num_shards = num_proc if num_proc is not None else 1\r\n        if batched and drop_last_batch:\r\n            pbar_total = len(self) // num_shards // batch_size * num_shards * batch_size\r\n        else:\r\n            pbar_total = len(self)\r\n\r\n        shards_done = 0\r\n        if num_proc is None or num_proc == 1:\r\n            transformed_dataset = None\r\n            try:\r\n                transformed_dataset = load_processed_shard_from_cache(dataset_kwargs)\r\n                logger.info(f\"Loading cached processed dataset at {dataset_kwargs['cache_file_name']}\")\r\n            except NonExistentDatasetError:\r\n                pass\r\n            if transformed_dataset is None:\r\n                with hf_tqdm(\r\n                    unit=\" examples\",\r\n                    total=pbar_total,\r\n                    desc=desc or \"Map\",\r\n                ) as pbar:\r\n                    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n                        if done:\r\n                            shards_done += 1\r\n                            logger.debug(f\"Finished processing shard number {rank} of {num_shards}.\")\r\n                            transformed_dataset = content\r\n                        else:\r\n                            pbar.update(content)\r\n            assert transformed_dataset is not None, \"Failed to retrieve the result from map\"\r\n            # update fingerprint if the dataset changed\r\n            if transformed_dataset._fingerprint != self._fingerprint:\r\n                transformed_dataset._fingerprint = new_fingerprint\r\n            return transformed_dataset\r\n        else:\r\n\r\n            def format_cache_file_name(\r\n                cache_file_name: Optional[str],\r\n                rank: Union[int, Literal[\"*\"]],  # noqa: F722\r\n            ) -> Optional[str]:\r\n                if not cache_file_name:\r\n                    return cache_file_name\r\n                sep = cache_file_name.rindex(\".\")\r\n                base_name, extension = cache_file_name[:sep], cache_file_name[sep:]\r\n                if isinstance(rank, int):\r\n                    cache_file_name = base_name + suffix_template.format(rank=rank, num_proc=num_proc) + extension\r\n                    logger.info(f\"Process #{rank} will write at {cache_file_name}\")\r\n                else:\r\n                    cache_file_name = (\r\n                        base_name\r\n                        + suffix_template.replace(\"{rank:05d}\", \"{rank}\").format(rank=rank, num_proc=num_proc)\r\n                        + extension\r\n                    )\r\n                return cache_file_name\r\n\r\n            def format_new_fingerprint(new_fingerprint: str, rank: int) -> str:\r\n                new_fingerprint = new_fingerprint + suffix_template.format(rank=rank, num_proc=num_proc)\r\n                validate_fingerprint(new_fingerprint)\r\n                return new_fingerprint\r\n\r\n            prev_env = deepcopy(os.environ)\r\n            # check if parallelism if off\r\n            # from https://github.com/huggingface/tokenizers/blob/bb668bc439dc34389b71dbb8ce0c597f15707b53/tokenizers/src/utils/parallelism.rs#L22\r\n            if prev_env.get(\"TOKENIZERS_PARALLELISM\", \"false\").lower() not in (\r\n                \"\",\r\n                \"off\",\r\n                \"false\",\r\n                \"f\",\r\n                \"no\",\r\n                \"n\",\r\n                \"0\",\r\n            ):\r\n                logger.warning(\"Setting TOKENIZERS_PARALLELISM=false for forked processes.\")\r\n            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n            shards = [\r\n                self.shard(num_shards=num_proc, index=rank, contiguous=True, keep_in_memory=keep_in_memory)\r\n                for rank in range(num_proc)\r\n            ]\r\n            kwargs_per_job = [\r\n                {\r\n                    **dataset_kwargs,\r\n                    \"shard\": shards[rank],\r\n                    \"cache_file_name\": format_cache_file_name(cache_file_name, rank),\r\n                    \"rank\": rank,\r\n                    \"offset\": sum(len(s) for s in shards[:rank]),\r\n                    \"new_fingerprint\": format_new_fingerprint(new_fingerprint, rank),\r\n                }\r\n                for rank in range(num_shards)\r\n            ]\r\n\r\n            transformed_shards = [None] * num_shards\r\n            for rank in range(num_shards):\r\n                try:\r\n                    transformed_shards[rank] = load_processed_shard_from_cache(kwargs_per_job[rank])\r\n                    kwargs_per_job[rank] = None\r\n                except NonExistentDatasetError:\r\n                    pass\r\n\r\n            kwargs_per_job = [kwargs for kwargs in kwargs_per_job if kwargs is not None]\r\n\r\n            # We try to create a pool with as many workers as dataset not yet cached.\r\n            if kwargs_per_job:\r\n                if len(kwargs_per_job) < num_shards:\r\n                    logger.info(\r\n                        f\"Reprocessing {len(kwargs_per_job)}/{num_shards} shards because some of them were missing from the cache.\"\r\n                    )\r\n                with Pool(len(kwargs_per_job)) as pool:\r\n                    os.environ = prev_env\r\n                    logger.info(f\"Spawning {num_proc} processes\")\r\n                    with hf_tqdm(\r\n                        unit=\" examples\",\r\n                        total=pbar_total,\r\n                        desc=(desc or \"Map\") + f\" (num_proc={num_proc})\",\r\n                    ) as pbar:\r\n                        for rank, done, content in iflatmap_unordered(\r\n                            pool, Dataset._map_single, kwargs_iterable=kwargs_per_job\r\n                        ):\r\n                            if done:\r\n                                shards_done += 1\r\n                                logger.debug(f\"Finished processing shard number {rank} of {num_shards}.\")\r\n                                transformed_shards[rank] = content\r\n                            else:\r\n                                pbar.update(content)\r\n                # Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\r\n                for kwargs in kwargs_per_job:\r\n                    del kwargs[\"shard\"]\r\n            else:\r\n                logger.info(f\"Loading cached processed dataset at {format_cache_file_name(cache_file_name, '*')}\")\r\n            assert None not in transformed_shards, (\r\n                f\"Failed to retrieve results from map: result list {transformed_shards} still contains None - at least one worker failed to return its results\"\r\n            )\r\n            logger.info(f\"Concatenating {num_proc} shards\")\r\n            result = _concatenate_map_style_datasets(transformed_shards)\r\n            # update fingerprint if the dataset changed\r\n            if any(\r\n                transformed_shard._fingerprint != shard._fingerprint\r\n                for transformed_shard, shard in zip(transformed_shards, shards)\r\n            ):\r\n                result._fingerprint = new_fingerprint\r\n            else:\r\n                result._fingerprint = self._fingerprint\r\n            return result",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def main(args):\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_dir=logging_dir,\r\n    )\r\n\r\n    if args.report_to == \"wandb\":\r\n        wandb_init = {\r\n            \"wandb\": {\r\n                \"name\": args.wandb_run_name,\r\n                \"mode\": \"online\",\r\n            }\r\n        }\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n\r\n    logger.info(accelerator.state, main_process_only=False)\r\n\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDIMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionNewModel.from_pretrained(\r\n        args.pretrained_model_name_or_path,\r\n        subfolder=\"unet\",\r\n        revision=args.revision,\r\n    )\r\n\r\n    controlnet = ControlNetModel()\r\n\r\n    if args.controlnet_model_name_or_path != \"\":\r\n        logger.info(f\"Loading existing controlnet weights from {args.controlnet_model_name_or_path}\")\r\n        controlnet.load_state_dict(torch.load(args.controlnet_model_name_or_path))\r\n\r\n    if args.use_boft:\r\n        config = BOFTConfig(\r\n            boft_block_size=args.boft_block_size,\r\n            boft_block_num=args.boft_block_num,\r\n            boft_n_butterfly_factor=args.boft_n_butterfly_factor,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            boft_dropout=args.boft_dropout,\r\n            bias=args.boft_bias,\r\n        )\r\n        unet = get_peft_model(unet, config)\r\n        unet.print_trainable_parameters()\r\n\r\n    vae.requires_grad_(False)\r\n    controlnet.requires_grad_(True)\r\n\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n\r\n    unet.train()\r\n    controlnet.train()\r\n\r\n    if args.train_text_encoder and args.use_boft:\r\n        config = BOFTConfig(\r\n            boft_block_size=args.boft_block_size,\r\n            boft_block_num=args.boft_block_num,\r\n            boft_n_butterfly_factor=args.boft_n_butterfly_factor,\r\n            target_modules=TEXT_ENCODER_TARGET_MODULES,\r\n            boft_dropout=args.boft_dropout,\r\n            bias=args.boft_bias,\r\n        )\r\n        text_encoder = get_peft_model(text_encoder, config, adapter_name=args.wandb_run_name)\r\n        text_encoder.print_trainable_parameters()\r\n\r\n    if args.train_text_encoder:\r\n        text_encoder.train()\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move unet, vae and text_encoder to device and cast to weight_dtype\r\n    unet.to(accelerator.device, dtype=weight_dtype)\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    controlnet.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            import xformers\r\n\r\n            xformers_version = version.parse(xformers.__version__)\r\n            if xformers_version == version.parse(\"0.0.16\"):\r\n                logger.warning(\r\n                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\r\n                )\r\n            unet.enable_xformers_memory_efficient_attention()\r\n            controlnet.enable_xformers_memory_efficient_attention()\r\n            if args.train_text_encoder and not (args.use_lora or args.use_boft or args.use_oft):\r\n                text_encoder.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        controlnet.enable_gradient_checkpointing()\r\n        unet.enable_gradient_checkpointing()\r\n        if args.train_text_encoder and not (args.use_lora or args.use_boft or args.use_oft):\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Check that all trainable models are in full precision\r\n    low_precision_error_string = (\r\n        \" Please make sure to always have all model weights in full float32 precision when starting training - even if\"\r\n        \" doing mixed precision training, copy of the weights should still be float32.\"\r\n    )\r\n\r\n    if accelerator.unwrap_model(controlnet).dtype != torch.float32:\r\n        raise ValueError(\r\n            f\"Controlnet loaded as datatype {accelerator.unwrap_model(controlnet).dtype}. {low_precision_error_string}\"\r\n        )\r\n\r\n    if accelerator.unwrap_model(unet).dtype != torch.float32:\r\n        raise ValueError(\r\n            f\"UNet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}\"\r\n        )\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    params_to_optimize = [param for param in controlnet.parameters() if param.requires_grad]\r\n    params_to_optimize += [param for param in unet.parameters() if param.requires_grad]\r\n\r\n    if args.train_text_encoder:\r\n        params_to_optimize += [param for param in text_encoder.parameters() if param.requires_grad]\r\n\r\n    # Optimizer creation\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Load the dataset\r\n    train_dataset = make_dataset(args, tokenizer, accelerator, \"train\")\r\n    val_dataset = make_dataset(args, tokenizer, accelerator, \"test\")\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        shuffle=True,\r\n        collate_fn=collate_fn,\r\n        batch_size=args.train_batch_size,\r\n        num_workers=args.dataloader_num_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n        controlnet, optimizer, train_dataloader, lr_scheduler\r\n    )\r\n\r\n    if args.train_text_encoder:\r\n        text_encoder = accelerator.prepare(text_encoder)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(args.wandb_project_name, config=vars(args), init_kwargs=wandb_init)\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            if \"checkpoint-current\" in dirs:\r\n                path = \"checkpoint-current\"\r\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\") and d.endswith(\"0\")]\r\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n\r\n            else:\r\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n                path = dirs[-1] if len(dirs) > 0 else None\r\n\r\n        if path is None:\r\n            accelerator.print(\r\n                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\r\n            )\r\n            args.resume_from_checkpoint = None\r\n            initial_global_step = 0\r\n        else:\r\n            accelerator.print(f\"Resuming from checkpoint {path}\")\r\n            accelerator.load_state(os.path.join(args.output_dir, path))\r\n            if path.split(\"-\")[1] == \"current\":\r\n                global_step = int(dirs[-1].split(\"-\")[1])\r\n            else:\r\n                global_step = int(path.split(\"-\")[1])\r\n\r\n            initial_global_step = global_step\r\n            resume_global_step = global_step * args.gradient_accumulation_steps\r\n            first_epoch = global_step // num_update_steps_per_epoch\r\n            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\r\n    else:\r\n        initial_global_step = 0\r\n\r\n    progress_bar = tqdm(\r\n        range(0, args.max_train_steps),\r\n        initial=initial_global_step,\r\n        desc=\"Steps\",\r\n        disable=not accelerator.is_local_main_process,\r\n    )\r\n\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        with TorchTracemalloc() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(controlnet), accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * vae.config.scaling_factor\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    controlnet_image = batch[\"conditioning_pixel_values\"].to(dtype=weight_dtype)\r\n\r\n                    # Get the guided hint for the UNet (320 dim)\r\n                    guided_hint = controlnet(\r\n                        controlnet_cond=controlnet_image,\r\n                    )\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(\r\n                        noisy_latents,\r\n                        timesteps,\r\n                        guided_hint=guided_hint,\r\n                        encoder_hidden_states=encoder_hidden_states,\r\n                    ).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(controlnet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else itertools.chain(\r\n                                controlnet.parameters(),\r\n                            )\r\n                        )\r\n\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad(set_to_none=args.set_grads_to_none)\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                    step_save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\r\n\r\n                    if accelerator.is_main_process:\r\n                        if global_step % args.validation_steps == 0 or global_step == 1:\r\n                            logger.info(f\"Running validation... \\n Generating {args.num_validation_images} images.\")\r\n                            logger.info(\"Running validation... \")\r\n\r\n                            with torch.no_grad():\r\n                                log_validation(val_dataset, text_encoder, unet, controlnet, args, accelerator)\r\n\r\n                        if global_step % args.checkpointing_steps == 0:\r\n                            save_adaptor(accelerator, step_save_path, {\"controlnet\": controlnet, \"unet\": unet})\r\n\r\n                            # save text_encoder if any\r\n                            if args.train_text_encoder:\r\n                                save_adaptor(accelerator, step_save_path, {\"text_encoder\": text_encoder})\r\n\r\n                            accelerator.save_state(step_save_path)\r\n\r\n                            logger.info(f\"Saved {global_step} state to {step_save_path}\")\r\n                            logger.info(f\"Saved current state to {step_save_path}\")\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    accelerator.wait_for_everyone()\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    validation_prompts = list(filter(None, args.validation_prompt[0].split(\".\")))\r\n\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_dir=accelerator_project_config,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        import wandb\r\n\r\n        wandb_init = {\r\n            \"wandb\": {\r\n                \"name\": args.wandb_run_name,\r\n                \"mode\": \"online\",\r\n            }\r\n        }\r\n\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    global_seed = hash(args.wandb_run_name) % (2**32)\r\n    set_seed(global_seed)\r\n\r\n    # Generate class images if prior preservation is enabled.\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            if args.prior_generation_precision == \"fp32\":\r\n                torch_dtype = torch.float32\r\n            elif args.prior_generation_precision == \"fp16\":\r\n                torch_dtype = torch.float16\r\n            elif args.prior_generation_precision == \"bf16\":\r\n                torch_dtype = torch.bfloat16\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            if args.hub_model_id is None:\r\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\r\n            else:\r\n                repo_name = args.hub_model_id\r\n            repo = Repository(args.output_dir, clone_from=repo_name)  # noqa: F841\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDIMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n\r\n    if args.use_boft:\r\n        config = BOFTConfig(\r\n            boft_block_size=args.boft_block_size,\r\n            boft_block_num=args.boft_block_num,\r\n            boft_n_butterfly_factor=args.boft_n_butterfly_factor,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            boft_dropout=args.boft_dropout,\r\n            bias=args.boft_bias,\r\n        )\r\n        unet = get_peft_model(unet, config, adapter_name=args.wandb_run_name)\r\n        unet.print_trainable_parameters()\r\n\r\n    vae.requires_grad_(False)\r\n    unet.train()\r\n\r\n    if args.train_text_encoder and args.use_boft:\r\n        config = BOFTConfig(\r\n            boft_block_size=args.boft_block_size,\r\n            boft_block_num=args.boft_block_num,\r\n            boft_n_butterfly_factor=args.boft_n_butterfly_factor,\r\n            target_modules=TEXT_ENCODER_TARGET_MODULES,\r\n            boft_dropout=args.boft_dropout,\r\n            bias=args.boft_bias,\r\n        )\r\n        text_encoder = get_peft_model(text_encoder, config, adapter_name=args.wandb_run_name)\r\n        text_encoder.print_trainable_parameters()\r\n        text_encoder.train()\r\n    else:\r\n        text_encoder.requires_grad_(False)\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move unet, vae and text_encoder to device and cast to weight_dtype\r\n    unet.to(accelerator.device, dtype=weight_dtype)\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        # below fails when using boft so commenting it out\r\n        if args.train_text_encoder and not args.use_boft:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    # Optimizer creation\r\n    params_to_optimize = [param for param in unet.parameters() if param.requires_grad]\r\n\r\n    if args.train_text_encoder:\r\n        params_to_optimize += [param for param in text_encoder.parameters() if param.requires_grad]\r\n\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Download the official dreambooth dataset from the official repository: https://github.com/google/dreambooth.git\r\n    data_path = os.path.join(os.getcwd(), \"data\", \"dreambooth\")\r\n    if not os.path.exists(data_path):\r\n        os.makedirs(os.path.join(os.getcwd(), \"data\"), exist_ok=True)\r\n        os.system(f\"git clone https://github.com/google/dreambooth.git '{data_path}'\")\r\n\r\n    # Dataset and DataLoaders creation:\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=args.train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\r\n        num_workers=args.num_dataloader_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move vae and text_encoder to device and cast to weight_dtype\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(args.wandb_project_name, config=vars(args), init_kwargs=wandb_init)\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1] if len(dirs) > 0 else None\r\n        accelerator.print(f\"Resuming from checkpoint {path}\")\r\n        accelerator.load_state(os.path.join(args.output_dir, path))\r\n        global_step = int(path.split(\"-\")[1])\r\n\r\n        resume_global_step = global_step * args.gradient_accumulation_steps\r\n        first_epoch = resume_global_step // num_update_steps_per_epoch\r\n        resume_step = resume_global_step % num_update_steps_per_epoch\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    if args.train_text_encoder:\r\n        text_encoder.train()\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n\r\n        with TorchTracemalloc() if not args.no_tracemalloc else nullcontext() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * vae.config.scaling_factor\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    if args.with_prior_preservation:\r\n                        # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                        target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                        # Compute instance loss\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                        # Compute prior loss\r\n                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                        # Add the prior loss to the instance loss.\r\n                        loss = loss + args.prior_loss_weight * prior_loss\r\n                    else:\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else unet.parameters()\r\n                        )\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                if global_step % args.checkpointing_steps == 0 and global_step != 0:\r\n                    if accelerator.is_main_process:\r\n                        save_adaptor(accelerator, global_step, unet, text_encoder, args)\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if (\r\n                    args.validation_prompt is not None\r\n                    and (step + num_update_steps_per_epoch * epoch) % args.validation_steps == 0\r\n                    and global_step > 10\r\n                ):\r\n                    unet.eval()\r\n\r\n                    logger.info(\r\n                        f\"Running validation... \\n Generating {len(validation_prompts)} images with prompt:\"\r\n                        f\" {validation_prompts[0]}, ......\"\r\n                    )\r\n                    # create pipeline\r\n                    pipeline = DiffusionPipeline.from_pretrained(\r\n                        args.pretrained_model_name_or_path,\r\n                        safety_checker=None,\r\n                        revision=args.revision,\r\n                    )\r\n                    # set `keep_fp32_wrapper` to True because we do not want to remove\r\n                    # mixed precision hooks while we are still training\r\n                    pipeline.unet = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\r\n                    pipeline.text_encoder = accelerator.unwrap_model(text_encoder, keep_fp32_wrapper=True)\r\n                    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\r\n                    pipeline = pipeline.to(accelerator.device)\r\n                    pipeline.set_progress_bar_config(disable=True)\r\n\r\n                    # run inference\r\n                    if args.seed is not None:\r\n                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\r\n                    else:\r\n                        generator = None\r\n                    # images = []\r\n                    # for _ in range(args.num_validation_images):\r\n                    #     image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\r\n                    #     images.append(image)\r\n\r\n                    images = []\r\n                    val_img_dir = os.path.join(\r\n                        args.output_dir,\r\n                        f\"validation/{global_step}\",\r\n                        args.wandb_run_name,\r\n                    )\r\n                    os.makedirs(val_img_dir, exist_ok=True)\r\n\r\n                    for val_promot in validation_prompts:\r\n                        image = pipeline(val_promot, num_inference_steps=50, generator=generator).images[0]\r\n                        image.save(os.path.join(val_img_dir, f\"{'_'.join(val_promot.split(' '))}.png\"[1:]))\r\n                        images.append(image)\r\n\r\n                    for tracker in accelerator.trackers:\r\n                        if tracker.name == \"tensorboard\":\r\n                            np_images = np.stack([np.asarray(img) for img in images])\r\n                            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                        if tracker.name == \"wandb\":\r\n                            import wandb\r\n\r\n                            tracker.log(\r\n                                {\r\n                                    \"validation\": [\r\n                                        wandb.Image(image, caption=f\"{i}: {validation_prompts[i]}\")\r\n                                        for i, image in enumerate(images)\r\n                                    ]\r\n                                }\r\n                            )\r\n\r\n                    del pipeline\r\n                    torch.cuda.empty_cache()\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        if not args.no_tracemalloc:\r\n            accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n            accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n            accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n            accelerator.print(\r\n                f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n            )\r\n\r\n            accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n            accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n            accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n            accelerator.print(\r\n                f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n            )\r\n\r\n    if args.push_to_hub:\r\n        repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    accelerator = Accelerator()\r\n    model_name_or_path = \"bigscience/bloomz-7b1\"\r\n    dataset_name = \"twitter_complaints\"\r\n    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\r\n    text_column = \"Tweet text\"\r\n    label_column = \"text_label\"\r\n    lr = 3e-3\r\n    num_epochs = 20\r\n    batch_size = 8\r\n    seed = 42\r\n    max_length = 64\r\n    do_test = False\r\n    set_seed(seed)\r\n\r\n    dataset = load_dataset(\"ought/raft\", dataset_name)\r\n    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\r\n    dataset = dataset.map(\r\n        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\r\n        batched=True,\r\n        num_proc=1,\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n\r\n    def preprocess_function(examples):\r\n        batch_size = len(examples[text_column])\r\n        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\r\n        targets = [str(x) for x in examples[label_column]]\r\n        model_inputs = tokenizer(inputs)\r\n        labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\r\n        for i in range(batch_size):\r\n            sample_input_ids = model_inputs[\"input_ids\"][i]\r\n            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\r\n            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\r\n            labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\r\n            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\r\n        for i in range(batch_size):\r\n            sample_input_ids = model_inputs[\"input_ids\"][i]\r\n            label_input_ids = labels[\"input_ids\"][i]\r\n            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\r\n                max_length - len(sample_input_ids)\r\n            ) + sample_input_ids\r\n            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\r\n                \"attention_mask\"\r\n            ][i]\r\n            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\r\n            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\r\n            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\r\n            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n\r\n    def test_preprocess_function(examples):\r\n        batch_size = len(examples[text_column])\r\n        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\r\n        model_inputs = tokenizer(inputs)\r\n        # print(model_inputs)\r\n        for i in range(batch_size):\r\n            sample_input_ids = model_inputs[\"input_ids\"][i]\r\n            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\r\n                max_length - len(sample_input_ids)\r\n            ) + sample_input_ids\r\n            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\r\n                \"attention_mask\"\r\n            ][i]\r\n            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\r\n            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\r\n        return model_inputs\r\n\r\n    with accelerator.main_process_first():\r\n        processed_datasets = dataset.map(\r\n            preprocess_function,\r\n            batched=True,\r\n            num_proc=1,\r\n            remove_columns=dataset[\"train\"].column_names,\r\n            load_from_cache_file=True,\r\n            desc=\"Running tokenizer on dataset\",\r\n        )\r\n    accelerator.wait_for_everyone()\r\n\r\n    train_dataset = processed_datasets[\"train\"]\r\n\r\n    with accelerator.main_process_first():\r\n        processed_datasets = dataset.map(\r\n            test_preprocess_function,\r\n            batched=True,\r\n            num_proc=1,\r\n            remove_columns=dataset[\"train\"].column_names,\r\n            load_from_cache_file=False,\r\n            desc=\"Running tokenizer on dataset\",\r\n        )\r\n    eval_dataset = processed_datasets[\"train\"]\r\n    test_dataset = processed_datasets[\"test\"]\r\n\r\n    train_dataloader = DataLoader(\r\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\r\n    )\r\n    test_dataloader = DataLoader(\r\n        test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\r\n    )\r\n\r\n    print(next(iter(train_dataloader)))\r\n\r\n    # creating model\r\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\r\n    model = get_peft_model(model, peft_config)\r\n    model.print_trainable_parameters()\r\n\r\n    # optimizer\r\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\r\n\r\n    # lr scheduler\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=0,\r\n        num_training_steps=(len(train_dataloader) * num_epochs),\r\n    )\r\n\r\n    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\r\n        model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\r\n    )\r\n    accelerator.print(model)\r\n\r\n    is_ds_zero_3 = False\r\n    if getattr(accelerator.state, \"deepspeed_plugin\", None):\r\n        is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\r\n\r\n    for epoch in range(num_epochs):\r\n        with TorchTracemalloc() as tracemalloc:\r\n            model.train()\r\n            total_loss = 0\r\n            for step, batch in enumerate(tqdm(train_dataloader)):\r\n                outputs = model(**batch)\r\n                loss = outputs.loss\r\n                total_loss += loss.detach().float()\r\n                accelerator.backward(loss)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n        train_epoch_loss = total_loss / len(train_dataloader)\r\n        train_ppl = torch.exp(train_epoch_loss)\r\n        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\r\n\r\n        model.eval()\r\n        eval_preds = []\r\n        with TorchTracemalloc() as tracemalloc:\r\n            for _, batch in enumerate(tqdm(eval_dataloader)):\r\n                batch = {k: v for k, v in batch.items() if k != \"labels\"}\r\n                with torch.no_grad():\r\n                    outputs = accelerator.unwrap_model(model).generate(\r\n                        **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\r\n                    )  # synced_gpus=True for DS-stage 3\r\n                outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\r\n                preds = accelerator.gather_for_metrics(outputs)\r\n                preds = preds[:, max_length:].detach().cpu().numpy()\r\n                eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\r\n\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the eval (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the eval (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the eval (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the eval (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the eval (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the eval (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n\r\n        correct = 0\r\n        total = 0\r\n        assert len(eval_preds) == len(dataset[\"train\"][label_column]), (\r\n            f\"{len(eval_preds)} != {len(dataset['train'][label_column])}\"\r\n        )\r\n        for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\r\n            if pred.strip() == true.strip():\r\n                correct += 1\r\n            total += 1\r\n        accuracy = correct / total * 100\r\n        accelerator.print(f\"{accuracy=}\")\r\n        accelerator.print(f\"{eval_preds[:10]=}\")\r\n        accelerator.print(f\"{dataset['train'][label_column][:10]=}\")\r\n\r\n    if do_test:\r\n        model.eval()\r\n        test_preds = []\r\n        for _, batch in enumerate(tqdm(test_dataloader)):\r\n            batch = {k: v for k, v in batch.items() if k != \"labels\"}\r\n            with torch.no_grad():\r\n                outputs = accelerator.unwrap_model(model).generate(\r\n                    **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\r\n                )  # synced_gpus=True for DS-stage 3\r\n            outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\r\n            preds = accelerator.gather(outputs)\r\n            preds = preds[:, max_length:].detach().cpu().numpy()\r\n            test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\r\n\r\n        test_preds_cleaned = []\r\n        for _, pred in enumerate(test_preds):\r\n            test_preds_cleaned.append(get_closest_label(pred, classes))\r\n\r\n        test_df = dataset[\"test\"].to_pandas()\r\n        assert len(test_preds_cleaned) == len(test_df), f\"{len(test_preds_cleaned)} != {len(test_df)}\"\r\n        test_df[label_column] = test_preds_cleaned\r\n        test_df[\"text_labels_orig\"] = test_preds\r\n        accelerator.print(test_df[[text_column, label_column]].sample(20))\r\n\r\n        pred_df = test_df[[\"ID\", label_column]]\r\n        pred_df.columns = [\"ID\", \"Label\"]\r\n\r\n        os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\r\n        pred_df.to_csv(f\"data/{dataset_name}/predictions.csv\", index=False)\r\n\r\n    accelerator.wait_for_everyone()\r\n    # Option1: Pushing the model to Hugging Face Hub\r\n    # model.push_to_hub(\r\n    #     f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\"),\r\n    #     token = \"hf_...\"\r\n    # )\r\n    # token (`bool` or `str`, *optional*):\r\n    #     `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\r\n    #     when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\r\n    #     is not specified.\r\n    #     Or you can get your token from https://huggingface.co/settings/token\r\n    # Option2: Saving the model locally\r\n    peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\r\n        \"/\", \"_\"\r\n    )\r\n    model.save_pretrained(peft_model_id)\r\n    accelerator.wait_for_everyone()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    accelerator = Accelerator()\r\n    # model_name_or_path = \"bigscience/T0_3B\"\r\n    model_name_or_path = \"facebook/bart-large\"\r\n    dataset_name = \"twitter_complaints\"\r\n    peft_config = LoraConfig(\r\n        task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\r\n    )\r\n    text_column = \"Tweet text\"\r\n    label_column = \"text_label\"\r\n    lr = 3e-3\r\n    num_epochs = 5\r\n    batch_size = 8\r\n    seed = 42\r\n    do_test = False\r\n    set_seed(seed)\r\n\r\n    dataset = load_dataset(\"ought/raft\", dataset_name)\r\n    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\r\n    dataset = dataset.map(\r\n        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\r\n        batched=True,\r\n        num_proc=1,\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n    target_max_length = max(len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes)\r\n\r\n    def preprocess_function(examples):\r\n        inputs = examples[text_column]\r\n        targets = examples[label_column]\r\n        model_inputs = tokenizer(inputs, truncation=True)\r\n        labels = tokenizer(\r\n            targets, max_length=target_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\r\n        )\r\n        labels = labels[\"input_ids\"]\r\n        labels[labels == tokenizer.pad_token_id] = -100\r\n        model_inputs[\"labels\"] = labels\r\n        return model_inputs\r\n\r\n    with accelerator.main_process_first():\r\n        processed_datasets = dataset.map(\r\n            preprocess_function,\r\n            batched=True,\r\n            num_proc=1,\r\n            remove_columns=dataset[\"train\"].column_names,\r\n            load_from_cache_file=True,\r\n            desc=\"Running tokenizer on dataset\",\r\n        )\r\n    accelerator.wait_for_everyone()\r\n\r\n    train_dataset = processed_datasets[\"train\"]\r\n    eval_dataset = processed_datasets[\"train\"]\r\n    test_dataset = processed_datasets[\"test\"]\r\n\r\n    def collate_fn(examples):\r\n        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\r\n\r\n    train_dataloader = DataLoader(\r\n        train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True\r\n    )\r\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True)\r\n    test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True)\r\n\r\n    # creating model\r\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\r\n    model = get_peft_model(model, peft_config)\r\n    model.print_trainable_parameters()\r\n\r\n    # optimizer\r\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\r\n\r\n    # lr scheduler\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=0,\r\n        num_training_steps=(len(train_dataloader) * num_epochs),\r\n    )\r\n\r\n    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\r\n        model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\r\n    )\r\n    accelerator.print(model)\r\n\r\n    is_ds_zero_3 = False\r\n    if getattr(accelerator.state, \"deepspeed_plugin\", None):\r\n        is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\r\n\r\n    for epoch in range(num_epochs):\r\n        with TorchTracemalloc() as tracemalloc:\r\n            model.train()\r\n            total_loss = 0\r\n            for step, batch in enumerate(tqdm(train_dataloader)):\r\n                outputs = model(**batch)\r\n                loss = outputs.loss\r\n                total_loss += loss.detach().float()\r\n                accelerator.backward(loss)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n        train_epoch_loss = total_loss / len(train_dataloader)\r\n        train_ppl = torch.exp(train_epoch_loss)\r\n        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\r\n\r\n        model.eval()\r\n        eval_preds = []\r\n        with TorchTracemalloc() as tracemalloc:\r\n            for _, batch in enumerate(tqdm(eval_dataloader)):\r\n                batch = {k: v for k, v in batch.items() if k != \"labels\"}\r\n                with torch.no_grad():\r\n                    outputs = accelerator.unwrap_model(model).generate(\r\n                        **batch, synced_gpus=is_ds_zero_3\r\n                    )  # synced_gpus=True for DS-stage 3\r\n                outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\r\n                preds = accelerator.gather_for_metrics(outputs).detach().cpu().numpy()\r\n                eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\r\n\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the eval (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the eval (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the eval (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the eval (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the eval (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the eval (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n\r\n        correct = 0\r\n        total = 0\r\n        assert len(eval_preds) == len(dataset[\"train\"][label_column]), (\r\n            f\"{len(eval_preds)} != {len(dataset['train'][label_column])}\"\r\n        )\r\n        for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\r\n            if pred.strip() == true.strip():\r\n                correct += 1\r\n            total += 1\r\n        accuracy = correct / total * 100\r\n        accelerator.print(f\"{accuracy=}\")\r\n        accelerator.print(f\"{eval_preds[:10]=}\")\r\n        accelerator.print(f\"{dataset['train'][label_column][:10]=}\")\r\n\r\n    if do_test:\r\n        model.eval()\r\n        test_preds = []\r\n        for _, batch in enumerate(tqdm(test_dataloader)):\r\n            batch = {k: v for k, v in batch.items() if k != \"labels\"}\r\n            with torch.no_grad():\r\n                outputs = accelerator.unwrap_model(model).generate(\r\n                    **batch, synced_gpus=is_ds_zero_3\r\n                )  # synced_gpus=True for DS-stage 3\r\n            outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\r\n            preds = accelerator.gather(outputs).detach().cpu().numpy()\r\n            test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\r\n\r\n        test_preds_cleaned = []\r\n        for _, pred in enumerate(test_preds):\r\n            test_preds_cleaned.append(get_closest_label(pred, classes))\r\n\r\n        test_df = dataset[\"test\"].to_pandas()\r\n        assert len(test_preds_cleaned) == len(test_df), f\"{len(test_preds_cleaned)} != {len(test_df)}\"\r\n        test_df[label_column] = test_preds_cleaned\r\n        test_df[\"text_labels_orig\"] = test_preds\r\n        accelerator.print(test_df[[text_column, label_column]].sample(20))\r\n\r\n        pred_df = test_df[[\"ID\", label_column]]\r\n        pred_df.columns = [\"ID\", \"Label\"]\r\n\r\n        os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\r\n        pred_df.to_csv(f\"data/{dataset_name}/predictions.csv\", index=False)\r\n\r\n    accelerator.wait_for_everyone()\r\n    # Option1: Pushing the model to Hugging Face Hub\r\n    # model.push_to_hub(\r\n    #     f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\"),\r\n    #     token = \"hf_...\"\r\n    # )\r\n    # token (`bool` or `str`, *optional*):\r\n    #     `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\r\n    #     when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\r\n    #     is not specified.\r\n    #     Or you can get your token from https://huggingface.co/settings/token\r\n\r\n    # Option2: Saving the model locally\r\n    peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\r\n        \"/\", \"_\"\r\n    )\r\n    model.save_pretrained(peft_model_id)\r\n    accelerator.wait_for_everyone()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    accelerator = Accelerator()\r\n    model_name_or_path = \"t5-base\"\r\n    batch_size = 8\r\n    text_column = \"sentence\"\r\n    label_column = \"label\"\r\n    max_length = 64\r\n    lr = 1e-3\r\n    num_epochs = 1\r\n    base_path = \"temp/data/FinancialPhraseBank-v1.0\"\r\n\r\n    peft_config = LoraConfig(\r\n        task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\r\n    )\r\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\r\n    model = get_peft_model(model, peft_config)\r\n    accelerator.print(model.print_trainable_parameters())\r\n\r\n    dataset = load_dataset(\r\n        \"json\",\r\n        data_files={\r\n            \"train\": os.path.join(base_path, \"financial_phrase_bank_train.jsonl\"),\r\n            \"validation\": os.path.join(base_path, \"financial_phrase_bank_val.jsonl\"),\r\n        },\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n\r\n    def preprocess_function(examples):\r\n        inputs = examples[text_column]\r\n        targets = examples[label_column]\r\n        model_inputs = tokenizer(\r\n            inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\r\n        )\r\n        labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\r\n        labels = labels[\"input_ids\"]\r\n        labels[labels == tokenizer.pad_token_id] = -100\r\n        model_inputs[\"labels\"] = labels\r\n        return model_inputs\r\n\r\n    with accelerator.main_process_first():\r\n        processed_datasets = dataset.map(\r\n            preprocess_function,\r\n            batched=True,\r\n            num_proc=1,\r\n            remove_columns=dataset[\"train\"].column_names,\r\n            load_from_cache_file=False,\r\n            desc=\"Running tokenizer on dataset\",\r\n        )\r\n\r\n    train_dataset = processed_datasets[\"train\"]\r\n    eval_dataset = processed_datasets[\"validation\"]\r\n\r\n    train_dataloader = DataLoader(\r\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\r\n    )\r\n\r\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=0,\r\n        num_training_steps=(len(train_dataloader) * num_epochs),\r\n    )\r\n\r\n    if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\r\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\r\n\r\n    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\r\n        model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\r\n    )\r\n    accelerator.print(model)\r\n\r\n    for epoch in range(num_epochs):\r\n        model.train()\r\n        total_loss = 0\r\n        for step, batch in enumerate(tqdm(train_dataloader)):\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            total_loss += loss.detach().float()\r\n            loss.backward()\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        model.eval()\r\n        eval_loss = 0\r\n        eval_preds = []\r\n        for step, batch in enumerate(tqdm(eval_dataloader)):\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            loss = outputs.loss\r\n            eval_loss += loss.detach().float()\r\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\r\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\r\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\r\n        eval_ppl = torch.exp(eval_epoch_loss)\r\n        train_epoch_loss = total_loss / len(train_dataloader)\r\n        train_ppl = torch.exp(train_epoch_loss)\r\n        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\r\n\r\n        correct = 0\r\n        total = 0\r\n        for pred, true in zip(eval_preds, dataset[\"validation\"][label_column]):\r\n            if pred.strip() == true.strip():\r\n                correct += 1\r\n            total += 1\r\n        accuracy = correct / total * 100\r\n        accelerator.print(f\"{accuracy=}\")\r\n        accelerator.print(f\"{eval_preds[:10]=}\")\r\n        accelerator.print(f\"{dataset['validation'][label_column][:10]=}\")\r\n        accelerator.wait_for_everyone()\r\n        # Option1: Pushing the model to Hugging Face Hub\r\n        # model.push_to_hub(\r\n        #     f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\"),\r\n        #     token = \"hf_...\"\r\n        # )\r\n        # token (`bool` or `str`, *optional*):\r\n        #     `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\r\n        #     when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\r\n        #     is not specified.\r\n        #     Or you can get your token from https://huggingface.co/settings/token\r\n        # Option2: Saving the model locally\r\n        peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\")\r\n        model.save_pretrained(peft_model_id)\r\n        accelerator.wait_for_everyone()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    validation_prompts = list(filter(None, args.validation_prompt[0].split(\".\")))\r\n\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to if args.report_to != \"none\" else None,\r\n        project_dir=accelerator_project_config,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        import wandb\r\n\r\n        args.wandb_project_name = args.project_name\r\n        args.wandb_run_name = args.run_name\r\n        wandb_init = {\r\n            \"wandb\": {\r\n                \"name\": args.wandb_run_name,\r\n                \"mode\": \"online\",\r\n            }\r\n        }\r\n\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    global_seed = hash(args.run_name) % (2**32)\r\n    set_seed(global_seed)\r\n\r\n    # Generate class images if prior preservation is enabled.\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            if args.prior_generation_precision == \"fp32\":\r\n                torch_dtype = torch.float32\r\n            elif args.prior_generation_precision == \"fp16\":\r\n                torch_dtype = torch.float16\r\n            elif args.prior_generation_precision == \"bf16\":\r\n                torch_dtype = torch.bfloat16\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            if args.hub_model_id is None:\r\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\r\n            else:\r\n                repo_name = args.hub_model_id\r\n            repo = Repository(args.output_dir, clone_from=repo_name)  # noqa: F841\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDIMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n\r\n    if args.use_hra:\r\n        config = HRAConfig(\r\n            r=args.hra_r,\r\n            apply_GS=args.hra_apply_GS,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            bias=args.hra_bias,\r\n        )\r\n        unet = get_peft_model(unet, config, adapter_name=args.run_name)\r\n        unet.print_trainable_parameters()\r\n\r\n    vae.requires_grad_(False)\r\n    unet.train()\r\n\r\n    if args.train_text_encoder and args.use_hra:\r\n        config = HRAConfig(\r\n            r=args.hra_r,\r\n            apply_GS=args.hra_apply_GS,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            bias=args.hra_bias,\r\n        )\r\n        text_encoder = get_peft_model(text_encoder, config, adapter_name=args.run_name)\r\n        text_encoder.print_trainable_parameters()\r\n        text_encoder.train()\r\n    else:\r\n        text_encoder.requires_grad_(False)\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move unet, vae and text_encoder to device and cast to weight_dtype\r\n    unet.to(accelerator.device, dtype=weight_dtype)\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        # below fails when using hra so commenting it out\r\n        if args.train_text_encoder and not args.use_hra:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    # Optimizer creation\r\n    params_to_optimize = [param for param in unet.parameters() if param.requires_grad]\r\n\r\n    if args.train_text_encoder:\r\n        params_to_optimize += [param for param in text_encoder.parameters() if param.requires_grad]\r\n\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Download the official dreambooth dataset from the official repository: https://github.com/google/dreambooth.git\r\n    data_path = os.path.join(os.getcwd(), \"data\", \"dreambooth\")\r\n    if not os.path.exists(data_path):\r\n        os.makedirs(os.path.join(os.getcwd(), \"data\"), exist_ok=True)\r\n        os.system(f\"git clone https://github.com/google/dreambooth.git '{data_path}'\")\r\n\r\n    # Dataset and DataLoaders creation:\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=args.train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\r\n        num_workers=args.num_dataloader_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move vae and text_encoder to device and cast to weight_dtype\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        if args.report_to == \"wandb\":\r\n            accelerator.init_trackers(args.wandb_project_name, config=vars(args), init_kwargs=wandb_init)\r\n        else:\r\n            accelerator.init_trackers(args.project_name, config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1] if len(dirs) > 0 else None\r\n        accelerator.print(f\"Resuming from checkpoint {path}\")\r\n        accelerator.load_state(os.path.join(args.output_dir, path))\r\n        global_step = int(path.split(\"-\")[1])\r\n\r\n        resume_global_step = global_step * args.gradient_accumulation_steps\r\n        first_epoch = resume_global_step // num_update_steps_per_epoch\r\n        resume_step = resume_global_step % num_update_steps_per_epoch\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    if args.train_text_encoder:\r\n        text_encoder.train()\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n\r\n        with TorchTracemalloc() if not args.no_tracemalloc else nullcontext() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * vae.config.scaling_factor\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    if args.with_prior_preservation:\r\n                        # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                        target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                        # Compute instance loss\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                        # Compute prior loss\r\n                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                        # Add the prior loss to the instance loss.\r\n                        loss = loss + args.prior_loss_weight * prior_loss\r\n                    else:\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else unet.parameters()\r\n                        )\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                if global_step % args.checkpointing_steps == 0 and global_step != 0:\r\n                    if accelerator.is_main_process:\r\n                        save_adaptor(accelerator, global_step, unet, text_encoder, args)\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if (\r\n                    args.validation_prompt is not None\r\n                    and (step + num_update_steps_per_epoch * epoch) % args.validation_steps == 0\r\n                    and global_step > 10\r\n                ):\r\n                    unet.eval()\r\n\r\n                    logger.info(\r\n                        f\"Running validation... \\n Generating {len(validation_prompts)} images with prompt:\"\r\n                        f\" {validation_prompts[0]}, ......\"\r\n                    )\r\n                    # create pipeline\r\n                    pipeline = DiffusionPipeline.from_pretrained(\r\n                        args.pretrained_model_name_or_path,\r\n                        safety_checker=None,\r\n                        revision=args.revision,\r\n                    )\r\n                    # set `keep_fp32_wrapper` to True because we do not want to remove\r\n                    # mixed precision hooks while we are still training\r\n                    pipeline.unet = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\r\n                    pipeline.text_encoder = accelerator.unwrap_model(text_encoder, keep_fp32_wrapper=True)\r\n                    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\r\n                    pipeline = pipeline.to(accelerator.device)\r\n                    pipeline.set_progress_bar_config(disable=True)\r\n\r\n                    # run inference\r\n                    if args.seed is not None:\r\n                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\r\n                    else:\r\n                        generator = None\r\n\r\n                    images = []\r\n                    val_img_dir = os.path.join(\r\n                        args.output_dir,\r\n                        f\"validation/{global_step}\",\r\n                        args.run_name,\r\n                    )\r\n                    os.makedirs(val_img_dir, exist_ok=True)\r\n\r\n                    for val_promot in validation_prompts:\r\n                        image = pipeline(val_promot, num_inference_steps=50, generator=generator).images[0]\r\n                        image.save(os.path.join(val_img_dir, f\"{'_'.join(val_promot.split(' '))}.png\"[1:]))\r\n                        images.append(image)\r\n\r\n                    for tracker in accelerator.trackers:\r\n                        if tracker.name == \"tensorboard\":\r\n                            np_images = np.stack([np.asarray(img) for img in images])\r\n                            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                        if tracker.name == \"wandb\":\r\n                            import wandb\r\n\r\n                            tracker.log(\r\n                                {\r\n                                    \"validation\": [\r\n                                        wandb.Image(image, caption=f\"{i}: {validation_prompts[i]}\")\r\n                                        for i, image in enumerate(images)\r\n                                    ]\r\n                                }\r\n                            )\r\n\r\n                    del pipeline\r\n                    torch.cuda.empty_cache()\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        if not args.no_tracemalloc:\r\n            accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n            accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n            accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n            accelerator.print(\r\n                f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n            )\r\n\r\n            accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n            accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n            accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n            accelerator.print(\r\n                f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n            )\r\n\r\n    if args.push_to_hub:\r\n        repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n\r\n    accelerator_kwargs = {\"gradient_accumulation_steps\": args.gradient_accumulation_steps}\r\n    if args.with_tracking:\r\n        accelerator_kwargs[\"log_with\"] = args.report_to\r\n        accelerator_kwargs[\"project_dir\"] = args.output_dir\r\n    accelerator = Accelerator(**accelerator_kwargs)\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n    accelerator.wait_for_everyone()\r\n\r\n    # get the tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\r\n\r\n    # dataset download and preprocessing\r\n    if args.sanity_test:\r\n        train_dataset = load_dataset(\"smangrul/amazon_esci\", split=\"train[:1024]\")\r\n        val_dataset = load_dataset(\"smangrul/amazon_esci\", split=\"validation[:1024]\")\r\n\r\n        dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\r\n    else:\r\n        dataset = load_dataset(args.dataset_name, revision=\"main\")\r\n\r\n    def preprocess_function(examples):\r\n        queries = examples[\"query\"]\r\n        result = tokenizer(queries, padding=\"max_length\", max_length=70, truncation=True)\r\n        result = {f\"query_{k}\": v for k, v in result.items()}\r\n\r\n        products = examples[\"product_title\"]\r\n        result_products = tokenizer(products, padding=\"max_length\", max_length=70, truncation=True)\r\n        for k, v in result_products.items():\r\n            result[f\"product_{k}\"] = v\r\n\r\n        result[\"labels\"] = examples[\"relevance_label\"]\r\n        return result\r\n\r\n    processed_datasets = dataset.map(\r\n        preprocess_function,\r\n        batched=True,\r\n        remove_columns=dataset[\"train\"].column_names,\r\n        desc=\"Running tokenizer on dataset\",\r\n    )\r\n\r\n    # Log a few random samples from the training set:\r\n    for index in random.sample(range(len(processed_datasets[\"train\"])), 3):\r\n        logger.info(f\"Sample {index} of the training set: {processed_datasets['train'][index]}.\")\r\n\r\n    # base model\r\n    model = AutoModelForSentenceEmbedding(args.model_name_or_path, tokenizer)\r\n\r\n    if args.use_peft:\r\n        # peft config and wrapping\r\n        peft_config = LoraConfig(\r\n            r=8,\r\n            lora_alpha=16,\r\n            bias=\"none\",\r\n            task_type=TaskType.FEATURE_EXTRACTION,\r\n            target_modules=[\"key\", \"query\", \"value\"],\r\n        )\r\n        model = get_peft_model(model, peft_config)\r\n        model.print_trainable_parameters()\r\n\r\n    accelerator.print(model)\r\n\r\n    # get dataloaders\r\n    train_dataloader = DataLoader(\r\n        processed_datasets[\"train\"],\r\n        shuffle=True,\r\n        collate_fn=default_data_collator,\r\n        batch_size=args.per_device_train_batch_size,\r\n        pin_memory=True,\r\n    )\r\n\r\n    eval_dataloader = DataLoader(\r\n        processed_datasets[\"validation\"],\r\n        shuffle=False,\r\n        collate_fn=default_data_collator,\r\n        batch_size=args.per_device_eval_batch_size,\r\n        pin_memory=True,\r\n    )\r\n\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        name=args.lr_scheduler_type,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.num_warmup_steps,\r\n        num_training_steps=args.max_train_steps,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\r\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\r\n    )\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # Figure out how many steps we should save the Accelerator states\r\n    checkpointing_steps = args.checkpointing_steps\r\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\r\n        checkpointing_steps = int(checkpointing_steps)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if args.with_tracking:\r\n        experiment_config = vars(args)\r\n        # TensorBoard cannot log Enums, need the raw value\r\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\r\n        accelerator.init_trackers(\"peft_semantic_search\", experiment_config)\r\n\r\n    metric = evaluate.load(\"roc_auc\")\r\n\r\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    if args.use_peft:\r\n        # saving and loading checkpoints for resuming training\r\n        accelerator.register_save_state_pre_hook(save_model_hook)\r\n        accelerator.register_load_state_pre_hook(load_model_hook)\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(processed_datasets['train'])}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    completed_steps = 0\r\n    starting_epoch = 0\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\r\n            accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\r\n            accelerator.load_state(args.resume_from_checkpoint)\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\r\n            dirs.sort(key=os.path.getctime)\r\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\r\n        # Extract `epoch_{i}` or `step_{i}`\r\n        training_difference = os.path.splitext(path)[0]\r\n\r\n        if \"epoch\" in training_difference:\r\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\r\n            resume_step = None\r\n            completed_steps = starting_epoch * num_update_steps_per_epoch\r\n        else:\r\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\r\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\r\n            starting_epoch = resume_step // len(train_dataloader)\r\n            resume_step -= starting_epoch * len(train_dataloader)\r\n            completed_steps = resume_step // args.gradient_accumulation_steps\r\n\r\n    # update the progress_bar if load from checkpoint\r\n    progress_bar.update(completed_steps)\r\n\r\n    for epoch in range(starting_epoch, args.num_train_epochs):\r\n        model.train()\r\n        if args.with_tracking:\r\n            total_loss = 0\r\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\r\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\r\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\r\n        else:\r\n            active_dataloader = train_dataloader\r\n        for step, batch in enumerate(active_dataloader):\r\n            with accelerator.accumulate(model):\r\n                query_embs = model(**{k.replace(\"query_\", \"\"): v for k, v in batch.items() if \"query\" in k})\r\n                product_embs = model(**{k.replace(\"product_\", \"\"): v for k, v in batch.items() if \"product\" in k})\r\n                loss = get_loss(get_cosing_embeddings(query_embs, product_embs), batch[\"labels\"])\r\n                total_loss += accelerator.reduce(loss.detach().float(), reduction=\"sum\")\r\n                accelerator.backward(loss)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                model.zero_grad()\r\n\r\n            # Checks if the accelerator has performed an optimization step behind the scenes\r\n            if accelerator.sync_gradients:\r\n                progress_bar.update(1)\r\n                completed_steps += 1\r\n\r\n            if (step + 1) % 100 == 0:\r\n                logger.info(f\"Step: {step + 1}, Loss: {total_loss / (step + 1)}\")\r\n                if args.with_tracking:\r\n                    accelerator.log({\"train/loss\": total_loss / (step + 1)}, step=completed_steps)\r\n\r\n            if isinstance(checkpointing_steps, int):\r\n                if completed_steps % checkpointing_steps == 0:\r\n                    output_dir = f\"step_{completed_steps}\"\r\n                    if args.output_dir is not None:\r\n                        output_dir = os.path.join(args.output_dir, output_dir)\r\n                    accelerator.save_state(output_dir)\r\n\r\n            if completed_steps >= args.max_train_steps:\r\n                break\r\n\r\n        model.eval()\r\n        for step, batch in enumerate(eval_dataloader):\r\n            with torch.no_grad():\r\n                query_embs = model(**{k.replace(\"query_\", \"\"): v for k, v in batch.items() if \"query\" in k})\r\n                product_embs = model(**{k.replace(\"product_\", \"\"): v for k, v in batch.items() if \"product\" in k})\r\n                prediction_scores = get_cosing_embeddings(query_embs, product_embs)\r\n            prediction_scores, references = accelerator.gather_for_metrics((prediction_scores, batch[\"labels\"]))\r\n            metric.add_batch(\r\n                prediction_scores=prediction_scores,\r\n                references=references,\r\n            )\r\n\r\n        result = metric.compute()\r\n        result = {f\"eval/{k}\": v for k, v in result.items()}\r\n        # Use accelerator.print to print only on the main process.\r\n        accelerator.print(f\"epoch {epoch}:\", result)\r\n        if args.with_tracking:\r\n            result[\"train/epoch_loss\"] = total_loss.item() / len(train_dataloader)\r\n            accelerator.log(result, step=completed_steps)\r\n\r\n        if args.output_dir is not None:\r\n            accelerator.wait_for_everyone()\r\n            if accelerator.is_main_process:\r\n                if isinstance(checkpointing_steps, str):\r\n                    accelerator.save_state(os.path.join(args.output_dir, f\"epoch_{epoch}\"))\r\n                accelerator.unwrap_model(model).save_pretrained(\r\n                    args.output_dir, state_dict=accelerator.get_state_dict(accelerator.unwrap_model(model))\r\n                )\r\n                tokenizer.save_pretrained(args.output_dir)\r\n                if args.push_to_hub:\r\n                    commit_message = (\r\n                        f\"Training in progress epoch {epoch}\"\r\n                        if epoch < args.num_train_epochs - 1\r\n                        else \"End of training\"\r\n                    )\r\n                    api.upload_folder(\r\n                        repo_id=repo_id,\r\n                        folder_path=args.output_dir,\r\n                        commit_message=commit_message,\r\n                        run_as_future=True,\r\n                    )\r\n            accelerator.wait_for_everyone()\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n\r\n    accelerator_kwargs = {\"gradient_accumulation_steps\": args.gradient_accumulation_steps}\r\n    if args.with_tracking:\r\n        accelerator_kwargs[\"log_with\"] = args.report_to\r\n        accelerator_kwargs[\"project_dir\"] = args.output_dir\r\n    accelerator = Accelerator(**accelerator_kwargs)\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n    accelerator.wait_for_everyone()\r\n\r\n    # load dataset either in streaming mode or not\r\n    processor = WhisperProcessor.from_pretrained(args.model_name_or_path, language=args.language, task=args.task)\r\n    normalizer = BasicTextNormalizer()\r\n    prepare_dataset = prepare_dataset_wrapper(args.do_lower_case, args.do_remove_punctuation, processor, normalizer)\r\n    is_audio_in_length_range = get_audio_length_processor(args.max_audio_input_length)\r\n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\r\n\r\n    if args.dataset_in_streaming_mode:\r\n        raw_datasets = IterableDatasetDict()\r\n        loading_method = load_streaming_dataset\r\n    else:\r\n        raw_datasets = DatasetDict()\r\n        loading_method = load_dataset\r\n\r\n    if args.debug_mode:\r\n        train_split = \"train[:100]\"\r\n        test_split = \"test[:10]\"\r\n    else:\r\n        train_split = \"train+validation\"\r\n        test_split = \"test\"\r\n\r\n    raw_datasets[\"train\"] = loading_method(\r\n        args.dataset_name, args.language_abbr, split=train_split, use_auth_token=True\r\n    )\r\n    raw_datasets[\"test\"] = loading_method(args.dataset_name, args.language_abbr, split=test_split, use_auth_token=True)\r\n    raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))\r\n\r\n    logger.info(\"Dataset loaded: %s\", raw_datasets)\r\n    logger.info(f\"{raw_datasets['train'][0]}\")\r\n\r\n    vectorized_datasets = raw_datasets.map(\r\n        prepare_dataset,\r\n        remove_columns=list(next(iter(raw_datasets.values())).features),\r\n        num_proc=args.preprocessing_num_workers,\r\n    ).with_format(\"torch\")\r\n\r\n    if args.dataset_in_streaming_mode:\r\n        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(\r\n            buffer_size=args.buffer_size,\r\n            seed=args.seed,\r\n        )\r\n\r\n    # filter out audio files that are too long from the training set\r\n    is_audio_in_length_range = get_audio_length_processor(args.max_audio_input_length)\r\n    vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].filter(\r\n        is_audio_in_length_range, input_columns=[\"input_length\"]\r\n    )\r\n\r\n    # get dataloaders\r\n    train_dataloader = DataLoader(\r\n        vectorized_datasets[\"train\"],\r\n        batch_size=args.per_device_train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=data_collator,\r\n        num_workers=args.dataloader_num_workers,\r\n        pin_memory=args.dataloader_pin_memory,\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        vectorized_datasets[\"test\"],\r\n        batch_size=args.per_device_eval_batch_size,\r\n        collate_fn=data_collator,\r\n        num_workers=args.dataloader_num_workers,\r\n        pin_memory=args.dataloader_pin_memory,\r\n    )\r\n\r\n    # metric\r\n    metric = evaluate.load(\"wer\")\r\n\r\n    # model\r\n    model = WhisperForConditionalGeneration.from_pretrained(\r\n        args.model_name_or_path, quantization_config=BitsAndBytesConfig(load_in_8bit=True)\r\n    )\r\n    model.config.forced_decoder_ids = None\r\n    model.config.suppress_tokens = []\r\n    if len(set(model.hf_device_map.values()).intersection({\"cpu\", \"disk\"})) > 0:\r\n        raise ValueError(\"Training on CPU or disk is not supported.\")\r\n    if len(set(model.hf_device_map.values())) > 1:\r\n        device_map = model.hf_device_map.copy()\r\n        # required because `labels` are on main execution device (0) while the output of `proj_out` is on other device.\r\n        # So, this leads to device mismatch error when calculation cross-entropy between logits and labels.\r\n        # Won't arise during inference as `labels` aren't supplied during that time\r\n        # instead of changing device of one of the tied modules, I have to do this for all tied modules\r\n        # else the execution device of remaining tied modules isn't changed\r\n        device_map[\"model.decoder.embed_tokens\"] = model._hf_hook.execution_device\r\n        device_map[\"model.decoder.embed_positions\"] = model._hf_hook.execution_device\r\n        device_map[\"proj_out\"] = model._hf_hook.execution_device\r\n        dispatch_model(model, device_map=device_map)\r\n\r\n    # preparing peft model\r\n    if args.use_peft:\r\n        from peft import prepare_model_for_kbit_training\r\n\r\n        model = prepare_model_for_kbit_training(model)\r\n\r\n        # as Whisper model uses Conv layer in encoder, checkpointing disables grad computation\r\n        # to avoid this, make the inputs trainable\r\n        def make_inputs_require_grad(module, input, output):\r\n            output.requires_grad_(True)\r\n\r\n        model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\r\n\r\n        # wrapping model with adalora tuner\r\n        if args.use_adalora:\r\n            config = AdaLoraConfig(\r\n                init_r=args.init_r,\r\n                target_r=args.target_r,\r\n                beta1=0.85,\r\n                beta2=0.85,\r\n                tinit=args.tinit,\r\n                tfinal=args.tfinal,\r\n                deltaT=args.delta_t,\r\n                lora_alpha=args.lora_alpha,\r\n                lora_dropout=args.lora_dropout,\r\n                target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\r\n                orth_reg_weight=args.orth_reg_weight,\r\n            )\r\n        else:\r\n            config = LoraConfig(\r\n                r=args.r,\r\n                lora_alpha=args.lora_alpha,\r\n                target_modules=[\"q_proj\", \"v_proj\"],\r\n                lora_dropout=args.lora_dropout,\r\n            )\r\n\r\n        model = get_peft_model(model, config)\r\n        model.print_trainable_parameters()\r\n\r\n    # optimizer\r\n    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\r\n\r\n    if args.max_train_steps is None:\r\n        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    else:\r\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # scheduler\r\n    lr_scheduler = get_scheduler(\r\n        name=args.lr_scheduler_type,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.num_warmup_steps,\r\n        num_training_steps=args.max_train_steps,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\r\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\r\n    )\r\n\r\n    accelerator.print(model)\r\n\r\n    # Note here that the max steps is adjusted by the accelerator's num_processes\r\n    args.max_train_steps = math.ceil(args.max_train_steps / accelerator.num_processes)\r\n    if args.use_peft and args.use_adalora:\r\n        model.base_model.peft_config[\"default\"].total_step = args.max_train_steps\r\n        # model.base_model.peft_config.total_step = args.max_train_steps\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if args.with_tracking:\r\n        run_name = f\"run-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\r\n        experiment_config = vars(args)\r\n        # TensorBoard cannot log Enums, need the raw value\r\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\r\n        accelerator.init_trackers(\r\n            \"Whisper PEFT Fine-Tuning\", config=experiment_config, init_kwargs={\"wandb\": {\"name\": run_name}}\r\n        )\r\n\r\n    # saving and loading checkpoints for resuming training\r\n    accelerator.register_save_state_pre_hook(save_model_hook)\r\n    accelerator.register_load_state_pre_hook(load_model_hook)\r\n\r\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    global_step = 0\r\n    starting_epoch = 0\r\n    best_metric = None\r\n    resume_step = 0\r\n    forced_decoder_ids = processor.get_decoder_prompt_ids(language=args.language, task=args.task)\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        accelerator.load_state(args.resume_from_checkpoint)\r\n        path = os.path.basename(args.resume_from_checkpoint)\r\n        training_difference = os.path.splitext(path)[0]\r\n        global_step = resume_step = int(training_difference.replace(\"step_\", \"\"))\r\n        starting_epoch = resume_step // len(train_dataloader)\r\n        resume_step -= starting_epoch * len(train_dataloader)\r\n\r\n    # We need to adjust the progress bar to the current step\r\n    progress_bar.update(resume_step)\r\n    for epoch in range(starting_epoch, args.num_train_epochs):\r\n        model.train()\r\n        if args.with_tracking:\r\n            total_loss = 0\r\n            running_loss = 0\r\n        for step, batch in enumerate(accelerator.skip_first_batches(train_dataloader, num_batches=resume_step)):\r\n            with accelerator.accumulate(model):\r\n                outputs = model(**batch)\r\n                loss = outputs.loss\r\n                accelerator.backward(loss)\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n\r\n                # Update the importance of low-rank matrices\r\n                # and allocate the budget accordingly.\r\n                # This is only needed for AdaLora.\r\n                # Note that this requires parameter gradients.\r\n                # Hence being called before optimizer.zero_grad().\r\n                if args.use_peft and args.use_adalora:\r\n                    model.update_and_allocate(global_step)\r\n\r\n                optimizer.zero_grad()\r\n                global_step += 1\r\n                progress_bar.update(1)\r\n\r\n            if args.with_tracking:\r\n                step_loss = accelerator.reduce(loss.detach().clone()).item()\r\n                total_loss += step_loss\r\n                running_loss += step_loss\r\n\r\n            if global_step % args.checkpointing_steps == 0:\r\n                output_dir = os.path.join(args.output_dir, f\"step_{global_step}\")\r\n                accelerator.save_state(output_dir)\r\n\r\n            if global_step % args.logging_steps == 0:\r\n                if args.with_tracking:\r\n                    accelerator.log({\"train/running_loss\": running_loss / args.logging_steps}, step=global_step)\r\n                    running_loss = 0\r\n\r\n            if global_step % args.evaluation_steps == 0:\r\n                eval_metrics = evaluation_loop(\r\n                    model, eval_dataloader, processor, normalizer, metric, forced_decoder_ids, accelerator\r\n                )\r\n                if args.with_tracking:\r\n                    logger.info(f\"Step {global_step} eval metrics: {eval_metrics}\")\r\n                    accelerator.log(eval_metrics, step=global_step)\r\n                if best_metric is None or eval_metrics[\"eval/wer\"] < best_metric:\r\n                    best_metric = eval_metrics[\"eval/wer\"]\r\n                    accelerator.save_state(os.path.join(args.output_dir, \"best_checkpoint\"))\r\n                model.train()\r\n\r\n            if global_step >= args.max_train_steps:\r\n                break\r\n\r\n        if args.with_tracking:\r\n            train_epoch_loss = total_loss / (step + 1)\r\n            logger.info(f\"Epoch {epoch} train loss: {train_epoch_loss}\")\r\n            accelerator.log({\"epoch/train_loss\": train_epoch_loss}, step=epoch)\r\n\r\n        if args.push_to_hub and epoch <= args.num_train_epochs - 1:\r\n            accelerator.wait_for_everyone()\r\n            unwrapped_model = accelerator.unwrap_model(model)\r\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process)\r\n            # evaluate the model at the end of training\r\n            eval_metrics = evaluation_loop(\r\n                model, eval_dataloader, processor, normalizer, metric, forced_decoder_ids, accelerator\r\n            )\r\n            if args.with_tracking:\r\n                logger.info(f\"Step {global_step} eval metrics: {eval_metrics}\")\r\n                accelerator.log(eval_metrics, step=global_step)\r\n            if best_metric is None or eval_metrics[\"eval/wer\"] < best_metric:\r\n                best_metric = eval_metrics[\"eval/wer\"]\r\n                accelerator.save_state(os.path.join(args.output_dir, \"best_checkpoint\"))\r\n\r\n            if accelerator.is_main_process:\r\n                processor.tokenizer.save_pretrained(args.output_dir)\r\n                api.upload_folder(\r\n                    repo_id=repo_id,\r\n                    folder_path=args.output_dir,\r\n                    commit_message=f\"Training in progress epoch {epoch}\",\r\n                    run_as_future=True,\r\n                )\r\n\r\n    if args.load_best_model:\r\n        # load the best model\r\n        accelerator.load_state(os.path.join(args.output_dir, \"best_checkpoint\"))\r\n        model.resize_modules_by_rank_pattern(model.peft_config[\"default\"].rank_pattern, \"default\")\r\n        eval_metrics = evaluation_loop(\r\n            model, eval_dataloader, processor, normalizer, metric, forced_decoder_ids, accelerator\r\n        )\r\n        if args.with_tracking:\r\n            best_metrics = {\"best_\" + k: v for k, v in eval_metrics.items()}\r\n            accelerator.log(best_metrics, step=global_step)\r\n\r\n    accelerator.wait_for_everyone()\r\n    unwrapped_model = accelerator.unwrap_model(model)\r\n    unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process)\r\n    if accelerator.is_main_process:\r\n        processor.tokenizer.save_pretrained(args.output_dir)\r\n        if args.push_to_hub:\r\n            api.upload_folder(\r\n                repo_id=repo_id,\r\n                folder_path=args.output_dir,\r\n                commit_message=\"End of training\",\r\n            )\r\n\r\n    with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\r\n        eval_metrics.pop(\"eval_samples\")\r\n        json.dump(eval_metrics, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n\r\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\r\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\r\n    send_example_telemetry(\"run_clm_no_trainer\", args)\r\n\r\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\r\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\r\n    # in the environment\r\n    accelerator_log_kwargs = {}\r\n\r\n    if args.with_tracking:\r\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\r\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\r\n\r\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n    accelerator.wait_for_everyone()\r\n\r\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\r\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\r\n    # (the dataset will be downloaded automatically from the datasets Hub).\r\n    #\r\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\r\n    # 'text' is found. You can easily tweak this behavior (see below).\r\n    #\r\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\r\n    # download the dataset.\r\n    if args.dataset_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\r\n        if \"validation\" not in raw_datasets.keys():\r\n            raw_datasets[\"validation\"] = load_dataset(\r\n                args.dataset_name,\r\n                args.dataset_config_name,\r\n                split=f\"train[:{args.validation_split_percentage}%]\",\r\n            )\r\n            raw_datasets[\"train\"] = load_dataset(\r\n                args.dataset_name,\r\n                args.dataset_config_name,\r\n                split=f\"train[{args.validation_split_percentage}%:]\",\r\n            )\r\n    else:\r\n        data_files = {}\r\n        dataset_args = {}\r\n        if args.train_file is not None:\r\n            data_files[\"train\"] = args.train_file\r\n        if args.validation_file is not None:\r\n            data_files[\"validation\"] = args.validation_file\r\n        extension = args.train_file.split(\".\")[-1]\r\n        if extension == \"txt\":\r\n            extension = \"text\"\r\n            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\r\n        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\r\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\r\n        if \"validation\" not in raw_datasets.keys():\r\n            raw_datasets[\"validation\"] = load_dataset(\r\n                extension,\r\n                data_files=data_files,\r\n                split=f\"train[:{args.validation_split_percentage}%]\",\r\n                **dataset_args,\r\n            )\r\n            raw_datasets[\"train\"] = load_dataset(\r\n                extension,\r\n                data_files=data_files,\r\n                split=f\"train[{args.validation_split_percentage}%:]\",\r\n                **dataset_args,\r\n            )\r\n\r\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\r\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\r\n\r\n    # Load pretrained model and tokenizer\r\n    #\r\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\r\n    # download model & vocab.\r\n    if args.config_name:\r\n        config = AutoConfig.from_pretrained(\r\n            args.config_name,\r\n            trust_remote_code=args.trust_remote_code,\r\n        )\r\n    elif args.model_name_or_path:\r\n        config = AutoConfig.from_pretrained(\r\n            args.model_name_or_path,\r\n            trust_remote_code=args.trust_remote_code,\r\n        )\r\n    else:\r\n        config = CONFIG_MAPPING[args.model_type]()\r\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\r\n\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\r\n        )\r\n    elif args.model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.model_name_or_path,\r\n            use_fast=not args.use_slow_tokenizer,\r\n            trust_remote_code=args.trust_remote_code,\r\n        )\r\n    else:\r\n        raise ValueError(\r\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\r\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\r\n        )\r\n\r\n    ##########################\r\n    #        Tokenizer       #\r\n    ##########################\r\n    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\r\n    tokenizer.padding_side = \"left\"  # Allow batched inference\r\n    tokenizer.truncation_side = \"left\"\r\n\r\n    if args.model_name_or_path:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            args.model_name_or_path,\r\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\r\n            config=config,\r\n            low_cpu_mem_usage=True,\r\n            quantization_config=BitsAndBytesConfig(\r\n                load_in_4bit=True,\r\n                bnb_4bit_use_double_quant=False,\r\n                bnb_4bit_quant_type=\"nf4\",\r\n                bnb_4bit_compute_dtype=config.torch_dtype,\r\n            ),\r\n        )\r\n    else:\r\n        logger.info(\"Training new model from scratch\")\r\n        model = AutoModelForCausalLM.from_config(config, trust_remote_code=args.trust_remote_code)\r\n\r\n    ##########################\r\n    #       Peft Model       #\r\n    ##########################\r\n    if args.adapter_name_or_path is None:\r\n        model = PeftModel.from_pretrained(model, args.model_name_or_path, subfolder=\"loftq_init\", is_trainable=True)\r\n    else:\r\n        model = PeftModel.from_pretrained(model, args.adapter_name_or_path, is_trainable=True)\r\n    model.print_trainable_parameters()\r\n\r\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\r\n    # on a small vocab and want a smaller embedding size, remove this test.\r\n    embedding_size = model.get_input_embeddings().weight.shape[0]\r\n    if len(tokenizer) > embedding_size:\r\n        model.resize_token_embeddings(len(tokenizer))\r\n\r\n    # Preprocessing the datasets.\r\n    # First we tokenize all the texts.\r\n    ##########################\r\n    #      GSM8K dataset     #\r\n    ##########################\r\n\r\n    # Preprocessing the datasets.\r\n    # First we tokenize all the texts.\r\n    column_names = raw_datasets[\"train\"].column_names\r\n\r\n    # Get the column names for source/target.\r\n    source_column, target_column = \"question\", \"answer\"\r\n\r\n    # Temporarily set max_target_length for training.\r\n    padding = \"max_length\" if args.pad_to_max_length else False\r\n    task_prompt = \"\\nAnswer the above question. First think step by step and then answer the final number.\\n\"\r\n\r\n    def prompt_process(sent_1, sent_2, prompt_1=\"\", prompt_2=\"\", prompt_3=\"\"):\r\n        sent_2 = sent_2.replace(\"####\", \"The final answer is\")\r\n        return prompt_1 + sent_1 + prompt_2 + sent_2 + prompt_3\r\n\r\n    def preprocess_function_train(examples):\r\n        sources = examples[source_column]\r\n        targets = examples[target_column]\r\n\r\n        inputs = [prompt_process(source, target, prompt_2=task_prompt) for (source, target) in zip(sources, targets)]\r\n\r\n        model_inputs = tokenizer(\r\n            inputs,\r\n            max_length=args.max_source_length + args.max_target_length,\r\n            padding=padding,\r\n            truncation=True,\r\n            return_tensors=\"pt\",\r\n        )\r\n\r\n        labels = copy.deepcopy(model_inputs)\r\n\r\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\r\n        # padding in the loss.\r\n        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\r\n            # get the length of the target tokens. -1 to kick out the <BOS> token\r\n            target_tokens = tokenizer(targets, padding=False)\r\n            target_len = [len(label) - 1 for label in target_tokens[\"input_ids\"]]\r\n\r\n            # don't calculate the loss from source and padding (left padding)\r\n            for i in range(len(labels[\"input_ids\"])):\r\n                labels[\"input_ids\"][i, : -target_len[i]] = -100\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n\r\n    def preprocess_function_test(examples):\r\n        sources = examples[source_column]\r\n        labels = examples[target_column]\r\n\r\n        inputs = [source + task_prompt for source in sources]\r\n\r\n        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\r\n        labels = tokenizer(labels, max_length=args.max_target_length, padding=padding, truncation=True)\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n\r\n        return model_inputs\r\n\r\n    with accelerator.main_process_first():\r\n        train_dataset = raw_datasets[\"train\"].map(\r\n            preprocess_function_train,\r\n            batched=True,\r\n            num_proc=args.preprocessing_num_workers,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=not args.overwrite_cache,\r\n            desc=\"Running tokenizer on training dataset\",\r\n        )\r\n\r\n        eval_dataset = raw_datasets[\"test\"].map(\r\n            preprocess_function_test,\r\n            batched=True,\r\n            num_proc=args.preprocessing_num_workers,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=not args.overwrite_cache,\r\n            desc=\"Running tokenizer on test dataset\",\r\n        )\r\n\r\n    # Log a few random samples from the set:\r\n    for index in random.sample(range(len(train_dataset)), 2):\r\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\r\n    for index in random.sample(range(len(eval_dataset)), 2):\r\n        logger.info(f\"Sample {index} of the validation set: {eval_dataset[index]}.\")\r\n\r\n    # DataLoaders creation:\r\n    train_dataloader = DataLoader(\r\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\r\n    )\r\n\r\n    # Optimizer\r\n    # Split weights in two groups, one with weight decay and the other not.\r\n    no_decay = [\"bias\", \"layer_norm.weight\"]\r\n    optimizer_grouped_parameters = [\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and \"lora\" in n],\r\n            \"weight_decay\": args.weight_decay,\r\n        },\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n            \"weight_decay\": 0.0,\r\n        },\r\n    ]\r\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        name=args.lr_scheduler_type,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\r\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\r\n    )\r\n\r\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\r\n    if accelerator.distributed_type == DistributedType.TPU:\r\n        model.tie_weights()\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # Figure out how many steps we should save the Accelerator states\r\n    checkpointing_steps = args.checkpointing_steps\r\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\r\n        checkpointing_steps = int(checkpointing_steps)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if args.with_tracking:\r\n        experiment_config = vars(args)\r\n        # TensorBoard cannot log Enums, need the raw value\r\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\r\n        accelerator.init_trackers(\"clm_no_trainer\", experiment_config)\r\n\r\n    # Train!\r\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    completed_steps = 0\r\n    starting_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\r\n            checkpoint_path = args.resume_from_checkpoint\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the most recent checkpoint\r\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\r\n            dirs.sort(key=os.path.getctime)\r\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\r\n            checkpoint_path = path\r\n            path = os.path.basename(checkpoint_path)\r\n\r\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\r\n        accelerator.load_state(path)\r\n        # Extract `epoch_{i}` or `step_{i}`\r\n        training_difference = os.path.splitext(path)[0]\r\n\r\n        if \"epoch\" in training_difference:\r\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\r\n            resume_step = None\r\n            completed_steps = starting_epoch * num_update_steps_per_epoch\r\n        else:\r\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\r\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\r\n            starting_epoch = resume_step // len(train_dataloader)\r\n            resume_step -= starting_epoch * len(train_dataloader)\r\n            completed_steps = resume_step // args.gradient_accumulation_steps\r\n\r\n    # update the progress_bar if load from checkpoint\r\n    progress_bar.update(completed_steps)\r\n\r\n    for epoch in range(starting_epoch, args.num_train_epochs):\r\n        model.train()\r\n        if args.with_tracking:\r\n            total_loss = 0\r\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\r\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\r\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\r\n        else:\r\n            active_dataloader = train_dataloader\r\n        for step, batch in enumerate(active_dataloader):\r\n            with accelerator.accumulate(model):\r\n                outputs = model(**batch)\r\n                loss = outputs.loss\r\n                # We keep track of the loss at each epoch\r\n                if args.with_tracking:\r\n                    total_loss += loss.detach().float()\r\n                accelerator.backward(loss)\r\n                if completed_steps % 50:\r\n                    accelerator.print(f\"Epoch: {epoch} | Step: {completed_steps} | Loss: {loss}\")\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n\r\n            # Checks if the accelerator has performed an optimization step behind the scenes\r\n            if accelerator.sync_gradients:\r\n                progress_bar.update(1)\r\n                completed_steps += 1\r\n\r\n            if isinstance(checkpointing_steps, int):\r\n                if completed_steps % checkpointing_steps == 0:\r\n                    output_dir = f\"step_{completed_steps}\"\r\n                    if args.output_dir is not None:\r\n                        output_dir = os.path.join(args.output_dir, output_dir)\r\n                    accelerator.save_state(output_dir)\r\n            if completed_steps >= args.max_train_steps:\r\n                break\r\n\r\n        model.eval()\r\n        gen_kwargs = {\r\n            \"max_new_tokens\": args.max_target_length,\r\n            \"temperature\": args.temperature,\r\n            \"top_k\": args.k,\r\n            \"top_p\": args.p,\r\n            \"do_sample\": True,\r\n        }\r\n        ans_pred_list = []\r\n        ans_gold_list = []\r\n        for step, batch in enumerate(eval_dataloader):\r\n            with torch.no_grad():\r\n                gen_kwargs[\"input_ids\"] = batch[\"input_ids\"]\r\n                gen_kwargs[\"attention_mask\"] = batch[\"attention_mask\"]\r\n                generated_tokens = accelerator.unwrap_model(model).generate(**gen_kwargs)\r\n\r\n            pred_tokens = generated_tokens[:, args.max_source_length :]\r\n            pred_tokens = accelerator.pad_across_processes(pred_tokens, dim=1, pad_index=tokenizer.pad_token_id)\r\n            gold_tokens = batch[\"labels\"]\r\n\r\n            if not args.pad_to_max_length:\r\n                # If we did not pad to max length, we need to pad the labels too\r\n                gold_tokens = accelerator.pad_across_processes(\r\n                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\r\n                )\r\n\r\n            pred_tokens, gold_tokens = accelerator.gather_for_metrics((pred_tokens, gold_tokens))\r\n            pred_tokens, gold_tokens = pred_tokens.cpu().numpy(), gold_tokens.cpu().numpy()\r\n\r\n            if isinstance(pred_tokens, tuple):\r\n                pred_tokens = pred_tokens[0]\r\n            decoded_pred = tokenizer.batch_decode(pred_tokens, skip_special_tokens=True)\r\n            decoded_gold = tokenizer.batch_decode(gold_tokens, skip_special_tokens=True)\r\n\r\n            # Extract the numbers in sentences\r\n            accelerator.print(decoded_pred)\r\n            ans_pred_list += [extract_answer_number(sentence_pred) for sentence_pred in decoded_pred]\r\n            ans_gold_list += [extract_answer_number(sentence_gold) for sentence_gold in decoded_gold]\r\n\r\n        accelerator.print(ans_pred_list)\r\n        accelerator.print(ans_gold_list)\r\n        accuracy = compute_accuracy(ans_gold_list, ans_pred_list)\r\n\r\n        logger.info(f\"epoch {epoch}: accuracy: {accuracy}\")\r\n\r\n        if args.with_tracking:\r\n            accelerator.log(\r\n                {\r\n                    \"accuracy\": accuracy,\r\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\r\n                    \"epoch\": epoch,\r\n                    \"step\": completed_steps,\r\n                },\r\n                step=completed_steps,\r\n            )\r\n\r\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\r\n            accelerator.wait_for_everyone()\r\n            unwrapped_model = accelerator.unwrap_model(model)\r\n            unwrapped_model.save_pretrained(\r\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\r\n            )\r\n            if accelerator.is_main_process:\r\n                tokenizer.save_pretrained(args.output_dir)\r\n                api.upload_folder(\r\n                    repo_id=repo_id,\r\n                    folder_path=args.output_dir,\r\n                    commit_message=f\"Training in progress epoch {epoch}\",\r\n                    run_as_future=True,\r\n                )\r\n\r\n        if args.checkpointing_steps == \"epoch\":\r\n            output_dir = f\"epoch_{epoch}\"\r\n            if args.output_dir is not None:\r\n                output_dir = os.path.join(args.output_dir, output_dir)\r\n            accelerator.save_state(output_dir)\r\n\r\n    if args.with_tracking:\r\n        accelerator.end_training()\r\n\r\n    if args.output_dir is not None:\r\n        accelerator.wait_for_everyone()\r\n        unwrapped_model = accelerator.unwrap_model(model)\r\n        unwrapped_model.save_pretrained(\r\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\r\n        )\r\n        if accelerator.is_main_process:\r\n            tokenizer.save_pretrained(args.output_dir)\r\n            if args.push_to_hub:\r\n                api.upload_folder(\r\n                    repo_id=repo_id,\r\n                    folder_path=args.output_dir,\r\n                    commit_message=\"End of training\",\r\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_dir=logging_dir,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        import wandb\r\n\r\n        wandb.login(key=args.wandb_key)\r\n        wandb.init(project=args.wandb_project_name)\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Generate class images if prior preservation is enabled.\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            if args.prior_generation_precision == \"fp32\":\r\n                torch_dtype = torch.float32\r\n            elif args.prior_generation_precision == \"fp16\":\r\n                torch_dtype = torch.float16\r\n            elif args.prior_generation_precision == \"bf16\":\r\n                torch_dtype = torch.bfloat16\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDPMScheduler(\r\n        beta_start=0.00085,\r\n        beta_end=0.012,\r\n        beta_schedule=\"scaled_linear\",\r\n        num_train_timesteps=1000,\r\n    )  # DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n\r\n    if args.use_lora:\r\n        config = LoraConfig(\r\n            r=args.lora_r,\r\n            lora_alpha=args.lora_alpha,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            lora_dropout=args.lora_dropout,\r\n            bias=args.lora_bias,\r\n        )\r\n        unet = get_peft_model(unet, config)\r\n        unet.print_trainable_parameters()\r\n        print(unet)\r\n\r\n    vae.requires_grad_(False)\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n    elif args.train_text_encoder and args.use_lora:\r\n        config = LoraConfig(\r\n            r=args.lora_text_encoder_r,\r\n            lora_alpha=args.lora_text_encoder_alpha,\r\n            target_modules=TEXT_ENCODER_TARGET_MODULES,\r\n            lora_dropout=args.lora_text_encoder_dropout,\r\n            bias=args.lora_text_encoder_bias,\r\n        )\r\n        text_encoder = get_peft_model(text_encoder, config)\r\n        text_encoder.print_trainable_parameters()\r\n        print(text_encoder)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        # below fails when using lora so commenting it out\r\n        if args.train_text_encoder and not args.use_lora:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    # Optimizer creation\r\n    params_to_optimize = (\r\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\r\n    )\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Dataset and DataLoaders creation:\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=args.train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\r\n        num_workers=args.num_dataloader_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move vae and text_encoder to device and cast to weight_dtype\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the mos recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1]\r\n        accelerator.print(f\"Resuming from checkpoint {path}\")\r\n        accelerator.load_state(os.path.join(args.output_dir, path))\r\n        global_step = int(path.split(\"-\")[1])\r\n\r\n        resume_global_step = global_step * args.gradient_accumulation_steps\r\n        first_epoch = resume_global_step // num_update_steps_per_epoch\r\n        resume_step = resume_global_step % num_update_steps_per_epoch\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n        if args.train_text_encoder:\r\n            text_encoder.train()\r\n        with TorchTracemalloc() if not args.no_tracemalloc else nullcontext() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * 0.18215\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    if args.with_prior_preservation:\r\n                        # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                        target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                        # Compute instance loss\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                        # Compute prior loss\r\n                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                        # Add the prior loss to the instance loss.\r\n                        loss = loss + args.prior_loss_weight * prior_loss\r\n                    else:\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else unet.parameters()\r\n                        )\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                    # if global_step % args.checkpointing_steps == 0:\r\n                    #     if accelerator.is_main_process:\r\n                    #         save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\r\n                    #         accelerator.save_state(save_path)\r\n                    #         logger.info(f\"Saved state to {save_path}\")\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if (\r\n                    args.validation_prompt is not None\r\n                    and (step + num_update_steps_per_epoch * epoch) % args.validation_steps == 0\r\n                ):\r\n                    logger.info(\r\n                        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\r\n                        f\" {args.validation_prompt}.\"\r\n                    )\r\n                    # create pipeline\r\n                    pipeline = DiffusionPipeline.from_pretrained(\r\n                        args.pretrained_model_name_or_path,\r\n                        safety_checker=None,\r\n                        revision=args.revision,\r\n                    )\r\n                    # set `keep_fp32_wrapper` to True because we do not want to remove\r\n                    # mixed precision hooks while we are still training\r\n                    pipeline.unet = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\r\n                    pipeline.text_encoder = accelerator.unwrap_model(text_encoder, keep_fp32_wrapper=True)\r\n                    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\r\n                    pipeline = pipeline.to(accelerator.device)\r\n                    pipeline.set_progress_bar_config(disable=True)\r\n\r\n                    # run inference\r\n                    if args.seed is not None:\r\n                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\r\n                    else:\r\n                        generator = None\r\n                    images = []\r\n                    for _ in range(args.num_validation_images):\r\n                        image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\r\n                        images.append(image)\r\n\r\n                    for tracker in accelerator.trackers:\r\n                        if tracker.name == \"tensorboard\":\r\n                            np_images = np.stack([np.asarray(img) for img in images])\r\n                            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                        if tracker.name == \"wandb\":\r\n                            import wandb\r\n\r\n                            tracker.log(\r\n                                {\r\n                                    \"validation\": [\r\n                                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\r\n                                        for i, image in enumerate(images)\r\n                                    ]\r\n                                }\r\n                            )\r\n\r\n                    del pipeline\r\n                    torch.cuda.empty_cache()\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n\r\n        if not args.no_tracemalloc:\r\n            accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n            accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n            accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n            accelerator.print(\r\n                f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n            )\r\n\r\n            accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n            accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n            accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n            accelerator.print(\r\n                f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n            )\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    accelerator.wait_for_everyone()\r\n    if accelerator.is_main_process:\r\n        if args.use_lora:\r\n            unwarpped_unet = accelerator.unwrap_model(unet)\r\n            unwarpped_unet.save_pretrained(\r\n                os.path.join(args.output_dir, \"unet\"), state_dict=accelerator.get_state_dict(unet)\r\n            )\r\n            if args.train_text_encoder:\r\n                unwarpped_text_encoder = accelerator.unwrap_model(text_encoder)\r\n                unwarpped_text_encoder.save_pretrained(\r\n                    os.path.join(args.output_dir, \"text_encoder\"),\r\n                    state_dict=accelerator.get_state_dict(text_encoder),\r\n                )\r\n        else:\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                unet=accelerator.unwrap_model(unet),\r\n                text_encoder=accelerator.unwrap_model(text_encoder),\r\n                revision=args.revision,\r\n            )\r\n            pipeline.save_pretrained(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            api.upload_folder(\r\n                repo_id=repo_id,\r\n                folder_path=args.output_dir,\r\n                commit_message=\"End of training\",\r\n                run_as_future=True,\r\n            )\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\r\n    args = parse_args()\r\n    ddp_scaler = DistributedDataParallelKwargs(find_unused_parameters=True)\r\n    accelerator = Accelerator(kwargs_handlers=[ddp_scaler])\r\n\r\n    task = \"mrpc\"\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    if args.peft_type == \"p_tuning\":\r\n        peft_config = PromptEncoderConfig(\r\n            task_type=\"SEQ_CLS\",\r\n            num_virtual_tokens=args.num_virtual_tokens,\r\n            encoder_hidden_size=args.encoder_hidden_size,\r\n        )\r\n    elif args.peft_type == \"prefix_tuning\":\r\n        peft_config = PrefixTuningConfig(\r\n            task_type=\"SEQ_CLS\",\r\n            num_virtual_tokens=args.num_virtual_tokens,\r\n            encoder_hidden_size=args.encoder_hidden_size,\r\n        )\r\n    else:\r\n        peft_config = PromptTuningConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=args.num_virtual_tokens)\r\n\r\n    tokenizer_kwargs = {}\r\n\r\n    if any(k in args.model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\r\n        tokenizer_kwargs[\"padding_side\"] = \"left\"\r\n    else:\r\n        tokenizer_kwargs[\"padding_side\"] = \"right\"\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, **tokenizer_kwargs)\r\n    if getattr(tokenizer, \"pad_token_id\") is None:\r\n        tokenizer.pad_token_id = tokenizer.eos_token_id\r\n\r\n    datasets = load_dataset(\"glue\", task)\r\n    metric = evaluate.load(\"glue\", task)\r\n\r\n    def tokenize_function(examples):\r\n        # max_length=None => use the model max length (it's actually the default)\r\n        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\r\n        return outputs\r\n\r\n    def collate_fn(examples):\r\n        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\r\n\r\n    with accelerator.main_process_first():\r\n        tokenized_datasets = datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\r\n        )\r\n\r\n    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\r\n    # transformers library\r\n    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\r\n\r\n    # Instantiate dataloaders.\r\n    train_dataloader = DataLoader(\r\n        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        tokenized_datasets[\"validation\"],\r\n        shuffle=False,\r\n        collate_fn=collate_fn,\r\n        batch_size=args.per_device_eval_batch_size,\r\n    )\r\n\r\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path)\r\n    model = get_peft_model(model, peft_config)\r\n    model.print_trainable_parameters()\r\n\r\n    if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\r\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\r\n        model = accelerator.prepare(model)\r\n\r\n    optimizer = AdamW(params=model.parameters(), lr=args.learning_rate)\r\n\r\n    # Instantiate scheduler\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.num_warmup_steps,\r\n        num_training_steps=(len(train_dataloader) * args.num_train_epochs),\r\n    )\r\n\r\n    if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\r\n        train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\r\n            train_dataloader, eval_dataloader, optimizer, lr_scheduler\r\n        )\r\n    else:\r\n        model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\r\n            model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\r\n        )\r\n\r\n    for epoch in range(args.num_train_epochs):\r\n        model.train()\r\n        for step, batch in enumerate(tqdm(train_dataloader)):\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        model.eval()\r\n        samples_seen = 0\r\n        for step, batch in enumerate(tqdm(eval_dataloader)):\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            predictions = outputs.logits.argmax(dim=-1)\r\n            predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\r\n            # If we are in a multiprocess environment, the last batch has duplicates\r\n            if accelerator.num_processes > 1:\r\n                if step == len(eval_dataloader) - 1:\r\n                    predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\r\n                    references = references[: len(eval_dataloader.dataset) - samples_seen]\r\n                else:\r\n                    samples_seen += references.shape[0]\r\n            metric.add_batch(\r\n                predictions=predictions,\r\n                references=references,\r\n            )\r\n        eval_metric = metric.compute()\r\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\r\n\r\n    accelerator.wait_for_everyone()\r\n    unwrapped_model = accelerator.unwrap_model(model)\r\n    unwrapped_model.save_pretrained(args.output_dir, state_dict=accelerator.get_state_dict(model))\r\n    if accelerator.is_main_process:\r\n        tokenizer.save_pretrained(args.output_dir)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_dir=logging_dir,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        import wandb\r\n\r\n        wandb.login(key=args.wandb_key)\r\n        wandb.init(project=args.wandb_project_name)\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Generate class images if prior preservation is enabled.\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            if args.prior_generation_precision == \"fp32\":\r\n                torch_dtype = torch.float32\r\n            elif args.prior_generation_precision == \"fp16\":\r\n                torch_dtype = torch.float16\r\n            elif args.prior_generation_precision == \"bf16\":\r\n                torch_dtype = torch.bfloat16\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDPMScheduler(\r\n        beta_start=0.00085,\r\n        beta_end=0.012,\r\n        beta_schedule=\"scaled_linear\",\r\n        num_train_timesteps=1000,\r\n    )  # DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n\r\n    if args.use_oft:\r\n        config = OFTConfig(\r\n            r=args.oft_r,\r\n            alpha=args.oft_alpha,\r\n            target_modules=UNET_TARGET_MODULES,\r\n            module_dropout=args.oft_dropout,\r\n            init_weights=True,\r\n            coft=args.oft_use_coft,\r\n            eps=args.oft_eps,\r\n        )\r\n        unet = get_peft_model(unet, config)\r\n        unet.print_trainable_parameters()\r\n        print(unet)\r\n\r\n    vae.requires_grad_(False)\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n    elif args.train_text_encoder and args.use_oft:\r\n        config = OFTConfig(\r\n            r=args.oft_text_encoder_r,\r\n            alpha=args.oft_text_encoder_alpha,\r\n            target_modules=TEXT_ENCODER_TARGET_MODULES,\r\n            module_dropout=args.oft_text_encoder_dropout,\r\n            init_weights=True,\r\n            coft=args.oft_text_encoder_use_coft,\r\n            eps=args.oft_text_encoder_eps,\r\n        )\r\n        text_encoder = get_peft_model(text_encoder, config)\r\n        text_encoder.print_trainable_parameters()\r\n        print(text_encoder)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        # below fails when using oft so commenting it out\r\n        if args.train_text_encoder and not args.use_oft:\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    # Optimizer creation\r\n    params_to_optimize = (\r\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\r\n    )\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Dataset and DataLoaders creation:\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=args.train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\r\n        num_workers=args.num_dataloader_workers,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move vae and text_encoder to device and cast to weight_dtype\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the mos recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1]\r\n        accelerator.print(f\"Resuming from checkpoint {path}\")\r\n        accelerator.load_state(os.path.join(args.output_dir, path))\r\n        global_step = int(path.split(\"-\")[1])\r\n\r\n        resume_global_step = global_step * args.gradient_accumulation_steps\r\n        first_epoch = resume_global_step // num_update_steps_per_epoch\r\n        resume_step = resume_global_step % num_update_steps_per_epoch\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n        if args.train_text_encoder:\r\n            text_encoder.train()\r\n        with TorchTracemalloc() if not args.no_tracemalloc else nullcontext() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * 0.18215\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    if args.with_prior_preservation:\r\n                        # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                        target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                        # Compute instance loss\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                        # Compute prior loss\r\n                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                        # Add the prior loss to the instance loss.\r\n                        loss = loss + args.prior_loss_weight * prior_loss\r\n                    else:\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else unet.parameters()\r\n                        )\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if (\r\n                    args.validation_prompt is not None\r\n                    and (step + num_update_steps_per_epoch * epoch) % args.validation_steps == 0\r\n                ):\r\n                    logger.info(\r\n                        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\r\n                        f\" {args.validation_prompt}.\"\r\n                    )\r\n                    # create pipeline\r\n                    pipeline = DiffusionPipeline.from_pretrained(\r\n                        args.pretrained_model_name_or_path,\r\n                        safety_checker=None,\r\n                        revision=args.revision,\r\n                    )\r\n                    # set `keep_fp32_wrapper` to True because we do not want to remove\r\n                    # mixed precision hooks while we are still training\r\n                    pipeline.unet = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\r\n                    pipeline.text_encoder = accelerator.unwrap_model(text_encoder, keep_fp32_wrapper=True)\r\n                    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\r\n                    pipeline = pipeline.to(accelerator.device)\r\n                    pipeline.set_progress_bar_config(disable=True)\r\n\r\n                    # run inference\r\n                    if args.seed is not None:\r\n                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\r\n                    else:\r\n                        generator = None\r\n                    images = []\r\n                    for _ in range(args.num_validation_images):\r\n                        image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\r\n                        images.append(image)\r\n\r\n                    for tracker in accelerator.trackers:\r\n                        if tracker.name == \"tensorboard\":\r\n                            np_images = np.stack([np.asarray(img) for img in images])\r\n                            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                        if tracker.name == \"wandb\":\r\n                            import wandb\r\n\r\n                            tracker.log(\r\n                                {\r\n                                    \"validation\": [\r\n                                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\r\n                                        for i, image in enumerate(images)\r\n                                    ]\r\n                                }\r\n                            )\r\n\r\n                    del pipeline\r\n                    torch.cuda.empty_cache()\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n\r\n        if not args.no_tracemalloc:\r\n            accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n            accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n            accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n            accelerator.print(\r\n                f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n            )\r\n\r\n            accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n            accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n            accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n            accelerator.print(\r\n                f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n            )\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    accelerator.wait_for_everyone()\r\n    if accelerator.is_main_process:\r\n        if args.use_oft:\r\n            unwarpped_unet = accelerator.unwrap_model(unet)\r\n            unwarpped_unet.save_pretrained(\r\n                os.path.join(args.output_dir, \"unet\"), state_dict=accelerator.get_state_dict(unet)\r\n            )\r\n            if args.train_text_encoder:\r\n                unwarpped_text_encoder = accelerator.unwrap_model(text_encoder)\r\n                unwarpped_text_encoder.save_pretrained(\r\n                    os.path.join(args.output_dir, \"text_encoder\"),\r\n                    state_dict=accelerator.get_state_dict(text_encoder),\r\n                )\r\n        else:\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                unet=accelerator.unwrap_model(unet),\r\n                text_encoder=accelerator.unwrap_model(text_encoder),\r\n                revision=args.revision,\r\n            )\r\n            pipeline.save_pretrained(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            api.upload_folder(\r\n                repo_id=repo_id,\r\n                folder_path=args.output_dir,\r\n                commit_message=\"End of training\",\r\n                run_as_future=True,\r\n            )\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(args):\r\n    logging_dir = Path(args.output_dir, args.logging_dir)\r\n\r\n    accelerator = Accelerator(\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        mixed_precision=args.mixed_precision,\r\n        log_with=args.report_to,\r\n        project_dir=logging_dir,\r\n    )\r\n    if args.report_to == \"wandb\":\r\n        import wandb\r\n\r\n        wandb.login(key=args.wandb_key)\r\n        wandb.init(project=args.wandb_project_name)\r\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\r\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\r\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\r\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\r\n        raise ValueError(\r\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\r\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\r\n        )\r\n\r\n    # Make one log on every process with the configuration for debugging.\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n        level=logging.INFO,\r\n    )\r\n    logger.info(accelerator.state, main_process_only=False)\r\n    if accelerator.is_local_main_process:\r\n        datasets.utils.logging.set_verbosity_warning()\r\n        transformers.utils.logging.set_verbosity_warning()\r\n        diffusers.utils.logging.set_verbosity_info()\r\n    else:\r\n        datasets.utils.logging.set_verbosity_error()\r\n        transformers.utils.logging.set_verbosity_error()\r\n        diffusers.utils.logging.set_verbosity_error()\r\n\r\n    # If passed along, set the training seed now.\r\n    if args.seed is not None:\r\n        set_seed(args.seed)\r\n\r\n    # Generate class images if prior preservation is enabled.\r\n    if args.with_prior_preservation:\r\n        class_images_dir = Path(args.class_data_dir)\r\n        if not class_images_dir.exists():\r\n            class_images_dir.mkdir(parents=True)\r\n        cur_class_images = len(list(class_images_dir.iterdir()))\r\n\r\n        if cur_class_images < args.num_class_images:\r\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\r\n            if args.prior_generation_precision == \"fp32\":\r\n                torch_dtype = torch.float32\r\n            elif args.prior_generation_precision == \"fp16\":\r\n                torch_dtype = torch.float16\r\n            elif args.prior_generation_precision == \"bf16\":\r\n                torch_dtype = torch.bfloat16\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                torch_dtype=torch_dtype,\r\n                safety_checker=None,\r\n                revision=args.revision,\r\n            )\r\n            pipeline.set_progress_bar_config(disable=True)\r\n\r\n            num_new_images = args.num_class_images - cur_class_images\r\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\r\n\r\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\r\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\r\n\r\n            sample_dataloader = accelerator.prepare(sample_dataloader)\r\n            pipeline.to(accelerator.device)\r\n\r\n            for example in tqdm(\r\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\r\n            ):\r\n                images = pipeline(example[\"prompt\"]).images\r\n\r\n                for i, image in enumerate(images):\r\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\r\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\r\n                    image.save(image_filename)\r\n\r\n            del pipeline\r\n            if torch.cuda.is_available():\r\n                torch.cuda.empty_cache()\r\n\r\n    # Handle the repository creation\r\n    if accelerator.is_main_process:\r\n        if args.push_to_hub:\r\n            api = HfApi(token=args.hub_token)\r\n\r\n            # Create repo (repo_name from args or inferred)\r\n            repo_name = args.hub_model_id\r\n            if repo_name is None:\r\n                repo_name = Path(args.output_dir).absolute().name\r\n            repo_id = api.create_repo(repo_name, exist_ok=True).repo_id\r\n\r\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\r\n                if \"step_*\" not in gitignore:\r\n                    gitignore.write(\"step_*\\n\")\r\n                if \"epoch_*\" not in gitignore:\r\n                    gitignore.write(\"epoch_*\\n\")\r\n        elif args.output_dir is not None:\r\n            os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Load the tokenizer\r\n    if args.tokenizer_name:\r\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\r\n    elif args.pretrained_model_name_or_path:\r\n        tokenizer = AutoTokenizer.from_pretrained(\r\n            args.pretrained_model_name_or_path,\r\n            subfolder=\"tokenizer\",\r\n            revision=args.revision,\r\n            use_fast=False,\r\n        )\r\n\r\n    # import correct text encoder class\r\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\r\n\r\n    # Load scheduler and models\r\n    noise_scheduler = DDPMScheduler(\r\n        beta_start=0.00085,\r\n        beta_end=0.012,\r\n        beta_schedule=\"scaled_linear\",\r\n        num_train_timesteps=1000,\r\n    )  # DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\r\n    text_encoder = text_encoder_cls.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\r\n    )\r\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\r\n    unet = UNet2DConditionModel.from_pretrained(\r\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\r\n    )\r\n\r\n    if args.adapter != \"full\":\r\n        config = create_unet_adapter_config(args)\r\n        unet = get_peft_model(unet, config)\r\n        unet.print_trainable_parameters()\r\n        print(unet)\r\n\r\n    vae.requires_grad_(False)\r\n    if not args.train_text_encoder:\r\n        text_encoder.requires_grad_(False)\r\n    elif args.train_text_encoder and args.adapter != \"full\":\r\n        config = create_text_encoder_adapter_config(args)\r\n        text_encoder = get_peft_model(text_encoder, config)\r\n        text_encoder.print_trainable_parameters()\r\n        print(text_encoder)\r\n\r\n    if args.enable_xformers_memory_efficient_attention:\r\n        if is_xformers_available():\r\n            unet.enable_xformers_memory_efficient_attention()\r\n        else:\r\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\r\n\r\n    if args.gradient_checkpointing:\r\n        unet.enable_gradient_checkpointing()\r\n        if args.train_text_encoder and not args.adapter != \"full\":\r\n            text_encoder.gradient_checkpointing_enable()\r\n\r\n    # Enable TF32 for faster training on Ampere GPUs,\r\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\r\n    if args.allow_tf32:\r\n        torch.backends.cuda.matmul.allow_tf32 = True\r\n\r\n    if args.scale_lr:\r\n        args.learning_rate = (\r\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\r\n        )\r\n\r\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\r\n    if args.use_8bit_adam:\r\n        try:\r\n            import bitsandbytes as bnb\r\n        except ImportError:\r\n            raise ImportError(\r\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\r\n            )\r\n\r\n        optimizer_class = bnb.optim.AdamW8bit\r\n    else:\r\n        optimizer_class = torch.optim.AdamW\r\n\r\n    # Optimizer creation\r\n    params_to_optimize = (\r\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\r\n    )\r\n    optimizer = optimizer_class(\r\n        params_to_optimize,\r\n        lr=args.learning_rate,\r\n        betas=(args.adam_beta1, args.adam_beta2),\r\n        weight_decay=args.adam_weight_decay,\r\n        eps=args.adam_epsilon,\r\n    )\r\n\r\n    # Dataset and DataLoaders creation:\r\n    train_dataset = DreamBoothDataset(\r\n        instance_data_root=args.instance_data_dir,\r\n        instance_prompt=args.instance_prompt,\r\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\r\n        class_prompt=args.class_prompt,\r\n        tokenizer=tokenizer,\r\n        size=args.resolution,\r\n        center_crop=args.center_crop,\r\n    )\r\n\r\n    train_dataloader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=args.train_batch_size,\r\n        shuffle=True,\r\n        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\r\n        num_workers=1,\r\n    )\r\n\r\n    # Scheduler and math around the number of training steps.\r\n    overrode_max_train_steps = False\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if args.max_train_steps is None:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n        overrode_max_train_steps = True\r\n\r\n    lr_scheduler = get_scheduler(\r\n        args.lr_scheduler,\r\n        optimizer=optimizer,\r\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\r\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\r\n        num_cycles=args.lr_num_cycles,\r\n        power=args.lr_power,\r\n    )\r\n\r\n    # Prepare everything with our `accelerator`.\r\n    if args.train_text_encoder:\r\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n    else:\r\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\r\n            unet, optimizer, train_dataloader, lr_scheduler\r\n        )\r\n\r\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\r\n    # as these models are only used for inference, keeping weights in full precision is not required.\r\n    weight_dtype = torch.float32\r\n    if accelerator.mixed_precision == \"fp16\":\r\n        weight_dtype = torch.float16\r\n    elif accelerator.mixed_precision == \"bf16\":\r\n        weight_dtype = torch.bfloat16\r\n\r\n    # Move vae and text_encoder to device and cast to weight_dtype\r\n    vae.to(accelerator.device, dtype=weight_dtype)\r\n    if not args.train_text_encoder:\r\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\r\n\r\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\r\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\r\n    if overrode_max_train_steps:\r\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\r\n    # Afterwards we recalculate our number of training epochs\r\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\r\n\r\n    # We need to initialize the trackers we use, and also store our configuration.\r\n    # The trackers initializes automatically on the main process.\r\n    if accelerator.is_main_process:\r\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\r\n\r\n    # Train!\r\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\r\n\r\n    logger.info(\"***** Running training *****\")\r\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\r\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\r\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\r\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\r\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\r\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\r\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\r\n    global_step = 0\r\n    first_epoch = 0\r\n\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.resume_from_checkpoint:\r\n        if args.resume_from_checkpoint != \"latest\":\r\n            path = os.path.basename(args.resume_from_checkpoint)\r\n        else:\r\n            # Get the mos recent checkpoint\r\n            dirs = os.listdir(args.output_dir)\r\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\r\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\r\n            path = dirs[-1]\r\n        accelerator.print(f\"Resuming from checkpoint {path}\")\r\n        accelerator.load_state(os.path.join(args.output_dir, path))\r\n        global_step = int(path.split(\"-\")[1])\r\n\r\n        resume_global_step = global_step * args.gradient_accumulation_steps\r\n        first_epoch = resume_global_step // num_update_steps_per_epoch\r\n        resume_step = resume_global_step % num_update_steps_per_epoch\r\n\r\n    # Only show the progress bar once on each machine.\r\n    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\r\n    progress_bar.set_description(\"Steps\")\r\n\r\n    for epoch in range(first_epoch, args.num_train_epochs):\r\n        unet.train()\r\n        if args.train_text_encoder:\r\n            text_encoder.train()\r\n        with TorchTracemalloc() as tracemalloc:\r\n            for step, batch in enumerate(train_dataloader):\r\n                # Skip steps until we reach the resumed step\r\n                if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\r\n                    if step % args.gradient_accumulation_steps == 0:\r\n                        progress_bar.update(1)\r\n                        if args.report_to == \"wandb\":\r\n                            accelerator.print(progress_bar)\r\n                    continue\r\n\r\n                with accelerator.accumulate(unet):\r\n                    # Convert images to latent space\r\n                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\r\n                    latents = latents * 0.18215\r\n\r\n                    # Sample noise that we'll add to the latents\r\n                    noise = torch.randn_like(latents)\r\n                    bsz = latents.shape[0]\r\n                    # Sample a random timestep for each image\r\n                    timesteps = torch.randint(\r\n                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\r\n                    )\r\n                    timesteps = timesteps.long()\r\n\r\n                    # Add noise to the latents according to the noise magnitude at each timestep\r\n                    # (this is the forward diffusion process)\r\n                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\r\n\r\n                    # Get the text embedding for conditioning\r\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\r\n\r\n                    # Predict the noise residual\r\n                    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\r\n\r\n                    # Get the target for loss depending on the prediction type\r\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\r\n                        target = noise\r\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\r\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\r\n                    else:\r\n                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\r\n\r\n                    if args.with_prior_preservation:\r\n                        # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\r\n                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\r\n                        target, target_prior = torch.chunk(target, 2, dim=0)\r\n\r\n                        # Compute instance loss\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                        # Compute prior loss\r\n                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\r\n\r\n                        # Add the prior loss to the instance loss.\r\n                        loss = loss + args.prior_loss_weight * prior_loss\r\n                    else:\r\n                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\r\n\r\n                    accelerator.backward(loss)\r\n                    if accelerator.sync_gradients:\r\n                        params_to_clip = (\r\n                            itertools.chain(unet.parameters(), text_encoder.parameters())\r\n                            if args.train_text_encoder\r\n                            else unet.parameters()\r\n                        )\r\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\r\n                    optimizer.step()\r\n                    lr_scheduler.step()\r\n                    optimizer.zero_grad()\r\n\r\n                # Checks if the accelerator has performed an optimization step behind the scenes\r\n                if accelerator.sync_gradients:\r\n                    progress_bar.update(1)\r\n                    if args.report_to == \"wandb\":\r\n                        accelerator.print(progress_bar)\r\n                    global_step += 1\r\n\r\n                    # if global_step % args.checkpointing_steps == 0:\r\n                    #     if accelerator.is_main_process:\r\n                    #         save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\r\n                    #         accelerator.save_state(save_path)\r\n                    #         logger.info(f\"Saved state to {save_path}\")\r\n\r\n                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\r\n                progress_bar.set_postfix(**logs)\r\n                accelerator.log(logs, step=global_step)\r\n\r\n                if (\r\n                    args.validation_prompt is not None\r\n                    and (step + num_update_steps_per_epoch * epoch) % args.validation_steps == 0\r\n                ):\r\n                    logger.info(\r\n                        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\r\n                        f\" {args.validation_prompt}.\"\r\n                    )\r\n                    # create pipeline\r\n                    pipeline = DiffusionPipeline.from_pretrained(\r\n                        args.pretrained_model_name_or_path,\r\n                        safety_checker=None,\r\n                        revision=args.revision,\r\n                    )\r\n                    # set `keep_fp32_wrapper` to True because we do not want to remove\r\n                    # mixed precision hooks while we are still training\r\n                    pipeline.unet = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\r\n                    pipeline.text_encoder = accelerator.unwrap_model(text_encoder, keep_fp32_wrapper=True)\r\n                    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\r\n                    pipeline = pipeline.to(accelerator.device)\r\n                    pipeline.set_progress_bar_config(disable=True)\r\n\r\n                    # Set evaliation mode\r\n                    pipeline.unet.eval()\r\n                    pipeline.text_encoder.eval()\r\n\r\n                    # run inference\r\n                    if args.seed is not None:\r\n                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\r\n                    else:\r\n                        generator = None\r\n                    images = []\r\n                    for _ in range(args.num_validation_images):\r\n                        image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\r\n                        images.append(image)\r\n\r\n                    for tracker in accelerator.trackers:\r\n                        if tracker.name == \"tensorboard\":\r\n                            np_images = np.stack([np.asarray(img) for img in images])\r\n                            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\r\n                        if tracker.name == \"wandb\":\r\n                            import wandb\r\n\r\n                            tracker.log(\r\n                                {\r\n                                    \"validation\": [\r\n                                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\r\n                                        for i, image in enumerate(images)\r\n                                    ]\r\n                                }\r\n                            )\r\n\r\n                    # Set evaliation mode\r\n                    pipeline.unet.train()\r\n                    pipeline.text_encoder.train()\r\n\r\n                    del pipeline\r\n                    torch.cuda.empty_cache()\r\n\r\n                if global_step >= args.max_train_steps:\r\n                    break\r\n        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\r\n        accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\r\n        accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\r\n        accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\r\n        accelerator.print(\r\n            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\r\n        )\r\n\r\n        accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\r\n        accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\r\n        accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\r\n        accelerator.print(\r\n            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\r\n        )\r\n\r\n    # Create the pipeline using using the trained modules and save it.\r\n    accelerator.wait_for_everyone()\r\n    if accelerator.is_main_process:\r\n        if args.adapter != \"full\":\r\n            unwarpped_unet = accelerator.unwrap_model(unet)\r\n            unwarpped_unet.save_pretrained(\r\n                os.path.join(args.output_dir, \"unet\"), state_dict=accelerator.get_state_dict(unet)\r\n            )\r\n            if args.train_text_encoder:\r\n                unwarpped_text_encoder = accelerator.unwrap_model(text_encoder)\r\n                unwarpped_text_encoder.save_pretrained(\r\n                    os.path.join(args.output_dir, \"text_encoder\"),\r\n                    state_dict=accelerator.get_state_dict(text_encoder),\r\n                )\r\n        else:\r\n            pipeline = DiffusionPipeline.from_pretrained(\r\n                args.pretrained_model_name_or_path,\r\n                unet=accelerator.unwrap_model(unet),\r\n                text_encoder=accelerator.unwrap_model(text_encoder),\r\n                revision=args.revision,\r\n            )\r\n            pipeline.save_pretrained(args.output_dir)\r\n\r\n        if args.push_to_hub:\r\n            api.upload_folder(\r\n                repo_id=repo_id,\r\n                folder_path=args.output_dir,\r\n                commit_message=\"End of training\",\r\n                run_as_future=True,\r\n            )\r\n\r\n    accelerator.end_training()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def forward(self, *args, **kwargs):\r\n        outputs = self.model.forward(*args, **kwargs)\r\n\r\n        if (getattr(outputs, \"loss\", None) is not None) and isinstance(outputs.loss, torch.Tensor):\r\n            # Calculate the orthogonal regularization\r\n            orth_reg_weight = self.peft_config[self.trainable_adapter_name].orth_reg_weight\r\n\r\n            if orth_reg_weight <= 0:\r\n                raise ValueError(\"orth_reg_weight should be greater than 0. \")\r\n\r\n            regu_loss = 0\r\n            num_param = 0\r\n            for n, p in self.model.named_parameters():\r\n                if (\"lora_A\" in n or \"lora_B\" in n) and self.trainable_adapter_name in n:\r\n                    if p.shape == torch.Size([0]):\r\n                        with gather_params_ctx(p, fwd_module=self):\r\n                            para_cov = p @ p.T if \"lora_A\" in n else p.T @ p\r\n                    else:\r\n                        para_cov = p @ p.T if \"lora_A\" in n else p.T @ p\r\n                    I = torch.eye(*para_cov.size(), out=torch.empty_like(para_cov))  # noqa: E741\r\n                    I.requires_grad = False\r\n                    num_param += 1\r\n                    regu_loss += torch.norm(para_cov - I, p=\"fro\")\r\n            if num_param > 0:\r\n                regu_loss = regu_loss / num_param\r\n            else:\r\n                regu_loss = 0\r\n            outputs.loss += orth_reg_weight * regu_loss\r\n        return outputs",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(\r\n        self,\r\n        result,\r\n        input_ids: Optional[torch.LongTensor] = None,\r\n        inputs_embeds: Optional[torch.FloatTensor] = None,\r\n        *args,\r\n        **kwargs,\r\n    ) -> torch.Tensor:\r\n        \"\"\"\r\n        Using the hidden states of the model, predict `n_classes` LoRA alpha values. Returns the scalings.\r\n        \"\"\"\r\n        if input_ids is not None:\r\n            batch_size = input_ids.shape[0]\r\n            seq_len = input_ids.shape[1]\r\n        else:\r\n            batch_size = inputs_embeds.shape[0]\r\n            seq_len = inputs_embeds.shape[1]\r\n\r\n        hidden_states = result.hidden_states  # type: ignore\r\n\r\n        hidden_state = hidden_states[-1]  # Get the last hidden state\r\n\r\n        ### Classifier run\r\n        # hidden_state=[batch_size, seq_len, hidden_size]\r\n        logits = self.layers.forward(hidden_state)\r\n\r\n        ### Repeat to make layerwise scalings\r\n        ### If layerwise_scalings=False, then the classifier only outputs logits which are not layer-wise.\r\n        ### So, we expand them to the correct shape.\r\n        if not self.config.layerwise_scalings:\r\n            logits = logits.unsqueeze(2)\r\n            logits = logits.expand(-1, -1, self.n_layers, -1)\r\n\r\n        ### Classifier run\r\n\r\n        scalings = logits.reshape(batch_size, seq_len, self.n_layers, self.n_classes)\r\n        # scalings = [batch_size, seq_len, n_layers, n_classes]\r\n\r\n        if self.config.enable_softmax:\r\n            scalings = self.softmax(scalings)\r\n\r\n        if self.scalings_logging:\r\n            self.log_scalings.append(scalings)\r\n\r\n        return scalings",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def pre_forward(module, *args, **kwargs):\r\n            nonlocal handles_to_remove\r\n\r\n            # =========================== Forward pass with \"dummy\" scalings ==================\r\n\r\n            args_real = args[0]\r\n            kwargs_real = args[1]\r\n            kwargs_real.update(kwargs)\r\n\r\n            dummy_scalings = self.internal_xlora_classifier.make_dummy_scalings(*args_real, **kwargs_real)\r\n\r\n            hook_handles = []\r\n            for module in self.modules():\r\n                if isinstance(module, LoraLayer):\r\n                    pre_forward = partial(scalings_injection_hook, scalings=dummy_scalings)\r\n                    handle = module.register_forward_pre_hook(pre_forward, with_kwargs=True)\r\n                    hook_handles.append(handle)\r\n\r\n            with torch.no_grad():\r\n                self.lora_model.disable_adapter_layers()\r\n\r\n                try:\r\n                    scaling_pass_kwargs = kwargs_real.copy()\r\n                    scaling_pass_kwargs[\"output_hidden_states\"] = True\r\n                    scaling_pass_kwargs[\"return_dict\"] = True\r\n                    try:\r\n                        base_output = self.lora_model.model.forward(*args_real, **scaling_pass_kwargs)\r\n                    finally:\r\n                        # Clean everything up\r\n                        for handle in hook_handles:\r\n                            handle.remove()\r\n                finally:\r\n                    self.lora_model.enable_adapter_layers()\r\n\r\n            xlora_scalings = self.internal_xlora_classifier(result=base_output, *args_real, **kwargs_real)\r\n\r\n            # =========================== Real forward pass with calculated scalings ==================\r\n\r\n            hook_handles = []\r\n            for module in self.modules():\r\n                if isinstance(module, LoraLayer):\r\n                    pre_forward = partial(scalings_injection_hook, scalings=xlora_scalings)\r\n                    handle = module.register_forward_pre_hook(pre_forward, with_kwargs=True)\r\n                    hook_handles.append(handle)\r\n\r\n            handles_to_remove = hook_handles",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _enable_peft_forward_hooks(self, *generate_args, **generate_kwargs):\r\n        def scalings_injection_hook(target, args, kwargs, scalings):\r\n            # pre-forward hook to inject the adapter_names argument when using mixed adapter batches inference\r\n            kwargs[\"scalings\"] = scalings\r\n            return args, kwargs\r\n\r\n        handles_to_remove = None\r\n\r\n        def pre_forward(module, *args, **kwargs):\r\n            nonlocal handles_to_remove\r\n\r\n            # =========================== Forward pass with \"dummy\" scalings ==================\r\n\r\n            args_real = args[0]\r\n            kwargs_real = args[1]\r\n            kwargs_real.update(kwargs)\r\n\r\n            dummy_scalings = self.internal_xlora_classifier.make_dummy_scalings(*args_real, **kwargs_real)\r\n\r\n            hook_handles = []\r\n            for module in self.modules():\r\n                if isinstance(module, LoraLayer):\r\n                    pre_forward = partial(scalings_injection_hook, scalings=dummy_scalings)\r\n                    handle = module.register_forward_pre_hook(pre_forward, with_kwargs=True)\r\n                    hook_handles.append(handle)\r\n\r\n            with torch.no_grad():\r\n                self.lora_model.disable_adapter_layers()\r\n\r\n                try:\r\n                    scaling_pass_kwargs = kwargs_real.copy()\r\n                    scaling_pass_kwargs[\"output_hidden_states\"] = True\r\n                    scaling_pass_kwargs[\"return_dict\"] = True\r\n                    try:\r\n                        base_output = self.lora_model.model.forward(*args_real, **scaling_pass_kwargs)\r\n                    finally:\r\n                        # Clean everything up\r\n                        for handle in hook_handles:\r\n                            handle.remove()\r\n                finally:\r\n                    self.lora_model.enable_adapter_layers()\r\n\r\n            xlora_scalings = self.internal_xlora_classifier(result=base_output, *args_real, **kwargs_real)\r\n\r\n            # =========================== Real forward pass with calculated scalings ==================\r\n\r\n            hook_handles = []\r\n            for module in self.modules():\r\n                if isinstance(module, LoraLayer):\r\n                    pre_forward = partial(scalings_injection_hook, scalings=xlora_scalings)\r\n                    handle = module.register_forward_pre_hook(pre_forward, with_kwargs=True)\r\n                    hook_handles.append(handle)\r\n\r\n            handles_to_remove = hook_handles\r\n\r\n        if not self.disabled:\r\n            forward_handle = self.lora_model.model.register_forward_pre_hook(pre_forward, with_kwargs=True)\r\n\r\n        # Run the forward pass: first the scaling pass in the hook, and then with the base model\r\n        yield\r\n\r\n        if not self.disabled:\r\n            # TODO(EricLBuehler): If we get a forward exception, we may have multiple forward hooks.\r\n            for handle in handles_to_remove:\r\n                handle.remove()\r\n            forward_handle.remove()",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_mixed_adapter_batches_lora_training_mode_raises(self, mlp_lora):\r\n        inputs = {\r\n            \"X\": torch.arange(90).view(-1, 10).to(self.torch_device),\r\n            \"adapter_names\": [\"__base__\"] * 9,\r\n        }\r\n        mlp_lora = mlp_lora.train()\r\n        msg = r\"Cannot pass `adapter_names` when the model is in training mode.\"\r\n        with pytest.raises(ValueError, match=msg):\r\n            mlp_lora.forward(**inputs)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def path_merged_and_unmerged(self, tmp_path):\r\n        # Create 2 checkpoints:\r\n        # 1. merged: the model after calling merge_and_unload\r\n        # 2. unmerged: the PEFT model saved without calling merge_and_unload\r\n        path = tmp_path / \"model.pt\"\r\n\r\n        lora_config = LoraConfig(\r\n            target_modules=[\"lin\"],\r\n            # important: \"classifier\" is a substring of \"classifier2\", \"classifier3\", \"classifier4\"\r\n            modules_to_save=[\"classifier\", \"classifier2\", \"classifier3\", \"classifier4\"],\r\n        )\r\n        model = get_peft_model(self.get_model(), lora_config)\r\n        # mock training\r\n        for _ in range(5):\r\n            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\r\n            output = model(torch.randn(10, 5))\r\n            loss = output.sum()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n        # save the peft model without merging\r\n        path_unmerged = tmp_path / \"unmerged\"\r\n        model.save_pretrained(path_unmerged)\r\n\r\n        # merge the model and save state_dict\r\n        path_merged = tmp_path / \"merged\"\r\n        merged = model.merge_and_unload()\r\n        state_dict = merged.state_dict()\r\n        torch.save(state_dict, path_merged)\r\n\r\n        return path_merged, path_unmerged",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_poly(self):\r\n        torch.manual_seed(0)\r\n        model_name_or_path = \"google/flan-t5-small\"\r\n\r\n        atol, rtol = 1e-6, 1e-6\r\n        r = 8  # rank of lora in poly\r\n        n_tasks = 3  # number of tasks\r\n        n_skills = 2  # number of skills (loras)\r\n        n_splits = 4  # number of heads\r\n        lr = 1e-2\r\n        num_epochs = 10\r\n\r\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\r\n\r\n        peft_config = PolyConfig(\r\n            task_type=TaskType.SEQ_2_SEQ_LM,\r\n            poly_type=\"poly\",\r\n            r=r,\r\n            n_tasks=n_tasks,\r\n            n_skills=n_skills,\r\n            n_splits=n_splits,\r\n        )\r\n\r\n        model = get_peft_model(base_model, peft_config)\r\n\r\n        # generate some dummy data\r\n        text = os.__doc__.splitlines()\r\n        assert len(text) > 10\r\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\r\n        inputs[\"task_ids\"] = torch.arange(len(text)) % n_tasks\r\n        inputs[\"labels\"] = tokenizer(([\"A\", \"B\"] * 100)[: len(text)], return_tensors=\"pt\")[\"input_ids\"]\r\n\r\n        # simple training loop\r\n        model.train()\r\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n        losses = []\r\n        for _ in range(num_epochs):\r\n            outputs = model(**inputs)\r\n            loss = outputs.loss\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n            losses.append(loss.item())\r\n\r\n        # loss improved by at least 50%\r\n        assert losses[-1] < (0.5 * losses[0])\r\n\r\n        # check that saving and loading works\r\n        torch.manual_seed(0)\r\n        model.eval()\r\n        logits_before = model(**inputs).logits\r\n        tokens_before = model.generate(**inputs)\r\n\r\n        with model.disable_adapter():\r\n            logits_disabled = model(**inputs).logits\r\n            tokens_disabled = model.generate(**inputs)\r\n\r\n        assert not torch.allclose(logits_before, logits_disabled, atol=atol, rtol=rtol)\r\n        assert not torch.allclose(tokens_before, tokens_disabled, atol=atol, rtol=rtol)\r\n\r\n        # saving and loading\r\n        with tempfile.TemporaryDirectory() as tmp_dir:\r\n            model.save_pretrained(tmp_dir)\r\n            base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\r\n            loaded = PeftModel.from_pretrained(base_model, tmp_dir)\r\n\r\n        torch.manual_seed(0)\r\n        output_after = loaded(**inputs).logits\r\n        tokens_after = loaded.generate(**inputs)\r\n        assert torch.allclose(logits_before, output_after, atol=atol, rtol=rtol)\r\n        assert torch.allclose(tokens_before, tokens_after, atol=atol, rtol=rtol)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                # 'token_type_ids':  None if args.model_type == 'xlm' else batch[2],\n                \"token_type_ids\": batch[2],\n                \"labels\": batch[3],\n            }\n            # if args.model_type in ['xlnet', 'xlm']:\n            #     inputs.update({'cls_index': batch[5],\n            #                    'p_mask':       batch[6]})\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_vocabulary(output_dir)\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name\", type=str, default=\"openai-community/openai-gpt\", help=\"pretrained model name\")\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--train_dataset\", type=str, default=\"\")\n    parser.add_argument(\"--eval_dataset\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--num_train_epochs\", type=int, default=3)\n    parser.add_argument(\"--train_batch_size\", type=int, default=8)\n    parser.add_argument(\"--eval_batch_size\", type=int, default=16)\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--max_grad_norm\", type=int, default=1)\n    parser.add_argument(\n        \"--max_steps\",\n        default=-1,\n        type=int,\n        help=(\n            \"If > 0: set total number of training                         steps to perform. Override num_train_epochs.\"\n        ),\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before                        performing a backward/update pass.\",\n    )\n    parser.add_argument(\"--learning_rate\", type=float, default=6.25e-5)\n    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n    parser.add_argument(\"--lr_schedule\", type=str, default=\"warmup_linear\")\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--lm_coef\", type=float, default=0.9)\n    parser.add_argument(\"--n_valid\", type=int, default=374)\n\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    args = parser.parse_args()\n    print(args)\n\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    n_gpu = torch.cuda.device_count()\n    logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n\n    if not args.do_train and not args.do_eval:\n        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    # Load tokenizer and model\n    # This loading functions also add new tokens and embeddings called `special tokens`\n    # These new embeddings will be fine-tuned on the RocStories dataset\n    special_tokens = [\"_start_\", \"_delimiter_\", \"_classify_\"]\n    tokenizer = OpenAIGPTTokenizer.from_pretrained(args.model_name)\n    tokenizer.add_tokens(special_tokens)\n    special_tokens_ids = tokenizer.convert_tokens_to_ids(special_tokens)\n    model = OpenAIGPTDoubleHeadsModel.from_pretrained(args.model_name)\n    model.resize_token_embeddings(len(tokenizer))\n    model.to(device)\n\n    # Load and encode the datasets\n    def tokenize_and_encode(obj):\n        \"\"\"Tokenize and encode a nested object\"\"\"\n        if isinstance(obj, str):\n            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n        elif isinstance(obj, int):\n            return obj\n        return [tokenize_and_encode(o) for o in obj]\n\n    logger.info(\"Encoding dataset...\")\n    train_dataset = load_rocstories_dataset(args.train_dataset)\n    eval_dataset = load_rocstories_dataset(args.eval_dataset)\n    datasets = (train_dataset, eval_dataset)\n    encoded_datasets = tokenize_and_encode(datasets)\n\n    # Compute the max input length for the Transformer\n    max_length = model.config.n_positions // 2 - 2\n    input_length = max(\n        len(story[:max_length]) + max(len(cont1[:max_length]), len(cont2[:max_length])) + 3\n        for dataset in encoded_datasets\n        for story, cont1, cont2, _ in dataset\n    )\n    input_length = min(input_length, model.config.n_positions)  # Max size of input for the pre-trained model\n\n    # Prepare inputs tensors and dataloaders\n    tensor_datasets = pre_process_datasets(encoded_datasets, input_length, max_length, *special_tokens_ids)\n    train_tensor_dataset, eval_tensor_dataset = tensor_datasets[0], tensor_datasets[1]\n\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    eval_data = TensorDataset(*eval_tensor_dataset)\n    eval_sampler = SequentialSampler(eval_data)\n    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    # Prepare optimizer\n    if args.do_train:\n        if args.max_steps > 0:\n            t_total = args.max_steps\n            args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n        else:\n            t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": args.weight_decay,\n            },\n            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n        )\n\n    if args.do_train:\n        nb_tr_steps, tr_loss, exp_average_loss = 0, 0, None\n        model.train()\n        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n            tr_loss = 0\n            nb_tr_steps = 0\n            tqdm_bar = tqdm(train_dataloader, desc=\"Training\")\n            for step, batch in enumerate(tqdm_bar):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, mc_token_ids, lm_labels, mc_labels = batch\n                losses = model(input_ids, mc_token_ids=mc_token_ids, lm_labels=lm_labels, mc_labels=mc_labels)\n                loss = args.lm_coef * losses[0] + losses[1]\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                tr_loss += loss.item()\n                exp_average_loss = (\n                    loss.item() if exp_average_loss is None else 0.7 * exp_average_loss + 0.3 * loss.item()\n                )\n                nb_tr_steps += 1\n                tqdm_bar.desc = \"Training loss: {:.2e} lr: {:.2e}\".format(exp_average_loss, scheduler.get_lr()[0])\n\n    # Save a trained model\n    if args.do_train:\n        # Save a trained model, configuration and tokenizer\n        model_to_save = model.module if hasattr(model, \"module\") else model  # Only save the model itself\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n\n        torch.save(model_to_save.state_dict(), output_model_file)\n        model_to_save.config.to_json_file(output_config_file)\n        tokenizer.save_vocabulary(args.output_dir)\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = OpenAIGPTDoubleHeadsModel.from_pretrained(args.output_dir)\n        tokenizer = OpenAIGPTTokenizer.from_pretrained(args.output_dir)\n        model.to(device)\n\n    if args.do_eval:\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, mc_token_ids, lm_labels, mc_labels = batch\n            with torch.no_grad():\n                _, mc_loss, _, mc_logits = model(\n                    input_ids, mc_token_ids=mc_token_ids, lm_labels=lm_labels, mc_labels=mc_labels\n                )\n\n            mc_logits = mc_logits.detach().cpu().numpy()\n            mc_labels = mc_labels.to(\"cpu\").numpy()\n            tmp_eval_accuracy = accuracy(mc_logits, mc_labels)\n\n            eval_loss += mc_loss.mean().item()\n            eval_accuracy += tmp_eval_accuracy\n\n            nb_eval_examples += input_ids.size(0)\n            nb_eval_steps += 1\n\n        eval_loss = eval_loss / nb_eval_steps\n        eval_accuracy = eval_accuracy / nb_eval_examples\n        train_loss = tr_loss / nb_tr_steps if args.do_train else None\n        result = {\"eval_loss\": eval_loss, \"eval_accuracy\": eval_accuracy, \"train_loss\": train_loss}\n\n        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results *****\")\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 1\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to global_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    # Added here for reproducibility\n    set_seed(args)\n\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n                \"start_positions\": batch[3],\n                \"end_positions\": batch[4],\n            }\n\n            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n                del inputs[\"token_type_ids\"]\n\n            if args.model_type in [\"xlnet\", \"xlm\"]:\n                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({\"is_impossible\": batch[7]})\n                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n                    inputs.update(\n                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n                    )\n\n            outputs = model(**inputs)\n            # model outputs are always tuple in transformers (see doc)\n            loss = outputs[0]\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                # Log metrics\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Only evaluate when single GPU otherwise metrics may not average well\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                # Save model checkpoint\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    # Take care of distributed/parallel training\n                    model_to_save = model.module if hasattr(model, \"module\") else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_instance_segmentation_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    setup_logging(accelerator)\n\n    # If passed along, set the training seed now.\n    # We set device_specific to True as we want different data augmentation per device.\n    if args.seed is not None:\n        set_seed(args.seed, device_specific=True)\n\n    # Create repository if push ot hub is specified\n    repo_id = handle_repository_creation(accelerator, args)\n\n    if args.push_to_hub:\n        api = HfApi()\n\n    # ------------------------------------------------------------------------------------------------\n    # Load dataset, prepare splits\n    # ------------------------------------------------------------------------------------------------\n\n    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n    # download the dataset.\n    dataset = load_dataset(args.dataset_name, cache_dir=args.cache_dir, trust_remote_code=args.trust_remote_code)\n\n    # We need to specify the label2id mapping for the model\n    # it is a mapping from semantic class name to class index.\n    # In case your dataset does not provide it, you can create it manually:\n    # label2id = {\"background\": 0, \"cat\": 1, \"dog\": 2}\n    label2id = dataset[\"train\"][0][\"semantic_class_to_id\"]\n\n    if args.do_reduce_labels:\n        label2id = {name: idx for name, idx in label2id.items() if idx != 0}  # remove background class\n        label2id = {name: idx - 1 for name, idx in label2id.items()}  # shift class indices by -1\n\n    id2label = {v: k for k, v in label2id.items()}\n\n    # ------------------------------------------------------------------------------------------------\n    # Load pretrained model and image processor\n    # ------------------------------------------------------------------------------------------------\n    model = AutoModelForUniversalSegmentation.from_pretrained(\n        args.model_name_or_path,\n        label2id=label2id,\n        id2label=id2label,\n        ignore_mismatched_sizes=True,\n        token=args.hub_token,\n    )\n\n    image_processor = AutoImageProcessor.from_pretrained(\n        args.model_name_or_path,\n        do_resize=True,\n        size={\"height\": args.image_height, \"width\": args.image_width},\n        do_reduce_labels=args.do_reduce_labels,\n        reduce_labels=args.do_reduce_labels,  # TODO: remove when mask2former support `do_reduce_labels`\n        token=args.hub_token,\n    )\n\n    # ------------------------------------------------------------------------------------------------\n    # Define image augmentations and dataset transforms\n    # ------------------------------------------------------------------------------------------------\n    train_augment_and_transform = A.Compose(\n        [\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.5),\n            A.HueSaturationValue(p=0.1),\n        ],\n    )\n    validation_transform = A.Compose(\n        [A.NoOp()],\n    )\n\n    # Make transform functions for batch and apply for dataset splits\n    train_transform_batch = partial(\n        augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n    )\n    validation_transform_batch = partial(\n        augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n    )\n\n    with accelerator.main_process_first():\n        dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n        dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\n\n    dataloader_common_args = {\n        \"num_workers\": args.dataloader_num_workers,\n        \"persistent_workers\": True,\n        \"collate_fn\": collate_fn,\n    }\n    train_dataloader = DataLoader(\n        dataset[\"train\"], shuffle=True, batch_size=args.per_device_train_batch_size, **dataloader_common_args\n    )\n    valid_dataloader = DataLoader(\n        dataset[\"validation\"], shuffle=False, batch_size=args.per_device_eval_batch_size, **dataloader_common_args\n    )\n\n    # ------------------------------------------------------------------------------------------------\n    # Define optimizer, scheduler and prepare everything with the accelerator\n    # ------------------------------------------------------------------------------------------------\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        list(model.parameters()),\n        lr=args.learning_rate,\n        betas=[args.adam_beta1, args.adam_beta2],\n        eps=args.adam_epsilon,\n    )\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, valid_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, valid_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"instance_segmentation_no_trainer\", experiment_config)\n\n    # ------------------------------------------------------------------------------------------------\n    # Run training with evaluation on each epoch\n    # ------------------------------------------------------------------------------------------------\n\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(dataset['train'])}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n                    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n                        accelerator.wait_for_everyone()\n                        unwrapped_model = accelerator.unwrap_model(model)\n                        unwrapped_model.save_pretrained(\n                            args.output_dir,\n                            is_main_process=accelerator.is_main_process,\n                            save_function=accelerator.save,\n                        )\n                        if accelerator.is_main_process:\n                            image_processor.save_pretrained(args.output_dir)\n                            api.upload_folder(\n                                repo_id=repo_id,\n                                commit_message=f\"Training in progress epoch {epoch}\",\n                                folder_path=args.output_dir,\n                                repo_type=\"model\",\n                                token=args.hub_token,\n                            )\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        logger.info(\"***** Running evaluation *****\")\n        metrics = evaluation_loop(model, image_processor, accelerator, valid_dataloader, id2label)\n\n        logger.info(f\"epoch {epoch}: {metrics}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    **metrics,\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                image_processor.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    # ------------------------------------------------------------------------------------------------\n    # Run evaluation on test dataset and save the model\n    # ------------------------------------------------------------------------------------------------\n\n    logger.info(\"***** Running evaluation on test dataset *****\")\n    metrics = evaluation_loop(model, image_processor, accelerator, valid_dataloader, id2label)\n    metrics = {f\"test_{k}\": v for k, v in metrics.items()}\n\n    logger.info(f\"Test metrics: {metrics}\")\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n            image_processor.save_pretrained(args.output_dir)\n\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                    ignore_patterns=[\"epoch_*\"],\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_image_classification_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    logger.info(accelerator.state)\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own training and evaluation files (see below)\n    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n\n    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        dataset = load_dataset(args.dataset_name, trust_remote_code=args.trust_remote_code)\n    else:\n        data_files = {}\n        if args.train_dir is not None:\n            data_files[\"train\"] = os.path.join(args.train_dir, \"**\")\n        if args.validation_dir is not None:\n            data_files[\"validation\"] = os.path.join(args.validation_dir, \"**\")\n        dataset = load_dataset(\n            \"imagefolder\",\n            data_files=data_files,\n        )\n        # See more about loading custom images at\n        # https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder.\n\n    dataset_column_names = dataset[\"train\"].column_names if \"train\" in dataset else dataset[\"validation\"].column_names\n    if args.image_column_name not in dataset_column_names:\n        raise ValueError(\n            f\"--image_column_name {args.image_column_name} not found in dataset '{args.dataset_name}'. \"\n            \"Make sure to set `--image_column_name` to the correct audio column - one of \"\n            f\"{', '.join(dataset_column_names)}.\"\n        )\n    if args.label_column_name not in dataset_column_names:\n        raise ValueError(\n            f\"--label_column_name {args.label_column_name} not found in dataset '{args.dataset_name}'. \"\n            \"Make sure to set `--label_column_name` to the correct text column - one of \"\n            f\"{', '.join(dataset_column_names)}.\"\n        )\n\n    # If we don't have a validation split, split off a percentage of train as validation.\n    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n    if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n        split = dataset[\"train\"].train_test_split(args.train_val_split)\n        dataset[\"train\"] = split[\"train\"]\n        dataset[\"validation\"] = split[\"test\"]\n\n    # Prepare label mappings.\n    # We'll include these in the model's config to get human readable labels in the Inference API.\n    labels = dataset[\"train\"].features[args.label_column_name].names\n    label2id = {label: str(i) for i, label in enumerate(labels)}\n    id2label = {str(i): label for i, label in enumerate(labels)}\n\n    # Load pretrained model and image processor\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    config = AutoConfig.from_pretrained(\n        args.model_name_or_path,\n        num_labels=len(labels),\n        id2label=id2label,\n        label2id=label2id,\n        finetuning_task=\"image-classification\",\n        trust_remote_code=args.trust_remote_code,\n    )\n    image_processor = AutoImageProcessor.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=args.trust_remote_code,\n    )\n    model = AutoModelForImageClassification.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n        trust_remote_code=args.trust_remote_code,\n    )\n\n    # Preprocessing the datasets\n\n    # Define torchvision transforms to be applied to each image.\n    if \"shortest_edge\" in image_processor.size:\n        size = image_processor.size[\"shortest_edge\"]\n    else:\n        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n    normalize = (\n        Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\")\n        else Lambda(lambda x: x)\n    )\n    train_transforms = Compose(\n        [\n            RandomResizedCrop(size),\n            RandomHorizontalFlip(),\n            ToTensor(),\n            normalize,\n        ]\n    )\n    val_transforms = Compose(\n        [\n            Resize(size),\n            CenterCrop(size),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\n    def preprocess_train(example_batch):\n        \"\"\"Apply _train_transforms across a batch.\"\"\"\n        example_batch[\"pixel_values\"] = [\n            train_transforms(image.convert(\"RGB\")) for image in example_batch[args.image_column_name]\n        ]\n        return example_batch\n\n    def preprocess_val(example_batch):\n        \"\"\"Apply _val_transforms across a batch.\"\"\"\n        example_batch[\"pixel_values\"] = [\n            val_transforms(image.convert(\"RGB\")) for image in example_batch[args.image_column_name]\n        ]\n        return example_batch\n\n    with accelerator.main_process_first():\n        if args.max_train_samples is not None:\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n        # Set the training transforms\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n        if args.max_eval_samples is not None:\n            dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=args.seed).select(range(args.max_eval_samples))\n        # Set the validation transforms\n        eval_dataset = dataset[\"validation\"].with_transform(preprocess_val)\n\n    # DataLoaders creation:\n    def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        labels = torch.tensor([example[args.label_column_name] for example in examples])\n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=collate_fn, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"image_classification_no_trainer\", experiment_config)\n\n    # Get the metric function\n    metric = evaluate.load(\"accuracy\")\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n                    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n                        accelerator.wait_for_everyone()\n                        unwrapped_model = accelerator.unwrap_model(model)\n                        unwrapped_model.save_pretrained(\n                            args.output_dir,\n                            is_main_process=accelerator.is_main_process,\n                            save_function=accelerator.save,\n                        )\n                        if accelerator.is_main_process:\n                            image_processor.save_pretrained(args.output_dir)\n                            api.upload_folder(\n                                commit_message=f\"Training in progress epoch {epoch}\",\n                                folder_path=args.output_dir,\n                                repo_id=repo_id,\n                                repo_type=\"model\",\n                                token=args.hub_token,\n                            )\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metric = metric.compute()\n        logger.info(f\"epoch {epoch}: {eval_metric}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"accuracy\": eval_metric,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                image_processor.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            image_processor.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n            all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(all_results, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_mim_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        **accelerator_log_kwargs,\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Initialize our dataset.\n    ds = load_dataset(\n        args.dataset_name,\n        args.dataset_config_name,\n        data_files=args.data_files,\n        cache_dir=args.cache_dir,\n        token=args.token,\n        trust_remote_code=args.trust_remote_code,\n    )\n\n    # If we don't have a validation split, split off a percentage of train as validation.\n    args.train_val_split = None if \"validation\" in ds.keys() else args.train_val_split\n    if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n        split = ds[\"train\"].train_test_split(args.train_val_split)\n        ds[\"train\"] = split[\"train\"]\n        ds[\"validation\"] = split[\"test\"]\n\n    # Create config\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    config_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"revision\": args.model_revision,\n        \"token\": args.token,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    if args.config_name_or_path:\n        config = AutoConfig.from_pretrained(args.config_name_or_path, **config_kwargs)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, **config_kwargs)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n        if args.config_overrides is not None:\n            logger.info(f\"Overriding config: {args.config_overrides}\")\n            config.update_from_string(args.config_overrides)\n            logger.info(f\"New config: {config}\")\n\n    # make sure the decoder_type is \"simmim\" (only relevant for BEiT)\n    if hasattr(config, \"decoder_type\"):\n        config.decoder_type = \"simmim\"\n\n    # adapt config\n    args.image_size = args.image_size if args.image_size is not None else config.image_size\n    args.patch_size = args.patch_size if args.patch_size is not None else config.patch_size\n    args.encoder_stride = args.encoder_stride if args.encoder_stride is not None else config.encoder_stride\n\n    config.update(\n        {\n            \"image_size\": args.image_size,\n            \"patch_size\": args.patch_size,\n            \"encoder_stride\": args.encoder_stride,\n        }\n    )\n\n    # create image processor\n    if args.image_processor_name:\n        image_processor = AutoImageProcessor.from_pretrained(args.image_processor_name, **config_kwargs)\n    elif args.model_name_or_path:\n        image_processor = AutoImageProcessor.from_pretrained(args.model_name_or_path, **config_kwargs)\n    else:\n        IMAGE_PROCESSOR_TYPES = {\n            conf.model_type: image_processor_class for conf, image_processor_class in IMAGE_PROCESSOR_MAPPING.items()\n        }\n        image_processor = IMAGE_PROCESSOR_TYPES[args.model_type]()\n\n    # create model\n    if args.model_name_or_path:\n        model = AutoModelForMaskedImageModeling.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            cache_dir=args.cache_dir,\n            revision=args.model_revision,\n            token=args.token,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedImageModeling.from_config(\n            config,\n            token=args.token,\n            trust_remote_code=args.trust_remote_code,\n        )\n\n    column_names = ds[\"train\"].column_names\n\n    if args.image_column_name is not None:\n        image_column_name = args.image_column_name\n    elif \"image\" in column_names:\n        image_column_name = \"image\"\n    elif \"img\" in column_names:\n        image_column_name = \"img\"\n    else:\n        image_column_name = column_names[0]\n\n    # transformations as done in original SimMIM paper\n    # source: https://github.com/microsoft/SimMIM/blob/main/data/data_simmim.py\n    transforms = Compose(\n        [\n            Lambda(lambda img: img.convert(\"RGB\")),\n            RandomResizedCrop(args.image_size, scale=(0.67, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),\n            RandomHorizontalFlip(),\n            ToTensor(),\n            Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n        ]\n    )\n\n    # create mask generator\n    mask_generator = MaskGenerator(\n        input_size=args.image_size,\n        mask_patch_size=args.mask_patch_size,\n        model_patch_size=args.patch_size,\n        mask_ratio=args.mask_ratio,\n    )\n\n    def preprocess_images(examples):\n        \"\"\"Preprocess a batch of images by applying transforms + creating a corresponding mask, indicating\n        which patches to mask.\"\"\"\n\n        examples[\"pixel_values\"] = [transforms(image) for image in examples[image_column_name]]\n        examples[\"mask\"] = [mask_generator() for i in range(len(examples[image_column_name]))]\n\n        return examples\n\n    if args.max_train_samples is not None:\n        ds[\"train\"] = ds[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n    # Set the training transforms\n    ds[\"train\"].set_transform(preprocess_images)\n\n    if args.max_eval_samples is not None:\n        ds[\"validation\"] = ds[\"validation\"].shuffle(seed=args.seed).select(range(args.max_eval_samples))\n    # Set the validation transforms\n    ds[\"validation\"].set_transform(preprocess_images)\n\n    # DataLoaders creation:\n    train_dataloader = DataLoader(\n        ds[\"train\"],\n        shuffle=True,\n        collate_fn=collate_fn,\n        batch_size=args.per_device_train_batch_size,\n    )\n    eval_dataloader = DataLoader(\n        ds[\"validation\"],\n        collate_fn=collate_fn,\n        batch_size=args.per_device_eval_batch_size,\n    )\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n    # shorter in multiprocess)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model,\n        optimizer,\n        train_dataloader,\n        eval_dataloader,\n        lr_scheduler,\n    )\n\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n    if accelerator.distributed_type == DistributedType.TPU:\n        model.tie_weights()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"mim_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(ds['train'])}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(int(args.max_train_steps)), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        eval_loss = torch.mean(losses)\n\n        logger.info(f\"epoch {epoch}: eval_loss: {eval_loss}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"eval_loss\": eval_loss,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                image_processor.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            image_processor.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_clm_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n    else:\n        data_files = {}\n        dataset_args = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        if extension == \"txt\":\n            extension = \"text\"\n            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                **dataset_args,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                **dataset_args,\n            )\n\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(\n            args.config_name,\n            trust_remote_code=args.trust_remote_code,\n        )\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(\n            args.model_name_or_path,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForCausalLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            low_cpu_mem_usage=args.low_cpu_mem_usage,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForCausalLM.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    with accelerator.main_process_first():\n        tokenized_datasets = raw_datasets.map(\n            tokenize_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    if args.block_size is None:\n        block_size = tokenizer.model_max_length\n        if block_size > config.max_position_embeddings:\n            logger.warning(\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n            )\n            block_size = min(1024, config.max_position_embeddings)\n    else:\n        if args.block_size > tokenizer.model_max_length:\n            logger.warning(\n                f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model \"\n                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n            )\n        block_size = min(args.block_size, tokenizer.model_max_length)\n\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n    def group_texts(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n        total_length = (total_length // block_size) * block_size\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n    # to preprocess.\n    #\n    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n    # https://huggingface.co/docs/datasets/process#map\n\n    with accelerator.main_process_first():\n        lm_datasets = tokenized_datasets.map(\n            group_texts,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=f\"Grouping texts in chunks of {block_size}\",\n        )\n\n    train_dataset = lm_datasets[\"train\"]\n    eval_dataset = lm_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(\n        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n    if accelerator.distributed_type == DistributedType.TPU:\n        model.tie_weights()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"clm_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        try:\n            eval_loss = torch.mean(losses)\n            perplexity = math.exp(eval_loss)\n        except OverflowError:\n            perplexity = float(\"inf\")\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"perplexity\": perplexity,\n                    \"eval_loss\": eval_loss,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump({\"perplexity\": perplexity}, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_mlm_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        if extension == \"txt\":\n            extension = \"text\"\n        raw_datasets = load_dataset(extension, data_files=data_files)\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n            )\n\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForMaskedLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            low_cpu_mem_usage=args.low_cpu_mem_usage,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedLM.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    if args.max_seq_length is None:\n        max_seq_length = tokenizer.model_max_length\n        if max_seq_length > 1024:\n            logger.warning(\n                \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n                \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n                \" override this default with `--block_size xxx`.\"\n            )\n            max_seq_length = 1024\n    else:\n        if args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    if args.line_by_line:\n        # When using line_by_line, we just tokenize each nonempty line.\n        padding = \"max_length\" if args.pad_to_max_length else False\n\n        def tokenize_function(examples):\n            # Remove empty lines\n            examples[text_column_name] = [\n                line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n            ]\n            return tokenizer(\n                examples[text_column_name],\n                padding=padding,\n                truncation=True,\n                max_length=max_seq_length,\n                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n                # receives the `special_tokens_mask`.\n                return_special_tokens_mask=True,\n            )\n\n        with accelerator.main_process_first():\n            tokenized_datasets = raw_datasets.map(\n                tokenize_function,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                remove_columns=[text_column_name],\n                load_from_cache_file=not args.overwrite_cache,\n                desc=\"Running tokenizer on dataset line_by_line\",\n            )\n    else:\n        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n        # efficient when it receives the `special_tokens_mask`.\n        def tokenize_function(examples):\n            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n\n        with accelerator.main_process_first():\n            tokenized_datasets = raw_datasets.map(\n                tokenize_function,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=\"Running tokenizer on every text in dataset\",\n            )\n\n        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n        # max_seq_length.\n        def group_texts(examples):\n            # Concatenate all texts.\n            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n            total_length = len(concatenated_examples[list(examples.keys())[0]])\n            # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\n            # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n            total_length = (total_length // max_seq_length) * max_seq_length\n            # Split by chunks of max_len.\n            result = {\n                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n                for k, t in concatenated_examples.items()\n            }\n            return result\n\n        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n        # might be slower to preprocess.\n        #\n        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n        # https://huggingface.co/docs/datasets/process#map\n\n        with accelerator.main_process_first():\n            tokenized_datasets = tokenized_datasets.map(\n                group_texts,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=f\"Grouping texts in chunks of {max_seq_length}\",\n            )\n\n    train_dataset = tokenized_datasets[\"train\"]\n    eval_dataset = tokenized_datasets[\"validation\"]\n\n    # Conditional for small test subsets\n    if len(train_dataset) > 3:\n        # Log a few random samples from the training set:\n        for index in random.sample(range(len(train_dataset)), 3):\n            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # Data collator\n    # This one will take care of randomly masking the tokens.\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n\n    # DataLoaders creation:\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n    # shorter in multiprocess)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n    if accelerator.distributed_type == DistributedType.TPU:\n        model.tie_weights()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"mlm_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        try:\n            eval_loss = torch.mean(losses)\n            perplexity = math.exp(eval_loss)\n        except OverflowError:\n            perplexity = float(\"inf\")\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"perplexity\": perplexity,\n                    \"eval_loss\": eval_loss,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump({\"perplexity\": perplexity}, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_swag_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # Trim a number of training examples\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    if raw_datasets[\"train\"] is not None:\n        column_names = raw_datasets[\"train\"].column_names\n    else:\n        column_names = raw_datasets[\"validation\"].column_names\n\n    # When using your own dataset or a different dataset from swag, you will probably need to change this.\n    ending_names = [f\"ending{i}\" for i in range(4)]\n    context_name = \"sent1\"\n    question_header_name = \"sent2\"\n    label_column_name = \"label\" if \"label\" in column_names else \"labels\"\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForMultipleChoice.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMultipleChoice.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    def preprocess_function(examples):\n        first_sentences = [[context] * 4 for context in examples[context_name]]\n        question_headers = examples[question_header_name]\n        second_sentences = [\n            [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n        ]\n        labels = examples[label_column_name]\n\n        # Flatten out\n        first_sentences = list(chain(*first_sentences))\n        second_sentences = list(chain(*second_sentences))\n\n        # Tokenize\n        tokenized_examples = tokenizer(\n            first_sentences,\n            second_sentences,\n            max_length=args.max_seq_length,\n            padding=padding,\n            truncation=True,\n        )\n        # Un-flatten\n        tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n        tokenized_inputs[\"labels\"] = labels\n        return tokenized_inputs\n\n    with accelerator.main_process_first():\n        processed_datasets = raw_datasets.map(\n            preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n        )\n\n    train_dataset = processed_datasets[\"train\"]\n    eval_dataset = processed_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorForMultipleChoice(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Use the device given by the `accelerator` object.\n    device = accelerator.device\n    model.to(device)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"swag_no_trainer\", experiment_config)\n\n    # Metrics\n    metric = evaluate.load(\"accuracy\")\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metric = metric.compute()\n        accelerator.print(f\"epoch {epoch}: {eval_metric}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"accuracy\": eval_metric,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n            all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(all_results, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_fim_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n        # Set a numpy random state for FIM transformations\n        np_rng = np.random.RandomState(seed=args.seed)\n    else:\n        # Still set a random state for FIM transformations\n        np_rng = np.random.RandomState(seed=42)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            # Clone repo locally\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                trust_remote_code=args.trust_remote_code,\n            )\n    else:\n        data_files = {}\n        dataset_args = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n        extension = args.train_file.split(\".\")[-1]\n        if extension == \"txt\":\n            extension = \"text\"\n            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                **dataset_args,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                **dataset_args,\n            )\n\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(\n            args.config_name,\n            trust_remote_code=args.trust_remote_code,\n        )\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(\n            args.model_name_or_path,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForCausalLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            low_cpu_mem_usage=args.low_cpu_mem_usage,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForCausalLM.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # Add the new FIM tokens to the tokenizer and resize model's vocab embeddings\n    special_tokens = [args.fim_prefix_token, args.fim_middle_token, args.fim_suffix_token]\n    if args.truncate_or_pad:\n        special_tokens.append(args.fim_pad_token)\n\n    # Get the factor by which the embedding layer should be padded based on the device\n    pad_factor = 1\n    if torch.cuda.is_availble():\n        pad_factor = 8\n\n    elif is_torch_tpu_available():\n        pad_factor = 128\n\n    # Add the new tokens to the tokenizer\n    tokenizer.add_tokens(special_tokens)\n    original_embeddings = model.get_input_embeddings()\n\n    if is_deepspeed_zero3_enabled():\n        import deepspeed\n\n        with deepspeed.zero.GatheredParameters(original_embeddings.weight, modifier_rank=0):\n            # Get the pre-expansion embeddings of the model and resize the embedding layer\n            model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=pad_factor)\n            embeddings = model.get_input_embeddings()\n\n            # Sample the embeddings for the new tokens from a multivariate normal distribution\n            # We do this so that the new embeddings are close to the original embeddings and not necessarily zero\n            # More on this: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n            mean = original_embeddings.mean(dim=0)\n            n = original_embeddings.size()[0]\n            sigma = ((original_embeddings - mean).T @ (original_embeddings - mean)) / n\n            dist = torch.distributions.multivariate_normal.MultivariateNormal(\n                mean,\n                covariance_matrix=1e-5 * sigma,\n            )\n            new_token_embeddings = torch.stack(\n                tuple((dist.sample() for _ in range(len(special_tokens)))),\n                dim=0,\n            )\n    else:\n        original_embeddings = model.get_input_embeddings()\n        # Get the pre-expansion embeddings of the model and resize the embedding layer\n        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=pad_factor)\n        embeddings = model.get_input_embeddings()\n\n        # Sample the embeddings for the new tokens from a multivariate normal distribution\n        # We do this so that the new embeddings are close to the original embeddings and not necessarily zero\n        # More on this: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n        mean = original_embeddings.mean(dim=0)\n        n = original_embeddings.size()[0]\n        sigma = ((original_embeddings - mean).T @ (original_embeddings - mean)) / n\n        dist = torch.distributions.multivariate_normal.MultivariateNormal(\n            mean,\n            covariance_matrix=1e-5 * sigma,\n        )\n        new_token_embeddings = torch.stack(\n            tuple((dist.sample() for _ in range(len(special_tokens)))),\n            dim=0,\n        )\n\n    if is_deepspeed_zero3_enabled():\n        import deepspeed\n\n        with deepspeed.zero.GatheredParameters(embeddings.weight, modifier_rank=0):\n            # Set the new tokens' embeddings to the newly sampled embeddings\n            embeddings.weight.data[-len(special_tokens) :] = new_token_embeddings\n    else:\n        # Set the new tokens' embeddings to the newly sampled embeddings\n        embeddings.weight.data[-len(special_tokens) :] = new_token_embeddings\n\n    # Update the model's embeddings with the new embeddings\n    model.set_input_embeddings(embeddings)\n\n    logger.info(\"Added special tokens to the tokenizer and resized model's embedding layer\")\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    with accelerator.main_process_first():\n        tokenized_datasets = raw_datasets.map(\n            tokenize_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    if args.block_size is None:\n        block_size = tokenizer.model_max_length\n        if block_size > config.max_position_embeddings:\n            logger.warning(\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n            )\n            block_size = min(1024, config.max_position_embeddings)\n    else:\n        if args.block_size > tokenizer.model_max_length:\n            logger.warning(\n                f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model \"\n                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n            )\n        block_size = min(args.block_size, tokenizer.model_max_length)\n\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n    def group_texts(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n        total_length = (total_length // block_size) * block_size\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    # Get the FIM-specific token ids\n    prefix_tok_id = tokenizer.convert_tokens_to_ids(args.fim_prefix_token)\n    middle_tok_id = tokenizer.convert_tokens_to_ids(args.fim_middle_token)\n    suffix_tok_id = tokenizer.convert_tokens_to_ids(args.fim_suffix_token)\n    pad_tok_id = None\n\n    # If truncate_or_pad is on, also get pad token id\n    if args.truncate_or_pad:\n        pad_tok_id = tokenizer.convert_tokens_to_ids(args.fim_pad_token)\n\n    # The two functions below perform the FIM transformation on the data (either PSM or SPM or PSM+SPM)\n    # Don't call fim_transform directly in .map()\n    # Adapted from https://github.com/loubnabnl/santacoder-finetuning/blob/main/fim.py#L22C13-L83\n    def fim_transform(example):\n        \"\"\"\n        This function performs FIM transformation on a single example (list of tokens)\n        \"\"\"\n        if np_rng.binomial(1, args.fim_rate):\n            boundaries = sorted(np_rng.randint(low=0, high=len(example) + 1, size=2))\n\n            prefix = example[: boundaries[0]]\n            middle = example[boundaries[0] : boundaries[1]]\n            suffix = example[boundaries[1] :]\n\n            if args.truncate_or_pad:\n                total_length = len(prefix) + len(middle) + len(suffix) + 3\n                diff = total_length - len(example)\n                if diff > 0:\n                    suffix = suffix[: max(0, len(suffix) - diff)]\n                elif diff < 0:\n                    suffix.extend([pad_tok_id] * (-diff))\n\n            if np_rng.binomial(1, args.fim_spm_rate):\n                # Apply Suffix-Prefix-Middle (SPM) transformation\n                transformed_example = [prefix_tok_id, suffix_tok_id] + suffix + [middle_tok_id] + prefix + middle\n            else:\n                # Apply Prefix-Suffix-Middle (PSM) transformation\n                transformed_example = [prefix_tok_id] + prefix + [suffix_tok_id] + suffix + [middle_tok_id] + middle\n        else:\n            transformed_example = example\n\n        return transformed_example\n\n    # Below function is the one you are supposed to call in the .map() function\n    def apply_fim(examples):\n        \"\"\"\n        Apply FIM transformation to a batch of examples\n        \"\"\"\n        fim_transform_ids = [fim_transform(ids) for ids in examples[\"input_ids\"]]\n        examples[\"input_ids\"] = fim_transform_ids\n        examples[\"labels\"] = fim_transform_ids\n        # If your application requires custom attention mask, please adjust this function's below line.\n        # Since FIM transformation increases the number of tokens in input_ids and labels\n        # but leaves the number of tokens unchanged in attention_masks which would cause problems\n        examples[\"attention_mask\"] = [[1] * len(mask) for mask in examples[\"input_ids\"]]\n        return examples\n\n    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n    # to preprocess.\n    #\n    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n    # https://huggingface.co/docs/datasets/process#map\n\n    # FIM transformations are only supposed to be applied before group_texts processing otherwise some sentences will\n    # have 3-4 more tokens than others due to probabilistic addition of FIM-specific tokens which will raise errors\n    with accelerator.main_process_first():\n        fim_datasets = tokenized_datasets.map(\n            apply_fim,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Performing FIM transformation\",\n        )\n        lm_datasets = fim_datasets.map(\n            group_texts,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=f\"Grouping texts in chunks of {block_size}\",\n        )\n\n    train_dataset = lm_datasets[\"train\"]\n    eval_dataset = lm_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(\n        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n    if accelerator.distributed_type == DistributedType.TPU:\n        model.tie_weights()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"fim_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        try:\n            eval_loss = torch.mean(losses)\n            perplexity = math.exp(eval_loss)\n        except OverflowError:\n            perplexity = float(\"inf\")\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"perplexity\": perplexity,\n                    \"eval_loss\": eval_loss,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(\n                    commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump({\"perplexity\": perplexity}, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_object_detection_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    # We set device_specific to True as we want different data augmentation per device.\n    if args.seed is not None:\n        set_seed(args.seed, device_specific=True)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Load dataset\n    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n    # download the dataset.\n    dataset = load_dataset(args.dataset_name, cache_dir=args.cache_dir, trust_remote_code=args.trust_remote_code)\n\n    # If we don't have a validation split, split off a percentage of train as validation.\n    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n    if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n        split = dataset[\"train\"].train_test_split(args.train_val_split, seed=args.seed)\n        dataset[\"train\"] = split[\"train\"]\n        dataset[\"validation\"] = split[\"test\"]\n\n    # Get dataset categories and prepare mappings for label_name <-> label_id\n    categories = dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n    id2label = dict(enumerate(categories))\n    label2id = {v: k for k, v in id2label.items()}\n\n    # ------------------------------------------------------------------------------------------------\n    # Load pretrained config, model and image processor\n    # ------------------------------------------------------------------------------------------------\n\n    common_pretrained_args = {\n        \"cache_dir\": args.cache_dir,\n        \"token\": args.hub_token,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    config = AutoConfig.from_pretrained(\n        args.model_name_or_path, label2id=label2id, id2label=id2label, **common_pretrained_args\n    )\n    model = AutoModelForObjectDetection.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n        **common_pretrained_args,\n    )\n    image_processor = AutoImageProcessor.from_pretrained(\n        args.model_name_or_path,\n        do_resize=True,\n        size={\"max_height\": args.image_square_size, \"max_width\": args.image_square_size},\n        do_pad=True,\n        pad_size={\"height\": args.image_square_size, \"width\": args.image_square_size},\n        **common_pretrained_args,\n    )\n\n    # ------------------------------------------------------------------------------------------------\n    # Define image augmentations and dataset transforms\n    # ------------------------------------------------------------------------------------------------\n    max_size = args.image_square_size\n    train_augment_and_transform = A.Compose(\n        [\n            A.Compose(\n                [\n                    A.SmallestMaxSize(max_size=max_size, p=1.0),\n                    A.RandomSizedBBoxSafeCrop(height=max_size, width=max_size, p=1.0),\n                ],\n                p=0.2,\n            ),\n            A.OneOf(\n                [\n                    A.Blur(blur_limit=7, p=0.5),\n                    A.MotionBlur(blur_limit=7, p=0.5),\n                    A.Defocus(radius=(1, 5), alias_blur=(0.1, 0.25), p=0.1),\n                ],\n                p=0.1,\n            ),\n            A.Perspective(p=0.1),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.5),\n            A.HueSaturationValue(p=0.1),\n        ],\n        bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n    )\n    validation_transform = A.Compose(\n        [A.NoOp()],\n        bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n    )\n\n    # Make transform functions for batch and apply for dataset splits\n    train_transform_batch = partial(\n        augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n    )\n    validation_transform_batch = partial(\n        augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n    )\n\n    with accelerator.main_process_first():\n        train_dataset = dataset[\"train\"].with_transform(train_transform_batch)\n        valid_dataset = dataset[\"validation\"].with_transform(validation_transform_batch)\n        test_dataset = dataset[\"test\"].with_transform(validation_transform_batch)\n\n    dataloader_common_args = {\n        \"num_workers\": args.dataloader_num_workers,\n        \"collate_fn\": collate_fn,\n    }\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, batch_size=args.per_device_train_batch_size, **dataloader_common_args\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset, shuffle=False, batch_size=args.per_device_eval_batch_size, **dataloader_common_args\n    )\n    test_dataloader = DataLoader(\n        test_dataset, shuffle=False, batch_size=args.per_device_eval_batch_size, **dataloader_common_args\n    )\n\n    # ------------------------------------------------------------------------------------------------\n    # Define optimizer, scheduler and prepare everything with the accelerator\n    # ------------------------------------------------------------------------------------------------\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        list(model.parameters()),\n        lr=args.learning_rate,\n        betas=[args.adam_beta1, args.adam_beta2],\n        eps=args.adam_epsilon,\n    )\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, valid_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, valid_dataloader, test_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"object_detection_no_trainer\", experiment_config)\n\n    # ------------------------------------------------------------------------------------------------\n    # Run training with evaluation on each epoch\n    # ------------------------------------------------------------------------------------------------\n\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n                    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n                        accelerator.wait_for_everyone()\n                        unwrapped_model = accelerator.unwrap_model(model)\n                        unwrapped_model.save_pretrained(\n                            args.output_dir,\n                            is_main_process=accelerator.is_main_process,\n                            save_function=accelerator.save,\n                        )\n                        if accelerator.is_main_process:\n                            image_processor.save_pretrained(args.output_dir)\n                            api.upload_folder(\n                                commit_message=f\"Training in progress epoch {epoch}\",\n                                folder_path=args.output_dir,\n                                repo_id=repo_id,\n                                repo_type=\"model\",\n                                token=args.hub_token,\n                            )\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        logger.info(\"***** Running evaluation *****\")\n        metrics = evaluation_loop(model, image_processor, accelerator, valid_dataloader, id2label)\n\n        logger.info(f\"epoch {epoch}: {metrics}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    **metrics,\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                image_processor.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    # ------------------------------------------------------------------------------------------------\n    # Run evaluation on test dataset and save the model\n    # ------------------------------------------------------------------------------------------------\n\n    logger.info(\"***** Running evaluation on test dataset *****\")\n    metrics = evaluation_loop(model, image_processor, accelerator, test_dataloader, id2label)\n    metrics = {f\"test_{k}\": v for k, v in metrics.items()}\n\n    logger.info(f\"Test metrics: {metrics}\")\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n            image_processor.save_pretrained(args.output_dir)\n\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                    ignore_patterns=[\"epoch_*\"],\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_qa_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        if args.test_file is not None:\n            data_files[\"test\"] = args.test_file\n            extension = args.test_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # Preprocessing the datasets.\n    # Preprocessing is slightly different for training and evaluation.\n\n    column_names = raw_datasets[\"train\"].column_names\n\n    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n\n    # Padding side determines if we do (question|context) or (context|question).\n    pad_on_right = tokenizer.padding_side == \"right\"\n\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(\n            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n        )\n\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    # Training preprocessing\n    def prepare_train_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_seq_length,\n            stride=args.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\" if args.pad_to_max_length else False,\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n        # The offset mappings will give us a map from token to character position in the original context. This will\n        # help us compute the start_positions and end_positions.\n        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n        # Let's label those examples!\n        tokenized_examples[\"start_positions\"] = []\n        tokenized_examples[\"end_positions\"] = []\n\n        for i, offsets in enumerate(offset_mapping):\n            # We will label impossible answers with the index of the CLS token.\n            input_ids = tokenized_examples[\"input_ids\"][i]\n            if tokenizer.cls_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.cls_token_id)\n            elif tokenizer.bos_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.bos_token_id)\n            else:\n                cls_index = 0\n\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples.sequence_ids(i)\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            # If no answers are given, set the cls_index as answer.\n            if len(answers[\"answer_start\"]) == 0:\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Start/end character index of the answer in the text.\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                # Start token index of the current span in the text.\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n\n                # End token index of the current span in the text.\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n\n                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples[\"start_positions\"].append(cls_index)\n                    tokenized_examples[\"end_positions\"].append(cls_index)\n                else:\n                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                    # Note: we could go after the last offset if the answer is the last word (edge case).\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n        return tokenized_examples\n\n    if \"train\" not in raw_datasets:\n        raise ValueError(\"--do_train requires a train dataset\")\n    train_dataset = raw_datasets[\"train\"]\n    if args.max_train_samples is not None:\n        # We will select sample from whole data if argument is specified\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    # Create train feature from dataset\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(\n            prepare_train_features,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on train dataset\",\n        )\n        if args.max_train_samples is not None:\n            # Number of samples might increase during Feature Creation, We select only specified max samples\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    # Validation preprocessing\n    def prepare_validation_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_seq_length,\n            stride=args.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\" if args.pad_to_max_length else False,\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n        # corresponding example_id and we will store the offset mappings.\n        tokenized_examples[\"example_id\"] = []\n\n        for i in range(len(tokenized_examples[\"input_ids\"])):\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n            # position is part of the context or not.\n            tokenized_examples[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_examples\n\n    if \"validation\" not in raw_datasets:\n        raise ValueError(\"--do_eval requires a validation dataset\")\n    eval_examples = raw_datasets[\"validation\"]\n    if args.max_eval_samples is not None:\n        # We will select sample from whole data\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    # Validation Feature Creation\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(\n            prepare_validation_features,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on validation dataset\",\n        )\n\n    if args.max_eval_samples is not None:\n        # During Feature creation dataset samples might increase, we will select required samples again\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n\n    if args.do_predict:\n        if \"test\" not in raw_datasets:\n            raise ValueError(\"--do_predict requires a test dataset\")\n        predict_examples = raw_datasets[\"test\"]\n        if args.max_predict_samples is not None:\n            # We will select sample from whole data\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        # Predict Feature Creation\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(\n                prepare_validation_features,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=\"Running tokenizer on prediction dataset\",\n            )\n            if args.max_predict_samples is not None:\n                # During Feature creation dataset samples might increase, we will select required samples again\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n\n    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n    eval_dataloader = DataLoader(\n        eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n        predict_dataloader = DataLoader(\n            predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n        )\n\n    # Post-processing:\n    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n        # Post-processing: we match the start logits and end logits to answers in the original context.\n        predictions = postprocess_qa_predictions(\n            examples=examples,\n            features=features,\n            predictions=predictions,\n            version_2_with_negative=args.version_2_with_negative,\n            n_best_size=args.n_best_size,\n            max_answer_length=args.max_answer_length,\n            null_score_diff_threshold=args.null_score_diff_threshold,\n            output_dir=args.output_dir,\n            prefix=stage,\n        )\n        # Format the result to the format the metric expects.\n        if args.version_2_with_negative:\n            formatted_predictions = [\n                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n            ]\n        else:\n            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n\n        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n\n    metric = evaluate.load(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n\n    # Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n\n        step = 0\n        # create a numpy array and fill it with -100.\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather_for_metrics\n        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n            # And after every iteration we have to change the step\n\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n\n            if step + batch_size < len(dataset):\n                logits_concat[step : step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n            step += batch_size\n\n        return logits_concat\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"qa_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n    # Evaluation\n    logger.info(\"***** Running Evaluation *****\")\n    logger.info(f\"  Num examples = {len(eval_dataset)}\")\n    logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n\n    all_start_logits = []\n    all_end_logits = []\n\n    model.eval()\n\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n\n    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n\n    # concatenate the numpy array\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n\n    # delete the list of numpy arrays\n    del all_start_logits\n    del all_end_logits\n\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f\"Evaluation metrics: {eval_metric}\")\n\n    # Prediction\n    if args.do_predict:\n        logger.info(\"***** Running Prediction *****\")\n        logger.info(f\"  Num examples = {len(predict_dataset)}\")\n        logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n\n        all_start_logits = []\n        all_end_logits = []\n\n        model.eval()\n\n        for step, batch in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n\n                if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n\n        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n        # concatenate the numpy array\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n\n        # delete the list of numpy arrays\n        del all_start_logits\n        del all_end_logits\n\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f\"Predict metrics: {predict_metric}\")\n\n    if args.with_tracking:\n        log = {\n            \"squad_v2\" if args.version_2_with_negative else \"squad\": eval_metric,\n            \"train_loss\": total_loss.item() / len(train_dataloader),\n            \"epoch\": epoch,\n            \"step\": completed_steps,\n        }\n    if args.do_predict:\n        log[\"squad_v2_predict\" if args.version_2_with_negative else \"squad_predict\"] = predict_metric\n\n        accelerator.log(log, step=completed_steps)\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_semantic_segmentation_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    # We set device_specific to True as we want different data augmentation per device.\n    if args.seed is not None:\n        set_seed(args.seed, device_specific=True)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Load dataset\n    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n    # download the dataset.\n    # TODO support datasets from local folders\n    dataset = load_dataset(args.dataset_name, cache_dir=args.cache_dir, trust_remote_code=args.trust_remote_code)\n\n    # Rename column names to standardized names (only \"image\" and \"label\" need to be present)\n    if \"pixel_values\" in dataset[\"train\"].column_names:\n        dataset = dataset.rename_columns({\"pixel_values\": \"image\"})\n    if \"annotation\" in dataset[\"train\"].column_names:\n        dataset = dataset.rename_columns({\"annotation\": \"label\"})\n\n    # If we don't have a validation split, split off a percentage of train as validation.\n    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n    if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n        split = dataset[\"train\"].train_test_split(args.train_val_split)\n        dataset[\"train\"] = split[\"train\"]\n        dataset[\"validation\"] = split[\"test\"]\n\n    # Prepare label mappings.\n    # We'll include these in the model's config to get human readable labels in the Inference API.\n    if args.dataset_name == \"scene_parse_150\":\n        repo_id = \"huggingface/label-files\"\n        filename = \"ade20k-id2label.json\"\n    else:\n        repo_id = args.dataset_name\n        filename = \"id2label.json\"\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n    id2label = {int(k): v for k, v in id2label.items()}\n    label2id = {v: k for k, v in id2label.items()}\n\n    # Load pretrained model and image processor\n    config = AutoConfig.from_pretrained(\n        args.model_name_or_path, id2label=id2label, label2id=label2id, trust_remote_code=args.trust_remote_code\n    )\n    image_processor = AutoImageProcessor.from_pretrained(\n        args.model_name_or_path, trust_remote_code=args.trust_remote_code\n    )\n    model = AutoModelForSemanticSegmentation.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        trust_remote_code=args.trust_remote_code,\n        do_reduce_labels=args.do_reduce_labels,\n    )\n\n    # Define transforms to be applied to each image and target.\n    if \"shortest_edge\" in image_processor.size:\n        # We instead set the target size as (shortest_edge, shortest_edge) to here to ensure all images are batchable.\n        height, width = image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"]\n    else:\n        height, width = image_processor.size[\"height\"], image_processor.size[\"width\"]\n    train_transforms = A.Compose(\n        [\n            A.Lambda(name=\"reduce_labels\", mask=reduce_labels_transform if args.do_reduce_labels else None, p=1.0),\n            # pad image with 255, because it is ignored by loss\n            A.PadIfNeeded(min_height=height, min_width=width, border_mode=0, value=255, p=1.0),\n            A.RandomCrop(height=height, width=width, p=1.0),\n            A.HorizontalFlip(p=0.5),\n            A.Normalize(mean=image_processor.image_mean, std=image_processor.image_std, max_pixel_value=255.0, p=1.0),\n            ToTensorV2(),\n        ]\n    )\n    val_transforms = A.Compose(\n        [\n            A.Lambda(name=\"reduce_labels\", mask=reduce_labels_transform if args.do_reduce_labels else None, p=1.0),\n            A.Resize(height=height, width=width, p=1.0),\n            A.Normalize(mean=image_processor.image_mean, std=image_processor.image_std, max_pixel_value=255.0, p=1.0),\n            ToTensorV2(),\n        ]\n    )\n\n    def preprocess_batch(example_batch, transforms: A.Compose):\n        pixel_values = []\n        labels = []\n        for image, target in zip(example_batch[\"image\"], example_batch[\"label\"]):\n            transformed = transforms(image=np.array(image.convert(\"RGB\")), mask=np.array(target))\n            pixel_values.append(transformed[\"image\"])\n            labels.append(transformed[\"mask\"])\n\n        encoding = {}\n        encoding[\"pixel_values\"] = torch.stack(pixel_values).to(torch.float)\n        encoding[\"labels\"] = torch.stack(labels).to(torch.long)\n\n        return encoding\n\n    # Preprocess function for dataset should have only one input argument,\n    # so we use partial to pass transforms\n    preprocess_train_batch_fn = partial(preprocess_batch, transforms=train_transforms)\n    preprocess_val_batch_fn = partial(preprocess_batch, transforms=val_transforms)\n\n    with accelerator.main_process_first():\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train_batch_fn)\n        eval_dataset = dataset[\"validation\"].with_transform(preprocess_val_batch_fn)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(\n        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        list(model.parameters()),\n        lr=args.learning_rate,\n        betas=[args.adam_beta1, args.adam_beta2],\n        eps=args.adam_epsilon,\n    )\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Instantiate metric\n    metric = evaluate.load(\"mean_iou\", cache_dir=args.cache_dir)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"semantic_segmentation_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n                    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n                        accelerator.wait_for_everyone()\n                        unwrapped_model = accelerator.unwrap_model(model)\n                        unwrapped_model.save_pretrained(\n                            args.output_dir,\n                            is_main_process=accelerator.is_main_process,\n                            save_function=accelerator.save,\n                        )\n                        if accelerator.is_main_process:\n                            image_processor.save_pretrained(args.output_dir)\n                            api.upload_folder(\n                                commit_message=f\"Training in progress epoch {epoch}\",\n                                folder_path=args.output_dir,\n                                repo_id=repo_id,\n                                repo_type=\"model\",\n                                token=args.hub_token,\n                            )\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        logger.info(\"***** Running evaluation *****\")\n        model.eval()\n        for step, batch in enumerate(tqdm(eval_dataloader, disable=not accelerator.is_local_main_process)):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            upsampled_logits = torch.nn.functional.interpolate(\n                outputs.logits, size=batch[\"labels\"].shape[-2:], mode=\"bilinear\", align_corners=False\n            )\n            predictions = upsampled_logits.argmax(dim=1)\n\n            predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metrics = metric.compute(\n            num_labels=len(id2label),\n            ignore_index=255,\n            reduce_labels=False,  # we've already reduced the labels before\n        )\n        logger.info(f\"epoch {epoch}: {eval_metrics}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"mean_iou\": eval_metrics[\"mean_iou\"],\n                    \"mean_accuracy\": eval_metrics[\"mean_accuracy\"],\n                    \"overall_accuracy\": eval_metrics[\"overall_accuracy\"],\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                image_processor.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            image_processor.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n            all_results = {\n                f\"eval_{k}\": v.tolist() if isinstance(v, np.ndarray) else v for k, v in eval_metrics.items()\n            }\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(all_results, f, indent=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    # See all possible arguments in src/transformers/args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_wav2vec2_pretraining_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    accelerator = Accelerator()\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n\n        # set up weights and biases if available\n        if is_wandb_available():\n            import wandb\n\n            wandb.init(project=args.output_dir.split(\"/\")[-1])\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub and not args.preprocessing_only:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # 1. Download and create train, validation dataset\n    # We load all dataset configuration and datset split pairs passed in\n    # ``args.dataset_config_names`` and ``args.dataset_split_names``\n    datasets_splits = []\n    for dataset_config_name, train_split_name in zip(args.dataset_config_names, args.dataset_split_names):\n        # load dataset\n        dataset_split = load_dataset(\n            args.dataset_name,\n            dataset_config_name,\n            split=train_split_name,\n            cache_dir=args.cache_dir,\n            trust_remote_code=args.trust_remote_code,\n        )\n        datasets_splits.append(dataset_split)\n\n    # Next, we concatenate all configurations and splits into a single training dataset\n    raw_datasets = DatasetDict()\n    if len(datasets_splits) > 1:\n        raw_datasets[\"train\"] = concatenate_datasets(datasets_splits).shuffle(seed=args.seed)\n    else:\n        raw_datasets[\"train\"] = datasets_splits[0]\n\n    # Take ``args.validation_split_percentage`` from the training dataset for the validation_split_percentage\n    num_validation_samples = raw_datasets[\"train\"].num_rows * args.validation_split_percentage // 100\n\n    if num_validation_samples == 0:\n        raise ValueError(\n            \"`args.validation_split_percentage` is less than a single sample \"\n            f\"for {len(raw_datasets['train'])} training samples. Increase \"\n            \"`args.num_validation_split_percentage`. \"\n        )\n\n    raw_datasets[\"validation\"] = raw_datasets[\"train\"].select(range(num_validation_samples))\n    raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(num_validation_samples, raw_datasets[\"train\"].num_rows))\n\n    # 2. Now we preprocess the datasets including loading the audio, resampling and normalization\n    # Thankfully, `datasets` takes care of automatically loading and resampling the audio,\n    # so that we just need to set the correct target sampling rate and normalize the input\n    # via the `feature_extractor`\n    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(args.model_name_or_path)\n\n    # make sure that dataset decodes audio with correct sampling rate\n    raw_datasets = raw_datasets.cast_column(\n        args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n    )\n\n    # only normalized-inputs-training is supported\n    if not feature_extractor.do_normalize:\n        raise ValueError(\n            \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n        )\n\n    # set max & min audio length in number of samples\n    max_length = int(args.max_duration_in_seconds * feature_extractor.sampling_rate)\n    min_length = int(args.min_duration_in_seconds * feature_extractor.sampling_rate)\n\n    def prepare_dataset(batch):\n        sample = batch[args.audio_column_name]\n\n        inputs = feature_extractor(\n            sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], max_length=max_length, truncation=True\n        )\n        batch[\"input_values\"] = inputs.input_values[0]\n        batch[\"input_length\"] = len(inputs.input_values[0])\n\n        return batch\n\n    # load via mapped files via path\n    cache_file_names = None\n    if args.train_cache_file_name is not None:\n        cache_file_names = {\"train\": args.train_cache_file_name, \"validation\": args.validation_cache_file_name}\n\n    # load audio files into numpy arrays\n    with accelerator.main_process_first():\n        vectorized_datasets = raw_datasets.map(\n            prepare_dataset,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=raw_datasets[\"train\"].column_names,\n            cache_file_names=cache_file_names,\n        )\n\n        if min_length > 0.0:\n            vectorized_datasets = vectorized_datasets.filter(\n                lambda x: x > min_length,\n                num_proc=args.preprocessing_num_workers,\n                input_columns=[\"input_length\"],\n            )\n\n        vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")\n\n    # for large datasets it is advised to run the preprocessing on a\n    # single machine first with ``args.preprocessing_only`` since there will mostly likely\n    # be a timeout when running the script in distributed mode.\n    # In a second step ``args.preprocessing_only`` can then be set to `False` to load the\n    # cached dataset\n    if args.preprocessing_only:\n        return\n\n    # 3. Load model\n    config = Wav2Vec2Config.from_pretrained(args.model_name_or_path)\n\n    # pretraining is only supported for \"newer\" stable layer norm architecture\n    # apply_spec_augment has to be True, mask_feature_prob has to be 0.0\n    if not config.do_stable_layer_norm or config.feat_extract_norm != \"layer\":\n        raise ValueError(\n            \"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and\"\n            \" ``config.feat_extract_norm='layer'\"\n        )\n\n    # initialize random model\n    model = Wav2Vec2ForPreTraining(config)\n\n    # Activate gradient checkpointing if needed\n    if args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    # 4. Define data collator, optimizer and scheduler\n\n    mask_time_prob = config.mask_time_prob if args.mask_time_prob is None else args.mask_time_prob\n    mask_time_length = config.mask_time_length if args.mask_time_length is None else args.mask_time_length\n\n    data_collator = DataCollatorForWav2Vec2Pretraining(\n        model=model,\n        feature_extractor=feature_extractor,\n        pad_to_multiple_of=args.pad_to_multiple_of,\n        mask_time_prob=mask_time_prob,\n        mask_time_length=mask_time_length,\n    )\n    train_dataloader = DataLoader(\n        vectorized_datasets[\"train\"],\n        shuffle=True,\n        collate_fn=data_collator,\n        batch_size=args.per_device_train_batch_size,\n    )\n    eval_dataloader = DataLoader(\n        vectorized_datasets[\"validation\"], collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    # Optimizer\n    optimizer = AdamW(\n        list(model.parameters()),\n        lr=args.learning_rate,\n        betas=[args.adam_beta1, args.adam_beta2],\n        eps=args.adam_epsilon,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    # Scheduler and math around the number of training steps.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # 5. Train\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(vectorized_datasets['train'])}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            # compute num of losses\n            num_losses = batch[\"mask_time_indices\"].sum()\n            sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n            sub_attention_mask = (\n                sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n            )\n            percent_masked = num_losses / sub_attention_mask.sum()\n\n            # forward\n            outputs = model(**batch)\n\n            # divide loss by gradient accumulation steps since gradients\n            # are accumulated for multiple backward passes in PyTorch\n            loss = outputs.loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n\n            # make sure that `num_losses` is summed for distributed training\n            # and average gradients over losses of all devices\n            if accelerator.state.num_processes > 1:\n                num_losses = accelerator.gather_for_metrics(num_losses).sum()\n                gradient_multiplier = accelerator.state.num_processes / num_losses\n                multiply_grads(model.module.parameters(), gradient_multiplier)\n            else:\n                multiply_grads(model.parameters(), 1 / num_losses)\n\n            # update step\n            if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                # compute grad norm for monitoring\n                scale = (\n                    accelerator.scaler._scale.item()\n                    if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n                    else 1\n                )\n                if accelerator.state.num_processes > 1:\n                    grad_norm = get_grad_norm(model.module.parameters(), scale)\n                else:\n                    grad_norm = get_grad_norm(model.parameters(), scale)\n\n                # update parameters\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if not accelerator.optimizer_step_was_skipped:\n                    lr_scheduler.step()\n                elif accelerator.is_local_main_process:\n                    progress_bar.write(\n                        f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n                    )\n\n                # update gumbel temperature\n                gumbel_temperature = max(\n                    args.max_gumbel_temperature * args.gumbel_temperature_decay**completed_steps,\n                    args.min_gumbel_temperature,\n                )\n                if hasattr(model, \"module\"):\n                    model.module.set_gumbel_temperature(gumbel_temperature)\n                else:\n                    model.set_gumbel_temperature(gumbel_temperature)\n\n                progress_bar.update(1)\n                completed_steps += 1\n\n            # 6. Log all results\n            if (step + 1) % (args.gradient_accumulation_steps * args.logging_steps) == 0:\n                loss.detach()\n                outputs.contrastive_loss.detach()\n                outputs.diversity_loss.detach()\n\n                if accelerator.state.num_processes > 1:\n                    loss = accelerator.gather_for_metrics(loss).sum()\n                    outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n                    outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n                    percent_masked = accelerator.gather_for_metrics(percent_masked).sum()\n\n                train_logs = {\n                    \"loss\": (loss * args.gradient_accumulation_steps) / num_losses,\n                    \"constrast_loss\": outputs.contrastive_loss / num_losses,\n                    \"div_loss\": outputs.diversity_loss / num_losses,\n                    \"%_mask_idx\": percent_masked / accelerator.num_processes,\n                    \"ppl\": outputs.codevector_perplexity,\n                    \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n                    \"temp\": torch.tensor(gumbel_temperature),\n                    \"grad_norm\": torch.tensor(grad_norm),\n                }\n                log_str = \"\"\n                for k, v in train_logs.items():\n                    log_str += \"| {}: {:.3e}\".format(k, v.item())\n\n                if accelerator.is_local_main_process:\n                    progress_bar.write(log_str)\n                    if is_wandb_available():\n                        wandb.log(train_logs)\n\n            # save model every `args.saving_steps` steps\n            if (step + 1) % (args.gradient_accumulation_steps * args.saving_steps) == 0:\n                if (args.push_to_hub and epoch < args.num_train_epochs - 1) or args.output_dir is not None:\n                    accelerator.wait_for_everyone()\n                    unwrapped_model = accelerator.unwrap_model(model)\n                    unwrapped_model.save_pretrained(\n                        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n                    )\n\n                if (args.push_to_hub and epoch < args.num_train_epochs - 1) and accelerator.is_main_process:\n                    api.upload_folder(\n                        commit_message=f\"Training in progress epoch {epoch}\",\n                        folder_path=args.output_dir,\n                        repo_id=repo_id,\n                        repo_type=\"model\",\n                        token=args.hub_token,\n                    )\n\n            # if completed steps > `args.max_train_steps` stop\n            if completed_steps >= args.max_train_steps:\n                break\n\n        # 7. Validate!\n        model.eval()\n\n        # init logs\n        val_logs = {\n            \"val_loss\": 0,\n            \"val_contrastive_loss\": 0,\n            \"val_diversity_loss\": 0,\n            \"val_num_losses\": 0,\n        }\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                batch.pop(\"sub_attention_mask\", None)\n                outputs = model(**batch)\n\n            val_logs[\"val_loss\"] += outputs.loss\n            val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n            val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n            val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n\n        # sum over devices in multi-processing\n        if accelerator.num_processes > 1:\n            val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n\n        val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n\n        log_str = \"\"\n        for k, v in val_logs.items():\n            log_str += \"| {}: {:.3e}\".format(k, v.item())\n\n        if accelerator.is_local_main_process:\n            progress_bar.write(log_str)\n            if is_wandb_available():\n                wandb.log(val_logs)\n\n        if args.output_dir is not None:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                if args.push_to_hub:\n                    api.upload_folder(\n                        commit_message=\"End of training\",\n                        folder_path=args.output_dir,\n                        repo_id=repo_id,\n                        repo_type=\"model\",\n                        token=args.hub_token,\n                    )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_glue_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator = (\n        Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()\n    )\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n\n    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n    # label if at least two columns are provided.\n\n    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n    # single column. You can easily tweak this behavior (see below)\n\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.task_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\"nyu-mll/glue\", args.task_name)\n    else:\n        # Loading the dataset from local csv or json file.\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n        extension = (args.train_file if args.train_file is not None else args.validation_file).split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # See more about loading any type of standard or custom dataset at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Labels\n    if args.task_name is not None:\n        is_regression = args.task_name == \"stsb\"\n        if not is_regression:\n            label_list = raw_datasets[\"train\"].features[\"label\"].names\n            num_labels = len(label_list)\n        else:\n            num_labels = 1\n    else:\n        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n        is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n        if is_regression:\n            num_labels = 1\n        else:\n            # A useful fast method:\n            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n            label_list = raw_datasets[\"train\"].unique(\"label\")\n            label_list.sort()  # Let's sort it for determinism\n            num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    config = AutoConfig.from_pretrained(\n        args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=args.task_name,\n        trust_remote_code=args.trust_remote_code,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n    )\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    config.pad_token_id = tokenizer.pad_token_id\n    model = AutoModelForSequenceClassification.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n        trust_remote_code=args.trust_remote_code,\n    )\n\n    # Preprocessing the datasets\n    if args.task_name is not None:\n        sentence1_key, sentence2_key = task_to_keys[args.task_name]\n    else:\n        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n        else:\n            if len(non_label_column_names) >= 2:\n                sentence1_key, sentence2_key = non_label_column_names[:2]\n            else:\n                sentence1_key, sentence2_key = non_label_column_names[0], None\n\n    # Some models have set the order of the labels to use, so let's make sure we do use it.\n    label_to_id = None\n    if (\n        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n        and args.task_name is not None\n        and not is_regression\n    ):\n        # Some have all caps in their config, some don't.\n        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n        if sorted(label_name_to_id.keys()) == sorted(label_list):\n            logger.info(\n                f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n                \"Using it!\"\n            )\n            label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n        else:\n            logger.warning(\n                \"Your model seems to have been trained with labels, but they don't match the dataset: \"\n                f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n                \"\\nIgnoring the model labels as a result.\",\n            )\n    elif args.task_name is None and not is_regression:\n        label_to_id = {v: i for i, v in enumerate(label_list)}\n\n    if label_to_id is not None:\n        model.config.label2id = label_to_id\n        model.config.id2label = {id: label for label, id in config.label2id.items()}\n    elif args.task_name is not None and not is_regression:\n        model.config.label2id = {l: i for i, l in enumerate(label_list)}\n        model.config.id2label = {id: label for label, id in config.label2id.items()}\n\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    def preprocess_function(examples):\n        # Tokenize the texts\n        texts = (\n            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n        )\n        result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)\n\n        if \"label\" in examples:\n            if label_to_id is not None:\n                # Map labels to IDs (not necessary for GLUE tasks)\n                result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n            else:\n                # In all cases, rename the column to labels because the model will expect that.\n                result[\"labels\"] = examples[\"label\"]\n        return result\n\n    with accelerator.main_process_first():\n        processed_datasets = raw_datasets.map(\n            preprocess_function,\n            batched=True,\n            remove_columns=raw_datasets[\"train\"].column_names,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    train_dataset = processed_datasets[\"train\"]\n    eval_dataset = processed_datasets[\"validation_matched\" if args.task_name == \"mnli\" else \"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"glue_no_trainer\", experiment_config)\n\n    # Get the metric function\n    if args.task_name is not None:\n        metric = evaluate.load(\"glue\", args.task_name)\n    else:\n        metric = evaluate.load(\"accuracy\")\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            # We keep track of the loss at each epoch\n            if args.with_tracking:\n                total_loss += loss.detach().float()\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        samples_seen = 0\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()\n            predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n            # If we are in a multiprocess environment, the last batch has duplicates\n            if accelerator.num_processes > 1:\n                if step == len(eval_dataloader) - 1:\n                    predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n                    references = references[: len(eval_dataloader.dataset) - samples_seen]\n                else:\n                    samples_seen += references.shape[0]\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metric = metric.compute()\n        logger.info(f\"epoch {epoch}: {eval_metric}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"accuracy\" if args.task_name is not None else \"glue\": eval_metric,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n    if args.task_name == \"mnli\":\n        # Final evaluation on mismatched validation set\n        eval_dataset = processed_datasets[\"validation_mismatched\"]\n        eval_dataloader = DataLoader(\n            eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n        )\n        eval_dataloader = accelerator.prepare(eval_dataloader)\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            metric.add_batch(\n                predictions=accelerator.gather(predictions),\n                references=accelerator.gather(batch[\"labels\"]),\n            )\n\n        eval_metric = metric.compute()\n        logger.info(f\"mnli-mm: {eval_metric}\")\n\n    if args.output_dir is not None:\n        all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n            json.dump(all_results, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_qa_beam_search_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        if args.test_file is not None:\n            data_files[\"test\"] = args.test_file\n            extension = args.test_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n\n    config = XLNetConfig.from_pretrained(args.model_name_or_path)\n    tokenizer = XLNetTokenizerFast.from_pretrained(args.model_name_or_path)\n    model = XLNetForQuestionAnswering.from_pretrained(\n        args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config\n    )\n\n    # Preprocessing the datasets.\n    # Preprocessing is slightly different for training and evaluation.\n    column_names = raw_datasets[\"train\"].column_names\n\n    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n\n    # Padding side determines if we do (question|context) or (context|question).\n    pad_on_right = tokenizer.padding_side == \"right\"\n\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(\n            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n        )\n\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    # Training preprocessing\n    def prepare_train_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_seq_length,\n            stride=args.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            return_special_tokens_mask=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n        # The offset mappings will give us a map from token to character position in the original context. This will\n        # help us compute the start_positions and end_positions.\n        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n        # The special tokens will help us build the p_mask (which indicates the tokens that can't be in answers).\n        special_tokens = tokenized_examples.pop(\"special_tokens_mask\")\n\n        # Let's label those examples!\n        tokenized_examples[\"start_positions\"] = []\n        tokenized_examples[\"end_positions\"] = []\n        tokenized_examples[\"is_impossible\"] = []\n        tokenized_examples[\"cls_index\"] = []\n        tokenized_examples[\"p_mask\"] = []\n\n        for i, offsets in enumerate(offset_mapping):\n            # We will label impossible answers with the index of the CLS token.\n            input_ids = tokenized_examples[\"input_ids\"][i]\n            if tokenizer.cls_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.cls_token_id)\n            elif tokenizer.bos_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.bos_token_id)\n            else:\n                cls_index = 0\n            tokenized_examples[\"cls_index\"].append(cls_index)\n\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples[\"token_type_ids\"][i]\n            for k, s in enumerate(special_tokens[i]):\n                if s:\n                    sequence_ids[k] = 3\n            context_idx = 1 if pad_on_right else 0\n\n            # Build the p_mask: non special tokens and context gets 0.0, the others get 1.0.\n            # The cls token gets 1.0 too (for predictions of empty answers).\n            tokenized_examples[\"p_mask\"].append(\n                [\n                    0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0\n                    for k, s in enumerate(sequence_ids)\n                ]\n            )\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            # If no answers are given, set the cls_index as answer.\n            if len(answers[\"answer_start\"]) == 0:\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n                tokenized_examples[\"is_impossible\"].append(1.0)\n            else:\n                # Start/end character index of the answer in the text.\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                # Start token index of the current span in the text.\n                token_start_index = 0\n                while sequence_ids[token_start_index] != context_idx:\n                    token_start_index += 1\n\n                # End token index of the current span in the text.\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != context_idx:\n                    token_end_index -= 1\n                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples[\"start_positions\"].append(cls_index)\n                    tokenized_examples[\"end_positions\"].append(cls_index)\n                    tokenized_examples[\"is_impossible\"].append(1.0)\n                else:\n                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                    # Note: we could go after the last offset if the answer is the last word (edge case).\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n                    tokenized_examples[\"is_impossible\"].append(0.0)\n\n        return tokenized_examples\n\n    if \"train\" not in raw_datasets:\n        raise ValueError(\"--do_train requires a train dataset\")\n    train_dataset = raw_datasets[\"train\"]\n    if args.max_train_samples is not None:\n        # We will select sample from whole data if argument is specified\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    # Create train feature from dataset\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(\n            prepare_train_features,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on train dataset\",\n        )\n    if args.max_train_samples is not None:\n        # Number of samples might increase during Feature Creation, We select only specified max samples\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    # Validation preprocessing\n    def prepare_validation_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_seq_length,\n            stride=args.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            return_special_tokens_mask=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n        # The special tokens will help us build the p_mask (which indicates the tokens that can't be in answers).\n        special_tokens = tokenized_examples.pop(\"special_tokens_mask\")\n\n        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n        # corresponding example_id and we will store the offset mappings.\n        tokenized_examples[\"example_id\"] = []\n\n        # We still provide the index of the CLS token and the p_mask to the model, but not the is_impossible label.\n        tokenized_examples[\"cls_index\"] = []\n        tokenized_examples[\"p_mask\"] = []\n\n        for i, input_ids in enumerate(tokenized_examples[\"input_ids\"]):\n            # Find the CLS token in the input ids.\n            if tokenizer.cls_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.cls_token_id)\n            elif tokenizer.bos_token_id in input_ids:\n                cls_index = input_ids.index(tokenizer.bos_token_id)\n            else:\n                cls_index = 0\n            tokenized_examples[\"cls_index\"].append(cls_index)\n\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples[\"token_type_ids\"][i]\n            for k, s in enumerate(special_tokens[i]):\n                if s:\n                    sequence_ids[k] = 3\n            context_idx = 1 if pad_on_right else 0\n\n            # Build the p_mask: non special tokens and context gets 0.0, the others 1.0.\n            tokenized_examples[\"p_mask\"].append(\n                [\n                    0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0\n                    for k, s in enumerate(sequence_ids)\n                ]\n            )\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n            # position is part of the context or not.\n            tokenized_examples[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_idx else None)\n                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_examples\n\n    if \"validation\" not in raw_datasets:\n        raise ValueError(\"--do_eval requires a validation dataset\")\n    eval_examples = raw_datasets[\"validation\"]\n    if args.max_eval_samples is not None:\n        # We will select sample from whole data\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    # Validation Feature Creation\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(\n            prepare_validation_features,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on validation dataset\",\n        )\n\n    if args.max_eval_samples is not None:\n        # During Feature creation dataset samples might increase, we will select required samples again\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n\n    if args.do_predict:\n        if \"test\" not in raw_datasets:\n            raise ValueError(\"--do_predict requires a test dataset\")\n        predict_examples = raw_datasets[\"test\"]\n        if args.max_predict_samples is not None:\n            # We will select sample from whole data\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        # Predict Feature Creation\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(\n                prepare_validation_features,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=\"Running tokenizer on prediction dataset\",\n            )\n            if args.max_predict_samples is not None:\n                # During Feature creation dataset samples might increase, we will select required samples again\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n\n    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n    eval_dataloader = DataLoader(\n        eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n    )\n\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n        predict_dataloader = DataLoader(\n            predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n        )\n\n    # Post-processing:\n    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n        # Post-processing: we match the start logits and end logits to answers in the original context.\n        predictions, scores_diff_json = postprocess_qa_predictions_with_beam_search(\n            examples=examples,\n            features=features,\n            predictions=predictions,\n            version_2_with_negative=args.version_2_with_negative,\n            n_best_size=args.n_best_size,\n            max_answer_length=args.max_answer_length,\n            start_n_top=model.config.start_n_top,\n            end_n_top=model.config.end_n_top,\n            output_dir=args.output_dir,\n            prefix=stage,\n        )\n        # Format the result to the format the metric expects.\n        if args.version_2_with_negative:\n            formatted_predictions = [\n                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": scores_diff_json[k]}\n                for k, v in predictions.items()\n            ]\n        else:\n            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n\n        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n\n    metric = evaluate.load(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n\n        step = 0\n        # create a numpy array and fill it with -100.\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float32)\n        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather_for_metrics\n        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n            # And after every iteration we have to change the step\n\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step : step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n            step += batch_size\n\n        return logits_concat\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"qa_beam_search_no_trainer\", experiment_config)\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n\n                accelerator.backward(loss)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    accelerator.save_state(f\"step_{completed_steps}\")\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n    # initialize all lists to collect the batches\n    all_start_top_log_probs = []\n    all_start_top_index = []\n    all_end_top_log_probs = []\n    all_end_top_index = []\n    all_cls_logits = []\n\n    model.eval()\n\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_top_log_probs = outputs.start_top_log_probs\n            start_top_index = outputs.start_top_index\n            end_top_log_probs = outputs.end_top_log_probs\n            end_top_index = outputs.end_top_index\n            cls_logits = outputs.cls_logits\n\n            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n                start_top_log_probs = accelerator.pad_across_processes(start_top_log_probs, dim=1, pad_index=-100)\n                start_top_index = accelerator.pad_across_processes(start_top_index, dim=1, pad_index=-100)\n                end_top_log_probs = accelerator.pad_across_processes(end_top_log_probs, dim=1, pad_index=-100)\n                end_top_index = accelerator.pad_across_processes(end_top_index, dim=1, pad_index=-100)\n                cls_logits = accelerator.pad_across_processes(cls_logits, dim=1, pad_index=-100)\n\n            all_start_top_log_probs.append(accelerator.gather_for_metrics(start_top_log_probs).cpu().numpy())\n            all_start_top_index.append(accelerator.gather_for_metrics(start_top_index).cpu().numpy())\n            all_end_top_log_probs.append(accelerator.gather_for_metrics(end_top_log_probs).cpu().numpy())\n            all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())\n            all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())\n\n    max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor\n\n    # concatenate all numpy arrays collected above\n    start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, eval_dataset, max_len)\n    start_top_index_concat = create_and_fill_np_array(all_start_top_index, eval_dataset, max_len)\n    end_top_log_probs_concat = create_and_fill_np_array(all_end_top_log_probs, eval_dataset, max_len)\n    end_top_index_concat = create_and_fill_np_array(all_end_top_index, eval_dataset, max_len)\n    cls_logits_concat = np.concatenate(all_cls_logits, axis=0)\n\n    # delete the list of numpy arrays\n    del start_top_log_probs\n    del start_top_index\n    del end_top_log_probs\n    del end_top_index\n    del cls_logits\n\n    outputs_numpy = (\n        start_top_log_probs_concat,\n        start_top_index_concat,\n        end_top_log_probs_concat,\n        end_top_index_concat,\n        cls_logits_concat,\n    )\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f\"Evaluation metrics: {eval_metric}\")\n\n    if args.do_predict:\n        # initialize all lists to collect the batches\n\n        all_start_top_log_probs = []\n        all_start_top_index = []\n        all_end_top_log_probs = []\n        all_end_top_index = []\n        all_cls_logits = []\n\n        model.eval()\n\n        for step, batch in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_top_log_probs = outputs.start_top_log_probs\n                start_top_index = outputs.start_top_index\n                end_top_log_probs = outputs.end_top_log_probs\n                end_top_index = outputs.end_top_index\n                cls_logits = outputs.cls_logits\n\n                if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n                    start_top_log_probs = accelerator.pad_across_processes(start_top_log_probs, dim=1, pad_index=-100)\n                    start_top_index = accelerator.pad_across_processes(start_top_index, dim=1, pad_index=-100)\n                    end_top_log_probs = accelerator.pad_across_processes(end_top_log_probs, dim=1, pad_index=-100)\n                    end_top_index = accelerator.pad_across_processes(end_top_index, dim=1, pad_index=-100)\n                    cls_logits = accelerator.pad_across_processes(cls_logits, dim=1, pad_index=-100)\n\n                all_start_top_log_probs.append(accelerator.gather_for_metrics(start_top_log_probs).cpu().numpy())\n                all_start_top_index.append(accelerator.gather_for_metrics(start_top_index).cpu().numpy())\n                all_end_top_log_probs.append(accelerator.gather_for_metrics(end_top_log_probs).cpu().numpy())\n                all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())\n                all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())\n\n        max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor\n\n        # concatenate all numpy arrays collected above\n        start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, predict_dataset, max_len)\n        start_top_index_concat = create_and_fill_np_array(all_start_top_index, predict_dataset, max_len)\n        end_top_log_probs_concat = create_and_fill_np_array(all_end_top_log_probs, predict_dataset, max_len)\n        end_top_index_concat = create_and_fill_np_array(all_end_top_index, predict_dataset, max_len)\n        cls_logits_concat = np.concatenate(all_cls_logits, axis=0)\n\n        # delete the list of numpy arrays\n        del start_top_log_probs\n        del start_top_index\n        del end_top_log_probs\n        del end_top_index\n        del cls_logits\n\n        outputs_numpy = (\n            start_top_log_probs_concat,\n            start_top_index_concat,\n            end_top_log_probs_concat,\n            end_top_index_concat,\n            cls_logits_concat,\n        )\n\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f\"Predict metrics: {predict_metric}\")\n\n    if args.with_tracking:\n        log = {\n            \"squad_v2\" if args.version_2_with_negative else \"squad\": eval_metric,\n            \"train_loss\": total_loss,\n            \"epoch\": epoch,\n            \"step\": completed_steps,\n        }\n        if args.do_predict:\n            log[\"squad_v2_predict\" if args.version_2_with_negative else \"squad_predict\"] = predict_metric\n\n        accelerator.log(log)\n\n    if args.checkpointing_steps == \"epoch\":\n        accelerator.save_state(f\"epoch_{epoch}\")\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_summarization_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    if args.source_prefix is None and args.model_name_or_path in [\n        \"google-t5/t5-small\",\n        \"google-t5/t5-base\",\n        \"google-t5/t5-large\",\n        \"google-t5/t5-3b\",\n        \"google-t5/t5-11b\",\n    ]:\n        logger.warning(\n            \"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with \"\n            \"`--source_prefix 'summarize: ' `\"\n        )\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForSeq2SeqLM.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n    if model.config.decoder_start_token_id is None:\n        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\n    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    column_names = raw_datasets[\"train\"].column_names\n\n    # Get the column names for input/target.\n    dataset_columns = summarization_name_mapping.get(args.dataset_name, None)\n    if args.text_column is None:\n        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n    else:\n        text_column = args.text_column\n        if text_column not in column_names:\n            raise ValueError(\n                f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n            )\n    if args.summary_column is None:\n        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n    else:\n        summary_column = args.summary_column\n        if summary_column not in column_names:\n            raise ValueError(\n                f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n            )\n\n    if args.val_max_target_length is None:\n        args.val_max_target_length = args.max_target_length\n\n    # Temporarily set max_target_length for training.\n    max_target_length = args.max_target_length\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[summary_column]\n        inputs = [prefix + inp for inp in inputs]\n        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n\n        # Tokenize targets with the `text_target` keyword argument\n        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n        # padding in the loss.\n        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\n            labels[\"input_ids\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n            ]\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    with accelerator.main_process_first():\n        train_dataset = raw_datasets[\"train\"].map(\n            preprocess_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n        # Temporarily set max_target_length for validation.\n        max_target_length = args.val_max_target_length\n        eval_dataset = raw_datasets[\"validation\"].map(\n            preprocess_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 1):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    if accelerator.mixed_precision == \"fp8\":\n        pad_to_multiple_of = 16\n    elif accelerator.mixed_precision != \"no\":\n        pad_to_multiple_of = 8\n    else:\n        pad_to_multiple_of = None\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer,\n        model=model,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=pad_to_multiple_of,\n    )\n\n    def postprocess_text(preds, labels):\n        preds = [pred.strip() for pred in preds]\n        labels = [label.strip() for label in labels]\n\n        # rougeLSum expects newline after each sentence\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n        return preds, labels\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps\n        if overrode_max_train_steps\n        else args.max_train_steps * accelerator.num_processes,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"summarization_no_trainer\", experiment_config)\n\n    # Metric\n    metric = evaluate.load(\"rouge\")\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n\n        gen_kwargs = {\n            \"max_length\": args.val_max_target_length,\n            \"num_beams\": args.num_beams,\n        }\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                generated_tokens = accelerator.unwrap_model(model).generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    **gen_kwargs,\n                )\n\n                generated_tokens = accelerator.pad_across_processes(\n                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n                )\n                labels = batch[\"labels\"]\n                if not args.pad_to_max_length:\n                    # If we did not pad to max length, we need to pad the labels too\n                    labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n\n                generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))\n                generated_tokens = generated_tokens.cpu().numpy()\n                labels = labels.cpu().numpy()\n\n                if args.ignore_pad_token_for_loss:\n                    # Replace -100 in the labels as we can't decode them.\n                    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n                if isinstance(generated_tokens, tuple):\n                    generated_tokens = generated_tokens[0]\n                decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n                decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n                metric.add_batch(\n                    predictions=decoded_preds,\n                    references=decoded_labels,\n                )\n        result = metric.compute(use_stemmer=True)\n        result = {k: round(v * 100, 4) for k, v in result.items()}\n\n        logger.info(result)\n\n        if args.with_tracking:\n            result[\"train_loss\"] = total_loss.item() / len(train_dataloader)\n            result[\"epoch\"] = epoch\n            result[\"step\"] = completed_steps\n            accelerator.log(result, step=completed_steps)\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n            all_results = {f\"eval_{k}\": v for k, v in result.items()}\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump(all_results, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    # Parse the arguments\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_translation_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator = (\n        Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForSeq2SeqLM.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Set decoder_start_token_id\n    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n        assert (\n            args.target_lang is not None and args.source_lang is not None\n        ), \"mBart requires --target_lang and --source_lang\"\n        if isinstance(tokenizer, MBartTokenizer):\n            model.config.decoder_start_token_id = tokenizer.lang_code_to_id[args.target_lang]\n        else:\n            model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(args.target_lang)\n\n    if model.config.decoder_start_token_id is None:\n        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\n    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    column_names = raw_datasets[\"train\"].column_names\n\n    # For translation we set the codes of our source and target languages (only useful for mBART, the others will\n    # ignore those attributes).\n    if isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n        if args.source_lang is not None:\n            tokenizer.src_lang = args.source_lang\n        if args.target_lang is not None:\n            tokenizer.tgt_lang = args.target_lang\n\n    # Get the language codes for input/target.\n    source_lang = args.source_lang.split(\"_\")[0]\n    target_lang = args.target_lang.split(\"_\")[0]\n\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    # Temporarily set max_target_length for training.\n    max_target_length = args.max_target_length\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    def preprocess_function(examples):\n        inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n        targets = [ex[target_lang] for ex in examples[\"translation\"]]\n        inputs = [prefix + inp for inp in inputs]\n        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n\n        # Tokenize targets with the `text_target` keyword argument\n        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n        # padding in the loss.\n        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\n            labels[\"input_ids\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n            ]\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    with accelerator.main_process_first():\n        processed_datasets = raw_datasets.map(\n            preprocess_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    train_dataset = processed_datasets[\"train\"]\n    eval_dataset = processed_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorForSeq2Seq(\n            tokenizer,\n            model=model,\n            label_pad_token_id=label_pad_token_id,\n            pad_to_multiple_of=pad_to_multiple_of,\n        )\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # We initialize the trackers only on main process because `accelerator.log`\n    # only logs on main process and we don't want empty logs/runs on other processes.\n    if args.with_tracking:\n        if accelerator.is_main_process:\n            experiment_config = vars(args)\n            # TensorBoard cannot log Enums, need the raw value\n            experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n            accelerator.init_trackers(\"translation_no_trainer\", experiment_config)\n\n    metric = evaluate.load(\"sacrebleu\")\n\n    def postprocess_text(preds, labels):\n        preds = [pred.strip() for pred in preds]\n        labels = [[label.strip()] for label in labels]\n\n        return preds, labels\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            # We keep track of the loss at each epoch\n            if args.with_tracking:\n                total_loss += loss.detach().float()\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n\n        if args.val_max_target_length is None:\n            args.val_max_target_length = args.max_target_length\n\n        gen_kwargs = {\n            \"max_length\": args.val_max_target_length if args is not None else config.max_length,\n            \"num_beams\": args.num_beams,\n        }\n        samples_seen = 0\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                generated_tokens = accelerator.unwrap_model(model).generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    **gen_kwargs,\n                )\n\n                generated_tokens = accelerator.pad_across_processes(\n                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n                )\n                labels = batch[\"labels\"]\n                if not args.pad_to_max_length:\n                    # If we did not pad to max length, we need to pad the labels too\n                    labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n\n                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n                labels = accelerator.gather(labels).cpu().numpy()\n\n                if args.ignore_pad_token_for_loss:\n                    # Replace -100 in the labels as we can't decode them.\n                    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n                decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n                decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n                # If we are in a multiprocess environment, the last batch has duplicates\n                if accelerator.num_processes > 1:\n                    if step == len(eval_dataloader) - 1:\n                        decoded_preds = decoded_preds[: len(eval_dataloader.dataset) - samples_seen]\n                        decoded_labels = decoded_labels[: len(eval_dataloader.dataset) - samples_seen]\n                    else:\n                        samples_seen += len(decoded_labels)\n\n                metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n        eval_metric = metric.compute()\n        logger.info({\"bleu\": eval_metric[\"score\"]})\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"bleu\": eval_metric[\"score\"],\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n            json.dump({\"eval_bleu\": eval_metric[\"score\"]}, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    send_example_telemetry(\"run_ner_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    accelerator = (\n        Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()\n    )\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            api = HfApi()\n            repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n    # 'tokens' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # Trim a number of training examples\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    if raw_datasets[\"train\"] is not None:\n        column_names = raw_datasets[\"train\"].column_names\n        features = raw_datasets[\"train\"].features\n    else:\n        column_names = raw_datasets[\"validation\"].column_names\n        features = raw_datasets[\"validation\"].features\n\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif \"tokens\" in column_names:\n        text_column_name = \"tokens\"\n    else:\n        text_column_name = column_names[0]\n\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f\"{args.task_name}_tags\" in column_names:\n        label_column_name = f\"{args.task_name}_tags\"\n    else:\n        label_column_name = column_names[1]\n\n    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n    # unique labels.\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n\n    # If the labels are of type ClassLabel, they are already integers and we have the map stored somewhere.\n    # Otherwise, we have to get the list of labels manually.\n    labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n    if labels_are_int:\n        label_list = features[label_column_name].feature.names\n        label_to_id = {i: i for i in range(len(label_list))}\n    else:\n        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n        label_to_id = {l: i for i, l in enumerate(label_list)}\n\n    num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = AutoConfig.from_pretrained(\n            args.config_name, num_labels=num_labels, trust_remote_code=args.trust_remote_code\n        )\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(\n            args.model_name_or_path, num_labels=num_labels, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if config.model_type in {\"bloom\", \"gpt2\", \"roberta\"}:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_name_or_path, use_fast=True, add_prefix_space=True, trust_remote_code=args.trust_remote_code\n        )\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForTokenClassification.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n            ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n            trust_remote_code=args.trust_remote_code,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForTokenClassification.from_config(config, trust_remote_code=args.trust_remote_code)\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) > embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Model has labels -> use them.\n    if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n        if sorted(model.config.label2id.keys()) == sorted(label_list):\n            # Reorganize `label_list` to match the ordering of the model.\n            if labels_are_int:\n                label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}\n                label_list = [model.config.id2label[i] for i in range(num_labels)]\n            else:\n                label_list = [model.config.id2label[i] for i in range(num_labels)]\n                label_to_id = {l: i for i, l in enumerate(label_list)}\n        else:\n            logger.warning(\n                \"Your model seems to have been trained with labels, but they don't match the dataset: \"\n                f\"model labels: {sorted(model.config.label2id.keys())}, dataset labels:\"\n                f\" {sorted(label_list)}.\\nIgnoring the model labels as a result.\",\n            )\n\n    # Set the correspondences label/ID inside the model config\n    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n    model.config.id2label = dict(enumerate(label_list))\n\n    # Map that sends B-Xxx label to its I-Xxx counterpart\n    b_to_i_label = []\n    for idx, label in enumerate(label_list):\n        if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n            b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n        else:\n            b_to_i_label.append(idx)\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    # Tokenize all texts and align the labels with them.\n\n    def tokenize_and_align_labels(examples):\n        tokenized_inputs = tokenizer(\n            examples[text_column_name],\n            max_length=args.max_length,\n            padding=padding,\n            truncation=True,\n            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n            is_split_into_words=True,\n        )\n\n        labels = []\n        for i, label in enumerate(examples[label_column_name]):\n            word_ids = tokenized_inputs.word_ids(batch_index=i)\n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:\n                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n                # ignored in the loss function.\n                if word_idx is None:\n                    label_ids.append(-100)\n                # We set the label for the first token of each word.\n                elif word_idx != previous_word_idx:\n                    label_ids.append(label_to_id[label[word_idx]])\n                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n                # the label_all_tokens flag.\n                else:\n                    if args.label_all_tokens:\n                        label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n                    else:\n                        label_ids.append(-100)\n                previous_word_idx = word_idx\n\n            labels.append(label_ids)\n        tokenized_inputs[\"labels\"] = labels\n        return tokenized_inputs\n\n    with accelerator.main_process_first():\n        processed_raw_datasets = raw_datasets.map(\n            tokenize_and_align_labels,\n            batched=True,\n            remove_columns=raw_datasets[\"train\"].column_names,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    train_dataset = processed_raw_datasets[\"train\"]\n    eval_dataset = processed_raw_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Use the device given by the `accelerator` object.\n    device = accelerator.device\n    model.to(device)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n        accelerator.init_trackers(\"ner_no_trainer\", experiment_config)\n\n    # Metrics\n    metric = evaluate.load(\"seqeval\")\n\n    def get_labels(predictions, references):\n        # Transform predictions and references tensos to numpy arrays\n        if device.type == \"cpu\":\n            y_pred = predictions.detach().clone().numpy()\n            y_true = references.detach().clone().numpy()\n        else:\n            y_pred = predictions.detach().cpu().clone().numpy()\n            y_true = references.detach().cpu().clone().numpy()\n\n        # Remove ignored index (special tokens)\n        true_predictions = [\n            [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n            for pred, gold_label in zip(y_pred, y_true)\n        ]\n        true_labels = [\n            [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n            for pred, gold_label in zip(y_pred, y_true)\n        ]\n        return true_predictions, true_labels\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            # Unpack nested dictionaries\n            final_results = {}\n            for key, value in results.items():\n                if isinstance(value, dict):\n                    for n, v in value.items():\n                        final_results[f\"{key}_{n}\"] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {\n                \"precision\": results[\"overall_precision\"],\n                \"recall\": results[\"overall_recall\"],\n                \"f1\": results[\"overall_f1\"],\n                \"accuracy\": results[\"overall_accuracy\"],\n            }\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for step, batch in enumerate(active_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            # We keep track of the loss at each epoch\n            if args.with_tracking:\n                total_loss += loss.detach().float()\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0 and accelerator.sync_gradients:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        samples_seen = 0\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            labels = batch[\"labels\"]\n            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n                predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n                labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n            predictions_gathered, labels_gathered = accelerator.gather((predictions, labels))\n            # If we are in a multiprocess environment, the last batch has duplicates\n            if accelerator.num_processes > 1:\n                if step == len(eval_dataloader) - 1:\n                    predictions_gathered = predictions_gathered[: len(eval_dataloader.dataset) - samples_seen]\n                    labels_gathered = labels_gathered[: len(eval_dataloader.dataset) - samples_seen]\n                else:\n                    samples_seen += labels_gathered.shape[0]\n            preds, refs = get_labels(predictions_gathered, labels_gathered)\n            metric.add_batch(\n                predictions=preds,\n                references=refs,\n            )  # predictions and preferences are expected to be a nested list of labels, not label_ids\n\n        eval_metric = compute_metrics()\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"seqeval\": eval_metric,\n                    \"train_loss\": total_loss.item() / len(train_dataloader),\n                    \"epoch\": epoch,\n                    \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                api.upload_folder(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n        if args.checkpointing_steps == \"epoch\":\n            output_dir = f\"epoch_{epoch}\"\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                api.upload_folder(\n                    commit_message=\"End of training\",\n                    folder_path=args.output_dir,\n                    repo_id=repo_id,\n                    repo_type=\"model\",\n                    token=args.hub_token,\n                )\n\n            all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n            if args.with_tracking:\n                all_results.update({\"train_loss\": total_loss.item() / len(train_dataloader)})\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                # Convert all float64 & int64 type numbers to float & int for json serialization\n                for key, value in all_results.items():\n                    if isinstance(value, np.float64):\n                        all_results[key] = float(value)\n                    elif isinstance(value, np.int64):\n                        all_results[key] = int(value)\n                json.dump(all_results, f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[args.local_rank],\n            output_device=args.local_rank,\n            find_unused_parameters=True,\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path):\n        # set global_step to global_step of last saved checkpoint from model path\n        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n        logger.info(\"  Continuing training from global step %d\", global_step)\n        logger.info(\n            \"  Will skip the first %d steps in the first epoch\",\n            steps_trained_in_current_epoch,\n        )\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained,\n        int(args.num_train_epochs),\n        desc=\"Epoch\",\n        disable=args.local_rank not in [-1, 0],\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            inputs[\"token_type_ids\"] = batch[2]\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    logs = {}\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            eval_key = \"eval_{}\".format(key)\n                            logs[eval_key] = value\n\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()[0]\n                    logs[\"learning_rate\"] = learning_rate_scalar\n                    logs[\"loss\"] = loss_scalar\n                    logging_loss = tr_loss\n\n                    for key, value in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{\"step\": global_step}}))\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer, train_highway=False):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    if train_highway:\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if (\"highway\" in n) and (not any(nd in n for nd in no_decay))\n                ],\n                \"weight_decay\": args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p for n, p in model.named_parameters() if (\"highway\" in n) and (any(nd in n for nd in no_decay))\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n    else:\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if (\"highway\" not in n) and (not any(nd in n for nd in no_decay))\n                ],\n                \"weight_decay\": args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if (\"highway\" not in n) and (any(nd in n for nd in no_decay))\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n            inputs[\"train_highway\"] = train_highway\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def finetune(\n    model,\n    train_dataset,\n    test_dataset,\n    context_len=32,\n    max_steps=1000,\n    batch_size=16,\n    threshold=1.0,\n    recopy_model=recopy_gpt2,\n    secondary_learner=None,\n    eval_interval=10,\n    finetuned_model_name=\"openai-community/gpt2_finetuned.pt\",\n):\n    \"\"\"\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\n\n    Args:\n        model: pre-trained GPT-2 model\n        train_dataset: Data set to train GPT-2 model\n        test_dataset: Evaluate GPT-2 model\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\n                    than this will be truncated, sequences shorter will be padded\n        max_steps: To calculate training epochs\n        batch_size: Batch size to train GPT-2 model\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\n                    informative data as input to the model\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\n        secondary_learner: Selection of IGF as fine-tuning method if not None\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\n                        1 standard deviation above average to 1 below average\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\n\n    Returns:\n        Fine-tuned GPT-2 model\n\n    \"\"\"\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n\n    num_train_epochs = max_steps // (len(train_dataset)) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    model, lm_optimizer, lm_scheduler = recopy_model(model, device, max_steps)\n\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n\n    observed_qs = []\n    test_perps = []\n\n    # Compute the performance of the transformer model at the beginning\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print(\"Test perplexity, step\", global_step, \":\", real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for step, example in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start : start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(\n                    torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)\n                )[0].item()\n                observed_qs.append(float(predicted_q))\n\n                # Here we implement the simple non-constant threshold for the predicted IG(X) value\n                # We will decay the selectivity of our secondary learner filter from\n                # 1 standard deviation above average to 1 below average after 10 batches.\n\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n\n            # If we passed the filter, add the context to the batch!\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n\n            del outputs\n\n            # Once the batch is filled with enough contexts, backprop on the batch.\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                # Do LM backprop\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()  # Update learning rate schedule\n                global_step += 1\n                # Compute the performance of the transformer model at this batch\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n\n                    print(\"Test perplexity, step\", global_step, \":\", real_perp)\n            # Break out of the loop after 60 batches\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n\n    # save finetuned transformer model\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    # Do some cleaning up so we can reinitialize for the next run of this function\n    del lm_optimizer\n    del lm_scheduler\n    return model",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer, teacher=None):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 1\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to global_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    # Added here for reproducibility\n    set_seed(args)\n\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            model.train()\n            if teacher is not None:\n                teacher.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"start_positions\": batch[3],\n                \"end_positions\": batch[4],\n            }\n            if args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = None if args.model_type == \"xlm\" else batch[2]\n            if args.model_type in [\"xlnet\", \"xlm\"]:\n                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({\"is_impossible\": batch[7]})\n            outputs = model(**inputs)\n            loss, start_logits_stu, end_logits_stu = outputs\n\n            # Distillation loss\n            if teacher is not None:\n                if \"token_type_ids\" not in inputs:\n                    inputs[\"token_type_ids\"] = None if args.teacher_type == \"xlm\" else batch[2]\n                with torch.no_grad():\n                    start_logits_tea, end_logits_tea = teacher(\n                        input_ids=inputs[\"input_ids\"],\n                        token_type_ids=inputs[\"token_type_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                    )\n                assert start_logits_tea.size() == start_logits_stu.size()\n                assert end_logits_tea.size() == end_logits_stu.size()\n\n                loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n                loss_start = loss_fct(\n                    nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1),\n                    nn.functional.softmax(start_logits_tea / args.temperature, dim=-1),\n                ) * (args.temperature**2)\n                loss_end = loss_fct(\n                    nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1),\n                    nn.functional.softmax(end_logits_tea / args.temperature, dim=-1),\n                ) * (args.temperature**2)\n                loss_ce = (loss_start + loss_end) / 2.0\n\n                loss = args.alpha_ce * loss_ce + args.alpha_squad * loss\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                # Log metrics\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Only evaluate when single GPU otherwise metrics may not average well\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def collect_objective_set(\n    model,\n    orig_perp,\n    context_len,\n    train_data,\n    objective_set,\n    max_steps,\n    device,\n    filename=\"dev.jbl\",\n    recopy_model=recopy_gpt2,\n):\n    \"\"\"\n    Collect individual IGF values from pre-trained transformer model\n    max_steps samples of training data to train secondary model\n\n    Args:\n        model: Pre-trained GPT2 model\n        orig_perp: Perplexity of original pretrained GPT-2 model\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\n                    than this will be truncated, sequences shorter will be padded\n        train_data: Data to train model\n        objective_set: Contexts used to create (X,IG(X)) pairs which is the training data for secondary learner\n        max_steps: To calculate training epochs of model\n        device: GPU/CPU\n        filename: To store intermediate perplexity differences\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\n\n    Returns:\n        file stored intermediate perplexity differences in intermediate stages\n\n    \"\"\"\n\n    # initialize variables to record relevant information\n    contexts = []\n    real_perps = []\n    past_perps = []\n\n    # Initialize the transformer model\n    orig_model = copy.deepcopy(model)\n    orig_model.to(device=\"cpu\")\n    torch.cuda.empty_cache()\n\n    # Compute perplexity of initial transformer model for comparison\n    model.train()\n    model, lm_optimizer, lm_scheduler = recopy_model(orig_model, device, max_steps)\n\n    for step in tqdm(range(max_steps)):\n        context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n        story = random.choice(train_data)\n        start = random.randint(0, len(story[0]) - context_len - 1)\n        context[0, :] = story[0][start : start + context_len]\n        lm_optimizer.zero_grad()\n        outputs = model(context, labels=context)\n        lm_loss = outputs[0]\n        past_perp = compute_perplexity(model, context, context_len)\n        model.train()\n        lm_loss.backward()\n        # Do LM backprop\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n        lm_optimizer.step()\n        lm_scheduler.step()  # Update learning rate schedule\n\n        # Compute perplexity after back-propagating on the selected context\n        real_perp = compute_perplexity(model, objective_set, context_len)\n\n        # Periodically save the stored (X, IG(X)) pairs\n        if step % 1000 == 0 and step > 1:\n            intermittent_save(contexts, real_perps, past_perps, filename)\n\n        # Reset the pretrained model to the original pretrained GPT-2 weights after each iteration\n        model, lm_optimizer, lm_scheduler = recopy_model(orig_model, device, max_steps)\n\n        past_perps.append(past_perp.item())\n        real_perps.append(orig_perp - real_perp.item())\n        contexts.append(np.array(context.cpu()))\n\n    intermittent_save(contexts, real_perps, past_perps, filename)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main():\n    args = parse_args()\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state)\n\n    # Setup logging, we only want one process per machine to log things on the screen.\n    # accelerator.is_local_main_process is only True for one process per machine.\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            # Clone repo locally\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n    # 'tokens' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n            extension = args.train_file.split(\".\")[-1]\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n            extension = args.validation_file.split(\".\")[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    # Trim a number of training examples\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    if raw_datasets[\"train\"] is not None:\n        column_names = raw_datasets[\"train\"].column_names\n        features = raw_datasets[\"train\"].features\n    else:\n        column_names = raw_datasets[\"validation\"].column_names\n        features = raw_datasets[\"validation\"].features\n\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif \"tokens\" in column_names:\n        text_column_name = \"tokens\"\n    else:\n        text_column_name = column_names[0]\n\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f\"{args.task_name}_tags\" in column_names:\n        label_column_name = f\"{args.task_name}_tags\"\n    else:\n        label_column_name = column_names[1]\n\n    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n    # unique labels.\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n        # No need to convert the labels since they are already ints.\n    else:\n        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n    num_labels = len(label_list)\n\n    # Map that sends B-Xxx label to its I-Xxx counterpart\n    b_to_i_label = []\n\n    for idx, label in enumerate(label_list):\n        if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n            b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n        else:\n            b_to_i_label.append(idx)\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    tokenizer = LukeTokenizer.from_pretrained(\n        tokenizer_name_or_path,\n        use_fast=False,\n        task=\"entity_span_classification\",\n        max_entity_length=args.max_entity_length,\n        max_mention_length=args.max_mention_length,\n    )\n\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = LukeForEntitySpanClassification.from_config(config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    padding = \"max_length\" if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n\n        examples[\"sentence_boundaries\"] = sentence_boundaries\n\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n\n        for labels, tokens, sentence_boundaries in zip(\n            examples[label_column_name], examples[text_column_name], examples[\"sentence_boundaries\"]\n        ):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            _, context_end = sentence_boundaries\n\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n\n            text = \"\"\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n\n            for word, label in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += \" \"\n                labels_positions[(word_start_char_positions[-1], word_end_char_positions[-1])] = label\n\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if (\n                        sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length\n                        and len(entity_spans) < tokenizer.max_entity_length\n                    ):\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (\n                            word_start_char_positions[word_start],\n                            word_end_char_positions[word_end],\n                        ) in labels_positions:\n                            labels_entity_spans.append(\n                                labels_positions[\n                                    (word_start_char_positions[word_start], word_end_char_positions[word_end])\n                                ]\n                            )\n                        else:\n                            labels_entity_spans.append(0)\n\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n\n        examples[\"entity_spans\"] = all_entity_spans\n        examples[\"text\"] = texts\n        examples[\"labels_entity_spans\"] = all_labels_entity_spans\n        examples[\"original_entity_spans\"] = all_original_entity_spans\n\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n\n        for v in examples[\"entity_spans\"]:\n            entity_spans.append(list(map(tuple, v)))\n\n        tokenized_inputs = tokenizer(\n            examples[\"text\"],\n            entity_spans=entity_spans,\n            max_length=args.max_length,\n            padding=padding,\n            truncation=True,\n        )\n\n        if padding == \"max_length\":\n            tokenized_inputs[\"labels\"] = padding_tensor(\n                examples[\"labels_entity_spans\"], -100, tokenizer.padding_side, tokenizer.max_entity_length\n            )\n            tokenized_inputs[\"original_entity_spans\"] = padding_tensor(\n                examples[\"original_entity_spans\"], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length\n            )\n            tokenized_inputs[label_column_name] = padding_tensor(\n                examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length\n            )\n        else:\n            tokenized_inputs[\"labels\"] = [ex[: tokenizer.max_entity_length] for ex in examples[\"labels_entity_spans\"]]\n            tokenized_inputs[\"original_entity_spans\"] = [\n                ex[: tokenizer.max_entity_length] for ex in examples[\"original_entity_spans\"]\n            ]\n            tokenized_inputs[label_column_name] = [\n                ex[: tokenizer.max_entity_length] for ex in examples[label_column_name]\n            ]\n\n        return tokenized_inputs\n\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(\n            compute_sentence_boundaries_for_luke,\n            batched=True,\n            desc=\"Adding sentence boundaries\",\n        )\n        raw_datasets = raw_datasets.map(\n            compute_entity_spans_for_luke,\n            batched=True,\n            desc=\"Adding sentence spans\",\n        )\n\n        processed_raw_datasets = raw_datasets.map(\n            tokenize_and_align_labels,\n            batched=True,\n            remove_columns=raw_datasets[\"train\"].column_names,\n            desc=\"Running tokenizer on dataset\",\n        )\n\n    train_dataset = processed_raw_datasets[\"train\"]\n    eval_dataset = processed_raw_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n\n    # DataLoaders creation:\n    if args.pad_to_max_length:\n        # If padding was already done ot max length, we use the default data collator that will just convert everything\n        # to tensors.\n        data_collator = default_data_collator\n    else:\n        # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n        # For fp8, we pad to multiple of 16.\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    # Use the device given by the `accelerator` object.\n    device = accelerator.device\n    model.to(device)\n\n    # Prepare everything with our `accelerator`.\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n    # shorter in multiprocess)\n\n    # Scheduler and math around the number of training steps.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    # Metrics\n    metric = load_metric(\"seqeval\")\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n\n        for output, original_spans, tags in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n\n            for logit, index, span in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n\n            predicted_sequence = [label_list[0]] * len(true_tags)\n\n            for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all(o == label_list[0] for o in predicted_sequence[span[0] : span[1]]):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1 : span[1]] = [label] * (span[1] - span[0] - 1)\n\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n\n        return true_predictions, true_labels\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            # Unpack nested dictionaries\n            final_results = {}\n            for key, value in results.items():\n                if isinstance(value, dict):\n                    for n, v in value.items():\n                        final_results[f\"{key}_{n}\"] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {\n                \"precision\": results[\"overall_precision\"],\n                \"recall\": results[\"overall_recall\"],\n                \"f1\": results[\"overall_f1\"],\n                \"accuracy\": results[\"overall_accuracy\"],\n            }\n\n    # Train!\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            _ = batch.pop(\"original_entity_spans\")\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop(\"original_entity_spans\")\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            preds, refs = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n\n            metric.add_batch(\n                predictions=preds,\n                references=refs,\n            )  # predictions and preferences are expected to be a nested list of labels, not label_ids\n\n        eval_metric = compute_metrics()\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(\n                    commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n                )\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n    model.train()\n    # make iterator\n    if curriculum:\n        train_sampler = SequentialSampler(dataset)\n    else:\n        train_sampler = RandomSampler(dataset)\n    model_collate_fn = functools.partial(\n        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n    )\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    for step, batch_inputs in enumerate(epoch_iterator):\n        pre_loss = model(**batch_inputs)[0]\n        loss = pre_loss.sum() / pre_loss.shape[0]\n        loss.backward()\n        # optimizer\n        if step % args.backward_freq == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        # some printing within the epoch\n        loc_loss += loss.item()\n        loc_steps += 1\n        if step % args.print_freq == 0 or step == 1:\n            print(\n                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                    e,\n                    step,\n                    len(dataset) // args.batch_size,\n                    loc_loss / loc_steps,\n                    time() - st_time,\n                )\n            )\n            loc_loss = 0\n            loc_steps = 0",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_qa_retriever_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n    model.train()\n    # make iterator\n    train_sampler = RandomSampler(dataset)\n    model_collate_fn = functools.partial(\n        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n    )\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    for step, batch in enumerate(epoch_iterator):\n        q_ids, q_mask, a_ids, a_mask = batch\n        pre_loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n        loss = pre_loss.sum()\n        # optimizer\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        model.zero_grad()\n        # some printing within the epoch\n        loc_loss += loss.item()\n        loc_steps += 1\n        if step % args.print_freq == 0 or step == 1:\n            print(\n                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                    e,\n                    step,\n                    len(dataset) // args.batch_size,\n                    loc_loss / loc_steps,\n                    time() - st_time,\n                )\n            )\n            loc_loss = 0\n            loc_steps = 0",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_qa_retriever_joint_epoch(model, dataset_list, tokenizer, optimizer, scheduler, args, e=0):\n    model.train()\n    model_collate_fn = functools.partial(\n        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n    )\n    # make iterator\n    train_samplers = [RandomSampler(dataset) for dataset in dataset_list]\n    data_loaders = [\n        DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n        for dataset, train_sampler in zip(dataset_list, train_samplers)\n    ]\n    iterators = [iter(dloader) for dloader in data_loaders]\n    joint_iter = zip(*iterators)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    for step, (batches,) in enumerate(zip(joint_iter)):\n        for batch in batches:\n            q_ids, q_mask, a_ids, a_mask = batch\n            loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n            # optimizer\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n            # some printing within the epoch\n            loc_loss += loss.item()\n            loc_steps += 1\n        if step % args.print_freq == 0:\n            print(\n                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                    e,\n                    step,\n                    len(dataset_list[0]) // args.batch_size,\n                    loc_loss / loc_steps,\n                    time() - st_time,\n                )\n            )\n            loc_loss = 0\n            loc_steps = 0",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer, teacher=None):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if \"mask_score\" in n and p.requires_grad],\n            \"lr\": args.mask_scores_learning_rate,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if \"mask_score\" not in n and p.requires_grad and not any(nd in n for nd in no_decay)\n            ],\n            \"lr\": args.learning_rate,\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if \"mask_score\" not in n and p.requires_grad and any(nd in n for nd in no_decay)\n            ],\n            \"lr\": args.learning_rate,\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[args.local_rank],\n            output_device=args.local_rank,\n            find_unused_parameters=True,\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n    # Distillation\n    if teacher is not None:\n        logger.info(\"  Training with distillation\")\n\n    global_step = 1\n    # Global TopK\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path):\n        # set global_step to global_step of last saved checkpoint from model path\n        try:\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    # Added here for reproducibility\n    set_seed(args)\n\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            threshold, regu_lambda = schedule_threshold(\n                step=global_step,\n                total_step=t_total,\n                warmup_steps=args.warmup_steps,\n                final_threshold=args.final_threshold,\n                initial_threshold=args.initial_threshold,\n                final_warmup=args.final_warmup,\n                initial_warmup=args.initial_warmup,\n                final_lambda=args.final_lambda,\n            )\n            # Global TopK\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -1e2  # Or an indefinitely low quantity\n                else:\n                    if (threshold_mem is None) or (global_step % args.global_topk_frequency_compute == 0):\n                        # Sort all the values to get the global topK\n                        concat = torch.cat(\n                            [param.view(-1) for name, param in model.named_parameters() if \"mask_scores\" in name]\n                        )\n                        n = concat.numel()\n                        kth = max(n - (int(n * threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                        threshold = threshold_mem\n                    else:\n                        threshold = threshold_mem\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n                \"start_positions\": batch[3],\n                \"end_positions\": batch[4],\n            }\n\n            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n                del inputs[\"token_type_ids\"]\n\n            if args.model_type in [\"xlnet\", \"xlm\"]:\n                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({\"is_impossible\": batch[7]})\n                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n                    inputs.update(\n                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n                    )\n\n            if \"masked\" in args.model_type:\n                inputs[\"threshold\"] = threshold\n\n            outputs = model(**inputs)\n            # model outputs are always tuple in transformers (see doc)\n            loss, start_logits_stu, end_logits_stu = outputs\n\n            # Distillation loss\n            if teacher is not None:\n                with torch.no_grad():\n                    start_logits_tea, end_logits_tea = teacher(\n                        input_ids=inputs[\"input_ids\"],\n                        token_type_ids=inputs[\"token_type_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                    )\n\n                loss_start = nn.functional.kl_div(\n                    input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1),\n                    target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1),\n                    reduction=\"batchmean\",\n                ) * (args.temperature**2)\n                loss_end = nn.functional.kl_div(\n                    input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1),\n                    target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1),\n                    reduction=\"batchmean\",\n                ) * (args.temperature**2)\n                loss_logits = (loss_start + loss_end) / 2.0\n\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n\n            # Regularization\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    tb_writer.add_scalar(\"threshold\", threshold, global_step)\n                    for name, param in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar(\"parameter_mean/\" + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar(\"parameter_std/\" + name, param.data.std(), global_step)\n                        tb_writer.add_scalar(\"parameter_min/\" + name, param.data.min(), global_step)\n                        tb_writer.add_scalar(\"parameter_max/\" + name, param.data.max(), global_step)\n                        if \"pooler\" in name:\n                            continue\n                        tb_writer.add_scalar(\"grad_mean/\" + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar(\"grad_std/\" + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and \"mask_scores\" in name:\n                            if args.regularization == \"l1\":\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == \"l0\":\n                                perc = (torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1))).sum().item() / param.numel()\n                            tb_writer.add_scalar(\"retained_weights_perc/\" + name, perc, global_step)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                # Log metrics\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Only evaluate when single GPU otherwise metrics may not average well\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar(\"lr\", learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for idx, lr in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f\"lr/{idx+1}\", lr, global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar(\"loss/distil\", loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar(\"loss/regularization\", regu_.item(), global_step)\n                    if (teacher is not None) or (args.regularization is not None):\n                        if (teacher is not None) and (args.regularization is not None):\n                            tb_writer.add_scalar(\n                                \"loss/instant_ce\",\n                                (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item())\n                                / args.alpha_ce,\n                                global_step,\n                            )\n                        elif teacher is not None:\n                            tb_writer.add_scalar(\n                                \"loss/instant_ce\",\n                                (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce,\n                                global_step,\n                            )\n                        else:\n                            tb_writer.add_scalar(\n                                \"loss/instant_ce\", loss.item() - regu_lambda * regu_.item(), global_step\n                            )\n                    logging_loss = tr_loss\n\n                # Save model checkpoint\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    # Take care of distributed/parallel training\n                    model_to_save = model.module if hasattr(model, \"module\") else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer, criterion):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=args.train_batch_size,\n        collate_fn=collate_fn,\n        num_workers=args.num_workers,\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    best_f1, n_no_improve = 0, 0\n    model.zero_grad()\n    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            labels = batch[5]\n            inputs = {\n                \"input_ids\": batch[0],\n                \"input_modal\": batch[2],\n                \"attention_mask\": batch[1],\n                \"modal_start_tokens\": batch[3],\n                \"modal_end_tokens\": batch[4],\n            }\n            outputs = model(**inputs)\n            logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n            loss = criterion(logits, labels)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    logs = {}\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer, criterion)\n                        for key, value in results.items():\n                            eval_key = \"eval_{}\".format(key)\n                            logs[eval_key] = value\n\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()[0]\n                    logs[\"learning_rate\"] = learning_rate_scalar\n                    logs[\"loss\"] = loss_scalar\n                    logging_loss = tr_loss\n\n                    for key, value in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{\"step\": global_step}}))\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    torch.save(model_to_save.state_dict(), os.path.join(output_dir, WEIGHTS_NAME))\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n        if args.local_rank == -1:\n            results = evaluate(args, model, tokenizer, criterion)\n            if results[\"micro_f1\"] > best_f1:\n                best_f1 = results[\"micro_f1\"]\n                n_no_improve = 0\n            else:\n                n_no_improve += 1\n\n            if n_no_improve > args.patience:\n                train_iterator.close()\n                break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(args, train_dataset, model, tokenizer, teacher=None):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if \"mask_score\" in n and p.requires_grad],\n            \"lr\": args.mask_scores_learning_rate,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if \"mask_score\" not in n and p.requires_grad and not any(nd in n for nd in no_decay)\n            ],\n            \"lr\": args.learning_rate,\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if \"mask_score\" not in n and p.requires_grad and any(nd in n for nd in no_decay)\n            ],\n            \"lr\": args.learning_rate,\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[args.local_rank],\n            output_device=args.local_rank,\n            find_unused_parameters=True,\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n    # Distillation\n    if teacher is not None:\n        logger.info(\"  Training with distillation\")\n\n    global_step = 0\n    # Global TopK\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path):\n        # set global_step to global_step of last saved checkpoint from model path\n        try:\n            global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n        logger.info(\"  Continuing training from global step %d\", global_step)\n        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained,\n        int(args.num_train_epochs),\n        desc=\"Epoch\",\n        disable=args.local_rank not in [-1, 0],\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            threshold, regu_lambda = schedule_threshold(\n                step=global_step,\n                total_step=t_total,\n                warmup_steps=args.warmup_steps,\n                final_threshold=args.final_threshold,\n                initial_threshold=args.initial_threshold,\n                final_warmup=args.final_warmup,\n                initial_warmup=args.initial_warmup,\n                final_lambda=args.final_lambda,\n            )\n            # Global TopK\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -1e2  # Or an indefinitely low quantity\n                else:\n                    if (threshold_mem is None) or (global_step % args.global_topk_frequency_compute == 0):\n                        # Sort all the values to get the global topK\n                        concat = torch.cat(\n                            [param.view(-1) for name, param in model.named_parameters() if \"mask_scores\" in name]\n                        )\n                        n = concat.numel()\n                        kth = max(n - (int(n * threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                        threshold = threshold_mem\n                    else:\n                        threshold = threshold_mem\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if args.model_type in [\"bert\", \"masked_bert\", \"xlnet\", \"albert\"] else None\n                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n\n            if \"masked\" in args.model_type:\n                inputs[\"threshold\"] = threshold\n\n            outputs = model(**inputs)\n            loss, logits_stu = outputs  # model outputs are always tuple in transformers (see doc)\n\n            # Distillation loss\n            if teacher is not None:\n                if \"token_type_ids\" not in inputs:\n                    inputs[\"token_type_ids\"] = None if args.teacher_type == \"xlm\" else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(\n                        input_ids=inputs[\"input_ids\"],\n                        token_type_ids=inputs[\"token_type_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                    )\n\n                loss_logits = nn.functional.kl_div(\n                    input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1),\n                    target=nn.functional.softmax(logits_tea / args.temperature, dim=-1),\n                    reduction=\"batchmean\",\n                ) * (args.temperature**2)\n\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n\n            # Regularization\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                # last step in epoch but step is always smaller than gradient_accumulation_steps\n                len(epoch_iterator) <= args.gradient_accumulation_steps and (step + 1) == len(epoch_iterator)\n            ):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    tb_writer.add_scalar(\"threshold\", threshold, global_step)\n                    for name, param in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar(\"parameter_mean/\" + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar(\"parameter_std/\" + name, param.data.std(), global_step)\n                        tb_writer.add_scalar(\"parameter_min/\" + name, param.data.min(), global_step)\n                        tb_writer.add_scalar(\"parameter_max/\" + name, param.data.max(), global_step)\n                        tb_writer.add_scalar(\"grad_mean/\" + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar(\"grad_std/\" + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and \"mask_scores\" in name:\n                            if args.regularization == \"l1\":\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == \"l0\":\n                                perc = (torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1))).sum().item() / param.numel()\n                            tb_writer.add_scalar(\"retained_weights_perc/\" + name, perc, global_step)\n\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    logs = {}\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            eval_key = \"eval_{}\".format(key)\n                            logs[eval_key] = value\n\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs[\"learning_rate\"] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for idx, lr in enumerate(learning_rate_scalar[1:]):\n                            logs[f\"learning_rate/{idx+1}\"] = lr\n                    logs[\"loss\"] = loss_scalar\n                    if teacher is not None:\n                        logs[\"loss/distil\"] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs[\"loss/regularization\"] = regu_.item()\n                    if (teacher is not None) or (args.regularization is not None):\n                        if (teacher is not None) and (args.regularization is not None):\n                            logs[\"loss/instant_ce\"] = (\n                                loss.item()\n                                - regu_lambda * logs[\"loss/regularization\"]\n                                - args.alpha_distil * logs[\"loss/distil\"]\n                            ) / args.alpha_ce\n                        elif teacher is not None:\n                            logs[\"loss/instant_ce\"] = (\n                                loss.item() - args.alpha_distil * logs[\"loss/distil\"]\n                            ) / args.alpha_ce\n                        else:\n                            logs[\"loss/instant_ce\"] = loss.item() - regu_lambda * logs[\"loss/regularization\"]\n                    logging_loss = tr_loss\n\n                    for key, value in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{\"step\": global_step}}))\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def perturb_past(\n    past,\n    model,\n    last,\n    unpert_past=None,\n    unpert_logits=None,\n    accumulated_hidden=None,\n    grad_norms=None,\n    stepsize=0.01,\n    one_hot_bows_vectors=None,\n    classifier=None,\n    class_label=None,\n    loss_type=0,\n    num_iterations=3,\n    horizon_length=1,\n    window_length=0,\n    decay=False,\n    gamma=1.5,\n    kl_scale=0.01,\n    device=\"cuda\",\n):\n    # Generate inital perturbed past\n    grad_accumulator = [(np.zeros(p.shape).astype(\"float32\")) for p in past]\n\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / (window_length))[1:]\n    else:\n        decay_mask = 1.0\n\n    # TODO fix this comment (SUMANTH)\n    # Generate a mask is gradient perturbated is based on a past window\n    _, _, _, curr_length, _ = past[0].shape\n\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n\n    # accumulate perturbations for num_iterations\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print(\"Iteration \", i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        # make sure p_.grad is not None\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n\n        # Compute hidden using perturbed past\n        perturbed_past = list(map(add, past, curr_perturbation))\n        _, _, _, curr_length, _ = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        all_logits, all_hidden = lm_output[\"logits\"], lm_output[\"hidden_states\"]\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                curr_all_logits, curr_unpert_past, curr_all_hidden = (\n                    lm_output[\"logits\"],\n                    lm_output[\"past_key_values\"],\n                    lm_output[\"hidden_states\"],\n                )\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * ((corrected_probs * (corrected_probs / unpert_probs).log()).sum())\n            print(\" kl_loss\", kl_loss.data.cpu().numpy())\n            loss += kl_loss\n\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(\" pplm_loss\", (loss - kl_loss).data.cpu().numpy())\n\n        # compute gradients\n        loss.backward()\n\n        # calculate gradient norms\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [\n                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n                for index, p_ in enumerate(curr_perturbation)\n            ]\n        else:\n            grad_norms = [\n                (torch.norm(p_.grad * window_mask) + SMALL_CONST) for index, p_ in enumerate(curr_perturbation)\n            ]\n\n        # normalize gradients\n        grad = [\n            -stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy()\n            for index, p_ in enumerate(curr_perturbation)\n        ]\n\n        # accumulate gradient\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n\n        # reset gradients, just to make sure\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n\n        # removing past from the graph\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n\n    # apply the accumulated perturbations to the past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n\n    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def evaluate(args, accelerator, dataloader, eval_set, model, checkpoint, has_labels=True, write_to_file=True):\n    \"\"\"Evaluate a model checkpoint on the given evaluation data.\"\"\"\n\n    num_examples = args.num_examples[eval_set]\n    eval_metric = None\n    completed_steps = 0\n    eval_loss = 0.0\n    all_predictions = None\n    all_references = None\n    all_probabilities = None\n\n    if has_labels:\n        # Get the metric function\n        eval_metric = load_metric(args.eval_metric)\n\n    eval_results = {}\n    model.eval()\n    for _, batch in enumerate(dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        eval_loss += outputs.loss.item()\n        logits = outputs.logits\n        predictions = logits.argmax(dim=-1) if not args.is_regression else logits.squeeze()\n        predictions = accelerator.gather(predictions)\n\n        if all_predictions is None:\n            all_predictions = predictions.detach().cpu().numpy()\n        else:\n            all_predictions = np.append(all_predictions, predictions.detach().cpu().numpy(), axis=0)\n\n        if not args.is_regression:\n            probabilities = logits.softmax(dim=-1).max(dim=-1).values\n            probabilities = accelerator.gather(probabilities)\n            if all_probabilities is None:\n                all_probabilities = probabilities.detach().cpu().numpy()\n            else:\n                all_probabilities = np.append(all_probabilities, probabilities.detach().cpu().numpy(), axis=0)\n\n        if has_labels:\n            references = batch[\"labels\"]\n            references = accelerator.gather(references)\n            if all_references is None:\n                all_references = references.detach().cpu().numpy()\n            else:\n                all_references = np.append(all_references, references.detach().cpu().numpy(), axis=0)\n\n            eval_metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n        completed_steps += 1\n\n    if has_labels:\n        eval_results.update(eval_metric.compute())\n        eval_results[\"completed_steps\"] = completed_steps\n        eval_results[\"avg_eval_loss\"] = eval_loss / completed_steps\n\n        if write_to_file:\n            accelerator.wait_for_everyone()\n            if accelerator.is_main_process:\n                results_file = os.path.join(args.output_dir, f\"{eval_set}_results_{checkpoint}.json\")\n                with open(results_file, \"w\") as f:\n                    json.dump(eval_results, f, indent=4, sort_keys=True)\n\n    if write_to_file:\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            output_file = os.path.join(args.output_dir, f\"{eval_set}_output_{checkpoint}.csv\")\n            if not args.is_regression:\n                assert len(all_predictions) == len(all_probabilities)\n                df = pd.DataFrame(list(zip(all_predictions, all_probabilities)), columns=[\"prediction\", \"probability\"])\n            else:\n                df = pd.DataFrame(all_predictions, columns=[\"prediction\"])\n            df = df.head(num_examples)\n            df.to_csv(output_file, header=True, index=False)\n    return eval_results",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def on_train_end(self, args, state, control, model=None, processing_class=None, **kwargs):\n        if self._wandb is None:\n            return\n        if self._log_model.is_enabled and self._initialized and state.is_world_process_zero:\n            from ..trainer import Trainer\n\n            fake_trainer = Trainer(args=args, model=model, processing_class=processing_class, eval_dataset=[\"fake\"])\n            with tempfile.TemporaryDirectory() as temp_dir:\n                fake_trainer.save_model(temp_dir)\n                metadata = (\n                    {\n                        k: v\n                        for k, v in dict(self._wandb.summary).items()\n                        if isinstance(v, numbers.Number) and not k.startswith(\"_\")\n                    }\n                    if not args.load_best_model_at_end\n                    else {\n                        f\"eval/{args.metric_for_best_model}\": state.best_metric,\n                        \"train/total_floss\": state.total_flos,\n                        \"model/num_parameters\": self._wandb.config.get(\"model/num_parameters\"),\n                    }\n                )\n                metadata[\"final_model\"] = True\n                logger.info(\"Logging model artifacts. ...\")\n                model_name = (\n                    f\"model-{self._wandb.run.id}\"\n                    if (args.run_name is None or args.run_name == args.output_dir)\n                    else f\"model-{self._wandb.run.name}\"\n                )\n                # add the model architecture to a separate text file\n                save_model_architecture_to_file(model, temp_dir)\n\n                artifact = self._wandb.Artifact(name=model_name, type=\"model\", metadata=metadata)\n                for f in Path(temp_dir).glob(\"*\"):\n                    if f.is_file():\n                        with artifact.new_file(f.name, mode=\"wb\") as fa:\n                            fa.write(f.read_bytes())\n                self._wandb.run.log_artifact(artifact, aliases=[\"final_model\"])",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def on_train_end(self, args, state, control, **kwargs):\n        if self.log_artifacts:\n            if getattr(self, \"train_dataloader\", None):\n                torch.save(self.train_dataloader.dataset, os.path.join(args.output_dir, \"dataset.pt\"))\n\n            self.repo.directory(str(self.path)).add_dir(args.output_dir)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def forward(\n        self,\n        inputs_embeds: torch.Tensor = None,\n        multi_stage_positional_embeddings: torch.Tensor = None,\n        pixel_embeddings: torch.Tensor = None,\n        encoder_hidden_states: torch.Tensor = None,\n        query_position_embeddings: torch.Tensor = None,\n        feature_size_list: List = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(num_queries, batch_size, hidden_size)`):\n                The query embeddings that are passed into the decoder.\n            multi_stage_positional_embeddings (`torch.FloatTensor` of shape `(height*width, batch_size, num_channels)`):\n                Position embeddings that are added to the keys in each cross(masked)-attention layer.\n            pixel_embeddings (`torch.FloatTensor`):\n                Tensor of shape `(batch_size, num_channels, height, width)`, 1/4 scale features from the last Pixel\n                Decoder.\n            query_position_embeddings (`torch.FloatTensor` of shape `(num_queries, batch_size, hidden_size)`):\n                , *optional*): Position embeddings that are added to the queries and keys in each self-attention layer.\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the\n                cross(masked)-attention of the decoder.\n            feature_size_list (`List[torch.Size]`):\n                This is a list containing shapes (height & width) of multi-scale features from the Pixel Decoder.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if inputs_embeds is not None:\n            hidden_states = inputs_embeds\n\n        # intermediate hidden states with layernorm applied - required for predicting class logits\n        intermediate = ()\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        attentions = () if output_attentions else None\n\n        # intermediate mask predictions from transformer decoder layers\n        intermediate_mask_predictions = ()\n\n        intermediate_hidden_states = self.layernorm(inputs_embeds)\n        intermediate += (intermediate_hidden_states,)\n\n        predicted_mask, attention_mask = self.mask_predictor(\n            intermediate_hidden_states, pixel_embeddings, feature_size_list[0]\n        )\n        intermediate_mask_predictions += (predicted_mask,)\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            dropout_probability = torch.rand([])\n\n            if self.training and (dropout_probability < self.layerdrop):\n                continue\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,\n                    encoder_hidden_states,\n                    None,\n                    None,\n                    output_attentions,\n                )\n\n            else:\n                level_index = idx % self.num_feature_levels\n\n                where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)\n                # Multiply the attention mask instead of indexing to avoid issue in torch.export.\n                attention_mask = attention_mask * where.unsqueeze(-1)\n\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    level_index=level_index,\n                    position_embeddings=multi_stage_positional_embeddings,\n                    query_position_embeddings=query_position_embeddings,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=attention_mask,\n                    output_attentions=output_attentions,\n                )\n\n                intermediate_hidden_states = self.layernorm(layer_outputs[0])\n\n                predicted_mask, attention_mask = self.mask_predictor(\n                    intermediate_hidden_states,\n                    pixel_embeddings,\n                    feature_size_list[(idx + 1) % self.num_feature_levels],\n                )\n\n                intermediate_mask_predictions += (predicted_mask,)\n\n                # add intermediate hidden states with layer norm applied which will be used for predicting class logits\n                intermediate += (intermediate_hidden_states,)\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                attentions += (layer_outputs[1],)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        hidden_states = hidden_states.transpose(1, 0)\n        if not return_dict:\n            outputs = [hidden_states, all_hidden_states, attentions, intermediate, intermediate_mask_predictions]\n            return tuple(v for v in outputs if v is not None)\n\n        return Mask2FormerMaskedAttentionDecoderOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=attentions,\n            intermediate_hidden_states=intermediate,\n            masks_queries_logits=intermediate_mask_predictions,\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(\n        self,\n        inputs_embeds=None,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        object_queries=None,\n        query_position_embeddings=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                The query embeddings that are passed into the decoder.\n\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on certain queries. Mask values selected in `[0, 1]`:\n\n                - 1 for queries that are **not masked**,\n                - 0 for queries that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                of the decoder.\n            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n                in `[0, 1]`:\n\n                - 1 for pixels that are real (i.e. **not masked**),\n                - 0 for pixels that are padding (i.e. **masked**).\n\n            object_queries (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Position embeddings that are added to the queries and keys in each cross-attention layer.\n            query_position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n                , *optional*): Position embeddings that are added to the queries and keys in each self-attention layer.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if inputs_embeds is not None:\n            hidden_states = inputs_embeds\n            input_shape = inputs_embeds.size()[:-1]\n\n        # expand encoder attention mask\n        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            encoder_attention_mask = _prepare_4d_attention_mask(\n                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n            )\n\n        # optional intermediate hidden states\n        intermediate = () if self.config.auxiliary_loss else None\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            if self.training:\n                dropout_probability = torch.rand([])\n                if dropout_probability < self.layerdrop:\n                    continue\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    None,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    None,\n                    output_attentions,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=None,\n                    object_queries=object_queries,\n                    query_position_embeddings=query_position_embeddings,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if self.config.auxiliary_loss:\n                hidden_states = self.layernorm(hidden_states)\n                intermediate += (hidden_states,)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # finally, apply layernorm\n        hidden_states = self.layernorm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        # stack intermediate decoder activations\n        if self.config.auxiliary_loss:\n            intermediate = torch.stack(intermediate)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, all_hidden_states, all_self_attns, all_cross_attentions, intermediate]\n                if v is not None\n            )\n        return DetrDecoderOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n            intermediate_hidden_states=intermediate,\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, config: Mask2FormerConfig, feature_channels):\n        super().__init__()\n\n        self.config = config\n\n        feature_dim = config.feature_size\n        mask_dim = config.mask_feature_size\n        num_pos_features = feature_dim // 2\n\n        self.position_embedding = Mask2FormerSinePositionEmbedding(num_pos_feats=num_pos_features, normalize=True)\n        self.num_feature_levels = 3\n        transformer_in_channels = feature_channels[-self.num_feature_levels :]\n\n        self.transformer_feature_strides = config.feature_strides[-self.num_feature_levels :]\n        self.feature_channels = feature_channels\n        self.level_embed = nn.Parameter(torch.Tensor(self.num_feature_levels, feature_dim))\n\n        # Create input projection layers\n        if self.num_feature_levels > 1:\n            input_projections_list = []\n            for in_channels in transformer_in_channels[::-1]:\n                input_projections_list.append(\n                    nn.Sequential(\n                        nn.Conv2d(in_channels, feature_dim, kernel_size=1),\n                        nn.GroupNorm(32, feature_dim),\n                    )\n                )\n            self.input_projections = nn.ModuleList(input_projections_list)\n        else:\n            self.input_projections = nn.ModuleList(\n                [\n                    nn.Sequential(\n                        nn.Conv2d(transformer_in_channels[-1], feature_dim, kernel_size=1),\n                        nn.GroupNorm(32, feature_dim),\n                    )\n                ]\n            )\n\n        self.encoder = Mask2FormerPixelDecoderEncoderOnly(config)\n        self.mask_projection = nn.Conv2d(feature_dim, mask_dim, kernel_size=1, stride=1, padding=0)\n\n        # Extra FPN levels\n        stride = min(self.transformer_feature_strides)\n        self.common_stride = config.common_stride\n        self.num_fpn_levels = int(np.log2(stride) - np.log2(self.common_stride))\n\n        lateral_convs = []\n        output_convs = []\n\n        for idx, in_channels in enumerate(self.feature_channels[: self.num_fpn_levels]):\n            lateral_conv = nn.Sequential(\n                nn.Conv2d(in_channels, feature_dim, kernel_size=1, bias=False),\n                nn.GroupNorm(32, feature_dim),\n            )\n\n            output_conv = nn.Sequential(\n                nn.Conv2d(feature_dim, feature_dim, kernel_size=3, stride=1, padding=1, bias=False),\n                nn.GroupNorm(32, feature_dim),\n                nn.ReLU(),\n            )\n            self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n            self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n\n            lateral_convs.append(lateral_conv)\n            output_convs.append(output_conv)\n\n        # Order convolutional layers from low to high resolution\n        self.lateral_convolutions = lateral_convs[::-1]\n        self.output_convolutions = output_convs[::-1]",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def convert_musicgen_melody_checkpoint(\n    checkpoint, pytorch_dump_folder=None, repo_id=None, device=\"cpu\", test_same_output=False\n):\n    fairseq_model = MusicGen.get_pretrained(checkpoint, device=args.device)\n    decoder_config = decoder_config_from_checkpoint(checkpoint)\n\n    decoder_state_dict = fairseq_model.lm.state_dict()\n    decoder_state_dict, enc_dec_proj_state_dict, audio_enc_to_dec_proj_state_dict = rename_state_dict(\n        decoder_state_dict, hidden_size=decoder_config.hidden_size\n    )\n\n    text_encoder = T5EncoderModel.from_pretrained(\"t5-base\")\n    audio_encoder = EncodecModel.from_pretrained(\"facebook/encodec_32khz\")\n    decoder = MusicgenMelodyForCausalLM(decoder_config).eval()\n\n    # load all decoder weights - expect that we'll be missing embeddings and enc-dec projection\n    missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state_dict, strict=False)\n\n    for key in missing_keys.copy():\n        if key.startswith((\"text_encoder\", \"audio_encoder\")) or key in EXPECTED_MISSING_KEYS:\n            missing_keys.remove(key)\n\n    for key in unexpected_keys.copy():\n        if key in EXPECTED_ADDITIONAL_KEYS:\n            unexpected_keys.remove(key)\n\n    if len(missing_keys) > 0:\n        raise ValueError(f\"Missing key(s) in state_dict: {missing_keys}\")\n\n    if len(unexpected_keys) > 0:\n        raise ValueError(f\"Unexpected key(s) in state_dict: {unexpected_keys}\")\n\n    # init the composite model\n    model = MusicgenMelodyForConditionalGeneration(\n        text_encoder=text_encoder, audio_encoder=audio_encoder, decoder=decoder\n    ).to(args.device)\n\n    # load the pre-trained enc-dec projection (from the decoder state dict)\n    model.enc_to_dec_proj.load_state_dict(enc_dec_proj_state_dict)\n\n    # load the pre-trained audio encoder projection (from the decoder state dict)\n    model.audio_enc_to_dec_proj.load_state_dict(audio_enc_to_dec_proj_state_dict)\n\n    # check we can do a forward pass\n    input_ids = torch.arange(0, 2 * decoder_config.num_codebooks, dtype=torch.long).reshape(2, -1).to(device)\n    decoder_input_ids = input_ids.reshape(2 * decoder_config.num_codebooks, -1).to(device)\n\n    with torch.no_grad():\n        logits = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n\n    output_length = 1 + input_ids.shape[1] + model.config.chroma_length\n    if logits.shape != (2 * decoder_config.num_codebooks, output_length, 2048):\n        raise ValueError(\"Incorrect shape for logits\")\n\n    # now construct the processor\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n    feature_extractor = MusicgenMelodyFeatureExtractor()\n\n    processor = MusicgenMelodyProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n\n    # set the appropriate bos/pad token ids\n    model.generation_config.decoder_start_token_id = 2048\n    model.generation_config.pad_token_id = 2048\n\n    # set other default generation config params\n    model.generation_config.max_length = int(30 * audio_encoder.config.frame_rate)\n    model.generation_config.do_sample = True\n    model.generation_config.guidance_scale = 3.0\n\n    if test_same_output:\n        # check same output than original model\n        decoder_input_ids = torch.ones_like(decoder_input_ids).to(device) * model.generation_config.pad_token_id\n        with torch.no_grad():\n            decoder_input_ids = decoder_input_ids[: decoder_config.num_codebooks]\n            inputs = processor(text=[\"gen\"], return_tensors=\"pt\", padding=True).to(device)\n            logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits\n\n            attributes, prompt_tokens = fairseq_model._prepare_tokens_and_attributes([\"gen\"], None)\n            original_logits = fairseq_model.lm.forward(\n                decoder_input_ids.reshape(1, decoder_config.num_codebooks, -1), attributes\n            )\n\n            torch.testing.assert_close(\n                original_logits.squeeze(2).reshape(decoder_config.num_codebooks, -1),\n                logits[:, -1],\n                rtol=1e-5,\n                atol=5e-5,\n            )\n\n    if pytorch_dump_folder is not None:\n        Path(pytorch_dump_folder).mkdir(exist_ok=True)\n        logger.info(f\"Saving model {checkpoint} to {pytorch_dump_folder}\")\n        model.save_pretrained(pytorch_dump_folder)\n        processor.save_pretrained(pytorch_dump_folder)\n\n    if repo_id:\n        logger.info(f\"Pushing model {checkpoint} to {repo_id}\")\n        model.push_to_hub(repo_id, create_pr=True)\n        processor.push_to_hub(repo_id, create_pr=True)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(self, hidden_states: torch.Tensor):\n        for layer in self.layers:\n            if self.use_checkpoint:\n                hidden_states = self._gradient_checkpointing_func(layer, hidden_states)\n            else:\n                hidden_states = layer(hidden_states)\n        return hidden_states",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(\n        self,\n        task_token=None,\n        multi_stage_features=None,\n        multi_stage_positional_embeddings=None,\n        mask_features=None,\n        query_features=None,\n        query_embeddings=None,\n        query_embedder=None,\n        size_list=None,\n        output_attentions=None,\n    ):\n        if self.use_task_norm:\n            task_token = self.decoder_norm(task_token)\n\n        object_queries = self.query_transformer(\n            query_features,\n            None,\n            query_embedder.weight[:-1],\n            self.query_input_projection(mask_features),\n            task_token if self.use_task_norm else None,\n        )\n\n        object_queries = object_queries[0].permute(1, 0, 2)\n\n        queries = torch.cat([object_queries, task_token], dim=0)\n\n        output = queries.clone()\n\n        intermediate_class_predictions = []\n        intermediate_mask_predictions = []\n\n        # prediction heads on learnable query features\n        outputs_class, outputs_mask, attention_mask = self.forward_prediction_heads(\n            output, mask_features, attention_mask_target_size=size_list[0]\n        )\n        intermediate_class_predictions.append(outputs_class)\n        intermediate_mask_predictions.append(outputs_mask)\n\n        attentions = ()\n\n        for index, layer in enumerate(self.layers):\n            layer_outputs = layer(\n                index=index,\n                output=output,\n                multi_stage_features=multi_stage_features,\n                multi_stage_positional_embeddings=multi_stage_positional_embeddings,\n                attention_mask=attention_mask,\n                query_embeddings=query_embeddings,\n                output_attentions=output_attentions,\n            )\n\n            output = layer_outputs[0]\n            attentions += (layer_outputs[1:],)\n\n            outputs_class, outputs_mask, attention_mask = self.forward_prediction_heads(\n                output, mask_features, attention_mask_target_size=size_list[(index + 1) % self.num_feature_levels]\n            )\n            intermediate_class_predictions.append(outputs_class)\n            intermediate_mask_predictions.append(outputs_mask)\n\n        if not len(intermediate_mask_predictions) == len(self.layers) + 1:\n            raise ValueError(\n                \"Intermediate predictions in the transformer decoder must have the same number of elements as number\"\n                \" of layers\"\n            )\n\n        object_queries = layer_outputs[0].permute(1, 0, 2)\n\n        contrastive_logits = queries.permute(1, 0, 2)\n\n        return OneFormerTransformerDecoderOutput(\n            object_queries=object_queries,\n            contrastive_logits=contrastive_logits,\n            prediction_masks=intermediate_mask_predictions[-1],\n            prediction_class=intermediate_class_predictions[-1],\n            auxiliary_predictions=self._get_aux_predictions(\n                intermediate_class_predictions, intermediate_mask_predictions\n            )\n            if self.use_auxiliary_loss\n            else None,\n            attentions=attentions,\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(\n        self,\n        context_length: int,\n        width: int,\n        layers: int,\n        vocab_size,\n        use_checkpoint=False,\n        layer_norm_eps=1e-05,\n    ):\n        super().__init__()\n        heads = width // 64\n        self.context_length = context_length\n        self.width = width\n        self.transformer = OneFormerTextTransformer(\n            width=width,\n            layers=layers,\n            heads=heads,\n            attn_mask=self.build_attention_mask(),\n            use_checkpoint=use_checkpoint,\n            layer_norm_eps=layer_norm_eps,\n        )\n\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, width))\n        self.ln_final = nn.LayerNorm(width, eps=layer_norm_eps)\n        self.token_embedding = nn.Embedding(vocab_size, width)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, config: OneFormerConfig, feature_channels):\n        super().__init__()\n\n        self.config = config\n\n        #  positional encoding\n        self.position_embedding = OneFormerSinePositionEmbedding(num_pos_feats=config.conv_dim // 2, normalize=True)\n        self.num_feature_levels = 3\n        transformer_in_channels = feature_channels[-self.num_feature_levels :]\n        self.transformer_feature_strides = config.strides[-self.num_feature_levels :]\n        self.feature_channels = feature_channels\n        self.level_embed = nn.Parameter(torch.Tensor(self.num_feature_levels, config.conv_dim))\n\n        # Create input projection layers\n        if self.num_feature_levels > 1:\n            input_projections_list = []\n            for in_channels in transformer_in_channels[::-1]:\n                input_projections_list.append(\n                    nn.Sequential(\n                        nn.Conv2d(in_channels, config.conv_dim, kernel_size=1),\n                        nn.GroupNorm(32, config.conv_dim),\n                    )\n                )\n            self.input_projections = nn.ModuleList(input_projections_list)\n        else:\n            self.input_projections = nn.ModuleList(\n                [\n                    nn.Sequential(\n                        nn.Conv2d(transformer_in_channels[-1], config.conv_dim, kernel_size=1),\n                        nn.GroupNorm(32, config.conv_dim),\n                    )\n                ]\n            )\n\n        self.encoder = OneFormerPixelDecoderEncoderOnly(config)\n\n        self.mask_projection = nn.Conv2d(\n            config.conv_dim,\n            config.mask_dim,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n\n        self.common_stride = config.common_stride\n\n        # extra fpn levels\n        stride = min(self.transformer_feature_strides)\n        self.num_fpn_levels = int(np.log2(stride) - np.log2(self.common_stride))\n\n        lateral_convs = []\n        output_convs = []\n\n        for idx, in_channels in enumerate(self.feature_channels[: self.num_fpn_levels]):\n            lateral_conv = nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    config.conv_dim,\n                    kernel_size=1,\n                    bias=False,\n                ),\n                nn.GroupNorm(32, config.conv_dim),\n            )\n            output_conv = nn.Sequential(\n                nn.Conv2d(\n                    config.conv_dim,\n                    config.conv_dim,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.GroupNorm(32, config.conv_dim),\n                nn.ReLU(),\n            )\n            self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n            self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n\n            lateral_convs.append(lateral_conv)\n            output_convs.append(output_conv)\n        # Place convs into top-down order (from low to high resolution)\n        # to make the top-down computation in forward clearer.\n        self.lateral_convs = lateral_convs[::-1]\n        self.output_convs = output_convs[::-1]",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        attn_mask: torch.Tensor = None,\n        use_checkpoint=False,\n        layer_norm_eps=1e-05,\n    ):\n        super().__init__()\n        self.width = width\n        self.num_layers = layers\n        self.layers = nn.Sequential(\n            *[OneFormerTextTransformerLayer(width, heads, attn_mask, layer_norm_eps) for _ in range(layers)]\n        )\n        self.use_checkpoint = use_checkpoint",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def forward(\n        self,\n        output,\n        memory,\n        output_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        output_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        intermediate = []\n\n        for layer in self.layers:\n            output = layer(\n                output,\n                memory,\n                output_mask=output_mask,\n                memory_mask=memory_mask,\n                output_key_padding_mask=output_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n                pos=pos,\n                query_pos=query_pos,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate.pop()\n                intermediate.append(output)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n\n        return output.unsqueeze(0)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_problem_types(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        problem_types = [\n            {\"title\": \"multi_label_classification\", \"num_labels\": 2, \"dtype\": torch.float},\n            {\"title\": \"single_label_classification\", \"num_labels\": 1, \"dtype\": torch.long},\n            {\"title\": \"regression\", \"num_labels\": 1, \"dtype\": torch.float},\n        ]\n\n        for model_class in self.all_model_classes:\n            if model_class.__name__ not in [\n                *get_values(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES),\n                *get_values(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES),\n            ]:\n                continue\n\n            for problem_type in problem_types:\n                with self.subTest(msg=f\"Testing {model_class} with {problem_type['title']}\"):\n                    config.problem_type = problem_type[\"title\"]\n                    config.num_labels = problem_type[\"num_labels\"]\n\n                    model = model_class(config)\n                    model.to(torch_device)\n                    model.train()\n\n                    inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n\n                    if problem_type[\"num_labels\"] > 1:\n                        inputs[\"labels\"] = inputs[\"labels\"].unsqueeze(1).repeat(1, problem_type[\"num_labels\"])\n\n                    inputs[\"labels\"] = inputs[\"labels\"].to(problem_type[\"dtype\"])\n\n                    # This tests that we do not trigger the warning form PyTorch \"Using a target size that is different\n                    # to the input size. This will likely lead to incorrect results due to broadcasting. Please ensure\n                    # they have the same size.\" which is a symptom something in wrong for the regression problem.\n                    # See https://github.com/huggingface/transformers/issues/11780\n                    with warnings.catch_warnings(record=True) as warning_list:\n                        loss = model(**inputs).loss\n                    for w in warning_list:\n                        if \"Using a target size that is different to the input size\" in str(w.message):\n                            raise ValueError(\n                                f\"Something is going wrong in the regression problem: intercepted {w.message}\"\n                            )\n\n                    loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        if not self.has_attentions:\n            self.skipTest(reason=\"Model architecture does not support attentions\")\n\n        if len(self.all_generative_model_classes) == 0:\n            self.skipTest(\n                reason=\"Model architecture has no generative classes, and thus not necessarily supporting 4D masks\"\n            )\n\n        set_model_tester_for_less_flaky_test(self)\n\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n            set_config_for_less_flaky_test(config)\n            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n            set_model_for_less_flaky_test(model)\n\n            (\n                input_ids,\n                position_ids,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                position_ids_shared_prefix,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(input_ids, position_ids=position_ids).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids_shared_prefix,\n                attention_mask=mask_shared_prefix,\n                position_ids=position_ids_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n\n        for model_class in self.all_model_classes:\n            with self.subTest(model_class.__name__):\n                if (\n                    model_class.__name__\n                    in [\n                        *get_values(MODEL_MAPPING_NAMES),\n                        *get_values(MODEL_FOR_BACKBONE_MAPPING_NAMES),\n                    ]\n                    or not model_class.supports_gradient_checkpointing\n                ):\n                    # TODO (ydshieh): use `skipTest` once pytest-dev/pytest-subtests/pull/169 is merged\n                    # self.skipTest(reason=f\"`supports_gradient_checkpointing` is False for {model_class.__name__}.\")\n                    continue\n\n                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                config.use_cache = False\n                config.return_dict = True\n                model = model_class(config)\n\n                model.to(torch_device)\n                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                model.train()\n\n                # unfreeze additional layers\n                for p in model.parameters():\n                    p.requires_grad_(True)\n\n                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n                loss = model(**inputs).loss\n                loss.backward()\n                optimizer.step()\n\n                if self.test_all_params_have_gradient:\n                    for k, v in model.named_parameters():\n                        if v.requires_grad:\n                            self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_headmasking(self):\n        if not self.test_head_masking:\n            self.skipTest(reason=\"Model does not support head masking\")\n\n        global_rng.seed(42)\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        global_rng.seed()\n\n        inputs_dict[\"output_attentions\"] = True\n        config.output_hidden_states = True\n        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            model.to(torch_device)\n            model.eval()\n\n            # Prepare head_mask\n            # Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior)\n            head_mask = torch.ones(\n                self.model_tester.num_hidden_layers,\n                self.model_tester.num_attention_heads,\n                device=torch_device,\n            )\n            head_mask[0, 0] = 0\n            head_mask[-1, :-1] = 0\n            head_mask.requires_grad_(requires_grad=True)\n            inputs = self._prepare_for_class(inputs_dict, model_class).copy()\n            inputs[\"head_mask\"] = head_mask\n            if model.config.is_encoder_decoder:\n                signature = inspect.signature(model.forward)\n                arg_names = [*signature.parameters.keys()]\n                if \"decoder_head_mask\" in arg_names:  # necessary diferentiation because of T5 model\n                    inputs[\"decoder_head_mask\"] = head_mask\n                if \"cross_attn_head_mask\" in arg_names:\n                    inputs[\"cross_attn_head_mask\"] = head_mask\n            outputs = model(**inputs, return_dict=True)\n\n            # Test that we can get a gradient back for importance score computation\n            output = sum(t.sum() for t in outputs[0])\n            output = output.sum()\n            output.backward()\n            multihead_outputs = head_mask.grad\n\n            self.assertIsNotNone(multihead_outputs)\n            self.assertEqual(len(multihead_outputs), self.model_tester.num_hidden_layers)\n\n            def check_attentions_validity(attentions):\n                # Remove Nan\n                for t in attentions:\n                    self.assertLess(\n                        torch.sum(torch.isnan(t)), t.numel() / 4\n                    )  # Check we don't have more than 25% nans (arbitrary)\n                attentions = [\n                    t.masked_fill(torch.isnan(t), 0.0) for t in attentions\n                ]  # remove them (the test is less complete)\n\n                self.assertAlmostEqual(attentions[0][..., 0, :, :].flatten().sum().item(), 0.0)\n                self.assertNotEqual(attentions[0][..., -1, :, :].flatten().sum().item(), 0.0)\n                if len(attentions) > 2:  # encoder-decoder models have only 2 layers in each module\n                    self.assertNotEqual(attentions[1][..., 0, :, :].flatten().sum().item(), 0.0)\n                self.assertAlmostEqual(attentions[-1][..., -2, :, :].flatten().sum().item(), 0.0)\n                self.assertNotEqual(attentions[-1][..., -1, :, :].flatten().sum().item(), 0.0)\n\n            if model.config.is_encoder_decoder:\n                check_attentions_validity(outputs.encoder_attentions)\n                check_attentions_validity(outputs.decoder_attentions)\n                check_attentions_validity(outputs.cross_attentions)\n            else:\n                check_attentions_validity(outputs.attentions)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_headmasking(self):\n        if not self.test_head_masking:\n            self.skipTest(reason=\"test_head_masking is set to False\")\n\n        global_rng.seed(42)\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        global_rng.seed()\n\n        inputs_dict[\"output_attentions\"] = True\n        config.output_hidden_states = True\n        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            model.to(torch_device)\n            model.eval()\n\n            # Prepare head_mask\n            # Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior)\n            head_mask = torch.ones(\n                self.model_tester.num_hidden_layers,\n                self.model_tester.num_attention_heads,\n                device=torch_device,\n            )\n            head_mask[0, 0] = 0\n            head_mask[-1, :-1] = 0\n            head_mask.requires_grad_(requires_grad=True)\n            inputs = self._prepare_for_class(inputs_dict, model_class).copy()\n            inputs[\"head_mask\"] = head_mask\n\n            outputs = model(**inputs, return_dict=True)\n\n            # Test that we can get a gradient back for importance score computation\n            output = sum(t.sum() for t in outputs[0])\n            output = output.sum()\n            output.backward()\n            multihead_outputs = head_mask.grad\n\n            self.assertIsNotNone(multihead_outputs)\n            self.assertEqual(len(multihead_outputs), self.model_tester.num_hidden_layers)\n\n            def check_attentions_validity(attentions):\n                # Remove Nan\n                for t in attentions:\n                    self.assertLess(\n                        torch.sum(torch.isnan(t)), t.numel() / 4\n                    )  # Check we don't have more than 25% nans (arbitrary)\n                attentions = [\n                    t.masked_fill(torch.isnan(t), 0.0) for t in attentions\n                ]  # remove them (the test is less complete)\n\n                self.assertAlmostEqual(attentions[1][..., 0, :, :].flatten().sum().item(), 0.0)\n                self.assertNotEqual(attentions[1][..., -1, :, :].flatten().sum().item(), 0.0)\n                self.assertAlmostEqual(attentions[-2][..., -2, :, :].flatten().sum().item(), 0.0)\n                self.assertNotEqual(attentions[-2][..., -1, :, :].flatten().sum().item(), 0.0)\n\n            check_attentions_validity(outputs.attentions)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_training_gradient_checkpointing(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        config.use_cache = False\n        config.return_dict = True\n\n        for model_class in self.all_model_classes:\n            if model_class.__name__ in MODEL_MAPPING_NAMES.values() or not model_class.supports_gradient_checkpointing:\n                continue\n            # TODO: remove the following 3 lines once we have a MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\n            # this can then be incorporated into _prepare_for_class in test_modeling_common.py\n            elif model_class.__name__ == \"Data2VecVisionForSemanticSegmentation\":\n                batch_size, num_channels, height, width = inputs_dict[\"pixel_values\"].shape\n                inputs_dict[\"labels\"] = torch.zeros(\n                    [self.model_tester.batch_size, height, width], device=torch_device\n                ).long()\n            model = model_class(config)\n            model.gradient_checkpointing_enable()\n            model.to(torch_device)\n            model.train()\n            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n            loss = model(**inputs).loss\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_problem_types(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        problem_types = [\n            {\"title\": \"multi_label_classification\", \"num_labels\": 2, \"dtype\": torch.float},\n            {\"title\": \"single_label_classification\", \"num_labels\": 1, \"dtype\": torch.long},\n            {\"title\": \"regression\", \"num_labels\": 1, \"dtype\": torch.float},\n        ]\n\n        for model_class in self.all_model_classes:\n            if (\n                model_class.__name__\n                not in [\n                    *MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values(),\n                    *MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES.values(),\n                ]\n                or model_class.__name__ == \"DeiTForImageClassificationWithTeacher\"\n            ):\n                continue\n\n            for problem_type in problem_types:\n                with self.subTest(msg=f\"Testing {model_class} with {problem_type['title']}\"):\n                    config.problem_type = problem_type[\"title\"]\n                    config.num_labels = problem_type[\"num_labels\"]\n\n                    model = model_class(config)\n                    model.to(torch_device)\n                    model.train()\n\n                    inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n\n                    if problem_type[\"num_labels\"] > 1:\n                        inputs[\"labels\"] = inputs[\"labels\"].unsqueeze(1).repeat(1, problem_type[\"num_labels\"])\n\n                    inputs[\"labels\"] = inputs[\"labels\"].to(problem_type[\"dtype\"])\n\n                    # This tests that we do not trigger the warning form PyTorch \"Using a target size that is different\n                    # to the input size. This will likely lead to incorrect results due to broadcasting. Please ensure\n                    # they have the same size.\" which is a symptom something in wrong for the regression problem.\n                    # See https://github.com/huggingface/transformers/issues/11780\n                    with warnings.catch_warnings(record=True) as warning_list:\n                        loss = model(**inputs).loss\n                    for w in warning_list:\n                        if \"Using a target size that is different to the input size\" in str(w.message):\n                            raise ValueError(\n                                f\"Something is going wrong in the regression problem: intercepted {w.message}\"\n                            )\n\n                    loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_partial_stacked_causal_mask(self):\n        # Same as the test above, but the input is passed in two groups. It tests that we can pass partial 4D attention masks\n\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # 2 forward runs with custom 4D masks\n        part_a = 3  # split point\n\n        input_1a = input_ids_shared_prefix[:, :part_a]\n        position_ids_1a = position_ids_shared_prefix[:, :part_a]\n        mask_1a = mask_shared_prefix[:, :, :part_a, :part_a]\n\n        outs_1a = self.model.forward(input_1a, attention_mask=mask_1a, position_ids=position_ids_1a)\n        past_key_values_a = outs_1a[\"past_key_values\"]\n\n        # Case 1: we pass a 4D attention mask regarding the current sequence length (i.e. [..., seq_len, full_len])\n        input_1b = input_ids_shared_prefix[:, part_a:]\n        position_ids_1b = position_ids_shared_prefix[:, part_a:]\n        mask_1b = mask_shared_prefix[:, :, part_a:, :]\n        outs_1b = self.model.forward(\n            input_1b,\n            attention_mask=mask_1b,\n            position_ids=position_ids_1b,\n            past_key_values=past_key_values_a,\n        )\n        decoded_1b = [\n            self.tokenizer.decode(t)\n            for t in outs_1b.logits.argmax(-1)[\n                0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1] - part_a\n            ]\n        ]\n        self.assertEqual(decoded, decoded_1b)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_stacked_causal_mask(self):\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # single forward run with 4D custom mask\n        logits_shared_prefix = self.model.forward(\n            input_ids_shared_prefix, attention_mask=mask_shared_prefix, position_ids=position_ids_shared_prefix\n        ).logits\n        logits_shared_prefix_last = logits_shared_prefix[\n            0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1], :\n        ]  # last three tokens\n        decoded_shared_prefix = [self.tokenizer.decode(t) for t in logits_shared_prefix_last.argmax(dim=-1)]\n\n        self.assertEqual(decoded, decoded_shared_prefix)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_stacked_causal_mask_static_cache(self):\n        \"\"\"same as above but with StaticCache\"\"\"\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # upgrade the model with StaticCache\n        max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n        past_key_values = StaticCache(\n            config=self.model.config,\n            batch_size=1,\n            max_cache_len=max_cache_len,\n            device=torch_device,\n            dtype=self.model.dtype,\n        )\n\n        padded_attention_mask = torch.nn.functional.pad(\n            input=mask_shared_prefix,\n            pad=(0, max_cache_len - mask_shared_prefix.shape[-1]),\n            mode=\"constant\",\n            value=torch.finfo(self.model_dtype).min,\n        )\n\n        # single forward run with 4D custom mask\n        logits_shared_prefix = self.model.forward(\n            input_ids_shared_prefix,\n            attention_mask=padded_attention_mask,\n            position_ids=position_ids_shared_prefix,\n            cache_position=torch.arange(input_ids_shared_prefix.shape[-1], device=torch_device),\n            past_key_values=past_key_values,\n        ).logits\n        logits_shared_prefix_last = logits_shared_prefix[\n            0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1], :\n        ]  # last three tokens\n        decoded_shared_prefix = [self.tokenizer.decode(t) for t in logits_shared_prefix_last.argmax(dim=-1)]\n\n        self.assertEqual(decoded, decoded_shared_prefix)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_partial_stacked_causal_mask_static_cache(self):\n        # Same as the test above, but the input is passed in two groups. It tests that we can pass partial 4D attention masks\n        # we pass a 4D attention mask shaped [..., seq_len, full_static_cache_len])\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # upgrade the model with StaticCache\n        max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n        past_key_values = StaticCache(\n            config=self.model.config,\n            batch_size=1,\n            max_cache_len=max_cache_len,\n            device=torch_device,\n            dtype=self.model.dtype,\n        )\n\n        # forward run for the first part of input\n        part_a = 3  # split point\n\n        input_1a = input_ids_shared_prefix[:, :part_a]\n        position_ids_1a = position_ids_shared_prefix[:, :part_a]\n        mask_1a = mask_shared_prefix[:, :, :part_a, :part_a]\n\n        padded_mask_1a = torch.nn.functional.pad(\n            input=mask_1a,\n            pad=(0, max_cache_len - mask_1a.shape[-1]),\n            mode=\"constant\",\n            value=torch.finfo(self.model_dtype).min,\n        )\n\n        _ = self.model.forward(\n            input_1a,\n            attention_mask=padded_mask_1a,\n            position_ids=position_ids_1a,\n            cache_position=torch.arange(part_a, device=torch_device),\n            past_key_values=past_key_values,\n        )\n\n        # forward run for the second part of input\n        input_1b = input_ids_shared_prefix[:, part_a:]\n        position_ids_1b = position_ids_shared_prefix[:, part_a:]\n        mask_1b = mask_shared_prefix[:, :, part_a:, :]\n\n        padded_mask_1b = torch.nn.functional.pad(\n            input=mask_1b, pad=(0, max_cache_len - mask_1b.shape[-1]), mode=\"constant\", value=0\n        )\n\n        outs_1b = self.model.forward(\n            input_1b,\n            attention_mask=padded_mask_1b,\n            position_ids=position_ids_1b,\n            cache_position=torch.arange(\n                part_a,\n                input_ids_shared_prefix.shape[-1],\n                device=torch_device,\n            ),\n            past_key_values=past_key_values,\n        )\n        decoded_1b = [\n            self.tokenizer.decode(t)\n            for t in outs_1b.logits.argmax(-1)[\n                0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1] - part_a\n            ]\n        ]\n        self.assertEqual(decoded, decoded_1b)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        \"\"\"Overwrite the common test to use atol=1e-3 instead of 1e-4. Can still rarely fail, thus flaky.\"\"\"\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                position_ids,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                position_ids_shared_prefix,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(input_ids, position_ids=position_ids).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids_shared_prefix,\n                attention_mask=mask_shared_prefix,\n                position_ids=position_ids_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = torch.nn.functional.softmax(out_last_tokens)\n            normalized_1 = torch.nn.functional.softmax(out_shared_prefix_last_tokens)\n            print(torch.abs(normalized_0 - normalized_1).max())\n\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_training(self):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        config.return_dict = True\n\n        for model_class in self.all_model_classes:\n            if model_class.__name__ in MODEL_MAPPING_NAMES.values():\n                continue\n            # TODO: remove the following 3 lines once we have a MODEL_FOR_DEPTH_ESTIMATION_MAPPING\n            # this can then be incorporated into _prepare_for_class in test_modeling_common.py\n            if model_class.__name__ == \"GLPNForDepthEstimation\":\n                batch_size, num_channels, height, width = inputs_dict[\"pixel_values\"].shape\n                inputs_dict[\"labels\"] = torch.zeros(\n                    [self.model_tester.batch_size, height, width], device=torch_device\n                ).long()\n            model = model_class(config)\n            model.to(torch_device)\n            model.train()\n            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n            loss = model(**inputs).loss\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_problem_types(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        problem_types = [\n            {\"title\": \"multi_label_classification\", \"num_labels\": 2, \"dtype\": torch.float},\n            {\"title\": \"single_label_classification\", \"num_labels\": 1, \"dtype\": torch.long},\n            {\"title\": \"regression\", \"num_labels\": 1, \"dtype\": torch.float},\n        ]\n\n        for model_class in self.all_model_classes:\n            if (\n                model_class.__name__\n                not in [\n                    *MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES.values(),\n                ]\n                or model_class.__name__ == \"LevitForImageClassificationWithTeacher\"\n            ):\n                continue\n\n            for problem_type in problem_types:\n                with self.subTest(msg=f\"Testing {model_class} with {problem_type['title']}\"):\n                    config.problem_type = problem_type[\"title\"]\n                    config.num_labels = problem_type[\"num_labels\"]\n\n                    model = model_class(config)\n                    model.to(torch_device)\n                    model.train()\n\n                    inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n\n                    if problem_type[\"num_labels\"] > 1:\n                        inputs[\"labels\"] = inputs[\"labels\"].unsqueeze(1).repeat(1, problem_type[\"num_labels\"])\n\n                    inputs[\"labels\"] = inputs[\"labels\"].to(problem_type[\"dtype\"])\n\n                    # This tests that we do not trigger the warning form PyTorch \"Using a target size that is different\n                    # to the input size. This will likely lead to incorrect results due to broadcasting. Please ensure\n                    # they have the same size.\" which is a symptom something in wrong for the regression problem.\n                    # See https://github.com/huggingface/transformers/issues/11780\n                    with warnings.catch_warnings(record=True) as warning_list:\n                        loss = model(**inputs).loss\n                    for w in warning_list:\n                        if \"Using a target size that is different to the input size\" in str(w.message):\n                            raise ValueError(\n                                f\"Something is going wrong in the regression problem: intercepted {w.message}\"\n                            )\n\n                    loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_partial_stacked_causal_mask(self):\n        # Same as the test above, but the input is passed in two groups. It tests that we can pass partial 4D attention masks\n\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # 2 forward runs with custom 4D masks\n        part_a = 3  # split point\n\n        input_1a = input_ids_shared_prefix[:, :part_a]\n        position_ids_1a = position_ids_shared_prefix[:, :part_a]\n        mask_1a = mask_shared_prefix[:, :, :part_a, :part_a]\n\n        outs_1a = self.model.forward(input_1a, attention_mask=mask_1a, position_ids=position_ids_1a)\n        past_key_values_a = outs_1a[\"past_key_values\"]\n\n        # Case 1: we pass a 4D attention mask regarding the current sequence length (i.e. [..., seq_len, full_len])\n        input_1b = input_ids_shared_prefix[:, part_a:]\n        position_ids_1b = position_ids_shared_prefix[:, part_a:]\n        mask_1b = mask_shared_prefix[:, :, part_a:, :]\n        outs_1b = self.model.forward(\n            input_1b,\n            attention_mask=mask_1b,\n            position_ids=position_ids_1b,\n            past_key_values=past_key_values_a,\n        )\n        decoded_1b = [\n            self.tokenizer.decode(t)\n            for t in outs_1b.logits.argmax(-1)[\n                0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1] - part_a\n            ]\n        ]\n        self.assertEqual(decoded, decoded_1b)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_stacked_causal_mask_static_cache(self):\n        \"\"\"same as above but with StaticCache\"\"\"\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # upgrade the model with StaticCache\n        max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n        past_key_values = StaticCache(\n            config=self.model.config,\n            batch_size=1,\n            max_cache_len=max_cache_len,\n            device=torch_device,\n            dtype=self.model.dtype,\n        )\n\n        padded_attention_mask = torch.nn.functional.pad(\n            input=mask_shared_prefix,\n            pad=(0, max_cache_len - mask_shared_prefix.shape[-1]),\n            mode=\"constant\",\n            value=torch.finfo(self.model_dtype).min,\n        )\n\n        # single forward run with 4D custom mask\n        logits_shared_prefix = self.model.forward(\n            input_ids_shared_prefix,\n            attention_mask=padded_attention_mask,\n            position_ids=position_ids_shared_prefix,\n            cache_position=torch.arange(input_ids_shared_prefix.shape[-1], device=torch_device),\n            past_key_values=past_key_values,\n        ).logits\n        logits_shared_prefix_last = logits_shared_prefix[\n            0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1], :\n        ]  # last three tokens\n        decoded_shared_prefix = [self.tokenizer.decode(t) for t in logits_shared_prefix_last.argmax(dim=-1)]\n\n        self.assertEqual(decoded, decoded_shared_prefix)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_partial_stacked_causal_mask_static_cache(self):\n        # Same as the test above, but the input is passed in two groups. It tests that we can pass partial 4D attention masks\n        # we pass a 4D attention mask shaped [..., seq_len, full_static_cache_len])\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # upgrade the model with StaticCache\n        max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n        past_key_values = StaticCache(\n            config=self.model.config,\n            batch_size=1,\n            max_cache_len=max_cache_len,\n            device=torch_device,\n            dtype=self.model.dtype,\n        )\n\n        # forward run for the first part of input\n        part_a = 3  # split point\n\n        input_1a = input_ids_shared_prefix[:, :part_a]\n        position_ids_1a = position_ids_shared_prefix[:, :part_a]\n        mask_1a = mask_shared_prefix[:, :, :part_a, :part_a]\n\n        padded_mask_1a = torch.nn.functional.pad(\n            input=mask_1a,\n            pad=(0, max_cache_len - mask_1a.shape[-1]),\n            mode=\"constant\",\n            value=torch.finfo(self.model_dtype).min,\n        )\n\n        _ = self.model.forward(\n            input_1a,\n            attention_mask=padded_mask_1a,\n            position_ids=position_ids_1a,\n            cache_position=torch.arange(part_a, device=torch_device),\n            past_key_values=past_key_values,\n        )\n\n        # forward run for the second part of input\n        input_1b = input_ids_shared_prefix[:, part_a:]\n        position_ids_1b = position_ids_shared_prefix[:, part_a:]\n        mask_1b = mask_shared_prefix[:, :, part_a:, :]\n\n        padded_mask_1b = torch.nn.functional.pad(\n            input=mask_1b, pad=(0, max_cache_len - mask_1b.shape[-1]), mode=\"constant\", value=0\n        )\n\n        outs_1b = self.model.forward(\n            input_1b,\n            attention_mask=padded_mask_1b,\n            position_ids=position_ids_1b,\n            cache_position=torch.arange(\n                part_a,\n                input_ids_shared_prefix.shape[-1],\n                device=torch_device,\n            ),\n            past_key_values=past_key_values,\n        )\n        decoded_1b = [\n            self.tokenizer.decode(t)\n            for t in outs_1b.logits.argmax(-1)[\n                0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1] - part_a\n            ]\n        ]\n        self.assertEqual(decoded, decoded_1b)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_stacked_causal_mask(self):\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # single forward run with 4D custom mask\n        logits_shared_prefix = self.model.forward(\n            input_ids_shared_prefix, attention_mask=mask_shared_prefix, position_ids=position_ids_shared_prefix\n        ).logits\n        logits_shared_prefix_last = logits_shared_prefix[\n            0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1], :\n        ]  # last three tokens\n        decoded_shared_prefix = [self.tokenizer.decode(t) for t in logits_shared_prefix_last.argmax(dim=-1)]\n\n        self.assertEqual(decoded, decoded_shared_prefix)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_stacked_causal_mask(self):\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # single forward run with 4D custom mask\n        logits_shared_prefix = self.model.forward(\n            input_ids_shared_prefix, attention_mask=mask_shared_prefix, position_ids=position_ids_shared_prefix\n        ).logits\n        logits_shared_prefix_last = logits_shared_prefix[\n            0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1], :\n        ]  # last three tokens\n        decoded_shared_prefix = [self.tokenizer.decode(t) for t in logits_shared_prefix_last.argmax(dim=-1)]\n\n        self.assertEqual(decoded, decoded_shared_prefix)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_partial_stacked_causal_mask(self):\n        # Same as the test above, but the input is passed in two groups. It tests that we can pass partial 4D attention masks\n\n        (\n            input_ids,\n            position_ids,\n            input_ids_shared_prefix,\n            mask_shared_prefix,\n            position_ids_shared_prefix,\n        ) = self.get_test_data()\n\n        # regular batch\n        logits = self.model.forward(input_ids, position_ids=position_ids).logits\n        logits_last = logits[:, -1, :]  # last tokens in each batch line\n        decoded = [self.tokenizer.decode(t) for t in logits_last.argmax(dim=-1)]\n\n        # 2 forward runs with custom 4D masks\n        part_a = 3  # split point\n\n        input_1a = input_ids_shared_prefix[:, :part_a]\n        position_ids_1a = position_ids_shared_prefix[:, :part_a]\n        mask_1a = mask_shared_prefix[:, :, :part_a, :part_a]\n\n        outs_1a = self.model.forward(input_1a, attention_mask=mask_1a, position_ids=position_ids_1a)\n        past_key_values_a = outs_1a[\"past_key_values\"]\n\n        # Case 1: we pass a 4D attention mask regarding the current sequence length (i.e. [..., seq_len, full_len])\n        input_1b = input_ids_shared_prefix[:, part_a:]\n        position_ids_1b = position_ids_shared_prefix[:, part_a:]\n        mask_1b = mask_shared_prefix[:, :, part_a:, :]\n        outs_1b = self.model.forward(\n            input_1b, attention_mask=mask_1b, position_ids=position_ids_1b, past_key_values=past_key_values_a\n        )\n        decoded_1b = [\n            self.tokenizer.decode(t)\n            for t in outs_1b.logits.argmax(-1)[\n                0, torch.where(position_ids_shared_prefix == position_ids_shared_prefix.max())[1] - part_a\n            ]\n        ]\n        self.assertEqual(decoded, decoded_1b)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        config.use_cache = False\n        config.return_dict = True\n        model = MusicgenForCausalLM(config)\n\n        model.to(torch_device)\n        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n        model.train()\n\n        # Contrarily to the initial method, we don't unfreeze freezed parameters.\n        # Indeed, sinusoidal position embeddings have frozen weights that should stay frozen.\n\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        inputs = self._prepare_for_class(inputs_dict, MusicgenForCausalLM, return_labels=True)\n        loss = model(**inputs).loss\n        loss.backward()\n        optimizer.step()\n\n        for k, v in model.named_parameters():\n            if v.requires_grad:\n                self.assertTrue(v.grad is not None, f\"{k} in {MusicgenForCausalLM.__name__} has no gradient!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        for model_class in self.all_model_classes:\n            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            config.use_cache = False\n            config.return_dict = True\n            model = model_class(config)\n\n            model.to(torch_device)\n            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n            model.train()\n\n            # The audio encoder weights are not used during the forward pass (only during the generate pass)\n            # So we need to freeze it to be able to train.\n            model.freeze_audio_encoder()\n\n            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n            loss = model(**inputs).loss\n            loss.backward()\n            optimizer.step()\n\n            for k, v in model.named_parameters():\n                if v.requires_grad:\n                    self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        for model_class in self.all_model_classes:\n            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            config.use_cache = False\n            config.return_dict = True\n            model = model_class(config)\n\n            model.to(torch_device)\n            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n            model.train()\n\n            # The audio encoder weights are not used during the forward pass (only during the generate pass)\n            # So we need to freeze it to be able to train.\n            model.freeze_audio_encoder()\n\n            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n            loss = model(**inputs).loss\n            loss.backward()\n            optimizer.step()\n\n            for k, v in model.named_parameters():\n                if v.requires_grad:\n                    self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"model_tester.is_training is set to False\")\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        config.use_cache = False\n        config.return_dict = True\n        model = MusicgenMelodyForCausalLM(config)\n\n        model.to(torch_device)\n        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n        model.train()\n\n        # Contrarily to the initial method, we don't unfreeze freezed parameters.\n        # Indeed, sinusoidal position embeddings have frozen weights that should stay frozen.\n\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        inputs = self._prepare_for_class(inputs_dict, MusicgenMelodyForCausalLM, return_labels=True)\n        loss = model(**inputs).loss\n        loss.backward()\n        optimizer.step()\n\n        for k, v in model.named_parameters():\n            if v.requires_grad:\n                self.assertTrue(v.grad is not None, f\"{k} in {MusicgenMelodyForCausalLM.__name__} has no gradient!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def generate(model: Phi3ForCausalLM, prompt_tokens: torch.LongTensor, max_seq_len: int) -> List[int]:\n            model = Phi3MiniWithStaticCache(model, 1, max_seq_len + prompt_tokens.shape[-1])\n\n            response_tokens = []\n\n            for input_pos in range(prompt_tokens.shape[-1]):\n                result = model.forward(\n                    input_ids=prompt_tokens[:, input_pos : input_pos + 1],\n                )\n                response_tokens.append(prompt_tokens[0][input_pos].item())\n\n            current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n            response_tokens.append(current_token)\n\n            while current_token != end_of_text_token and len(response_tokens) < max_seq_len:\n                result = model.forward(\n                    input_ids=torch.tensor([[current_token]], dtype=torch.long),\n                )\n                current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n                response_tokens.append(current_token)\n\n            return response_tokens",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n        ) -> torch.FloatTensor:\n            return self.model.forward(\n                input_ids=input_ids,\n                use_cache=True,\n                return_dict=True,\n                past_key_values=self.cache,\n            ).logits",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n        ) -> torch.FloatTensor:\n            return self.model.forward(\n                input_ids=input_ids,\n                use_cache=True,\n                return_dict=True,\n                past_key_values=self.cache,\n            ).logits",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_problem_types(self):\n        problem_types = [\n            {\"title\": \"multi_label_classification\", \"num_labels\": 2, \"dtype\": torch.float},\n            {\"title\": \"single_label_classification\", \"num_labels\": 1, \"dtype\": torch.long},\n            {\"title\": \"regression\", \"num_labels\": 1, \"dtype\": torch.float},\n        ]\n\n        for model_class in self.all_model_classes:\n            if model_class.__name__ not in MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values():\n                continue\n\n            config, inputs, input_mask, _, _ = self.model_tester.prepare_config_and_inputs(model_class=model_class)\n            inputs_dict = {\"inputs\": inputs, \"attention_mask\": input_mask}\n\n            for problem_type in problem_types:\n                with self.subTest(msg=f\"Testing {model_class} with {problem_type['title']}\"):\n                    config.problem_type = problem_type[\"title\"]\n                    config.num_labels = problem_type[\"num_labels\"]\n\n                    model = model_class(config)\n                    model.to(torch_device)\n                    model.train()\n\n                    inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n\n                    if problem_type[\"num_labels\"] > 1:\n                        inputs[\"labels\"] = inputs[\"labels\"].unsqueeze(1).repeat(1, problem_type[\"num_labels\"])\n\n                    inputs[\"labels\"] = inputs[\"labels\"].to(problem_type[\"dtype\"])\n\n                    # This tests that we do not trigger the warning form PyTorch \"Using a target size that is different\n                    # to the input size. This will likely lead to incorrect results due to broadcasting. Please ensure\n                    # they have the same size.\" which is a symptom something in wrong for the regression problem.\n                    # See https://github.com/huggingface/transformers/issues/11780\n                    with warnings.catch_warnings(record=True) as warning_list:\n                        loss = model(**inputs).loss\n                    for w in warning_list:\n                        if \"Using a target size that is different to the input size\" in str(w.message):\n                            raise ValueError(\n                                f\"Something is going wrong in the regression problem: intercepted {w.message}\"\n                            )\n\n                    loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def generate(model: PhimoeForCausalLM, prompt_tokens: torch.LongTensor, max_seq_len: int) -> List[int]:\n            model = PhimoeMiniWithStaticCache(model, 1, max_seq_len + prompt_tokens.shape[-1])\n\n            response_tokens = []\n\n            for input_pos in range(prompt_tokens.shape[-1]):\n                result = model.forward(\n                    input_ids=prompt_tokens[:, input_pos : input_pos + 1],\n                )\n                response_tokens.append(prompt_tokens[0][input_pos].item())\n\n            current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n            response_tokens.append(current_token)\n\n            while current_token != end_of_text_token and len(response_tokens) < max_seq_len:\n                result = model.forward(\n                    input_ids=torch.tensor([[current_token]], dtype=torch.long),\n                )\n                current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n                response_tokens.append(current_token)\n\n            return response_tokens",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n                bbox=input_dict[\"bbox\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                bbox=input_dict[\"bbox\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_4d_attention_mask(self):\n        for model_class in self.all_generative_model_classes:\n            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n\n            (\n                input_ids,\n                _,\n                input_ids_shared_prefix,\n                mask_shared_prefix,\n                _,\n            ) = self._get_custom_4d_mask_test_data()\n\n            logits = model.forward(\n                decoder_input_ids=input_ids,\n                input_ids=input_dict[\"input_ids\"][:3],\n            ).logits\n            # logits.shape == torch.Size([3, 4, ...])\n\n            logits_shared_prefix = model(\n                input_ids=input_dict[\"input_ids\"][:1],\n                decoder_input_ids=input_ids_shared_prefix,\n                decoder_attention_mask=mask_shared_prefix,\n            )[0]\n            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n\n            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n\n            # comparing softmax-normalized logits:\n            normalized_0 = F.softmax(out_last_tokens)\n            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_adam_w(self):\n        w = torch.tensor([0.1, -0.2, -0.1], requires_grad=True)\n        target = torch.tensor([0.4, 0.2, -0.5])\n        criterion = nn.MSELoss()\n        # No warmup, constant schedule, no gradient clipping\n        optimizer = AdamW(params=[w], lr=2e-1, weight_decay=0.0)\n        for _ in range(100):\n            loss = criterion(w, target)\n            loss.backward()\n            optimizer.step()\n            w.grad.detach_()  # No zero_grad() function on simple tensors. we do it ourselves.\n            w.grad.zero_()\n        self.assertListAlmostEqual(w.tolist(), [0.4, 0.2, -0.5], tol=1e-2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_adafactor(self):\n        w = torch.tensor([0.1, -0.2, -0.1], requires_grad=True)\n        target = torch.tensor([0.4, 0.2, -0.5])\n        criterion = nn.MSELoss()\n        # No warmup, constant schedule, no gradient clipping\n        optimizer = Adafactor(\n            params=[w],\n            lr=1e-2,\n            eps=(1e-30, 1e-3),\n            clip_threshold=1.0,\n            decay_rate=-0.8,\n            beta1=None,\n            weight_decay=0.0,\n            relative_step=False,\n            scale_parameter=False,\n            warmup_init=False,\n        )\n        for _ in range(1000):\n            loss = criterion(w, target)\n            loss.backward()\n            optimizer.step()\n            w.grad.detach_()  # No zero_grad() function on simple tensors. we do it ourselves.\n            w.grad.zero_()\n        self.assertListAlmostEqual(w.tolist(), [0.4, 0.2, -0.5], tol=1e-2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_peft_add_adapter_training_gradient_checkpointing(self):\n        \"\"\"\n        Simple test that tests if `add_adapter` works as expected when training with\n        gradient checkpointing.\n        \"\"\"\n        from peft import LoraConfig\n\n        for model_id in self.transformers_test_model_ids:\n            for transformers_class in self.transformers_test_model_classes:\n                model = transformers_class.from_pretrained(model_id).to(torch_device)\n\n                peft_config = LoraConfig(init_lora_weights=False)\n\n                model.add_adapter(peft_config)\n\n                self.assertTrue(self._check_lora_correctly_converted(model))\n\n                # When attaching adapters the input embeddings will stay frozen, this will\n                # lead to the output embedding having requires_grad=False.\n                dummy_input = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]]).to(torch_device)\n                frozen_output = model.get_input_embeddings()(dummy_input)\n                self.assertTrue(frozen_output.requires_grad is False)\n\n                model.gradient_checkpointing_enable()\n\n                # Since here we attached the hook, the input should have requires_grad to set\n                # properly\n                non_frozen_output = model.get_input_embeddings()(dummy_input)\n                self.assertTrue(non_frozen_output.requires_grad is True)\n\n                # To repro the Trainer issue\n                dummy_input.requires_grad = False\n\n                for name, param in model.named_parameters():\n                    if \"lora\" in name.lower():\n                        self.assertTrue(param.requires_grad)\n\n                logits = model(dummy_input).logits\n                loss = logits.mean()\n                loss.backward()\n\n                for name, param in model.named_parameters():\n                    if param.requires_grad:\n                        self.assertTrue(\"lora\" in name.lower())\n                        self.assertTrue(param.grad is not None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_peft_add_adapter_modules_to_save(self):\n        \"\"\"\n        Simple test that tests if `add_adapter` works as expected when training with\n        modules to save.\n        \"\"\"\n        from peft import LoraConfig\n        from peft.utils import ModulesToSaveWrapper\n\n        for model_id in self.transformers_test_model_ids:\n            for transformers_class in self.transformers_test_model_classes:\n                dummy_input = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]]).to(torch_device)\n\n                model = transformers_class.from_pretrained(model_id).to(torch_device)\n                peft_config = LoraConfig(init_lora_weights=False, modules_to_save=[\"lm_head\"])\n                model.add_adapter(peft_config)\n                self._check_lora_correctly_converted(model)\n\n                _has_modules_to_save_wrapper = False\n                for name, module in model.named_modules():\n                    if isinstance(module, ModulesToSaveWrapper):\n                        _has_modules_to_save_wrapper = True\n                        self.assertTrue(module.modules_to_save.default.weight.requires_grad)\n                        self.assertTrue(\"lm_head\" in name)\n                        break\n\n                self.assertTrue(_has_modules_to_save_wrapper)\n                state_dict = model.get_adapter_state_dict()\n\n                self.assertTrue(\"lm_head.weight\" in state_dict.keys())\n\n                logits = model(dummy_input).logits\n                loss = logits.mean()\n                loss.backward()\n\n                for _, param in model.named_parameters():\n                    if param.requires_grad:\n                        self.assertTrue(param.grad is not None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_training(self):\n        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.37.0\"):\n            self.skipTest(reason=\"This test requires bitsandbytes>=0.37.0\")\n\n        # Step 1: freeze all parameters\n        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n\n        if torch.cuda.is_available():\n            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n        elif torch.xpu.is_available():\n            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n        else:\n            self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n\n        for param in model.parameters():\n            param.requires_grad = False  # freeze the model - train adapters later\n            # cast all non INT8 parameters to fp32\n            if param.dtype in (torch.float16, torch.bfloat16) and param.__class__.__name__ != \"Params4bit\":\n                param.data = param.data.to(torch.float32)\n\n        # Step 2: add adapters\n        for _, module in model.named_modules():\n            if isinstance(module, OPTAttention):\n                module.q_proj = LoRALayer(module.q_proj, rank=16, dtype=model.dtype)\n                module.k_proj = LoRALayer(module.k_proj, rank=16, dtype=model.dtype)\n                module.v_proj = LoRALayer(module.v_proj, rank=16, dtype=model.dtype)\n\n        # Step 3: dummy batch\n        batch = self.tokenizer(\"Test batch \", return_tensors=\"pt\").to(torch_device)\n\n        # Step 4: Check if the gradient is not None\n        if torch_device in {\"xpu\", \"cpu\"}:\n            # XPU and CPU finetune do not support autocast for now.\n            out = model.forward(**batch)\n            out.logits.norm().backward()\n        else:\n            with torch.autocast(torch_device):\n                out = model.forward(**batch)\n                out.logits.norm().backward()\n\n        for module in model.modules():\n            if isinstance(module, LoRALayer):\n                self.assertTrue(module.adapter[1].weight.grad is not None)\n                self.assertTrue(module.adapter[1].weight.grad.norm().item() > 0)\n            elif isinstance(module, nn.Embedding):\n                self.assertTrue(module.weight.grad is None)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_training(self):\n        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.37.0\"):\n            self.skipTest(reason=\"This test requires bitsandbytes >= 0.37.0\")\n\n        # Step 1: freeze all parameters\n        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n\n        if torch.cuda.is_available():\n            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n        elif torch.xpu.is_available():\n            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n        else:\n            self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n\n        for param in model.parameters():\n            param.requires_grad = False  # freeze the model - train adapters later\n            if param.ndim == 1:\n                # cast the small parameters (e.g. layernorm) to fp32 for stability\n                param.data = param.data.to(torch.float32)\n\n        # Step 2: add adapters\n        for _, module in model.named_modules():\n            if isinstance(module, OPTAttention):\n                module.q_proj = LoRALayer(module.q_proj, rank=16)\n                module.k_proj = LoRALayer(module.k_proj, rank=16)\n                module.v_proj = LoRALayer(module.v_proj, rank=16)\n\n        # Step 3: dummy batch\n        batch = self.tokenizer(\"Test batch \", return_tensors=\"pt\").to(torch_device)\n\n        # Step 4: Check if the gradient is not None\n        with torch.autocast(torch_device):\n            out = model.forward(**batch)\n            out.logits.norm().backward()\n\n        for module in model.modules():\n            if isinstance(module, LoRALayer):\n                self.assertTrue(module.adapter[1].weight.grad is not None)\n                self.assertTrue(module.adapter[1].weight.grad.norm().item() > 0)\n            elif isinstance(module, nn.Embedding):\n                self.assertTrue(module.weight.grad is None)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_model_serialization(self):\n        \"\"\"\n        Simple HQQ LLM save/load test\n        \"\"\"\n        quant_config = HqqConfig(nbits=4, group_size=64)\n\n        hqq_runner = HQQLLMRunner(\n            model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n        )\n\n        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=torch_device)\n\n        with torch.no_grad():\n            logits_ref = hqq_runner.model.forward(input_tensor).logits\n\n        # Save\n        saved_model_id = \"quant_model\"\n        hqq_runner.model.save_pretrained(saved_model_id)\n\n        # Remove old model\n        del hqq_runner.model\n        torch.cuda.empty_cache()\n\n        # Load and check if the logits match\n        model_loaded = AutoModelForCausalLM.from_pretrained(\n            \"quant_model\", torch_dtype=torch.float16, device_map=torch_device, low_cpu_mem_usage=True\n        )\n\n        with torch.no_grad():\n            logits_loaded = model_loaded.forward(input_tensor).logits\n\n        self.assertEqual((logits_loaded - logits_ref).abs().mean().item(), 0)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_cumulative_partitioned_window(alltypes, func, df):\r\n    t = alltypes\r\n    df = df.sort_values(\"string_col\").reset_index(drop=True)\r\n    window = ibis.cumulative_window(group_by=t.string_col)\r\n    f = getattr(t.double_col, func)\r\n    expr = t.select((t.double_col - f().over(window)).name(\"double_col\"))\r\n    result = expr.execute().double_col\r\n    expected = df.groupby(df.string_col).double_col.transform(\r\n        lambda c: c - getattr(c, f\"cum{func}\")()\r\n    )\r\n    tm.assert_series_equal(result, expected)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_cumulative_partitioned_ordered_window(alltypes, func, df):\r\n    t = alltypes\r\n    df = df.sort_values([\"string_col\", \"timestamp_col\"]).reset_index(drop=True)\r\n    window = ibis.cumulative_window(order_by=t.timestamp_col, group_by=t.string_col)\r\n    f = getattr(t.double_col, func)\r\n    expr = t.select((t.double_col - f().over(window)).name(\"double_col\"))\r\n    result = expr.execute().double_col\r\n    method = operator.methodcaller(f\"cum{func}\")\r\n    expected = df.groupby(df.string_col).double_col.transform(lambda c: c - method(c))\r\n    tm.assert_series_equal(result, expected)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def __init__(self, weight, bias, split_num, use_split=False, group_size=0, asym=False):\r\n        super().__init__()\r\n        self.split_num = split_num\r\n        self.outC, self.inC = weight.shape\r\n        split_size = weight.size(1) // split_num // 2 * 2\r\n        self.lm_heads = nn.Sequential()\r\n        self.group_size = group_size\r\n        for i in range(split_num):\r\n            new_linear = torch.nn.Linear(0, 0, bias=False)\r\n            start_idx = i * split_size\r\n            end_idx = (i + 1) * split_size if i < split_num - 1 else weight.size(1)\r\n            new_weight = torch.nn.Parameter(weight[:, start_idx:end_idx],\r\n                                            requires_grad=False)\r\n            new_linear.weight = new_weight\r\n            new_linear.in_features = new_weight.size(1)\r\n            new_linear.out_features = new_weight.size(0)\r\n            self.lm_heads.append(new_linear)\r\n        self.bias = bias\r\n        self.use_split = use_split\r\n        self.asym = asym",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def main():\r\n    parser = argparse.ArgumentParser(description=\"check if the number of lines in html meets expectation\")\r\n    parser.add_argument(\"-c\", \"--csv_file\", type=str, dest=\"csv_name\", help=\"name of csv\")\r\n    parser.add_argument(\"-y\", \"--yaml_file\", type=str, dest=\"yaml_name\", help=\"name of yaml\")\r\n    parser.add_argument(\"-n\", \"--expected_lines\", type=int, dest=\"expected_lines\", help=\"the number of expected csv lines\")\r\n    args = parser.parse_args()\r\n\r\n    csv_file  = [file for file in os.listdir() if file.endswith('.csv') and args.csv_name in file][0]\r\n    csv_dataframe = pd.read_csv(csv_file, index_col=0)\r\n    actual_test_num = len(csv_dataframe)\r\n    actual_test_cases = []\r\n    for index, row in csv_dataframe.iterrows():\r\n        actual_test_cases.append(row['model'] + \":\" + row['input/output tokens'].split('-')[0] + \":\" + str(row['batch_size']))\r\n    if args.yaml_name:\r\n        yaml_name = args.yaml_name\r\n        conf = OmegaConf.load(yaml_name)\r\n        all_test_cases = []\r\n        for model in conf.repo_id:\r\n            for in_out in conf['in_out_pairs']:\r\n                if not OmegaConf.is_list(conf[\"batch_size\"]):\r\n                    batch_list = [conf[\"batch_size\"]]\r\n                else:\r\n                    batch_list = conf[\"batch_size\"]\r\n                for batch_size in batch_list:\r\n                    model_id_input = model + ':' + in_out.split('-')[0] + ':' + str(batch_size)\r\n                    all_test_cases.append(model_id_input)\r\n        exclude_test_cases = []\r\n        if 'exclude' in conf and conf['exclude'] is not None:\r\n            exclude_test_cases = conf['exclude']\r\n        expected_test_num = len(all_test_cases) - len(exclude_test_cases)\r\n        if actual_test_num != expected_test_num:\r\n            print(\"---------------The test cases should be tested!------------\")\r\n            for test_case in all_test_cases:\r\n                if test_case not in actual_test_cases and test_case not in exclude_test_cases:\r\n                    print(test_case)\r\n            raise ValueError(\"The above tests failed. Please check the errors in the log.\")\r\n    elif args.expected_lines:\r\n        expected_test_num = args.expected_lines\r\n        if actual_test_num != expected_test_num:\r\n            raise ValueError(\"Missing some expected test cases! Please check the yaml file and the given expected_lines manually.\")\r\n    else:\r\n        raise ValueError(\"You should provide the value of either yaml_name or expected_lines!\")",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def main():\r\n    parser = argparse.ArgumentParser(description=\"convert .csv file to .html file\")\r\n    parser.add_argument(\"-f\", \"--folder_path\", type=str, dest=\"folder_path\",\r\n                        help=\"The directory which stores the .csv file\", default=\"/home/arda/yibo/BigDL/python/llm/dev/benchmark/harness\")\r\n    parser.add_argument(\"-t\", \"--threshold\", type=float, dest=\"threshold\",\r\n                        help=\"the threshold of highlight values\", default=3.0)\r\n    parser.add_argument(\"-b\", \"--baseline_path\", type=str, dest=\"baseline_path\",\r\n                        help=\"the baseline path which stores the baseline.csv file\")\r\n    args = parser.parse_args()\r\n\r\n    # fp16.csv is downloaded previously under the parent folder of the folder_path\r\n    parent_dir = os.path.dirname((args.folder_path))\r\n    fp16_path = os.path.join(parent_dir, 'fp16.csv')\r\n    fp16_dict = create_fp16_dict(fp16_path)\r\n\r\n    csv_files = []\r\n    for file_name in os.listdir(args.folder_path):\r\n        file_path = os.path.join(args.folder_path, file_name)\r\n        if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\r\n            csv_files.append(file_path)\r\n    csv_files.sort(reverse=True)\r\n\r\n    highlight_threshold=args.threshold\r\n    \r\n    latest_csv = pd.read_csv(csv_files[0], index_col=0)\r\n    daily_html=csv_files[0].split(\".\")[0]+\".html\"\r\n\r\n    # Reset index\r\n    latest_csv.reset_index(inplace=True)\r\n\r\n    diffs_within_normal_range = True\r\n\r\n    # Add display of FP16 values for each model and add percentage difference column\r\n    for task in ['Arc', 'TruthfulQA', 'Winogrande']:\r\n        latest_csv[f'{task}_FP16'] = latest_csv['Model'].apply(lambda model: fp16_dict.get(model, {}).get(task, 'N/A'))\r\n        latest_csv[f'{task}_diff_FP16(%)'] = latest_csv.apply(lambda row: calculate_percentage_difference(row[task], row[f'{task}_FP16']), axis=1)\r\n\r\n    if len(csv_files)>1:\r\n        if args.baseline_path:\r\n            previous_csv = pd.read_csv(args.baseline_path, index_col=0)\r\n        else:\r\n            previous_csv = pd.read_csv(csv_files[1], index_col=0)\r\n\r\n        last_Arc=['']*len(latest_csv.index)\r\n        diff_Arc=['']*len(latest_csv.index)\r\n        last_TruthfulQA=['']*len(latest_csv.index)\r\n        diff_TruthfulQA=['']*len(latest_csv.index)\r\n        last_Winogrande=['']*len(latest_csv.index)\r\n        diff_Winogrande=['']*len(latest_csv.index)\r\n\r\n\r\n        Arc='Arc'\r\n        TruthfulQA='TruthfulQA'\r\n        Winogrande='Winogrande'\r\n\r\n        csv_dict = {}\r\n        for csv_file in csv_files:\r\n            current_csv = pd.read_csv(csv_file, index_col=0)\r\n            for current_csv_ind,current_csv_row in current_csv.iterrows():\r\n                current_csv_model=current_csv_row['Model'].strip()\r\n                current_csv_precision=current_csv_row['Precision'].strip()\r\n                current_csv_model_arc=current_csv_model+'-'+current_csv_precision+'-'+'Arc'\r\n                current_csv_model_truthfulqa=current_csv_model+'-'+current_csv_precision+'-'+'TruthfulQA'\r\n                current_csv_model_winogrande=current_csv_model+'-'+current_csv_precision+'-'+'Winogrande'\r\n                add_to_dict(csv_dict, current_csv_model_arc, current_csv_row[Arc])\r\n                add_to_dict(csv_dict, current_csv_model_truthfulqa, current_csv_row[TruthfulQA])\r\n                add_to_dict(csv_dict, current_csv_model_winogrande, current_csv_row[Winogrande])\r\n\r\n        for latest_csv_ind,latest_csv_row in latest_csv.iterrows():\r\n\r\n            latest_csv_model=latest_csv_row['Model'].strip()\r\n            latest_csv_precision=latest_csv_row['Precision'].strip()\r\n            latest_arc=latest_csv_row[Arc]\r\n            latest_truthfulqa=latest_csv_row[TruthfulQA]\r\n            latest_winogrande=latest_csv_row[Winogrande]\r\n\r\n            in_previous_flag=False\r\n\r\n            for previous_csv_ind,previous_csv_row in previous_csv.iterrows():\r\n\r\n                previous_csv_model=previous_csv_row['Model'].strip()\r\n                previous_csv_precision=previous_csv_row['Precision'].strip()\r\n\r\n                if latest_csv_model==previous_csv_model and latest_csv_precision==previous_csv_precision:\r\n\r\n                    previous_arc=previous_csv_row[Arc]\r\n                    previous_truthfulqa=previous_csv_row[TruthfulQA]\r\n                    previous_winogrande=previous_csv_row[Winogrande]\r\n                    if previous_arc > 0.0 and previous_truthfulqa > 0.0 and previous_winogrande > 0.0:\r\n                        last_Arc[latest_csv_ind]=previous_arc\r\n                        diff_Arc[latest_csv_ind]=round((latest_arc-previous_arc)*100/previous_arc,2)\r\n                        last_TruthfulQA[latest_csv_ind]=previous_truthfulqa\r\n                        diff_TruthfulQA[latest_csv_ind]=round((latest_truthfulqa-previous_truthfulqa)*100/previous_truthfulqa,2)\r\n                        last_Winogrande[latest_csv_ind]=previous_winogrande\r\n                        diff_Winogrande[latest_csv_ind]=round((latest_winogrande-previous_winogrande)*100/previous_winogrande,2)\r\n                        in_previous_flag=True\r\n\r\n            if not in_previous_flag:\r\n                last_Arc[latest_csv_ind]=pd.NA\r\n                diff_Arc[latest_csv_ind]=pd.NA\r\n                last_TruthfulQA[latest_csv_ind]=pd.NA\r\n                diff_TruthfulQA[latest_csv_ind]=pd.NA\r\n                last_Winogrande[latest_csv_ind]=pd.NA\r\n                diff_Winogrande[latest_csv_ind]=pd.NA\r\n\r\n        latest_csv.insert(loc=6,column='last_Arc',value=last_Arc)\r\n        latest_csv.insert(loc=7,column='diff_Arc(%)',value=diff_Arc)\r\n        latest_csv.insert(loc=8,column='last_TruthfulQA',value=last_TruthfulQA)\r\n        latest_csv.insert(loc=9,column='diff_TruthfulQA(%)',value=diff_TruthfulQA)\r\n        latest_csv.insert(loc=10,column='last_Winogrande',value=last_Winogrande)\r\n        latest_csv.insert(loc=11,column='diff_Winogrande(%)',value=diff_Winogrande)\r\n\r\n\r\n        diffs_within_normal_range = is_diffs_within_normal_range(diff_Arc, diff_TruthfulQA, diff_Winogrande, threshold=highlight_threshold)\r\n\r\n        subset1=['diff_Arc(%)','diff_TruthfulQA(%)','diff_Winogrande(%)' ]\r\n        \r\n        columns={'Arc': '{:.2f}', 'TruthfulQA': '{:.2f}', 'Winogrande': '{:.2f}', 'last_Arc': '{:.2f}', 'diff_Arc(%)': '{:.2f}',\r\n                'last_TruthfulQA': '{:.2f}', 'diff_TruthfulQA(%)': '{:.2f}', 'last_Winogrande': '{:.2f}', 'diff_Winogrande(%)': '{:.2f}'}\r\n\r\n        latest_csv.drop('Index', axis=1, inplace=True)\r\n\r\n        styled_df = latest_csv.style.format(columns).applymap(lambda val: highlight_vals(val, max=3.0, is_last=True), subset=subset1)\r\n        for task in ['Arc', 'TruthfulQA', 'Winogrande']:\r\n            styled_df = styled_df.applymap(lambda val: highlight_vals(val, max=highlight_threshold, is_last=False), subset=[f'{task}_diff_FP16(%)'])\r\n        \r\n        # add css style to restrict width and wrap text\r\n        styled_df.set_table_styles([{\r\n            'selector': 'th, td',\r\n            'props': [('max-width', '88px'), ('word-wrap', 'break-word')]\r\n        }], overwrite=False)\r\n        \r\n        html_output = styled_df.set_table_attributes(\"border=1\").to_html()\r\n\r\n        with open(daily_html, 'w') as f:\r\n            f.write(html_output)\r\n    else:\r\n        latest_csv.to_html(daily_html)\r\n\r\n    if args.baseline_path and not diffs_within_normal_range:\r\n        print(\"The diffs are outside the normal range: %\" + str(highlight_threshold))\r\n        return 1 \r\n    return 0",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def main():\r\n    parser = argparse.ArgumentParser(description=\"check if the number of lines in html meets expectation\")\r\n    parser.add_argument(\"-y\", \"--yaml-file\", type=str, dest=\"yaml_name\", help=\"name of yaml\", required=True)\r\n    parser.add_argument(\"--suffix\", type=str, dest=\"file_suffix\", help=\"the suffix of the csv_file\")\r\n    args = parser.parse_args()\r\n\r\n    all_csv_files:list[str] = [file for file in os.listdir() if file.endswith('.csv')]\r\n\r\n    conf = OmegaConf.load(args.yaml_name)\r\n    test_apis : list[str] = conf['test_api']\r\n    in_out_pairs : list[str] = conf['in_out_pairs']\r\n    for api in test_apis:\r\n        for in_out in in_out_pairs:\r\n            csv_name_info:str = f\"{in_out}-{api}-results-\"\r\n            csv_file = [file for file in all_csv_files if (csv_name_info in file) and file.endswith(f\"_{args.file_suffix}.csv\")][0]\r\n    \r\n            csv_dataframe = pd.read_csv(csv_file, index_col=0)\r\n            actual_test_num = len(csv_dataframe)\r\n            actual_test_cases = []\r\n            for index, row in csv_dataframe.iterrows():\r\n                actual_test_cases.append(row['model'] + \":\" + row['input/output tokens'].split('-')[0] + \":\" + str(row['batch_size']))\r\n\r\n            \r\n            all_test_cases = []\r\n            for model in conf.repo_id:\r\n                if not OmegaConf.is_list(conf[\"batch_size\"]):\r\n                    batch_list = [conf[\"batch_size\"]]\r\n                else:\r\n                    batch_list = conf[\"batch_size\"]\r\n                for batch_size in batch_list:\r\n                    model_id_input = model + ':' + in_out.split('-')[0] + ':' + str(batch_size)\r\n                    all_test_cases.append(model_id_input)\r\n            exclude_test_cases = []\r\n            if 'exclude' in conf and conf['exclude'] is not None:\r\n                exclude_test_cases = conf['exclude']\r\n            expected_test_num = len(all_test_cases) - len(exclude_test_cases)\r\n            if actual_test_num != expected_test_num:\r\n                print(\"---------------The test cases should be tested!------------\")\r\n                for test_case in all_test_cases:\r\n                    if test_case not in actual_test_cases and test_case not in exclude_test_cases:\r\n                        print(test_case)\r\n                raise ValueError(\"The above tests failed. Please check the errors in the log.\")\r\n            else:\r\n                print(\"Results integrity check completed\")",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def main():\r\n    parser = argparse.ArgumentParser(description=\"convert .csv file to .html file\")\r\n    parser.add_argument(\"-f\", \"--folder_path\", type=str, dest=\"folder_path\",\r\n                        help=\"The directory which stores the .csv file\", default=\"/home/arda/yibo/BigDL/python/llm/dev/benchmark/harness\")\r\n    parser.add_argument(\"-t\", \"--threshold\", type=float, dest=\"threshold\",\r\n                        help=\"the threshold of highlight values\", default=3.0)\r\n    parser.add_argument(\"-b\", \"--baseline_path\", type=str, dest=\"baseline_path\",\r\n                        help=\"the baseline path which stores the baseline.csv file\")\r\n    args = parser.parse_args()\r\n\r\n    # fp16.csv is downloaded previously under the parent folder of the folder_path\r\n    parent_dir = Path(args.folder_path).parent\r\n    fp16_path = os.path.join(parent_dir, 'fp16.csv')\r\n    fp16_dict = create_fp16_dict(fp16_path)\r\n\r\n    csv_files = []\r\n    for file_name in os.listdir(args.folder_path):\r\n        file_path = os.path.join(args.folder_path, file_name)\r\n        if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\r\n            csv_files.append(file_path)\r\n    csv_files.sort(reverse=True)\r\n\r\n    highlight_threshold=args.threshold\r\n    \r\n    latest_csv = pd.read_csv(csv_files[0], index_col=0)\r\n    daily_html=csv_files[0].split(\".\")[0]+\".html\"\r\n\r\n    # Reset index\r\n    latest_csv.reset_index(inplace=True)\r\n\r\n    diffs_within_normal_range = True\r\n\r\n    # Add display of FP16 values for each model and add percentage difference column\r\n    latest_csv['ppl_result_FP16'] = latest_csv['Model'].apply(lambda model: fp16_dict.get(model, {}).get('ppl_result', 'N/A'))\r\n    latest_csv['ppl_result_diff_FP16(%)'] = latest_csv.apply(lambda row: calculate_percentage_difference(row['ppl_result'], row['ppl_result_FP16']), axis=1)\r\n\r\n    if len(csv_files)>1:\r\n        if args.baseline_path:\r\n            previous_csv = pd.read_csv(args.baseline_path, index_col=0)\r\n        else:\r\n            previous_csv = pd.read_csv(csv_files[1], index_col=0)\r\n\r\n        last_ppl_result=['']*len(latest_csv.index)\r\n        diff_ppl_result=['']*len(latest_csv.index)\r\n\r\n        ppl_result = 'ppl_result'\r\n                \r\n        for latest_csv_ind,latest_csv_row in latest_csv.iterrows():\r\n\r\n            latest_csv_model=latest_csv_row['Model'].strip()\r\n            latest_csv_precision=latest_csv_row['Precision'].strip()\r\n            latest_ppl_result=latest_csv_row[ppl_result]\r\n\r\n            in_previous_flag=False\r\n\r\n            for previous_csv_ind,previous_csv_row in previous_csv.iterrows():\r\n\r\n                previous_csv_model=previous_csv_row['Model'].strip()\r\n                previous_csv_precision=previous_csv_row['Precision'].strip()\r\n\r\n                if latest_csv_model==previous_csv_model and latest_csv_precision==previous_csv_precision:\r\n\r\n                    previous_ppl_result=previous_csv_row[ppl_result] \r\n\r\n                    if previous_ppl_result > 0.0:\r\n                        last_ppl_result[latest_csv_ind]=previous_ppl_result\r\n                        diff_ppl_result[latest_csv_ind]=round((latest_ppl_result-previous_ppl_result)*100/previous_ppl_result,2)\r\n                        in_previous_flag=True\r\n\r\n            if not in_previous_flag:\r\n                last_ppl_result[latest_csv_ind]=pd.NA\r\n                diff_ppl_result[latest_csv_ind]=pd.NA\r\n\r\n\r\n        latest_csv.insert(loc=6,column='last_ppl_result',value=last_ppl_result)\r\n        latest_csv.insert(loc=7,column='ppl_result_diff_last(%)',value=diff_ppl_result)\r\n\r\n\r\n        diffs_within_normal_range = is_diffs_within_normal_range(diff_ppl_result, threshold=highlight_threshold)\r\n        \r\n        columns={'ppl_result': '{:.2f}', 'last_ppl_result': '{:.2f}', 'ppl_result_diff_last(%)': '{:.2f}'}\r\n        latest_csv.drop('Index', axis=1, inplace=True)\r\n\r\n        styled_df = latest_csv.style.format(columns).applymap(lambda val: highlight_vals(val, max=highlight_threshold, is_last=True), subset=['ppl_result_diff_last(%)'])\r\n        styled_df = styled_df.applymap(lambda val: highlight_vals(val, max=highlight_threshold, is_last=False), subset=['ppl_result_diff_FP16(%)'])\r\n \r\n    else:\r\n        columns={'ppl_result': '{:.2f}'}\r\n        latest_csv.drop('Index', axis=1, inplace=True)\r\n        styled_df = latest_csv.style.format(columns).applymap(lambda val: highlight_vals(val, max=highlight_threshold, is_last=False), subset=['ppl_result_diff_FP16(%)'])\r\n \r\n    # add css style to restrict width and wrap text\r\n    styled_df.set_table_styles([{\r\n        'selector': 'th, td',\r\n        'props': [('max-width', '88px'), ('word-wrap', 'break-word')]\r\n    }], overwrite=False)\r\n   \r\n    html_output = styled_df.set_table_attributes(\"border=1\").to_html()\r\n \r\n    with open(daily_html, 'w') as f:\r\n        f.write(html_output)\r\n \r\n    if args.baseline_path and not diffs_within_normal_range:\r\n        print(\"The diffs are outside the normal range: %\" + str(highlight_threshold))\r\n        return 1\r\n    return 0",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def __init__(self, image_channel, block_expansion, num_down_blocks, max_features, reshape_channel, reshape_depth, num_resblocks):\r\n        super(AppearanceFeatureExtractor, self).__init__()\r\n        self.image_channel = image_channel\r\n        self.block_expansion = block_expansion\r\n        self.num_down_blocks = num_down_blocks\r\n        self.max_features = max_features\r\n        self.reshape_channel = reshape_channel\r\n        self.reshape_depth = reshape_depth\r\n\r\n        self.first = SameBlock2d(image_channel, block_expansion, kernel_size=(3, 3), padding=(1, 1))\r\n\r\n        down_blocks = []\r\n        for i in range(num_down_blocks):\r\n            in_features = min(max_features, block_expansion * (2 ** i))\r\n            out_features = min(max_features, block_expansion * (2 ** (i + 1)))\r\n            down_blocks.append(DownBlock2d(in_features, out_features, kernel_size=(3, 3), padding=(1, 1)))\r\n        self.down_blocks = nn.ModuleList(down_blocks)\r\n\r\n        self.second = nn.Conv2d(in_channels=out_features, out_channels=max_features, kernel_size=1, stride=1)\r\n\r\n        self.resblocks_3d = torch.nn.Sequential()\r\n        for i in range(num_resblocks):\r\n            self.resblocks_3d.add_module('3dr' + str(i), ResBlock3d(reshape_channel, kernel_size=3, padding=1))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _get_datas(self, metrics_name: str, params: Optional[Dict[str, Any]] = None) -> pd.DataFrame:\r\n        datas = get_metrics_datas(\r\n            dataset=self.dataset,\r\n            metrics_name=metrics_name,\r\n            field_map=self.field_map,\r\n            params=params or self.params\r\n        )\r\n        return pd.DataFrame(json.loads(json.dumps(datas, cls=_JSONEncoder)))",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train_loop(model, train_loader, num_epochs, optimizer, loss_fn, framework):\r\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    model.to(device)\r\n    start = None\r\n    average_batch_time_per_epoch = []\r\n    for _ in range(num_epochs):\r\n        running_loss = 0.0\r\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\r\n            if batch_idx == 1:\r\n                start = time.time()\r\n            inputs = inputs.to(device)\r\n            targets = targets.to(device)\r\n            # Forward pass\r\n            outputs = model(inputs)\r\n            loss = loss_fn(outputs, targets)\r\n\r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            running_loss += loss.item()\r\n\r\n        end = time.time()\r\n        average_batch_time_per_epoch.append(\r\n            (end - start) / (len(train_loader) - 1)\r\n        )\r\n    average_time = np.mean(average_batch_time_per_epoch)\r\n\r\n    print(f\"Time per batch in {framework}: {average_time:.2f}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def get_model():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Dense(1))\r\n    model.compile(\r\n        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\r\n        loss=\"mean_squared_error\",\r\n        metrics=[\"mean_absolute_error\"],\r\n    )\r\n    return model",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_model_and_layer(self):\r\n        @object_registration.register_keras_serializable(package=\"my_package\")\r\n        class CustomLayer(layers.Layer):\r\n            def __call__(self, inputs):\r\n                return inputs\r\n\r\n        @object_registration.register_keras_serializable(package=\"my_package\")\r\n        class Model(models.Model):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.layer = CustomLayer()\r\n\r\n            @tf.function(input_signature=[tf.TensorSpec([None, 1])])\r\n            def call(self, inputs):\r\n                return self.layer(inputs)\r\n\r\n        model = Model()\r\n        inp = np.array([[1.0]])\r\n        result = model(inp)\r\n        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\r\n        tf.saved_model.save(model, path)\r\n        restored_model = tf.saved_model.load(path)\r\n        self.assertAllClose(\r\n            result,\r\n            restored_model.call(inp),\r\n            rtol=1e-4,\r\n            atol=1e-4,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stop_training_csv(self):\r\n        # Test that using the CSVLogger callback with the TerminateOnNaN\r\n        # callback does not result in invalid CSVs.\r\n        tmpdir = tempfile.TemporaryDirectory()\r\n        csv_logfile = os.path.join(tmpdir.name, \"csv_logger.csv\")\r\n        NUM_CLASSES = 2\r\n        np.random.seed(1337)\r\n        x_train = np.random.random((TRAIN_SAMPLES, INPUT_DIM))\r\n        y_train = np.random.choice(np.arange(NUM_CLASSES), size=TRAIN_SAMPLES)\r\n        x_test = np.random.random((TEST_SAMPLES, INPUT_DIM))\r\n        y_test = np.random.choice(np.arange(NUM_CLASSES), size=TEST_SAMPLES)\r\n\r\n        y_test = numerical_utils.to_categorical(y_test)\r\n        y_train = numerical_utils.to_categorical(y_train)\r\n        model = Sequential()\r\n        initializer = initializers.Constant(value=1e5)\r\n        for _ in range(5):\r\n            model.add(\r\n                layers.Dense(\r\n                    2,\r\n                    activation=\"relu\",\r\n                    kernel_initializer=initializer,\r\n                )\r\n            )\r\n        model.add(layers.Dense(NUM_CLASSES))\r\n        model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\r\n\r\n        history = model.fit(\r\n            x_train,\r\n            y_train,\r\n            batch_size=BATCH_SIZE,\r\n            validation_data=(x_test, y_test),\r\n            callbacks=[\r\n                callbacks.TerminateOnNaN(),\r\n                callbacks.CSVLogger(csv_logfile),\r\n            ],\r\n            epochs=20,\r\n        )\r\n        loss = history.history[\"loss\"]\r\n        self.assertEqual(len(loss), 1)\r\n        self.assertTrue(np.isnan(loss[0]) or np.isinf(loss[0]))\r\n\r\n        values = []\r\n        with open(csv_logfile) as f:\r\n            # On Windows, due to \\r\\n line ends, we may end up reading empty\r\n            # lines after each line. Skip empty lines.\r\n            values = [x for x in csv.reader(f) if x]\r\n        self.assertIn(\"nan\", values[-1], \"NaN not logged in CSV Logger.\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_saving_include_optimizer_false(self):\r\n        tf_keras_model = tf_keras.Sequential()\r\n        tf_keras_model.add(tf_keras.layers.Dense(1))\r\n        tf_keras_model.compile(\"adam\", loss=\"mse\")\r\n        x, y = np.ones((10, 10)), np.ones((10, 1))\r\n        tf_keras_model.fit(x, y)\r\n        ref_output = tf_keras_model(x)\r\n\r\n        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\r\n        tf_keras_model.save(temp_filepath, include_optimizer=False)\r\n        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\r\n        output = loaded(x)\r\n\r\n        # Assert that optimizer does not exist in loaded model\r\n        with self.assertRaises(AttributeError):\r\n            _ = loaded.optimizer\r\n\r\n        # Compare output\r\n        self.assertAllClose(ref_output, output, atol=1e-5)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_saving_include_optimizer_false(self):\r\n        model = models.Sequential()\r\n        model.add(layers.Dense(1))\r\n        model.compile(\"adam\", loss=\"mean_squared_error\")\r\n        x, y = np.ones((10, 10)), np.ones((10, 1))\r\n        model.fit(x, y)\r\n        ref_output = model(x)\r\n\r\n        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\r\n        legacy_h5_format.save_model_to_hdf5(\r\n            model, temp_filepath, include_optimizer=False\r\n        )\r\n        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\r\n        output = loaded(x)\r\n\r\n        # Assert that optimizer does not exist in loaded model\r\n        with self.assertRaises(AttributeError):\r\n            _ = loaded.optimizer\r\n\r\n        # Compare output\r\n        self.assertAllClose(ref_output, output, atol=1e-5)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_compiled_model_with_various_layers(self):\r\n        model = models.Sequential()\r\n        model.add(layers.Dense(2, input_shape=(3,)))\r\n        model.add(layers.RepeatVector(3))\r\n        model.add(layers.TimeDistributed(layers.Dense(3)))\r\n        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\r\n\r\n        tf_keras_model = tf_keras.Sequential()\r\n        tf_keras_model.add(tf_keras.layers.Dense(2, input_shape=(3,)))\r\n        tf_keras_model.add(tf_keras.layers.RepeatVector(3))\r\n        tf_keras_model.add(\r\n            tf_keras.layers.TimeDistributed(tf_keras.layers.Dense(3))\r\n        )\r\n        tf_keras_model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\r\n\r\n        ref_input = np.random.random((1, 3))\r\n        self._check_reloading_model(ref_input, model, tf_keras_model)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multiple_compiles(self):\r\n        # https://github.com/keras-team/keras/issues/20474\r\n        model1 = ExampleModel(units=3)\r\n        model2 = ExampleModel(units=3)\r\n        model1.compile(\r\n            optimizer=optimizers.SGD(),\r\n            loss=losses.MeanSquaredError(),\r\n            metrics=[metrics.MeanSquaredError()],\r\n        )\r\n\r\n        # Combine these 2 models into `combined`.\r\n        inputs = keras.Input(shape=(4,))\r\n        x = model1(inputs)\r\n        outputs = model2(x)\r\n        combined = models.Model(inputs, outputs)\r\n        combined.compile(\r\n            optimizer=optimizers.SGD(),\r\n            loss=losses.MeanSquaredError(),\r\n            metrics=[metrics.MeanSquaredError()],\r\n        )\r\n\r\n        self.assertLen(model1.metrics, 2)\r\n        self.assertIsNotNone(model1._loss_tracker)\r\n        self.assertEqual(model1.metrics[0], model1._loss_tracker)\r\n        self.assertEqual(model1.metrics[1], model1._compile_metrics)\r\n\r\n        # `combined.metrics` will not include `model1.metrics`.\r\n        self.assertLen(combined.metrics, 2)\r\n        self.assertIsNotNone(combined._loss_tracker)\r\n        self.assertEqual(combined.metrics[0], combined._loss_tracker)\r\n        self.assertEqual(combined.metrics[1], combined._compile_metrics)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_softmax_in_graph(self):\r\n        class SoftmaxLayer(keras.Layer):\r\n            def call(self, x):\r\n                return ops.softmax(x, axis=-1)\r\n\r\n        class Model(keras.Model):\r\n            def __init__(self):\r\n                x = keras.Input(shape=(None,))\r\n                y = SoftmaxLayer()(x)\r\n                super().__init__(inputs=x, outputs=y)\r\n\r\n        # Make sure Keras is able to compile the model graph\r\n        model = Model()\r\n        x = ops.array([[1.0, 2.0, 3.0, 4.0]])\r\n        model.predict(x)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nested_trainer_metrics_without_compile(self):\r\n        model = ExampleModel(units=3)\r\n        self.assertLen(model.metrics, 0)\r\n\r\n        inputs = keras.Input((4,))\r\n        outputs = model(inputs)\r\n        outputs = layers.Dense(8)(outputs)\r\n        new_model = models.Model(inputs, outputs)\r\n        new_model.compile(\r\n            optimizer=optimizers.SGD(),\r\n            loss=losses.MeanSquaredError(),\r\n            metrics=[metrics.MeanSquaredError()],\r\n        )\r\n        self.assertLen(new_model.metrics, 2)\r\n        self.assertEqual(new_model.metrics[0], new_model._loss_tracker)\r\n        self.assertEqual(new_model.metrics[1], new_model._compile_metrics)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nested_trainer_metrics(self):\r\n        # https://github.com/keras-team/keras/issues/20188\r\n        model = ExampleModel(units=3)\r\n        model.compile(\r\n            optimizer=optimizers.SGD(),\r\n            loss=losses.MeanSquaredError(),\r\n            metrics=[metrics.MeanSquaredError()],\r\n        )\r\n        self.assertLen(model.metrics, 2)\r\n        self.assertEqual(model.metrics[0], model._loss_tracker)\r\n        self.assertEqual(model.metrics[1], model._compile_metrics)\r\n\r\n        inputs = keras.Input((4,))\r\n        outputs = model(inputs)\r\n        outputs = layers.Dense(8)(outputs)\r\n        new_model = models.Model(inputs, outputs)\r\n        new_model.compile(\r\n            optimizer=optimizers.SGD(),\r\n            loss=losses.MeanSquaredError(),\r\n            metrics=[metrics.MeanSquaredError()],\r\n        )\r\n        self.assertLen(new_model.metrics, 2)\r\n        self.assertEqual(new_model.metrics[0], new_model._loss_tracker)\r\n        self.assertEqual(new_model.metrics[1], new_model._compile_metrics)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_model():\r\n    iris = load_iris()\r\n    y = iris[\"target\"]\r\n    X = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\r\n    dtrain = lgb.Dataset(X, label=y)\r\n\r\n    params = {\"objective\": \"multiclass\", \"metric\": \"softmax\", \"num_class\": 3}\r\n    lgb_model = lgb.train(params=params, train_set=dtrain)\r\n    model_file = os.path.join(model_dir, BST_FILE)\r\n    lgb_model.save_model(model_file)\r\n    model = LightGBMModel(\"model\", model_dir, NTHREAD)\r\n    model.load()\r\n\r\n    request = {\r\n        \"sepal_width_(cm)\": {0: 3.5},\r\n        \"petal_length_(cm)\": {0: 1.4},\r\n        \"petal_width_(cm)\": {0: 0.2},\r\n        \"sepal_length_(cm)\": {0: 5.1},\r\n    }\r\n\r\n    response = model.predict({\"inputs\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    response = model.predict({\"instances\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    request = [\r\n        {\"sepal_width_(cm)\": 3.5},\r\n        {\"petal_length_(cm)\": 1.4},\r\n        {\"petal_width_(cm)\": 0.2},\r\n        {\"sepal_length_(cm)\": 5.1},\r\n    ]\r\n    response = model.predict({\"inputs\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    response = model.predict({\"instances\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    request = [\r\n        {\"sepal_width_(cm)\": 3.5},\r\n        {\"petal_length_(cm)\": 1.4},\r\n        {\"petal_width_(cm)\": 0.2},\r\n    ]\r\n    response = model.predict({\"inputs\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    response = model.predict({\"instances\": [request, request]})\r\n    assert numpy.argmax(response[\"predictions\"][0]) == 0\r\n\r\n    # test v2 handler\r\n    infer_input = InferInput(\r\n        name=\"input-0\",\r\n        shape=[2, 4],\r\n        datatype=\"FP32\",\r\n        data=[[6.8, 2.8, 4.8, 1.6], [6.0, 3.4, 4.5, 1.6]],\r\n    )\r\n    infer_request = InferRequest(model_name=\"model\", infer_inputs=[infer_input])\r\n    infer_response = model.predict(infer_request)\r\n    infer_dict, _ = infer_response.to_rest()\r\n    assert infer_dict[\"outputs\"] == [\r\n        {\r\n            \"name\": \"output-0\",\r\n            \"shape\": [2, 3],\r\n            \"datatype\": \"FP64\",\r\n            \"data\": [\r\n                3.7899802486733807e-06,\r\n                0.9996982074114203,\r\n                0.00029800260833088297,\r\n                5.2172911836629736e-05,\r\n                0.99973341723876,\r\n                0.000214409849403366,\r\n            ],\r\n        }\r\n    ]",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def run(self):\r\n        \"\"\"\r\n        ### Training loop\r\n\r\n        We do full batch training since the dataset is small.\r\n        If we were to sample and train we will have to sample a set of\r\n        nodes for each training step along with the edges that span\r\n        across those selected nodes.\r\n        \"\"\"\r\n        # Move the feature vectors to the device\r\n        features = self.dataset.features.to(self.device)\r\n        # Move the labels to the device\r\n        labels = self.dataset.labels.to(self.device)\r\n        # Move the adjacency matrix to the device\r\n        edges_adj = self.dataset.adj_mat.to(self.device)\r\n        # Add an empty third dimension for the heads\r\n        edges_adj = edges_adj.unsqueeze(-1)\r\n\r\n        # Random indexes\r\n        idx_rand = torch.randperm(len(labels))\r\n        # Nodes for training\r\n        idx_train = idx_rand[:self.training_samples]\r\n        # Nodes for validation\r\n        idx_valid = idx_rand[self.training_samples:]\r\n\r\n        # Training loop\r\n        for epoch in monit.loop(self.epochs):\r\n            # Set the model to training mode\r\n            self.model.train()\r\n            # Make all the gradients zero\r\n            self.optimizer.zero_grad()\r\n            # Evaluate the model\r\n            output = self.model(features, edges_adj)\r\n            # Get the loss for training nodes\r\n            loss = self.loss_func(output[idx_train], labels[idx_train])\r\n            # Calculate gradients\r\n            loss.backward()\r\n            # Take optimization step\r\n            self.optimizer.step()\r\n            # Log the loss\r\n            tracker.add('loss.train', loss)\r\n            # Log the accuracy\r\n            tracker.add('accuracy.train', accuracy(output[idx_train], labels[idx_train]))\r\n\r\n            # Set mode to evaluation mode for validation\r\n            self.model.eval()\r\n\r\n            # No need to compute gradients\r\n            with torch.no_grad():\r\n                # Evaluate the model again\r\n                output = self.model(features, edges_adj)\r\n                # Calculate the loss for validation nodes\r\n                loss = self.loss_func(output[idx_valid], labels[idx_valid])\r\n                # Log the loss\r\n                tracker.add('loss.valid', loss)\r\n                # Log the accuracy\r\n                tracker.add('accuracy.valid', accuracy(output[idx_valid], labels[idx_valid]))\r\n\r\n            # Save logs\r\n            tracker.save()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test():\r\n    device_info = DeviceInfo(use_cuda=True, cuda_device=0)\r\n    print(device_info)\r\n    inp = torch.randn((64, 1, 28, 28), device=device_info.device)\r\n    target = torch.ones(64, dtype=torch.long, device=device_info.device)\r\n    loss_func = nn.CrossEntropyLoss()\r\n    model = Model().to(device_info.device)\r\n    my_adam = MyAdam(model.parameters())\r\n    torch_adam = TorchAdam(model.parameters())\r\n    loss = loss_func(model(inp), target)\r\n    loss.backward()\r\n    with monit.section('MyAdam warmup'):\r\n        for i in range(100):\r\n            my_adam.step()\r\n    with monit.section('MyAdam'):\r\n        for i in range(1000):\r\n            my_adam.step()\r\n    with monit.section('TorchAdam warmup'):\r\n        for i in range(100):\r\n            torch_adam.step()\r\n    with monit.section('TorchAdam'):\r\n        for i in range(1000):\r\n            torch_adam.step()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train(self, samples: Dict[str, torch.Tensor]):\r\n        \"\"\"\r\n        ### Train the model based on samples\r\n        \"\"\"\r\n\r\n        # It learns faster with a higher number of epochs,\r\n        #  but becomes a little unstable; that is,\r\n        #  the average episode reward does not monotonically increase\r\n        #  over time.\r\n        # May be reducing the clipping range might solve it.\r\n        for _ in range(self.epochs()):\r\n            # shuffle for each epoch\r\n            indexes = torch.randperm(self.batch_size)\r\n\r\n            # for each mini batch\r\n            for start in range(0, self.batch_size, self.mini_batch_size):\r\n                # get mini batch\r\n                end = start + self.mini_batch_size\r\n                mini_batch_indexes = indexes[start: end]\r\n                mini_batch = {}\r\n                for k, v in samples.items():\r\n                    mini_batch[k] = v[mini_batch_indexes]\r\n\r\n                # train\r\n                loss = self._calc_loss(mini_batch)\r\n\r\n                # Set learning rate\r\n                for pg in self.optimizer.param_groups:\r\n                    pg['lr'] = self.learning_rate()\r\n                # Zero out the previously calculated gradients\r\n                self.optimizer.zero_grad()\r\n                # Calculate gradients\r\n                loss.backward()\r\n                # Clip gradients\r\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\r\n                # Update parameters based on gradients\r\n                self.optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _synthetic_experiment(is_adam: bool):\r\n    \"\"\"\r\n    ## Synthetic Experiment\r\n\r\n    This is the synthetic experiment described in the paper,\r\n    that shows a scenario where *Adam* fails.\r\n\r\n    The paper (and Adam) formulates the problem of optimizing as\r\n    minimizing the expected value of a function, $\\mathbb{E}[f(\\theta)]$\r\n    with respect to the parameters $\\theta$.\r\n    In the stochastic training setting we do not get hold of the function $f$\r\n    it self; that is,\r\n    when you are optimizing a NN $f$ would be the function on  entire\r\n    batch of data.\r\n    What we actually evaluate is a mini-batch so the actual function is\r\n    realization of the stochastic $f$.\r\n    This is why we are talking about an expected value.\r\n    So let the function realizations be $f_1, f_2, ..., f_T$ for each time step\r\n    of training.\r\n\r\n    We measure the performance of the optimizer as the regret,\r\n    $$R(T) = \\sum_{t=1}^T \\big[ f_t(\\theta_t) - f_t(\\theta^*) \\big]$$\r\n    where $\\theta_t$ is the parameters at time step $t$, and  $\\theta^*$ is the\r\n    optimal parameters that minimize $\\mathbb{E}[f(\\theta)]$.\r\n\r\n    Now lets define the synthetic problem,\r\n\r\n    \\begin{align}\r\n    f_t(x) =\r\n    \\begin{cases}\r\n    1010 x,  & \\text{for } t \\mod 101 = 1 \\\\\r\n    -10  x, & \\text{otherwise}\r\n    \\end{cases}\r\n    \\end{align}\r\n\r\n    where $-1 \\le x \\le +1$.\r\n    The optimal solution is $x = -1$.\r\n\r\n    This code will try running *Adam* and *AMSGrad* on this problem.\r\n    \"\"\"\r\n\r\n    # Define $x$ parameter\r\n    x = nn.Parameter(torch.tensor([.0]))\r\n    # Optimal, $x^* = -1$\r\n    x_star = nn.Parameter(torch.tensor([-1]), requires_grad=False)\r\n\r\n    def func(t: int, x_: nn.Parameter):\r\n        \"\"\"\r\n        ### $f_t(x)$\r\n        \"\"\"\r\n        if t % 101 == 1:\r\n            return (1010 * x_).sum()\r\n        else:\r\n            return (-10 * x_).sum()\r\n\r\n    # Initialize the relevant optimizer\r\n    if is_adam:\r\n        optimizer = Adam([x], lr=1e-2, betas=(0.9, 0.99))\r\n    else:\r\n        optimizer = AMSGrad([x], lr=1e-2, betas=(0.9, 0.99))\r\n    # $R(T)$\r\n    total_regret = 0\r\n\r\n    from labml import monit, tracker, experiment\r\n\r\n    # Create experiment to record results\r\n    with experiment.record(name='synthetic', comment='Adam' if is_adam else 'AMSGrad'):\r\n        # Run for $10^7$ steps\r\n        for step in monit.loop(10_000_000):\r\n            # $f_t(\\theta_t) - f_t(\\theta^*)$\r\n            regret = func(step, x) - func(step, x_star)\r\n            # $R(T) = \\sum_{t=1}^T \\big[ f_t(\\theta_t) - f_t(\\theta^*) \\big]$\r\n            total_regret += regret.item()\r\n            # Track results every 1,000 steps\r\n            if (step + 1) % 1000 == 0:\r\n                tracker.save(loss=regret, x=x, regret=total_regret / (step + 1))\r\n            # Calculate gradients\r\n            regret.backward()\r\n            # Optimize\r\n            optimizer.step()\r\n            # Clear gradients\r\n            optimizer.zero_grad()\r\n\r\n            # Make sure $-1 \\le x \\le +1$\r\n            x.data.clamp_(-1., +1.)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __init__(self, *,\r\n                 updates: int, epochs: int,\r\n                 n_workers: int, worker_steps: int, mini_batch_size: int,\r\n                 update_target_model: int,\r\n                 learning_rate: FloatDynamicHyperParam,\r\n                 ):\r\n        # number of workers\r\n        self.n_workers = n_workers\r\n        # steps sampled on each update\r\n        self.worker_steps = worker_steps\r\n        # number of training iterations\r\n        self.train_epochs = epochs\r\n\r\n        # number of updates\r\n        self.updates = updates\r\n        # size of mini batch for training\r\n        self.mini_batch_size = mini_batch_size\r\n\r\n        # update target network every 250 update\r\n        self.update_target_model = update_target_model\r\n\r\n        # learning rate\r\n        self.learning_rate = learning_rate\r\n\r\n        # exploration as a function of updates\r\n        self.exploration_coefficient = Piecewise(\r\n            [\r\n                (0, 1.0),\r\n                (25_000, 0.1),\r\n                (self.updates / 2, 0.01)\r\n            ], outside_value=0.01)\r\n\r\n        # $\\beta$ for replay buffer as a function of updates\r\n        self.prioritized_replay_beta = Piecewise(\r\n            [\r\n                (0, 0.4),\r\n                (self.updates, 1)\r\n            ], outside_value=1)\r\n\r\n        # Replay buffer with $\\alpha = 0.6$. Capacity of the replay buffer must be a power of 2.\r\n        self.replay_buffer = ReplayBuffer(2 ** 14, 0.6)\r\n\r\n        # Model for sampling and training\r\n        self.model = Model().to(device)\r\n        # target model to get $\\textcolor{orange}Q(s';\\textcolor{orange}{\\theta_i^{-}})$\r\n        self.target_model = Model().to(device)\r\n\r\n        # create workers\r\n        self.workers = [Worker(47 + i) for i in range(self.n_workers)]\r\n\r\n        # initialize tensors for observations\r\n        self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8)\r\n\r\n        # reset the workers\r\n        for worker in self.workers:\r\n            worker.child.send((\"reset\", None))\r\n\r\n        # get the initial observations\r\n        for i, worker in enumerate(self.workers):\r\n            self.obs[i] = worker.child.recv()\r\n\r\n        # loss function\r\n        self.loss_func = QFuncLoss(0.99)\r\n        # optimizer\r\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2.5e-4)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train(self):\r\n        \"\"\"\r\n        ### Train the model\r\n        \"\"\"\r\n\r\n        # Loop for the given number of epochs\r\n        for _ in monit.loop(self.epochs):\r\n            # Iterate over the minibatches\r\n            for i, batch in monit.enum('Train', self.dataloader):\r\n                # Move data to the device\r\n                data, target = batch[0].to(self.device), batch[1].to(self.device)\r\n\r\n                # Set tracker step, as the number of characters trained on\r\n                tracker.add_global_step(data.shape[0] * data.shape[1])\r\n\r\n                # Set model state to training\r\n                self.model.train()\r\n                # Evaluate the model\r\n                output = self.model(data)\r\n\r\n                # Calculate loss\r\n                loss = self.loss_func(output.view(-1, output.shape[-1]), target.view(-1))\r\n                # Log the loss\r\n                tracker.add(\"loss.train\", loss)\r\n\r\n                # Calculate gradients\r\n                loss.backward()\r\n                # Clip gradients\r\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\r\n                # Take optimizer step\r\n                self.optimizer.step()\r\n                # Log the model parameters and gradients\r\n                if (i + 1) % 100 == 0:\r\n                    tracker.add('model', self.model)\r\n                # Clear the gradients\r\n                self.optimizer.zero_grad()\r\n\r\n                # Generate a sample\r\n                if (i + 1) % 100 == 0:\r\n                    self.model.eval()\r\n                    with torch.no_grad():\r\n                        self.sample()\r\n\r\n                # Save the tracked metrics\r\n                if (i + 1) % 10 == 0:\r\n                    tracker.save()\r\n\r\n            # Save the model\r\n            experiment.save_checkpoint()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(output_dir: str = \"data\"):\n    \"\"\"Download and prepare the dataset for use.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    kaggle.api.dataset_download_files(\"tboyle10/medicaltranscriptions\", \"data\", unzip=True)\n    mt_samples = preprocess(pd.read_csv(\"data/mtsamples.csv\"))\n    conversations = get_conversations(mt_samples)\n    random.shuffle(conversations)\n    train_limit = math.ceil(len(conversations) * 0.6)\n    dev_limit = math.ceil(len(conversations) * 0.8)\n    train, validation, test = (\n        conversations[:train_limit],\n        conversations[train_limit:dev_limit],\n        conversations[dev_limit:],\n    )\n    splits = {\"train\": train, \"validation\": validation, \"test\": test}\n    for split in [\"train\", \"validation\", \"test\"]:\n        with open(f\"{output_dir}/mt_note_generation_{split}.jsonl\", \"w\") as f:\n            for conversation in splits[split]:\n                f.write(f\"{json.dumps({'conversation': conversation})}\\n\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train():\r\n    L.seed_everything(42)\r\n\r\n    batch_size = 8\r\n    micro_batch_size = 1\r\n\r\n    max_steps = 100\r\n\r\n    dataset = WikiText2()\r\n    dataloader = DataLoader(dataset, num_workers=8, batch_size=micro_batch_size)\r\n\r\n    with torch.device(\"meta\"):\r\n        model = Transformer(\r\n            vocab_size=dataset.vocab_size,\r\n            nlayers=16,\r\n            nhid=4096,\r\n            ninp=1024,\r\n            nhead=32,\r\n        )\r\n\r\n    strategy = ModelParallelStrategy(data_parallel_size=4, tensor_parallel_size=1, parallelize_fn=configure_model)\r\n\r\n    fabric = L.Fabric(precision=\"bf16-true\", strategy=strategy)\r\n    fabric.launch()\r\n\r\n    model = fabric.setup(model)\r\n\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n    dataloader = fabric.setup_dataloaders(dataloader)\r\n\r\n    iterable = tqdm(enumerate(dataloader), total=len(dataloader)) if fabric.is_global_zero else enumerate(dataloader)\r\n\r\n    steps = 0\r\n\r\n    for i, batch in iterable:\r\n        input, target = batch\r\n\r\n        is_accumulating = i % (batch_size // micro_batch_size) != 0\r\n\r\n        with fabric.no_backward_sync(model, enabled=is_accumulating):\r\n            output = model(input, target)\r\n            loss = F.nll_loss(output, target.view(-1))\r\n            fabric.backward(loss)\r\n\r\n        if not is_accumulating:\r\n            fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n            steps += 1\r\n\r\n        if fabric.is_global_zero:\r\n            iterable.set_postfix_str(f\"train_loss={loss.item():.2f}\")\r\n\r\n        if steps == max_steps:\r\n            break\r\n\r\n    fabric.print(torch.cuda.memory_summary())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def main(\r\n    ways=5,\r\n    shots=5,\r\n    meta_lr=0.003,\r\n    fast_lr=0.5,\r\n    meta_batch_size=32,\r\n    adaptation_steps=1,\r\n    num_iterations=60000,\r\n    seed=42,\r\n):\r\n    # Create the Fabric object\r\n    # Arguments get parsed from the command line, see `fabric run --help`\r\n    fabric = Fabric()\r\n\r\n    meta_batch_size = meta_batch_size // fabric.world_size\r\n    seed_everything(seed + fabric.global_rank)\r\n\r\n    # Create Tasksets using the benchmark interface\r\n    tasksets = l2l.vision.benchmarks.get_tasksets(\r\n        # 'mini-imagenet' works too, but you need to download it manually due to license restrictions of ImageNet\r\n        \"omniglot\",\r\n        train_ways=ways,\r\n        train_samples=2 * shots,\r\n        test_ways=ways,\r\n        test_samples=2 * shots,\r\n        num_tasks=20000,\r\n        root=\"data\",\r\n    )\r\n\r\n    # Create model\r\n    # model = l2l.vision.models.MiniImagenetCNN(ways)\r\n    model = l2l.vision.models.OmniglotFC(28**2, ways)\r\n    model = fabric.to_device(model)\r\n    maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\r\n    optimizer = torch.optim.Adam(maml.parameters(), meta_lr)\r\n    optimizer = cherry.optim.Distributed(maml.parameters(), opt=optimizer, sync=1)\r\n\r\n    # model, optimizer = fabric.setup(model, optimizer)\r\n\r\n    optimizer.sync_parameters()\r\n    loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\r\n\r\n    for iteration in range(num_iterations):\r\n        optimizer.zero_grad()\r\n        meta_train_error = 0.0\r\n        meta_train_accuracy = 0.0\r\n        meta_valid_error = 0.0\r\n        meta_valid_accuracy = 0.0\r\n        for task in range(meta_batch_size):\r\n            # Compute meta-training loss\r\n            learner = maml.clone()\r\n            batch = fabric.to_device(tasksets.train.sample())\r\n            evaluation_error, evaluation_accuracy = fast_adapt(\r\n                batch,\r\n                learner,\r\n                loss,\r\n                adaptation_steps,\r\n                shots,\r\n                ways,\r\n            )\r\n            fabric.backward(evaluation_error)\r\n            meta_train_error += evaluation_error.item()\r\n            meta_train_accuracy += evaluation_accuracy.item()\r\n\r\n            # Compute meta-validation loss\r\n            learner = maml.clone()\r\n            batch = fabric.to_device(tasksets.validation.sample())\r\n            evaluation_error, evaluation_accuracy = fast_adapt(\r\n                batch,\r\n                learner,\r\n                loss,\r\n                adaptation_steps,\r\n                shots,\r\n                ways,\r\n            )\r\n            meta_valid_error += evaluation_error.item()\r\n            meta_valid_accuracy += evaluation_accuracy.item()\r\n\r\n        # Print some metrics\r\n        fabric.print(\"\\n\")\r\n        fabric.print(\"Iteration\", iteration)\r\n        fabric.print(\"Meta Train Error\", meta_train_error / meta_batch_size)\r\n        fabric.print(\"Meta Train Accuracy\", meta_train_accuracy / meta_batch_size)\r\n        fabric.print(\"Meta Valid Error\", meta_valid_error / meta_batch_size)\r\n        fabric.print(\"Meta Valid Accuracy\", meta_valid_accuracy / meta_batch_size)\r\n\r\n        # Average the accumulated gradients and optimize\r\n        for p in maml.parameters():\r\n            p.grad.data.mul_(1.0 / meta_batch_size)\r\n        optimizer.step()  # averages gradients across all workers\r\n\r\n    meta_test_error = 0.0\r\n    meta_test_accuracy = 0.0\r\n    for task in range(meta_batch_size):\r\n        # Compute meta-testing loss\r\n        learner = maml.clone()\r\n        batch = fabric.to_device(tasksets.test.sample())\r\n        evaluation_error, evaluation_accuracy = fast_adapt(\r\n            batch,\r\n            learner,\r\n            loss,\r\n            adaptation_steps,\r\n            shots,\r\n            ways,\r\n        )\r\n        meta_test_error += evaluation_error.item()\r\n        meta_test_accuracy += evaluation_accuracy.item()\r\n    fabric.print(\"Meta Test Error\", meta_test_error / meta_batch_size)\r\n    fabric.print(\"Meta Test Accuracy\", meta_test_accuracy / meta_batch_size)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(\r\n    agent: PPOAgent,\r\n    optimizer: torch.optim.Optimizer,\r\n    data: dict[str, Tensor],\r\n    logger: SummaryWriter,\r\n    global_step: int,\r\n    args: argparse.Namespace,\r\n):\r\n    indexes = list(range(data[\"obs\"].shape[0]))\r\n    if args.share_data:\r\n        sampler = DistributedSampler(\r\n            indexes,\r\n            num_replicas=distributed.get_world_size(),\r\n            rank=distributed.get_rank(),\r\n            shuffle=True,\r\n            seed=args.seed,\r\n        )\r\n    else:\r\n        sampler = RandomSampler(indexes)\r\n    sampler = BatchSampler(sampler, batch_size=args.per_rank_batch_size, drop_last=False)\r\n    per_epoch_losses = torch.tensor([0.0, 0.0, 0.0], device=data[\"obs\"].device)\r\n    for epoch in range(args.update_epochs):\r\n        if args.share_data:\r\n            sampler.sampler.set_epoch(epoch)\r\n        for batch_idxes in sampler:\r\n            _, newlogprob, entropy, newvalue = agent(data[\"obs\"][batch_idxes], data[\"actions\"].long()[batch_idxes])\r\n            logratio = newlogprob - data[\"logprobs\"][batch_idxes]\r\n            ratio = logratio.exp()\r\n\r\n            advantages = data[\"advantages\"][batch_idxes]\r\n            if args.normalize_advantages:\r\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\r\n\r\n            # Policy loss\r\n            pg_loss = policy_loss(advantages, ratio, args.clip_coef)\r\n            per_epoch_losses[0] += pg_loss.detach()\r\n\r\n            # Value loss\r\n            v_loss = value_loss(\r\n                newvalue,\r\n                data[\"values\"][batch_idxes],\r\n                data[\"returns\"][batch_idxes],\r\n                args.clip_coef,\r\n                args.clip_vloss,\r\n                args.vf_coef,\r\n            )\r\n            per_epoch_losses[1] += v_loss.detach()\r\n\r\n            # Entropy loss\r\n            ent_loss = entropy_loss(entropy, args.ent_coef)\r\n            per_epoch_losses[2] += ent_loss.detach()\r\n\r\n            # Overall loss\r\n            loss = pg_loss + ent_loss + v_loss\r\n\r\n            optimizer.zero_grad(set_to_none=True)\r\n            loss.backward()\r\n            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\r\n            optimizer.step()\r\n\r\n        # Log\r\n        distributed.reduce(per_epoch_losses, dst=0)\r\n        if logger is not None:\r\n            per_epoch_losses = per_epoch_losses / (len(sampler) * distributed.get_world_size())\r\n            logger.add_scalar(\"Loss/policy_loss\", per_epoch_losses[0], global_step)\r\n            logger.add_scalar(\"Loss/value_loss\", per_epoch_losses[1], global_step)\r\n            logger.add_scalar(\"Loss/entropy_loss\", per_epoch_losses[2], global_step)\r\n        per_epoch_losses.fill_(0)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def validation_step(self, batch, batch_idx):\r\n        # 1. Forward pass:\r\n        x, y = batch\r\n        y_logits = self.forward(x)\r\n        y_scores = torch.sigmoid(y_logits)\r\n        y_true = y.view((-1, 1)).type_as(x)\r\n\r\n        # 2. Compute loss\r\n        self.log(\"val_loss\", self.loss(y_logits, y_true), prog_bar=True)\r\n\r\n        # 3. Compute accuracy:\r\n        self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def training_step(self, batch, batch_idx):\r\n        # 1. Forward pass:\r\n        x, y = batch\r\n        y_logits = self.forward(x)\r\n        y_scores = torch.sigmoid(y_logits)\r\n        y_true = y.view((-1, 1)).type_as(x)\r\n\r\n        # 2. Compute loss\r\n        train_loss = self.loss(y_logits, y_true)\r\n\r\n        # 3. Compute accuracy:\r\n        self.log(\"train_acc\", self.train_acc(y_scores, y_true.int()), prog_bar=True)\r\n\r\n        return train_loss",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def vanilla_loop(cls_model, idx, device_type: str = \"cuda\", num_epochs=10):\r\n    device = torch.device(device_type)\r\n    # set seed\r\n    seed_everything(idx)\r\n\r\n    # init model parts\r\n    model = cls_model()\r\n    dl = model.train_dataloader()\r\n    optimizer = model.configure_optimizers()\r\n\r\n    # model to GPU\r\n    model = model.to(device)\r\n\r\n    epoch_losses = []\r\n    # as the first run is skipped, no need to run it long\r\n    for epoch in range(num_epochs if idx > 0 else 1):\r\n        # run through full training set\r\n        for j, batch in enumerate(dl):\r\n            batch = [x.to(device) for x in batch]\r\n            loss_dict = model.training_step(batch, j)\r\n            loss = loss_dict[\"loss\"]\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n        # track last epoch loss\r\n        epoch_losses.append(loss.item())\r\n\r\n    return epoch_losses[-1], _hook_memory()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_setup_module_move_to_device(setup_method, move_to_device, accelerator, initial_device, target_device):\r\n    \"\"\"Test that `move_to_device` leads to parameters being moved to the correct device and that the device attributes\r\n    on the wrapper are updated.\"\"\"\r\n    initial_device = torch.device(initial_device)\r\n    target_device = torch.device(target_device)\r\n    expected_device = target_device if move_to_device else initial_device\r\n\r\n    fabric = Fabric(accelerator=accelerator, devices=1)\r\n    model = nn.Linear(1, 2)\r\n    model.to(initial_device)\r\n    setup_method = getattr(fabric, setup_method)\r\n    fabric_model = setup_method(model, move_to_device=move_to_device)\r\n\r\n    # all parameters on the expected device\r\n    assert all(param.device == expected_device for param in model.parameters())\r\n    assert all(param.device == expected_device for param in fabric_model.parameters())\r\n\r\n    assert fabric_model.device == expected_device\r\n    assert fabric.device == target_device\r\n\r\n    # edge case: model has no parameters\r\n    model = nn.Sequential()\r\n    fabric_model = setup_method(model, move_to_device=move_to_device)\r\n    assert fabric_model.device == target_device if move_to_device else torch.device(\"cpu\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_load_quantized_checkpoint(tmp_path):\r\n    \"\"\"Test that a checkpoint saved from a quantized model can be loaded back into a quantized model.\"\"\"\r\n\r\n    class Model(torch.nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.linear = torch.nn.Linear(16, 16, bias=False)\r\n\r\n        def forward(self, x):\r\n            return self.linear(x)\r\n\r\n    fabric = Fabric(accelerator=\"cuda\", devices=1, plugins=BitsandbytesPrecision(\"nf4-dq\"))\r\n    model = Model()\r\n    model = fabric.setup(model)\r\n    model(torch.randn(2, 16, device=fabric.device))\r\n    state_dict = model.state_dict()\r\n    # The checkpoint contains quantized weights\r\n    assert state_dict[\"linear.weight\"].dtype == torch.uint8\r\n    assert state_dict[\"linear.weight\"].shape == (128, 1)\r\n    torch.save(state_dict, tmp_path / \"checkpoint.pt\")\r\n\r\n    fabric = Fabric(accelerator=\"cuda\", devices=1, plugins=BitsandbytesPrecision(\"nf4-dq\"))\r\n    model = Model()\r\n    model = fabric.setup(model)\r\n    state_dict = torch.load(tmp_path / \"checkpoint.pt\", weights_only=True)\r\n    model.load_state_dict(state_dict)\r\n    assert model.linear.weight.dtype == torch.uint8\r\n    assert model.linear.weight.shape == (128, 1)\r\n    # Shapes match during forward (weight is being dequantized during forward)\r\n    model(torch.randn(2, 16, device=fabric.device))\r\n\r\n    # Test with lazy load (LitGPT uses this)\r\n    # TODO: Replace `_lazy_load` with `torch.load(..., mmap=True)` in LitGPT\r\n    state_dict = _lazy_load(tmp_path / \"checkpoint.pt\")\r\n    model.load_state_dict(state_dict)\r\n    assert model.linear.weight.dtype == torch.uint8\r\n    assert model.linear.weight.shape == (128, 1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_deepspeed_with_bfloat16_precision():\r\n    \"\"\"Test that the DeepSpeed strategy works with bfloat16 precision.\"\"\"\r\n\r\n    class Model(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.layer = nn.Linear(32, 2)\r\n\r\n        def forward(self, x):\r\n            assert x.dtype == torch.bfloat16\r\n            return self.layer(x)\r\n\r\n    fabric = Fabric(accelerator=\"cuda\", devices=2, strategy=\"deepspeed_stage_3\", precision=\"bf16-mixed\")\r\n    assert isinstance(fabric._strategy.precision, DeepSpeedPrecision)\r\n    assert fabric._strategy.precision.precision == \"bf16-mixed\"\r\n    assert fabric._strategy.config[\"zero_optimization\"][\"stage\"] == 3\r\n    fabric.launch()\r\n\r\n    with fabric.init_module():\r\n        model = Model()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\r\n    model, optimizer = fabric.setup(model, optimizer)\r\n    assert fabric._strategy.config[\"bf16\"][\"enabled\"]\r\n    assert model.layer.weight.dtype == torch.bfloat16\r\n\r\n    batch = torch.rand(2, 32, device=fabric.device)\r\n    assert batch.dtype == torch.float32\r\n    loss = model(batch).sum()\r\n    fabric.backward(loss)\r\n    optimizer.step()\r\n    optimizer.zero_grad()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_activation_checkpointing():\r\n    \"\"\"Test that the FSDP strategy can apply activation checkpointing to the given layers.\"\"\"\r\n\r\n    class Block1(nn.Linear):\r\n        pass\r\n\r\n    class Block2(nn.Linear):\r\n        pass\r\n\r\n    class Model(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.layer0 = nn.Sequential(Block1(4, 4), Block1(5, 5))\r\n            self.layer1 = Block2(2, 2)\r\n            self.layer2 = nn.Linear(3, 3)\r\n\r\n    strategy = FSDPStrategy(activation_checkpointing_policy={Block1})\r\n    assert set(strategy._activation_checkpointing_kwargs) == {\"auto_wrap_policy\"}\r\n    assert isinstance(strategy._activation_checkpointing_kwargs[\"auto_wrap_policy\"], ModuleWrapPolicy)\r\n\r\n    strategy = FSDPStrategy(activation_checkpointing_policy=ModuleWrapPolicy({Block1, Block2}))\r\n    assert set(strategy._activation_checkpointing_kwargs) == {\"auto_wrap_policy\"}\r\n    assert isinstance(strategy._activation_checkpointing_kwargs[\"auto_wrap_policy\"], ModuleWrapPolicy)\r\n\r\n    strategy._parallel_devices = [torch.device(\"cuda\", 0)]\r\n    with (\r\n        mock.patch(\"torch.distributed.fsdp.FullyShardedDataParallel\", new=MagicMock),\r\n        mock.patch(\r\n            \"torch.distributed.algorithms._checkpoint.checkpoint_wrapper.apply_activation_checkpointing\"\r\n        ) as apply_mock,\r\n    ):\r\n        wrapped = strategy.setup_module(Model())\r\n    apply_mock.assert_called_with(wrapped, checkpoint_wrapper_fn=ANY, **strategy._activation_checkpointing_kwargs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_reapply_compile():\r\n    \"\"\"Test that Fabric can rewrap a compiled module such that compilation happens over the FSDP-wrapper.\"\"\"\r\n    strategy = FSDPStrategy(auto_wrap_policy=always_wrap_policy)\r\n    fabric = Fabric(accelerator=\"cuda\", devices=2, strategy=strategy)\r\n    fabric.launch()\r\n\r\n    model = BoringModel()\r\n    compile_kwargs = {\"mode\": \"reduce-overhead\"}\r\n    compiled_model = torch.compile(model, **compile_kwargs)\r\n    torch.compile.reset_mock()\r\n\r\n    fabric_model = fabric.setup(compiled_model, _reapply_compile=True)\r\n\r\n    assert isinstance(fabric_model._forward_module, OptimizedModule)\r\n    assert isinstance(fabric_model._forward_module._orig_mod, FullyShardedDataParallel)\r\n\r\n    # Assert we called compile again with the same arguments, but on the FSDP-wrapped module\r\n    torch.compile.assert_called_with(fabric_model._forward_module._orig_mod, **compile_kwargs)\r\n\r\n    assert fabric_model._original_module == model\r\n    assert fabric_model._forward_module._orig_mod.module == model\r\n    assert fabric_model.device == fabric.device\r\n\r\n    # Smoke-testing forward to ensure we don't get compilation errors\r\n    for _ in range(3):\r\n        loss = fabric_model(torch.randn(2, 32, device=fabric.device)).sum()\r\n        fabric.backward(loss)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_tensor_parallel(distributed, compile):\r\n    from torch.distributed._tensor import DTensor\r\n\r\n    parallelize = _parallelize_feed_forward_tp\r\n\r\n    if compile:\r\n        parallelize = _parallelize_with_compile(parallelize)\r\n\r\n    strategy = ModelParallelStrategy(parallelize_fn=parallelize)\r\n    fabric = Fabric(accelerator=\"auto\", devices=2, strategy=strategy)\r\n    fabric.launch()\r\n\r\n    fabric.seed_everything(0)\r\n\r\n    with fabric.init_module(empty_init=True):\r\n        model = FeedForward()\r\n\r\n    model = fabric.setup(model)\r\n    optimizer = torch.optim.AdamW(model.parameters())\r\n    optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n    device_mesh = fabric.strategy.device_mesh\r\n    assert all(tensor.device_mesh == device_mesh[\"tensor_parallel\"] for tensor in optimizer.param_groups[0][\"params\"])\r\n    assert all(isinstance(weight, DTensor) for weight in model.parameters())\r\n    assert model.w1.weight.device_mesh == device_mesh[\"tensor_parallel\"]\r\n\r\n    dataset_size = 6\r\n    dataset = RandomDataset(32, dataset_size)\r\n    dataloader = DataLoader(dataset, batch_size=2)\r\n    dataloader = fabric.setup_dataloaders(dataloader)\r\n\r\n    # No data sharding, all GPUs get the same input inside a TP group\r\n    assert len(dataloader) == dataset_size // dataloader.batch_size\r\n    assert isinstance(dataloader.sampler, DistributedSampler)\r\n\r\n    for _, batch in enumerate(dataloader):\r\n        # All batches must be identical across TP group\r\n        batches = fabric.all_gather(batch)\r\n        assert all(torch.equal(batches[0], batches[i]) for i in range(1, len(batches)))\r\n\r\n        output = model(batch)\r\n        fabric.backward(output.sum())\r\n        assert isinstance(model.w1.weight.grad, DTensor)\r\n        assert model.w1.weight.grad.device_mesh == device_mesh[\"tensor_parallel\"]\r\n        optimizer.step()\r\n        optimizer.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_fsdp2_tensor_parallel(distributed, compile):\r\n    from torch.distributed._tensor import DTensor\r\n\r\n    parallelize = _parallelize_feed_forward_fsdp2_tp\r\n\r\n    if compile:\r\n        parallelize = _parallelize_with_compile(parallelize)\r\n\r\n    strategy = ModelParallelStrategy(\r\n        parallelize_fn=_parallelize_feed_forward_fsdp2_tp,\r\n        data_parallel_size=2,\r\n        tensor_parallel_size=2,\r\n    )\r\n    fabric = Fabric(accelerator=\"auto\", devices=4, strategy=strategy)\r\n    fabric.launch()\r\n\r\n    fabric.seed_everything(0)\r\n\r\n    with fabric.init_module(empty_init=True):\r\n        model = FeedForward()\r\n\r\n    model = fabric.setup(model)\r\n    optimizer = torch.optim.AdamW(model.parameters())\r\n    optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n    assert all(isinstance(weight, DTensor) for weight in model.parameters())\r\n    assert all(isinstance(tensor, DTensor) for tensor in optimizer.param_groups[0][\"params\"])\r\n    assert model.w1.weight.device_mesh.ndim == 2\r\n    assert model.w1.weight.device_mesh.size(0) == 2\r\n    assert model.w1.weight.device_mesh.size(1) == 2\r\n    assert all(weight.device.type != \"meta\" for weight in model.parameters())\r\n    assert all(tensor.device_mesh.ndim == 2 for tensor in optimizer.param_groups[0][\"params\"])\r\n    assert all(tensor.device.type != \"meta\" for tensor in optimizer.param_groups[0][\"params\"])\r\n\r\n    dataset_size = 8\r\n    dataset = RandomDataset(32, dataset_size)\r\n    dataloader = DataLoader(dataset, batch_size=2)\r\n    dataloader = fabric.setup_dataloaders(dataloader)\r\n\r\n    # No data sharding across TP dimension, sharding across data-parallel dimension only\r\n    device_mesh = fabric.strategy.device_mesh\r\n    dp_mesh = device_mesh[\"data_parallel\"]\r\n    tp_mesh = device_mesh[\"tensor_parallel\"]\r\n    assert len(dataloader) == dataset_size // dataloader.batch_size // dp_mesh.size()\r\n    assert isinstance(dataloader.sampler, DistributedSampler)\r\n\r\n    for _, batch in enumerate(dataloader):\r\n        batches = fabric.all_gather(batch)\r\n        # Batches across the TP dimension must be identical\r\n        batches_tp = batches[tp_mesh.mesh]\r\n        assert all(torch.equal(batches_tp[0], batches_tp[i]) for i in range(1, len(batches_tp)))\r\n        # Batches across the DP dimension must be different\r\n        batches_dp = batches[dp_mesh.mesh]\r\n        assert all(not torch.equal(batches_dp[0], batches_dp[i]) for i in range(1, len(batches_dp)))\r\n\r\n        output = model(batch)\r\n        fabric.backward(output.sum())\r\n        assert isinstance(model.w1.weight.grad, DTensor)\r\n        assert model.w1.weight.grad.device_mesh == device_mesh\r\n        optimizer.step()\r\n        optimizer.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_module_init_context(precision, expected_dtype, tmp_path):\r\n    \"\"\"Test that the module under the init-context gets moved to the right device and dtype.\"\"\"\r\n\r\n    class Model(BoringModel):\r\n        def configure_optimizers(self):\r\n            return torch.optim.Adam(self.parameters(), lr=1e-2)\r\n\r\n        def on_train_start(self):\r\n            # Parameters get sharded in `FSDPStrategy.setup()` and moved to the target device\r\n            assert self.layer.weight.device == torch.device(\"cuda\", self.local_rank)\r\n            assert self.layer.weight.dtype == expected_dtype\r\n            optimizer = self.optimizers(use_pl_optimizer=False)\r\n            assert optimizer.param_groups[0][\"params\"][0].device.type == \"cuda\"\r\n\r\n    def _run_setup_assertions(empty_init, expected_device):\r\n        trainer = Trainer(\r\n            default_root_dir=tmp_path,\r\n            accelerator=\"cuda\",\r\n            devices=2,\r\n            strategy=FSDPStrategy(auto_wrap_policy={torch.nn.Linear}),\r\n            precision=precision,\r\n            max_steps=1,\r\n            barebones=True,\r\n            enable_checkpointing=False,\r\n            logger=False,\r\n        )\r\n        with trainer.init_module(empty_init=empty_init):\r\n            model = Model()\r\n\r\n        # The model is on the CPU/meta-device until after `FSDPStrategy.setup()`\r\n        assert model.layer.weight.device == expected_device\r\n        assert model.layer.weight.dtype == expected_dtype\r\n        trainer.fit(model)\r\n\r\n    # Case 1: No empty init\r\n    _run_setup_assertions(empty_init=False, expected_device=torch.device(\"cpu\"))\r\n\r\n    # Case 2: Empty-init with meta device\r\n    _run_setup_assertions(empty_init=True, expected_device=torch.device(\"meta\"))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _run_setup_assertions(empty_init, expected_device):\r\n        trainer = Trainer(\r\n            default_root_dir=tmp_path,\r\n            accelerator=\"cuda\",\r\n            devices=2,\r\n            strategy=FSDPStrategy(auto_wrap_policy={torch.nn.Linear}),\r\n            precision=precision,\r\n            max_steps=1,\r\n            barebones=True,\r\n            enable_checkpointing=False,\r\n            logger=False,\r\n        )\r\n        with trainer.init_module(empty_init=empty_init):\r\n            model = Model()\r\n\r\n        # The model is on the CPU/meta-device until after `FSDPStrategy.setup()`\r\n        assert model.layer.weight.device == expected_device\r\n        assert model.layer.weight.dtype == expected_dtype\r\n        trainer.fit(model)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fsdp_v1_modules_unsupported():\r\n    \"\"\"Test that the strategy won't allow setting up a module wrapped with the legacy FSDP API.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n    class Model(LightningModule):\r\n        def configure_model(self):\r\n            pass\r\n\r\n    model = Model()\r\n    model.modules = Mock(return_value=[Mock(spec=FullyShardedDataParallel)])\r\n    strategy = ModelParallelStrategy()\r\n    strategy.model = model\r\n    strategy._lightning_module = model\r\n    strategy._accelerator = Mock()\r\n\r\n    with pytest.raises(TypeError, match=\"only supports the new FSDP2 APIs in PyTorch >= 2.4\"):\r\n        strategy.setup(Mock())",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_activation_checkpointing():\r\n    \"\"\"Test that the FSDP strategy can apply activation checkpointing to the given layers.\"\"\"\r\n\r\n    class Block1(nn.Linear):\r\n        pass\r\n\r\n    class Block2(nn.Linear):\r\n        pass\r\n\r\n    class Model(BoringModel):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.layer0 = nn.Sequential(Block1(4, 4), Block1(5, 5))\r\n            self.layer1 = Block2(2, 2)\r\n            self.layer2 = nn.Linear(3, 3)\r\n\r\n    strategy = FSDPStrategy(activation_checkpointing_policy={Block1})\r\n    assert set(strategy._activation_checkpointing_kwargs) == {\"auto_wrap_policy\"}\r\n    assert isinstance(strategy._activation_checkpointing_kwargs[\"auto_wrap_policy\"], ModuleWrapPolicy)\r\n\r\n    strategy = FSDPStrategy(activation_checkpointing_policy=ModuleWrapPolicy({Block1, Block2}))\r\n    assert set(strategy._activation_checkpointing_kwargs) == {\"auto_wrap_policy\"}\r\n    assert isinstance(strategy._activation_checkpointing_kwargs[\"auto_wrap_policy\"], ModuleWrapPolicy)\r\n\r\n    model = Model()\r\n    strategy._parallel_devices = [torch.device(\"cuda\", 0)]\r\n    strategy._lightning_module = model\r\n    strategy._process_group = Mock()\r\n    with (\r\n        mock.patch(\"torch.distributed.fsdp.FullyShardedDataParallel\", new=MagicMock),\r\n        mock.patch(\r\n            \"torch.distributed.algorithms._checkpoint.checkpoint_wrapper.apply_activation_checkpointing\"\r\n        ) as apply_mock,\r\n    ):\r\n        wrapped = strategy._setup_model(model)\r\n    apply_mock.assert_called_with(wrapped, checkpoint_wrapper_fn=ANY, **strategy._activation_checkpointing_kwargs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plateau_scheduler_lr_step_interval_updated_after_saving(tmp_path, save_on_train_epoch_end):\r\n    batches = 4\r\n    trainer = Trainer(\r\n        default_root_dir=tmp_path,\r\n        enable_progress_bar=False,\r\n        logger=False,\r\n        max_epochs=1,\r\n        limit_train_batches=batches,\r\n        limit_val_batches=1,\r\n        callbacks=[ModelCheckpoint(dirpath=tmp_path, save_on_train_epoch_end=save_on_train_epoch_end)],\r\n    )\r\n\r\n    class Model(BoringModel):\r\n        def training_step(self, batch, batch_idx):\r\n            self.log(\"foo\", batch_idx)\r\n            return super().training_step(batch, batch_idx)\r\n\r\n        def configure_optimizers(self):\r\n            optimizer = torch.optim.Adam(self.parameters())\r\n\r\n            lr_scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\r\n            lr_scheduler_config_1 = {\"scheduler\": lr_scheduler1, \"interval\": \"step\", \"monitor\": \"foo\"}\r\n\r\n            lr_scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n            lr_scheduler_config_2 = {\"scheduler\": lr_scheduler2, \"interval\": \"step\"}\r\n            return [optimizer], [lr_scheduler_config_1, lr_scheduler_config_2]\r\n\r\n        def on_save_checkpoint(self, checkpoint):\r\n            lr_scheduler_config_1 = checkpoint[\"lr_schedulers\"][0]\r\n            last_epoch = lr_scheduler_config_1[\"last_epoch\"]\r\n            assert last_epoch == batches - (not save_on_train_epoch_end)  # last epoch starts at 0\r\n\r\n            lr_scheduler_config_2 = checkpoint[\"lr_schedulers\"][1]\r\n            assert lr_scheduler_config_2[\"_step_count\"] - 1 == batches  # step count starts at 1\r\n\r\n            self.on_save_checkpoint_called = True\r\n\r\n    model = Model()\r\n    trainer.fit(model)\r\n    assert model.on_save_checkpoint_called",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_manual_optimization_with_non_pytorch_scheduler(automatic_optimization):\r\n    \"\"\"In manual optimization, the user can provide a custom scheduler that doesn't follow PyTorch's interface.\"\"\"\r\n\r\n    class IncompatibleScheduler:\r\n        def __init__(self, optimizer):\r\n            self.optimizer = optimizer\r\n\r\n        def state_dict(self):\r\n            return {}\r\n\r\n        def load_state_dict(self, _):\r\n            pass\r\n\r\n    class Model(BoringModel):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.automatic_optimization = automatic_optimization\r\n\r\n        def configure_optimizers(self):\r\n            optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n            scheduler = IncompatibleScheduler(optimizer)\r\n            return [optimizer], [scheduler]\r\n\r\n    model = Model()\r\n    trainer = Trainer(accelerator=\"cpu\", max_epochs=0, logger=False, enable_checkpointing=False)\r\n    if automatic_optimization:\r\n        with pytest.raises(MisconfigurationException, match=\"doesn't follow PyTorch's LRScheduler\"):\r\n            trainer.fit(model)\r\n    else:\r\n        # No error for manual optimization\r\n        trainer.fit(model)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grad_norm_with_double_dtype():\r\n    class Model(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            dtype = torch.double\r\n            self.param = nn.Parameter(torch.tensor(1.0, dtype=dtype))\r\n            # grad norm of this would become infinite\r\n            self.param.grad = torch.tensor(1e23, dtype=dtype)\r\n\r\n    model = Model()\r\n    norms = grad_norm(model, 2)\r\n    assert all(torch.isfinite(torch.tensor(v)) for v in norms.values()), norms",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grad_norm(norm_type, expected):\r\n    \"\"\"Test utility function for computing the p-norm of individual parameter groups and norm in total.\"\"\"\r\n\r\n    class Model(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.param0 = nn.Parameter(torch.rand(3))\r\n            self.param1 = nn.Parameter(torch.rand(2, 1))\r\n            self.param0.grad = torch.tensor([-1.0, 2.0, -3.0])\r\n            self.param1.grad = torch.tensor([[-4.0], [5.0]])\r\n            # param without grad should not contribute to norm\r\n            self.param2 = nn.Parameter(torch.rand(1))\r\n\r\n    model = Model()\r\n    norms = grad_norm(model, norm_type)\r\n\r\n    assert norms.keys() == expected.keys()\r\n    for k in norms:\r\n        assert norms[k] == pytest.approx(expected[k])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_module_mode_restore_new_module(caplog):\r\n    \"\"\"Test that restoring ignores newly added submodules after the module was captured.\"\"\"\r\n\r\n    class Model(torch.nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.child = torch.nn.Linear(2, 2)\r\n\r\n    model = Model()\r\n    mode = _ModuleMode()\r\n    mode.capture(model)\r\n    model.child.eval()\r\n    model.new_child = torch.nn.Linear(2, 2)\r\n    with caplog.at_level(logging.DEBUG, logger=\"lightning.pytorch.utilities.model_helpers\"):\r\n        mode.restore(model)\r\n    assert \"Restoring training mode on module 'new_child' not possible\" in caplog.text",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_summary_restores_module_mode():\r\n    \"\"\"Test that the model summary puts the model in `eval()` mode, but restores the original mode once finished.\"\"\"\r\n\r\n    class Model(LightningModule):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.layer1 = torch.nn.Linear(2, 2)\r\n            self.layer2 = torch.nn.Linear(2, 2)\r\n            self.example_input_array = torch.rand(2, 2)\r\n\r\n        def forward(self, x):\r\n            assert not self.training\r\n            assert not self.layer1.training\r\n            assert not self.layer2.training\r\n            return self.layer2(self.layer1(x))\r\n\r\n    model = Model()\r\n    model.layer1.train()\r\n    model.layer2.eval()\r\n    summarize(model)\r\n    assert model.training\r\n    assert model.layer1.training\r\n    assert not model.layer2.training",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_module_mode_restore_missing_module():\r\n    \"\"\"Test that restoring still works if the module drops a layer after it was captured.\"\"\"\r\n\r\n    class Model(torch.nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.child1 = torch.nn.Linear(2, 2)\r\n            self.child2 = torch.nn.Linear(2, 2)\r\n\r\n    model = Model()\r\n    mode = _ModuleMode()\r\n    mode.capture(model)\r\n    model.child1.eval()\r\n    del model.child2\r\n    assert not hasattr(model, \"child2\")\r\n    mode.restore(model)\r\n    assert model.child1.training",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(\r\n        self,\r\n        net,\r\n        image_size,\r\n        hidden_layer = -2,\r\n        projection_hidden_size = 256,\r\n        num_classes_K = 65336,\r\n        projection_layers = 4,\r\n        student_temp = 0.9,\r\n        teacher_temp = 0.04,\r\n        local_upper_crop_scale = 0.4,\r\n        global_lower_crop_scale = 0.5,\r\n        moving_average_decay = 0.9,\r\n        center_moving_average_decay = 0.9,\r\n        augment_fn = None,\r\n        augment_fn2 = None\r\n    ):\r\n        super().__init__()\r\n        self.net = net\r\n\r\n        # default BYOL augmentation\r\n\r\n        DEFAULT_AUG = torch.nn.Sequential(\r\n            RandomApply(\r\n                T.ColorJitter(0.8, 0.8, 0.8, 0.2),\r\n                p = 0.3\r\n            ),\r\n            T.RandomGrayscale(p=0.2),\r\n            T.RandomHorizontalFlip(),\r\n            RandomApply(\r\n                T.GaussianBlur((3, 3), (1.0, 2.0)),\r\n                p = 0.2\r\n            ),\r\n            T.Normalize(\r\n                mean=torch.tensor([0.485, 0.456, 0.406]),\r\n                std=torch.tensor([0.229, 0.224, 0.225])),\r\n        )\r\n\r\n        self.augment1 = default(augment_fn, DEFAULT_AUG)\r\n        self.augment2 = default(augment_fn2, DEFAULT_AUG)\r\n\r\n        # local and global crops\r\n\r\n        self.local_crop = T.RandomResizedCrop((image_size, image_size), scale = (0.05, local_upper_crop_scale))\r\n        self.global_crop = T.RandomResizedCrop((image_size, image_size), scale = (global_lower_crop_scale, 1.))\r\n\r\n        self.student_encoder = NetWrapper(net, num_classes_K, projection_hidden_size, projection_layers, layer = hidden_layer)\r\n\r\n        self.teacher_encoder = None\r\n        self.teacher_ema_updater = EMA(moving_average_decay)\r\n\r\n        self.register_buffer('teacher_view_centers', torch.zeros(1, num_classes_K))\r\n        self.register_buffer('last_teacher_view_centers',  torch.zeros(1, num_classes_K))\r\n\r\n        self.register_buffer('teacher_region_centers', torch.zeros(1, num_classes_K))\r\n        self.register_buffer('last_teacher_region_centers',  torch.zeros(1, num_classes_K))\r\n\r\n        self.teacher_centering_ema_updater = EMA(center_moving_average_decay)\r\n\r\n        self.student_temp = student_temp\r\n        self.teacher_temp = teacher_temp\r\n\r\n        # get device of network and make wrapper same device\r\n        device = get_module_device(net)\r\n        self.to(device)\r\n\r\n        # send a mock image tensor to instantiate singleton parameters\r\n        self.forward(torch.randn(2, 3, image_size, image_size, device=device))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(\r\n        self,\r\n        net,\r\n        image_size,\r\n        hidden_layer = -2,\r\n        projection_hidden_size = 256,\r\n        num_classes_K = 65336,\r\n        projection_layers = 4,\r\n        student_temp = 0.9,\r\n        teacher_temp = 0.04,\r\n        local_upper_crop_scale = 0.4,\r\n        global_lower_crop_scale = 0.5,\r\n        moving_average_decay = 0.9,\r\n        center_moving_average_decay = 0.9,\r\n        augment_fn = None,\r\n        augment_fn2 = None\r\n    ):\r\n        super().__init__()\r\n        self.net = net\r\n\r\n        # default BYOL augmentation\r\n\r\n        DEFAULT_AUG = torch.nn.Sequential(\r\n            RandomApply(\r\n                T.ColorJitter(0.8, 0.8, 0.8, 0.2),\r\n                p = 0.3\r\n            ),\r\n            T.RandomGrayscale(p=0.2),\r\n            T.RandomHorizontalFlip(),\r\n            RandomApply(\r\n                T.GaussianBlur((3, 3), (1.0, 2.0)),\r\n                p = 0.2\r\n            ),\r\n            T.Normalize(\r\n                mean=torch.tensor([0.485, 0.456, 0.406]),\r\n                std=torch.tensor([0.229, 0.224, 0.225])),\r\n        )\r\n\r\n        self.augment1 = default(augment_fn, DEFAULT_AUG)\r\n        self.augment2 = default(augment_fn2, DEFAULT_AUG)\r\n\r\n        # local and global crops\r\n\r\n        self.local_crop = T.RandomResizedCrop((image_size, image_size), scale = (0.05, local_upper_crop_scale))\r\n        self.global_crop = T.RandomResizedCrop((image_size, image_size), scale = (global_lower_crop_scale, 1.))\r\n\r\n        self.student_encoder = NetWrapper(net, num_classes_K, projection_hidden_size, projection_layers, layer = hidden_layer)\r\n\r\n        self.teacher_encoder = None\r\n        self.teacher_ema_updater = EMA(moving_average_decay)\r\n\r\n        self.register_buffer('teacher_centers', torch.zeros(1, num_classes_K))\r\n        self.register_buffer('last_teacher_centers',  torch.zeros(1, num_classes_K))\r\n\r\n        self.teacher_centering_ema_updater = EMA(center_moving_average_decay)\r\n\r\n        self.student_temp = student_temp\r\n        self.teacher_temp = teacher_temp\r\n\r\n        # get device of network and make wrapper same device\r\n        device = get_module_device(net)\r\n        self.to(device)\r\n\r\n        # send a mock image tensor to instantiate singleton parameters\r\n        self.forward(torch.randn(2, 3, image_size, image_size, device=device))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def visualize_hierarchical_documents(\r\n    topic_model,\r\n    docs: List[str],\r\n    hierarchical_topics: pd.DataFrame,\r\n    topics: List[int] = None,\r\n    embeddings: np.ndarray = None,\r\n    reduced_embeddings: np.ndarray = None,\r\n    sample: Union[float, int] = None,\r\n    hide_annotations: bool = False,\r\n    hide_document_hover: bool = True,\r\n    nr_levels: int = 10,\r\n    level_scale: str = \"linear\",\r\n    custom_labels: Union[bool, str] = False,\r\n    title: str = \"<b>Hierarchical Documents and Topics</b>\",\r\n    width: int = 1200,\r\n    height: int = 750,\r\n) -> go.Figure:\r\n    \"\"\"Visualize documents and their topics in 2D at different levels of hierarchy.\r\n\r\n    Arguments:\r\n        topic_model: A fitted BERTopic instance.\r\n        docs: The documents you used when calling either `fit` or `fit_transform`\r\n        hierarchical_topics: A dataframe that contains a hierarchy of topics\r\n                             represented by their parents and their children\r\n        topics: A selection of topics to visualize.\r\n                Not to be confused with the topics that you get from `.fit_transform`.\r\n                For example, if you want to visualize only topics 1 through 5:\r\n                `topics = [1, 2, 3, 4, 5]`.\r\n        embeddings: The embeddings of all documents in `docs`.\r\n        reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\r\n        sample: The percentage of documents in each topic that you would like to keep.\r\n                Value can be between 0 and 1. Setting this value to, for example,\r\n                0.1 (10% of documents in each topic) makes it easier to visualize\r\n                millions of documents as a subset is chosen.\r\n        hide_annotations: Hide the names of the traces on top of each cluster.\r\n        hide_document_hover: Hide the content of the documents when hovering over\r\n                             specific points. Helps to speed up generation of visualizations.\r\n        nr_levels: The number of levels to be visualized in the hierarchy. First, the distances\r\n                   in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances.\r\n                   Then, for each list of distances, the merged topics are selected that have a\r\n                   distance less or equal to the maximum distance of the selected list of distances.\r\n                   NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to\r\n                   the length of `hierarchical_topics`.\r\n        level_scale: Whether to apply a linear or logarithmic (log) scale levels of the distance\r\n                     vector. Linear scaling will perform an equal number of merges at each level\r\n                     while logarithmic scaling will perform more mergers in earlier levels to\r\n                     provide more resolution at higher levels (this can be used for when the number\r\n                     of topics is large).\r\n        custom_labels: If bool, whether to use custom topic labels that were defined using\r\n                       `topic_model.set_topic_labels`.\r\n                       If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\r\n                       NOTE: Custom labels are only generated for the original\r\n                       un-merged topics.\r\n        title: Title of the plot.\r\n        width: The width of the figure.\r\n        height: The height of the figure.\r\n\r\n    Examples:\r\n    To visualize the topics simply run:\r\n\r\n    ```python\r\n    topic_model.visualize_hierarchical_documents(docs, hierarchical_topics)\r\n    ```\r\n\r\n    Do note that this re-calculates the embeddings and reduces them to 2D.\r\n    The advised and preferred pipeline for using this function is as follows:\r\n\r\n    ```python\r\n    from sklearn.datasets import fetch_20newsgroups\r\n    from sentence_transformers import SentenceTransformer\r\n    from bertopic import BERTopic\r\n    from umap import UMAP\r\n\r\n    # Prepare embeddings\r\n    docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\r\n    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\r\n    embeddings = sentence_model.encode(docs, show_progress_bar=False)\r\n\r\n    # Train BERTopic and extract hierarchical topics\r\n    topic_model = BERTopic().fit(docs, embeddings)\r\n    hierarchical_topics = topic_model.hierarchical_topics(docs)\r\n\r\n    # Reduce dimensionality of embeddings, this step is optional\r\n    # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\r\n\r\n    # Run the visualization with the original embeddings\r\n    topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings)\r\n\r\n    # Or, if you have reduced the original embeddings already:\r\n    topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\r\n    ```\r\n\r\n    Or if you want to save the resulting figure:\r\n\r\n    ```python\r\n    fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\r\n    fig.write_html(\"path/to/file.html\")\r\n    ```\r\n\r\n    Note:\r\n        This visualization was inspired by the scatter plot representation of Doc2Map:\r\n        https://github.com/louisgeisler/Doc2Map\r\n\r\n    <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\"\r\n    style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe>\r\n    \"\"\"\r\n    topic_per_doc = topic_model.topics_\r\n\r\n    # Sample the data to optimize for visualization and dimensionality reduction\r\n    if sample is None or sample > 1:\r\n        sample = 1\r\n\r\n    indices = []\r\n    for topic in set(topic_per_doc):\r\n        s = np.where(np.array(topic_per_doc) == topic)[0]\r\n        size = len(s) if len(s) < 100 else int(len(s) * sample)\r\n        indices.extend(np.random.choice(s, size=size, replace=False))\r\n    indices = np.array(indices)\r\n\r\n    df = pd.DataFrame({\"topic\": np.array(topic_per_doc)[indices]})\r\n    df[\"doc\"] = [docs[index] for index in indices]\r\n    df[\"topic\"] = [topic_per_doc[index] for index in indices]\r\n\r\n    # Extract embeddings if not already done\r\n    if sample is None:\r\n        if embeddings is None and reduced_embeddings is None:\r\n            embeddings_to_reduce = topic_model._extract_embeddings(df.doc.to_list(), method=\"document\")\r\n        else:\r\n            embeddings_to_reduce = embeddings\r\n    else:\r\n        if embeddings is not None:\r\n            embeddings_to_reduce = embeddings[indices]\r\n        elif embeddings is None and reduced_embeddings is None:\r\n            embeddings_to_reduce = topic_model._extract_embeddings(df.doc.to_list(), method=\"document\")\r\n\r\n    # Reduce input embeddings\r\n    if reduced_embeddings is None:\r\n        umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric=\"cosine\").fit(embeddings_to_reduce)\r\n        embeddings_2d = umap_model.embedding_\r\n    elif sample is not None and reduced_embeddings is not None:\r\n        embeddings_2d = reduced_embeddings[indices]\r\n    elif sample is None and reduced_embeddings is not None:\r\n        embeddings_2d = reduced_embeddings\r\n\r\n    # Combine data\r\n    df[\"x\"] = embeddings_2d[:, 0]\r\n    df[\"y\"] = embeddings_2d[:, 1]\r\n\r\n    # Create topic list for each level, levels are created by calculating the distance\r\n    distances = hierarchical_topics.Distance.to_list()\r\n    if level_scale == \"log\" or level_scale == \"logarithmic\":\r\n        log_indices = (\r\n            np.round(\r\n                np.logspace(\r\n                    start=math.log(1, 10),\r\n                    stop=math.log(len(distances) - 1, 10),\r\n                    num=nr_levels,\r\n                )\r\n            )\r\n            .astype(int)\r\n            .tolist()\r\n        )\r\n        log_indices.reverse()\r\n        max_distances = [distances[i] for i in log_indices]\r\n    elif level_scale == \"lin\" or level_scale == \"linear\":\r\n        max_distances = [\r\n            distances[indices[-1]] for indices in np.array_split(range(len(hierarchical_topics)), nr_levels)\r\n        ][::-1]\r\n    else:\r\n        raise ValueError(\"level_scale needs to be one of 'log' or 'linear'\")\r\n\r\n    for index, max_distance in enumerate(max_distances):\r\n        # Get topics below `max_distance`\r\n        mapping = {topic: topic for topic in df.topic.unique()}\r\n        selection = hierarchical_topics.loc[hierarchical_topics.Distance <= max_distance, :]\r\n        selection.Parent_ID = selection.Parent_ID.astype(int)\r\n        selection = selection.sort_values(\"Parent_ID\")\r\n\r\n        for row in selection.iterrows():\r\n            for topic in row[1].Topics:\r\n                mapping[topic] = row[1].Parent_ID\r\n\r\n        # Make sure the mappings are mapped 1:1\r\n        mappings = [True for _ in mapping]\r\n        while any(mappings):\r\n            for i, (key, value) in enumerate(mapping.items()):\r\n                if value in mapping.keys() and key != value:\r\n                    mapping[key] = mapping[value]\r\n                else:\r\n                    mappings[i] = False\r\n\r\n        # Create new column\r\n        df[f\"level_{index+1}\"] = df.topic.map(mapping)\r\n        df[f\"level_{index+1}\"] = df[f\"level_{index+1}\"].astype(int)\r\n\r\n    # Prepare topic names of original and merged topics\r\n    trace_names = []\r\n    topic_names = {}\r\n    for topic in range(hierarchical_topics.Parent_ID.astype(int).max()):\r\n        if topic < hierarchical_topics.Parent_ID.astype(int).min():\r\n            if topic_model.get_topic(topic):\r\n                if isinstance(custom_labels, str):\r\n                    trace_name = f\"{topic}_\" + \"_\".join(\r\n                        list(zip(*topic_model.topic_aspects_[custom_labels][topic]))[0][:3]\r\n                    )\r\n                elif topic_model.custom_labels_ is not None and custom_labels:\r\n                    trace_name = topic_model.custom_labels_[topic + topic_model._outliers]\r\n                else:\r\n                    trace_name = f\"{topic}_\" + \"_\".join([word[:20] for word, _ in topic_model.get_topic(topic)][:3])\r\n                topic_names[topic] = {\r\n                    \"trace_name\": trace_name[:40],\r\n                    \"plot_text\": trace_name[:40],\r\n                }\r\n                trace_names.append(trace_name)\r\n        else:\r\n            trace_name = (\r\n                f\"{topic}_\"\r\n                + hierarchical_topics.loc[hierarchical_topics.Parent_ID == str(topic), \"Parent_Name\"].values[0]\r\n            )\r\n            plot_text = \"_\".join([name[:20] for name in trace_name.split(\"_\")[:3]])\r\n            topic_names[topic] = {\r\n                \"trace_name\": trace_name[:40],\r\n                \"plot_text\": plot_text[:40],\r\n            }\r\n            trace_names.append(trace_name)\r\n\r\n    # Prepare traces\r\n    all_traces = []\r\n    for level in range(len(max_distances)):\r\n        traces = []\r\n\r\n        # Outliers\r\n        if topic_model._outliers:\r\n            traces.append(\r\n                go.Scattergl(\r\n                    x=df.loc[(df[f\"level_{level+1}\"] == -1), \"x\"],\r\n                    y=df.loc[df[f\"level_{level+1}\"] == -1, \"y\"],\r\n                    mode=\"markers+text\",\r\n                    name=\"other\",\r\n                    hoverinfo=\"text\",\r\n                    hovertext=df.loc[(df[f\"level_{level+1}\"] == -1), \"doc\"] if not hide_document_hover else None,\r\n                    showlegend=False,\r\n                    marker=dict(color=\"#CFD8DC\", size=5, opacity=0.5),\r\n                )\r\n            )\r\n\r\n        # Selected topics\r\n        if topics:\r\n            selection = df.loc[(df.topic.isin(topics)), :]\r\n            unique_topics = sorted([int(topic) for topic in selection[f\"level_{level+1}\"].unique()])\r\n        else:\r\n            unique_topics = sorted([int(topic) for topic in df[f\"level_{level+1}\"].unique()])\r\n\r\n        for topic in unique_topics:\r\n            if topic != -1:\r\n                if topics:\r\n                    selection = df.loc[(df[f\"level_{level+1}\"] == topic) & (df.topic.isin(topics)), :]\r\n                else:\r\n                    selection = df.loc[df[f\"level_{level+1}\"] == topic, :]\r\n\r\n                if not hide_annotations:\r\n                    selection.loc[len(selection), :] = None\r\n                    selection[\"text\"] = \"\"\r\n                    selection.loc[len(selection) - 1, \"x\"] = selection.x.mean()\r\n                    selection.loc[len(selection) - 1, \"y\"] = selection.y.mean()\r\n                    selection.loc[len(selection) - 1, \"text\"] = topic_names[int(topic)][\"plot_text\"]\r\n\r\n                traces.append(\r\n                    go.Scattergl(\r\n                        x=selection.x,\r\n                        y=selection.y,\r\n                        text=selection.text if not hide_annotations else None,\r\n                        hovertext=selection.doc if not hide_document_hover else None,\r\n                        hoverinfo=\"text\",\r\n                        name=topic_names[int(topic)][\"trace_name\"],\r\n                        mode=\"markers+text\",\r\n                        marker=dict(size=5, opacity=0.5),\r\n                    )\r\n                )\r\n\r\n        all_traces.append(traces)\r\n\r\n    # Track and count traces\r\n    nr_traces_per_set = [len(traces) for traces in all_traces]\r\n    trace_indices = [(0, nr_traces_per_set[0])]\r\n    for index, nr_traces in enumerate(nr_traces_per_set[1:]):\r\n        start = trace_indices[index][1]\r\n        end = nr_traces + start\r\n        trace_indices.append((start, end))\r\n\r\n    # Visualization\r\n    fig = go.Figure()\r\n    for traces in all_traces:\r\n        for trace in traces:\r\n            fig.add_trace(trace)\r\n\r\n    for index in range(len(fig.data)):\r\n        if index >= nr_traces_per_set[0]:\r\n            fig.data[index].visible = False\r\n\r\n    # Create and add slider\r\n    steps = []\r\n    for index, indices in enumerate(trace_indices):\r\n        step = dict(\r\n            method=\"update\",\r\n            label=str(index),\r\n            args=[{\"visible\": [False] * len(fig.data)}],\r\n        )\r\n        for index in range(indices[1] - indices[0]):\r\n            step[\"args\"][0][\"visible\"][index + indices[0]] = True\r\n        steps.append(step)\r\n\r\n    sliders = [dict(currentvalue={\"prefix\": \"Level: \"}, pad={\"t\": 20}, steps=steps)]\r\n\r\n    # Add grid in a 'plus' shape\r\n    x_range = (\r\n        df.x.min() - abs((df.x.min()) * 0.15),\r\n        df.x.max() + abs((df.x.max()) * 0.15),\r\n    )\r\n    y_range = (\r\n        df.y.min() - abs((df.y.min()) * 0.15),\r\n        df.y.max() + abs((df.y.max()) * 0.15),\r\n    )\r\n    fig.add_shape(\r\n        type=\"line\",\r\n        x0=sum(x_range) / 2,\r\n        y0=y_range[0],\r\n        x1=sum(x_range) / 2,\r\n        y1=y_range[1],\r\n        line=dict(color=\"#CFD8DC\", width=2),\r\n    )\r\n    fig.add_shape(\r\n        type=\"line\",\r\n        x0=x_range[0],\r\n        y0=sum(y_range) / 2,\r\n        x1=x_range[1],\r\n        y1=sum(y_range) / 2,\r\n        line=dict(color=\"#9E9E9E\", width=2),\r\n    )\r\n    fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\r\n    fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\r\n\r\n    # Stylize layout\r\n    fig.update_layout(\r\n        sliders=sliders,\r\n        template=\"simple_white\",\r\n        title={\r\n            \"text\": f\"{title}\",\r\n            \"x\": 0.5,\r\n            \"xanchor\": \"center\",\r\n            \"yanchor\": \"top\",\r\n            \"font\": dict(size=22, color=\"Black\"),\r\n        },\r\n        width=width,\r\n        height=height,\r\n    )\r\n\r\n    fig.update_xaxes(visible=False)\r\n    fig.update_yaxes(visible=False)\r\n    return fig",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration",
            "Empty Column Misinitialization"
        ]
    },
    {
        "code": "def test_topic_reduction_edge_cases(model, documents, request):\r\n    topic_model = copy.deepcopy(request.getfixturevalue(model))\r\n    topic_model.nr_topics = 100\r\n    nr_topics = 5\r\n    topics = np.random.randint(-1, nr_topics - 1, len(documents))\r\n    old_documents = pd.DataFrame({\"Document\": documents, \"ID\": range(len(documents)), \"Topic\": topics})\r\n    topic_model._update_topic_size(old_documents)\r\n    old_documents = topic_model._sort_mappings_by_frequency(old_documents)\r\n    topic_model._extract_topics(old_documents)\r\n    old_freq = topic_model.get_topic_freq()\r\n\r\n    new_documents = topic_model._reduce_topics(old_documents)\r\n    new_freq = topic_model.get_topic_freq()\r\n\r\n    assert not set(old_documents.Topic).difference(set(new_documents.Topic))\r\n    pd.testing.assert_frame_equal(old_documents, new_documents)\r\n    pd.testing.assert_frame_equal(old_freq, new_freq)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_extract_topics_custom_cv(model, documents, request):\r\n    topic_model = copy.deepcopy(request.getfixturevalue(model))\r\n    nr_topics = 5\r\n    documents = pd.DataFrame(\r\n        {\r\n            \"Document\": documents,\r\n            \"ID\": range(len(documents)),\r\n            \"Topic\": np.random.randint(-1, nr_topics - 1, len(documents)),\r\n        }\r\n    )\r\n\r\n    cv = CountVectorizer(ngram_range=(1, 2))\r\n    topic_model.vectorizer_model = cv\r\n    topic_model._update_topic_size(documents)\r\n    topic_model._extract_topics(documents)\r\n    freq = topic_model.get_topic_freq()\r\n\r\n    assert topic_model.c_tf_idf_.shape[0] == 5\r\n    assert topic_model.c_tf_idf_.shape[1] > 100\r\n    assert isinstance(freq, pd.DataFrame)\r\n    assert nr_topics == len(freq.Topic.unique())\r\n    assert freq.Count.sum() == len(documents)\r\n    assert len(freq.Topic.unique()) == len(freq)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_extract_topics(model, documents, request):\r\n    topic_model = copy.deepcopy(request.getfixturevalue(model))\r\n    nr_topics = 5\r\n    documents = pd.DataFrame(\r\n        {\r\n            \"Document\": documents,\r\n            \"ID\": range(len(documents)),\r\n            \"Topic\": np.random.randint(-1, nr_topics - 1, len(documents)),\r\n        }\r\n    )\r\n    topic_model._update_topic_size(documents)\r\n    topic_model._extract_topics(documents)\r\n    freq = topic_model.get_topic_freq()\r\n\r\n    assert topic_model.c_tf_idf_.shape[0] == 5\r\n    assert topic_model.c_tf_idf_.shape[1] > 100\r\n    assert isinstance(freq, pd.DataFrame)\r\n    assert nr_topics == len(freq.Topic.unique())\r\n    assert freq.Count.sum() == len(documents)\r\n    assert len(freq.Topic.unique()) == len(freq)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ctfidf(model, documents, request):\r\n    topic_model = copy.deepcopy(request.getfixturevalue(model))\r\n    topics = topic_model.topics_\r\n    documents = pd.DataFrame({\"Document\": documents, \"ID\": range(len(documents)), \"Topic\": topics})\r\n    documents_per_topic = documents.groupby([\"Topic\"], as_index=False).agg({\"Document\": \" \".join})\r\n    documents = topic_model._preprocess_text(documents_per_topic.Document.values)\r\n    count = topic_model.vectorizer_model.fit(documents)\r\n\r\n    # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\r\n    # and will be removed in 1.2. Please use get_feature_names_out instead.\r\n    if version.parse(sklearn_version) >= version.parse(\"1.0.0\"):\r\n        words = count.get_feature_names_out()\r\n    else:\r\n        words = count.get_feature_names()\r\n\r\n    X = count.transform(documents)\r\n    transformer = ClassTfidfTransformer().fit(X)\r\n    c_tf_idf = transformer.transform(X)\r\n\r\n    assert len(words) > 1000\r\n    assert all([isinstance(x, str) for x in words])\r\n\r\n    assert isinstance(X, csr_matrix)\r\n    assert isinstance(c_tf_idf, csr_matrix)\r\n\r\n    assert X.shape[0] == len(set(topics))\r\n    assert X.shape[1] == len(words)\r\n\r\n    assert c_tf_idf.shape[0] == len(set(topics))\r\n    assert c_tf_idf.shape[1] == len(words)\r\n\r\n    assert np.min(X) == 0",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_ctfidf_custom_cv(model, documents, request):\r\n    cv = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\r\n    topic_model = copy.deepcopy(request.getfixturevalue(model))\r\n    topic_model.vectorizer_model = cv\r\n    topics = topic_model.topics_\r\n    documents = pd.DataFrame({\"Document\": documents, \"ID\": range(len(documents)), \"Topic\": topics})\r\n    documents_per_topic = documents.groupby([\"Topic\"], as_index=False).agg({\"Document\": \" \".join})\r\n    documents = topic_model._preprocess_text(documents_per_topic.Document.values)\r\n    count = topic_model.vectorizer_model.fit(documents)\r\n\r\n    # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\r\n    # and will be removed in 1.2. Please use get_feature_names_out instead.\r\n    if version.parse(sklearn_version) >= version.parse(\"1.0.0\"):\r\n        words = count.get_feature_names_out()\r\n    else:\r\n        words = count.get_feature_names()\r\n\r\n    X = count.transform(documents)\r\n    transformer = ClassTfidfTransformer().fit(X)\r\n    c_tf_idf = transformer.transform(X)\r\n\r\n    assert len(words) > 1000\r\n    assert all([isinstance(x, str) for x in words])\r\n\r\n    assert isinstance(X, csr_matrix)\r\n    assert isinstance(c_tf_idf, csr_matrix)\r\n\r\n    assert X.shape[0] == len(set(topics))\r\n    assert X.shape[1] == len(words)\r\n\r\n    assert c_tf_idf.shape[0] == len(set(topics))\r\n    assert c_tf_idf.shape[1] == len(words)\r\n\r\n    assert np.min(X) == 0",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def merge_models(cls, models, min_similarity: float = 0.7, embedding_model=None):\r\n        \"\"\"Merge multiple pre-trained BERTopic models into a single model.\r\n\r\n        The models are merged as if they were all saved using pytorch or\r\n        safetensors, so a minimal version without c-TF-IDF.\r\n\r\n        To do this, we choose the first model in the list of\r\n        models as a baseline. Then, we check each model whether\r\n        they contain topics that are not in the baseline.\r\n        This check is based on the cosine similarity between\r\n        topics embeddings. If topic embeddings between two models\r\n        are similar, then the topic of the second model is re-assigned\r\n        to the first. If they are dissimilar, the topic of the second\r\n        model is assigned to the first.\r\n\r\n        In essence, we simply check whether sufficiently \"new\"\r\n        topics emerge and add them.\r\n\r\n        Arguments:\r\n            models: A list of fitted BERTopic models\r\n            min_similarity: The minimum similarity for when topics are merged.\r\n            embedding_model: Additionally load in an embedding model if necessary.\r\n\r\n        Returns:\r\n            A new BERTopic model that was created as if you were\r\n            loading a model from the HuggingFace Hub without c-TF-IDF\r\n\r\n        Examples:\r\n        ```python\r\n        from bertopic import BERTopic\r\n        from sklearn.datasets import fetch_20newsgroups\r\n\r\n        docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\r\n\r\n        # Create three separate models\r\n        topic_model_1 = BERTopic(min_topic_size=5).fit(docs[:4000])\r\n        topic_model_2 = BERTopic(min_topic_size=5).fit(docs[4000:8000])\r\n        topic_model_3 = BERTopic(min_topic_size=5).fit(docs[8000:])\r\n\r\n        # Combine all models into one\r\n        merged_model = BERTopic.merge_models([topic_model_1, topic_model_2, topic_model_3])\r\n        ```\r\n        \"\"\"\r\n        import torch\r\n\r\n        # Temporarily save model and push to HF\r\n        with TemporaryDirectory() as tmpdir:\r\n            # Save model weights and config.\r\n            all_topics, all_params, all_tensors = [], [], []\r\n            for index, model in enumerate(models):\r\n                model.save(tmpdir, serialization=\"pytorch\")\r\n                topics, params, tensors, _, _, _ = save_utils.load_local_files(Path(tmpdir))\r\n                all_topics.append(topics)\r\n                all_params.append(params)\r\n                all_tensors.append(np.array(tensors[\"topic_embeddings\"]))\r\n\r\n                # Create a base set of parameters\r\n                if index == 0:\r\n                    merged_topics = topics\r\n                    merged_params = params\r\n                    merged_tensors = np.array(tensors[\"topic_embeddings\"])\r\n                    merged_topics[\"custom_labels\"] = None\r\n\r\n        for tensors, selected_topics in zip(all_tensors[1:], all_topics[1:]):\r\n            # Calculate similarity matrix\r\n            sim_matrix = cosine_similarity(tensors, merged_tensors)\r\n            sims = np.max(sim_matrix, axis=1)\r\n\r\n            # Extract new topics\r\n            new_topics = sorted(\r\n                [index - selected_topics[\"_outliers\"] for index, sim in enumerate(sims) if sim < min_similarity]\r\n            )\r\n            max_topic = max(set(merged_topics[\"topics\"]))\r\n\r\n            # Merge Topic Representations\r\n            new_topics_dict = {}\r\n            for new_topic in new_topics:\r\n                if new_topic != -1:\r\n                    max_topic += 1\r\n                    new_topics_dict[new_topic] = max_topic\r\n                    merged_topics[\"topic_representations\"][str(max_topic)] = selected_topics[\"topic_representations\"][\r\n                        str(new_topic)\r\n                    ]\r\n                    merged_topics[\"topic_labels\"][str(max_topic)] = selected_topics[\"topic_labels\"][str(new_topic)]\r\n\r\n                    # Add new aspects\r\n                    if selected_topics[\"topic_aspects\"]:\r\n                        aspects_1 = set(merged_topics[\"topic_aspects\"].keys())\r\n                        aspects_2 = set(selected_topics[\"topic_aspects\"].keys())\r\n                        aspects_diff = aspects_2.difference(aspects_1)\r\n                        if aspects_diff:\r\n                            for aspect in aspects_diff:\r\n                                merged_topics[\"topic_aspects\"][aspect] = {}\r\n\r\n                        # If the original model does not have topic aspects but the to be added model does\r\n                        if not merged_topics.get(\"topic_aspects\"):\r\n                            merged_topics[\"topic_aspects\"] = selected_topics[\"topic_aspects\"]\r\n\r\n                        # If they both contain topic aspects, add to the existing set of aspects\r\n                        else:\r\n                            for aspect, values in selected_topics[\"topic_aspects\"].items():\r\n                                merged_topics[\"topic_aspects\"][aspect][str(max_topic)] = values[str(new_topic)]\r\n\r\n                    # Add new embeddings\r\n                    new_tensors = tensors[new_topic + selected_topics[\"_outliers\"]]\r\n                    merged_tensors = np.vstack([merged_tensors, new_tensors])\r\n\r\n            # Topic Mapper\r\n            merged_topics[\"topic_mapper\"] = TopicMapper(list(range(-1, max_topic + 1, 1))).mappings_\r\n\r\n            # Find similar topics and re-assign those from the new models\r\n            sims_idx = np.argmax(sim_matrix, axis=1)\r\n            sims = np.max(sim_matrix, axis=1)\r\n            to_merge = {\r\n                a - selected_topics[\"_outliers\"]: b - merged_topics[\"_outliers\"]\r\n                for a, (b, val) in enumerate(zip(sims_idx, sims))\r\n                if val >= min_similarity\r\n            }\r\n            to_merge.update(new_topics_dict)\r\n            to_merge[-1] = -1\r\n            topics = [to_merge[topic] for topic in selected_topics[\"topics\"]]\r\n            merged_topics[\"topics\"].extend(topics)\r\n            merged_topics[\"topic_sizes\"] = dict(Counter(merged_topics[\"topics\"]))\r\n\r\n        # Create a new model from the merged parameters\r\n        merged_tensors = {\"topic_embeddings\": torch.from_numpy(merged_tensors)}\r\n        merged_model = _create_model_from_files(\r\n            merged_topics,\r\n            merged_params,\r\n            merged_tensors,\r\n            None,\r\n            None,\r\n            None,\r\n            warn_no_backend=False,\r\n        )\r\n        merged_model.embedding_model = models[0].embedding_model\r\n\r\n        # Replace embedding model if one is specifically chosen\r\n        verbose = any([model.verbose for model in models])\r\n        if embedding_model is not None and type(merged_model.embedding_model) == BaseEmbedder:\r\n            merged_model.embedding_model = select_backend(embedding_model, verbose=verbose)\r\n        return merged_model",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def approximate_distribution(\r\n        self,\r\n        documents: Union[str, List[str]],\r\n        window: int = 4,\r\n        stride: int = 1,\r\n        min_similarity: float = 0.1,\r\n        batch_size: int = 1000,\r\n        padding: bool = False,\r\n        use_embedding_model: bool = False,\r\n        calculate_tokens: bool = False,\r\n        separator: str = \" \",\r\n    ) -> Tuple[np.ndarray, Union[List[np.ndarray], None]]:\r\n        \"\"\"A post-hoc approximation of topic distributions across documents.\r\n\r\n        In order to perform this approximation, each document is split into tokens\r\n        according to the provided tokenizer in the `CountVectorizer`. Then, a\r\n        sliding window is applied on each document creating subsets of the document.\r\n        For example, with a window size of 3 and stride of 1, the sentence:\r\n\r\n        `Solving the right problem is difficult.`\r\n\r\n        can be split up into `solving the right`, `the right problem`, `right problem is`,\r\n        and `problem is difficult`. These are called tokensets. For each of these\r\n        tokensets, we calculate their c-TF-IDF representation and find out\r\n        how similar they are to the previously generated topics. Then, the\r\n        similarities to the topics for each tokenset are summed up in order to\r\n        create a topic distribution for the entire document.\r\n\r\n        We can also dive into this a bit deeper by then splitting these tokensets\r\n        up into individual tokens and calculate how much a word, in a specific sentence,\r\n        contributes to the topics found in that document. This can be enabled by\r\n        setting `calculate_tokens=True` which can be used for visualization purposes\r\n        in `topic_model.visualize_approximate_distribution`.\r\n\r\n        The main output, `topic_distributions`, can also be used directly in\r\n        `.visualize_distribution(topic_distributions[index])` by simply selecting\r\n        a single distribution.\r\n\r\n        Arguments:\r\n            documents: A single document or a list of documents for which we\r\n                       approximate their topic distributions\r\n            window: Size of the moving window which indicates the number of\r\n                    tokens being considered.\r\n            stride: How far the window should move at each step.\r\n            min_similarity: The minimum similarity of a document's tokenset\r\n                            with respect to the topics.\r\n            batch_size: The number of documents to process at a time. If None,\r\n                        then all documents are processed at once.\r\n                        NOTE: With a large number of documents, it is not\r\n                        advised to process all documents at once.\r\n            padding: Whether to pad the beginning and ending of a document with\r\n                     empty tokens.\r\n            use_embedding_model: Whether to use the topic model's embedding\r\n                                 model to calculate the similarity between\r\n                                 tokensets and topics instead of using c-TF-IDF.\r\n            calculate_tokens: Calculate the similarity of tokens with all topics.\r\n                              NOTE: This is computation-wise more expensive and\r\n                              can require more memory. Using this over batches of\r\n                              documents might be preferred.\r\n            separator: The separator used to merge tokens into tokensets.\r\n\r\n        Returns:\r\n            topic_distributions: A `n` x `m` matrix containing the topic distributions\r\n                                 for all input documents with `n` being the documents\r\n                                 and `m` the topics.\r\n            topic_token_distributions: A list of `t` x `m` arrays with `t` being the\r\n                                       number of tokens for the respective document\r\n                                       and `m` the topics.\r\n\r\n        Examples:\r\n        After fitting the model, the topic distributions can be calculated regardless\r\n        of the clustering model and regardless of whether the documents were previously\r\n        seen or not:\r\n\r\n        ```python\r\n        topic_distr, _ = topic_model.approximate_distribution(docs)\r\n        ```\r\n\r\n        As a result, the topic distributions are calculated in `topic_distr` for the\r\n        entire document based on a token set with a specific window size and stride.\r\n\r\n        If you want to calculate the topic distributions on a token-level:\r\n\r\n        ```python\r\n        topic_distr, topic_token_distr = topic_model.approximate_distribution(docs, calculate_tokens=True)\r\n        ```\r\n\r\n        The `topic_token_distr` then contains, for each token, the best fitting topics.\r\n        As with `topic_distr`, it can contain multiple topics for a single token.\r\n        \"\"\"\r\n        if isinstance(documents, str):\r\n            documents = [documents]\r\n\r\n        if batch_size is None:\r\n            batch_size = len(documents)\r\n            batches = 1\r\n        else:\r\n            batches = math.ceil(len(documents) / batch_size)\r\n\r\n        topic_distributions = []\r\n        topic_token_distributions = []\r\n\r\n        for i in tqdm(range(batches), disable=not self.verbose):\r\n            doc_set = documents[i * batch_size : (i + 1) * batch_size]\r\n\r\n            # Extract tokens\r\n            analyzer = self.vectorizer_model.build_tokenizer()\r\n            tokens = [analyzer(document) for document in doc_set]\r\n\r\n            # Extract token sets\r\n            all_sentences = []\r\n            all_indices = [0]\r\n            all_token_sets_ids = []\r\n\r\n            for tokenset in tokens:\r\n                if len(tokenset) < window:\r\n                    token_sets = [tokenset]\r\n                    token_sets_ids = [list(range(len(tokenset)))]\r\n                else:\r\n                    # Extract tokensets using window and stride parameters\r\n                    stride_indices = list(range(len(tokenset)))[::stride]\r\n                    token_sets = []\r\n                    token_sets_ids = []\r\n                    for stride_index in stride_indices:\r\n                        selected_tokens = tokenset[stride_index : stride_index + window]\r\n\r\n                        if padding or len(selected_tokens) == window:\r\n                            token_sets.append(selected_tokens)\r\n                            token_sets_ids.append(\r\n                                list(\r\n                                    range(\r\n                                        stride_index,\r\n                                        stride_index + len(selected_tokens),\r\n                                    )\r\n                                )\r\n                            )\r\n\r\n                    # Add empty tokens at the beginning and end of a document\r\n                    if padding:\r\n                        padded = []\r\n                        padded_ids = []\r\n                        t = math.ceil(window / stride) - 1\r\n                        for i in range(math.ceil(window / stride) - 1):\r\n                            padded.append(tokenset[: window - ((t - i) * stride)])\r\n                            padded_ids.append(list(range(0, window - ((t - i) * stride))))\r\n\r\n                        token_sets = padded + token_sets\r\n                        token_sets_ids = padded_ids + token_sets_ids\r\n\r\n                # Join the tokens\r\n                sentences = [separator.join(token) for token in token_sets]\r\n                all_sentences.extend(sentences)\r\n                all_token_sets_ids.extend(token_sets_ids)\r\n                all_indices.append(all_indices[-1] + len(sentences))\r\n\r\n            # Calculate similarity between embeddings of token sets and the topics\r\n            if use_embedding_model:\r\n                embeddings = self._extract_embeddings(all_sentences, method=\"document\", verbose=True)\r\n                similarity = cosine_similarity(embeddings, self.topic_embeddings_[self._outliers :])\r\n\r\n            # Calculate similarity between c-TF-IDF of token sets and the topics\r\n            else:\r\n                bow_doc = self.vectorizer_model.transform(all_sentences)\r\n                c_tf_idf_doc = self.ctfidf_model.transform(bow_doc)\r\n                similarity = cosine_similarity(c_tf_idf_doc, self.c_tf_idf_[self._outliers :])\r\n\r\n            # Only keep similarities that exceed the minimum\r\n            similarity[similarity < min_similarity] = 0\r\n\r\n            # Aggregate results on an individual token level\r\n            if calculate_tokens:\r\n                topic_distribution = []\r\n                topic_token_distribution = []\r\n                for index, token in enumerate(tokens):\r\n                    start = all_indices[index]\r\n                    end = all_indices[index + 1]\r\n\r\n                    if start == end:\r\n                        end = end + 1\r\n\r\n                    # Assign topics to individual tokens\r\n                    token_id = [i for i in range(len(token))]\r\n                    token_val = {index: [] for index in token_id}\r\n                    for sim, token_set in zip(similarity[start:end], all_token_sets_ids[start:end]):\r\n                        for token in token_set:\r\n                            if token in token_val:\r\n                                token_val[token].append(sim)\r\n\r\n                    matrix = []\r\n                    for _, value in token_val.items():\r\n                        matrix.append(np.add.reduce(value))\r\n\r\n                    # Take empty documents into account\r\n                    matrix = np.array(matrix)\r\n                    if len(matrix.shape) == 1:\r\n                        matrix = np.zeros((1, len(self.topic_labels_) - self._outliers))\r\n\r\n                    topic_token_distribution.append(np.array(matrix))\r\n                    topic_distribution.append(np.add.reduce(matrix))\r\n\r\n                topic_distribution = normalize(topic_distribution, norm=\"l1\", axis=1)\r\n\r\n            # Aggregate on a tokenset level indicated by the window and stride\r\n            else:\r\n                topic_distribution = []\r\n                for index in range(len(all_indices) - 1):\r\n                    start = all_indices[index]\r\n                    end = all_indices[index + 1]\r\n\r\n                    if start == end:\r\n                        end = end + 1\r\n                    group = similarity[start:end].sum(axis=0)\r\n                    topic_distribution.append(group)\r\n                topic_distribution = normalize(np.array(topic_distribution), norm=\"l1\", axis=1)\r\n                topic_token_distribution = None\r\n\r\n            # Combine results\r\n            topic_distributions.append(topic_distribution)\r\n            if topic_token_distribution is None:\r\n                topic_token_distributions = None\r\n            else:\r\n                topic_token_distributions.extend(topic_token_distribution)\r\n\r\n        topic_distributions = np.vstack(topic_distributions)\r\n\r\n        return topic_distributions, topic_token_distributions",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def hierarchical_topics(\r\n        self,\r\n        docs: List[str],\r\n        use_ctfidf: bool = True,\r\n        linkage_function: Callable[[csr_matrix], np.ndarray] = None,\r\n        distance_function: Callable[[csr_matrix], csr_matrix] = None,\r\n    ) -> pd.DataFrame:\r\n        \"\"\"Create a hierarchy of topics.\r\n\r\n        To create this hierarchy, BERTopic needs to be already fitted once.\r\n        Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF or topic embeddings\r\n        representation using `scipy.cluster.hierarchy.linkage`.\r\n\r\n        Based on that hierarchy, we calculate the topic representation at each\r\n        merged step. This is a local representation, as we only assume that the\r\n        chosen step is merged and not all others which typically improves the\r\n        topic representation.\r\n\r\n        Arguments:\r\n            docs: The documents you used when calling either `fit` or `fit_transform`\r\n            use_ctfidf: Whether to calculate distances between topics based on c-TF-IDF embeddings. If False, the\r\n                        embeddings from the embedding model are used.\r\n            linkage_function: The linkage function to use. Default is:\r\n                              `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\r\n            distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\r\n                               `lambda x: 1 - cosine_similarity(x)`.\r\n                               You can pass any function that returns either a square matrix of\r\n                               shape (n_samples, n_samples) with zeros on the diagonal and\r\n                               non-negative values or condensed distance matrix of shape\r\n                               (n_samples * (n_samples - 1) / 2,) containing the upper\r\n                               triangular of the distance matrix.\r\n\r\n        Returns:\r\n            hierarchical_topics: A dataframe that contains a hierarchy of topics\r\n                                 represented by their parents and their children\r\n\r\n        Examples:\r\n        ```python\r\n        from bertopic import BERTopic\r\n        topic_model = BERTopic()\r\n        topics, probs = topic_model.fit_transform(docs)\r\n        hierarchical_topics = topic_model.hierarchical_topics(docs)\r\n        ```\r\n\r\n        A custom linkage function can be used as follows:\r\n\r\n        ```python\r\n        from scipy.cluster import hierarchy as sch\r\n        from bertopic import BERTopic\r\n        topic_model = BERTopic()\r\n        topics, probs = topic_model.fit_transform(docs)\r\n\r\n        # Hierarchical topics\r\n        linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\r\n        hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function)\r\n        ```\r\n        \"\"\"\r\n        check_documents_type(docs)\r\n        if distance_function is None:\r\n            distance_function = lambda x: 1 - cosine_similarity(x)\r\n\r\n        if linkage_function is None:\r\n            linkage_function = lambda x: sch.linkage(x, \"ward\", optimal_ordering=True)\r\n\r\n        # Calculate distance\r\n        embeddings = select_topic_representation(self.c_tf_idf_, self.topic_embeddings_, use_ctfidf)[0][\r\n            self._outliers :\r\n        ]\r\n        X = distance_function(embeddings)\r\n        X = validate_distance_matrix(X, embeddings.shape[0])\r\n\r\n        # Use the 1-D condensed distance matrix as an input instead of the raw distance matrix\r\n        Z = linkage_function(X)\r\n\r\n        # Ensuring that the distances between clusters are unique otherwise the flatting of the hierarchy with\r\n        # `sch.fcluster(...)` would produce incorrect values for \"Topics\" for these clusters\r\n        if len(Z[:, 2]) != len(np.unique(Z[:, 2])):\r\n            Z[:, 2] = get_unique_distances(Z[:, 2])\r\n\r\n        # Calculate basic bag-of-words to be iteratively merged later\r\n        documents = pd.DataFrame({\"Document\": docs, \"ID\": range(len(docs)), \"Topic\": self.topics_})\r\n        documents_per_topic = documents.groupby([\"Topic\"], as_index=False).agg({\"Document\": \" \".join})\r\n        documents_per_topic = documents_per_topic.loc[documents_per_topic.Topic != -1, :]\r\n        clean_documents = self._preprocess_text(documents_per_topic.Document.values)\r\n\r\n        # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\r\n        # and will be removed in 1.2. Please use get_feature_names_out instead.\r\n        if version.parse(sklearn_version) >= version.parse(\"1.0.0\"):\r\n            words = self.vectorizer_model.get_feature_names_out()\r\n        else:\r\n            words = self.vectorizer_model.get_feature_names()\r\n\r\n        bow = self.vectorizer_model.transform(clean_documents)\r\n\r\n        # Extract clusters\r\n        hier_topics = pd.DataFrame(\r\n            columns=[\r\n                \"Parent_ID\",\r\n                \"Parent_Name\",\r\n                \"Topics\",\r\n                \"Child_Left_ID\",\r\n                \"Child_Left_Name\",\r\n                \"Child_Right_ID\",\r\n                \"Child_Right_Name\",\r\n            ]\r\n        )\r\n        for index in tqdm(range(len(Z))):\r\n            # Find clustered documents\r\n            clusters = sch.fcluster(Z, t=Z[index][2], criterion=\"distance\") - self._outliers\r\n            nr_clusters = len(clusters)\r\n\r\n            # Extract first topic we find to get the set of topics in a merged topic\r\n            topic = None\r\n            val = Z[index][0]\r\n            while topic is None:\r\n                if val - len(clusters) < 0:\r\n                    topic = int(val)\r\n                else:\r\n                    val = Z[int(val - len(clusters))][0]\r\n            clustered_topics = [i for i, x in enumerate(clusters) if x == clusters[topic]]\r\n\r\n            # Group bow per cluster, calculate c-TF-IDF and extract words\r\n            grouped = csr_matrix(bow[clustered_topics].sum(axis=0))\r\n            c_tf_idf = self.ctfidf_model.transform(grouped)\r\n            selection = documents.loc[documents.Topic.isin(clustered_topics), :]\r\n            selection.Topic = 0\r\n            words_per_topic = self._extract_words_per_topic(words, selection, c_tf_idf, calculate_aspects=False)\r\n\r\n            # Extract parent's name and ID\r\n            parent_id = index + len(clusters)\r\n            parent_name = \"_\".join([x[0] for x in words_per_topic[0]][:5])\r\n\r\n            # Extract child's name and ID\r\n            Z_id = Z[index][0]\r\n            child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\r\n\r\n            if Z_id - nr_clusters < 0:\r\n                child_left_name = \"_\".join([x[0] for x in self.get_topic(Z_id)][:5])\r\n            else:\r\n                child_left_name = hier_topics.iloc[int(child_left_id)].Parent_Name\r\n\r\n            # Extract child's name and ID\r\n            Z_id = Z[index][1]\r\n            child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\r\n\r\n            if Z_id - nr_clusters < 0:\r\n                child_right_name = \"_\".join([x[0] for x in self.get_topic(Z_id)][:5])\r\n            else:\r\n                child_right_name = hier_topics.iloc[int(child_right_id)].Parent_Name\r\n\r\n            # Save results\r\n            hier_topics.loc[len(hier_topics), :] = [\r\n                parent_id,\r\n                parent_name,\r\n                clustered_topics,\r\n                int(Z[index][0]),\r\n                child_left_name,\r\n                int(Z[index][1]),\r\n                child_right_name,\r\n            ]\r\n\r\n        hier_topics[\"Distance\"] = Z[:, 2]\r\n        hier_topics = hier_topics.sort_values(\"Parent_ID\", ascending=False)\r\n        hier_topics[[\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]] = hier_topics[\r\n            [\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]\r\n        ].astype(str)\r\n\r\n        return hier_topics",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def errorbar(self, x, y, z, zerr=None, yerr=None, xerr=None, fmt='',\r\n                 barsabove=False, errorevery=1, ecolor=None, elinewidth=None,\r\n                 capsize=None, capthick=None, xlolims=False, xuplims=False,\r\n                 ylolims=False, yuplims=False, zlolims=False, zuplims=False,\r\n                 axlim_clip=False,\r\n                 **kwargs):\r\n        \"\"\"\r\n        Plot lines and/or markers with errorbars around them.\r\n\r\n        *x*/*y*/*z* define the data locations, and *xerr*/*yerr*/*zerr* define\r\n        the errorbar sizes. By default, this draws the data markers/lines as\r\n        well the errorbars. Use fmt='none' to draw errorbars only.\r\n\r\n        Parameters\r\n        ----------\r\n        x, y, z : float or array-like\r\n            The data positions.\r\n\r\n        xerr, yerr, zerr : float or array-like, shape (N,) or (2, N), optional\r\n            The errorbar sizes:\r\n\r\n            - scalar: Symmetric +/- values for all data points.\r\n            - shape(N,): Symmetric +/-values for each data point.\r\n            - shape(2, N): Separate - and + values for each bar. First row\r\n              contains the lower errors, the second row contains the upper\r\n              errors.\r\n            - *None*: No errorbar.\r\n\r\n            Note that all error arrays should have *positive* values.\r\n\r\n        fmt : str, default: ''\r\n            The format for the data points / data lines. See `.plot` for\r\n            details.\r\n\r\n            Use 'none' (case-insensitive) to plot errorbars without any data\r\n            markers.\r\n\r\n        ecolor : :mpltype:`color`, default: None\r\n            The color of the errorbar lines.  If None, use the color of the\r\n            line connecting the markers.\r\n\r\n        elinewidth : float, default: None\r\n            The linewidth of the errorbar lines. If None, the linewidth of\r\n            the current style is used.\r\n\r\n        capsize : float, default: :rc:`errorbar.capsize`\r\n            The length of the error bar caps in points.\r\n\r\n        capthick : float, default: None\r\n            An alias to the keyword argument *markeredgewidth* (a.k.a. *mew*).\r\n            This setting is a more sensible name for the property that\r\n            controls the thickness of the error bar cap in points. For\r\n            backwards compatibility, if *mew* or *markeredgewidth* are given,\r\n            then they will over-ride *capthick*. This may change in future\r\n            releases.\r\n\r\n        barsabove : bool, default: False\r\n            If True, will plot the errorbars above the plot\r\n            symbols. Default is below.\r\n\r\n        xlolims, ylolims, zlolims : bool, default: False\r\n            These arguments can be used to indicate that a value gives only\r\n            lower limits. In that case a caret symbol is used to indicate\r\n            this. *lims*-arguments may be scalars, or array-likes of the same\r\n            length as the errors. To use limits with inverted axes,\r\n            `~.set_xlim`, `~.set_ylim`, or `~.set_zlim` must be\r\n            called before `errorbar`. Note the tricky parameter names: setting\r\n            e.g. *ylolims* to True means that the y-value is a *lower* limit of\r\n            the True value, so, only an *upward*-pointing arrow will be drawn!\r\n\r\n        xuplims, yuplims, zuplims : bool, default: False\r\n            Same as above, but for controlling the upper limits.\r\n\r\n        errorevery : int or (int, int), default: 1\r\n            draws error bars on a subset of the data. *errorevery* =N draws\r\n            error bars on the points (x[::N], y[::N], z[::N]).\r\n            *errorevery* =(start, N) draws error bars on the points\r\n            (x[start::N], y[start::N], z[start::N]). e.g. *errorevery* =(6, 3)\r\n            adds error bars to the data at (x[6], x[9], x[12], x[15], ...).\r\n            Used to avoid overlapping error bars when two series share x-axis\r\n            values.\r\n\r\n        axlim_clip : bool, default: False\r\n            Whether to hide error bars that are outside the axes limits.\r\n\r\n            .. versionadded:: 3.10\r\n\r\n        Returns\r\n        -------\r\n        errlines : list\r\n            List of `~mpl_toolkits.mplot3d.art3d.Line3DCollection` instances\r\n            each containing an errorbar line.\r\n        caplines : list\r\n            List of `~mpl_toolkits.mplot3d.art3d.Line3D` instances each\r\n            containing a capline object.\r\n        limmarks : list\r\n            List of `~mpl_toolkits.mplot3d.art3d.Line3D` instances each\r\n            containing a marker with an upper or lower limit.\r\n\r\n        Other Parameters\r\n        ----------------\r\n        data : indexable object, optional\r\n            DATA_PARAMETER_PLACEHOLDER\r\n\r\n        **kwargs\r\n            All other keyword arguments for styling errorbar lines are passed\r\n            `~mpl_toolkits.mplot3d.art3d.Line3DCollection`.\r\n\r\n        Examples\r\n        --------\r\n        .. plot:: gallery/mplot3d/errorbar3d.py\r\n        \"\"\"\r\n        had_data = self.has_data()\r\n\r\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\r\n        # Drop anything that comes in as None to use the default instead.\r\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\r\n        kwargs.setdefault('zorder', 2)\r\n\r\n        self._process_unit_info([(\"x\", x), (\"y\", y), (\"z\", z)], kwargs,\r\n                                convert=False)\r\n\r\n        # make sure all the args are iterable; use lists not arrays to\r\n        # preserve units\r\n        x = x if np.iterable(x) else [x]\r\n        y = y if np.iterable(y) else [y]\r\n        z = z if np.iterable(z) else [z]\r\n\r\n        if not len(x) == len(y) == len(z):\r\n            raise ValueError(\"'x', 'y', and 'z' must have the same size\")\r\n\r\n        everymask = self._errorevery_to_mask(x, errorevery)\r\n\r\n        label = kwargs.pop(\"label\", None)\r\n        kwargs['label'] = '_nolegend_'\r\n\r\n        # Create the main line and determine overall kwargs for child artists.\r\n        # We avoid calling self.plot() directly, or self._get_lines(), because\r\n        # that would call self._process_unit_info again, and do other indirect\r\n        # data processing.\r\n        (data_line, base_style), = self._get_lines._plot_args(\r\n            self, (x, y) if fmt == '' else (x, y, fmt), kwargs, return_kwargs=True)\r\n        art3d.line_2d_to_3d(data_line, zs=z, axlim_clip=axlim_clip)\r\n\r\n        # Do this after creating `data_line` to avoid modifying `base_style`.\r\n        if barsabove:\r\n            data_line.set_zorder(kwargs['zorder'] - .1)\r\n        else:\r\n            data_line.set_zorder(kwargs['zorder'] + .1)\r\n\r\n        # Add line to plot, or throw it away and use it to determine kwargs.\r\n        if fmt.lower() != 'none':\r\n            self.add_line(data_line)\r\n        else:\r\n            data_line = None\r\n            # Remove alpha=0 color that _process_plot_format returns.\r\n            base_style.pop('color')\r\n\r\n        if 'color' not in base_style:\r\n            base_style['color'] = 'C0'\r\n        if ecolor is None:\r\n            ecolor = base_style['color']\r\n\r\n        # Eject any line-specific information from format string, as it's not\r\n        # needed for bars or caps.\r\n        for key in ['marker', 'markersize', 'markerfacecolor',\r\n                    'markeredgewidth', 'markeredgecolor', 'markevery',\r\n                    'linestyle', 'fillstyle', 'drawstyle', 'dash_capstyle',\r\n                    'dash_joinstyle', 'solid_capstyle', 'solid_joinstyle']:\r\n            base_style.pop(key, None)\r\n\r\n        # Make the style dict for the line collections (the bars).\r\n        eb_lines_style = {**base_style, 'color': ecolor}\r\n\r\n        if elinewidth:\r\n            eb_lines_style['linewidth'] = elinewidth\r\n        elif 'linewidth' in kwargs:\r\n            eb_lines_style['linewidth'] = kwargs['linewidth']\r\n\r\n        for key in ('transform', 'alpha', 'zorder', 'rasterized'):\r\n            if key in kwargs:\r\n                eb_lines_style[key] = kwargs[key]\r\n\r\n        # Make the style dict for caps (the \"hats\").\r\n        eb_cap_style = {**base_style, 'linestyle': 'None'}\r\n        if capsize is None:\r\n            capsize = mpl.rcParams[\"errorbar.capsize\"]\r\n        if capsize > 0:\r\n            eb_cap_style['markersize'] = 2. * capsize\r\n        if capthick is not None:\r\n            eb_cap_style['markeredgewidth'] = capthick\r\n        eb_cap_style['color'] = ecolor\r\n\r\n        def _apply_mask(arrays, mask):\r\n            # Return, for each array in *arrays*, the elements for which *mask*\r\n            # is True, without using fancy indexing.\r\n            return [[*itertools.compress(array, mask)] for array in arrays]\r\n\r\n        def _extract_errs(err, data, lomask, himask):\r\n            # For separate +/- error values we need to unpack err\r\n            if len(err.shape) == 2:\r\n                low_err, high_err = err\r\n            else:\r\n                low_err, high_err = err, err\r\n\r\n            lows = np.where(lomask | ~everymask, data, data - low_err)\r\n            highs = np.where(himask | ~everymask, data, data + high_err)\r\n\r\n            return lows, highs\r\n\r\n        # collect drawn items while looping over the three coordinates\r\n        errlines, caplines, limmarks = [], [], []\r\n\r\n        # list of endpoint coordinates, used for auto-scaling\r\n        coorderrs = []\r\n\r\n        # define the markers used for errorbar caps and limits below\r\n        # the dictionary key is mapped by the `i_xyz` helper dictionary\r\n        capmarker = {0: '|', 1: '|', 2: '_'}\r\n        i_xyz = {'x': 0, 'y': 1, 'z': 2}\r\n\r\n        # Calculate marker size from points to quiver length. Because these are\r\n        # not markers, and 3D Axes do not use the normal transform stack, this\r\n        # is a bit involved. Since the quiver arrows will change size as the\r\n        # scene is rotated, they are given a standard size based on viewing\r\n        # them directly in planar form.\r\n        quiversize = eb_cap_style.get('markersize',\r\n                                      mpl.rcParams['lines.markersize']) ** 2\r\n        quiversize *= self.get_figure(root=True).dpi / 72\r\n        quiversize = self.transAxes.inverted().transform([\r\n            (0, 0), (quiversize, quiversize)])\r\n        quiversize = np.mean(np.diff(quiversize, axis=0))\r\n        # quiversize is now in Axes coordinates, and to convert back to data\r\n        # coordinates, we need to run it through the inverse 3D transform. For\r\n        # consistency, this uses a fixed elevation, azimuth, and roll.\r\n        with cbook._setattr_cm(self, elev=0, azim=0, roll=0):\r\n            invM = np.linalg.inv(self.get_proj())\r\n        # elev=azim=roll=0 produces the Y-Z plane, so quiversize in 2D 'x' is\r\n        # 'y' in 3D, hence the 1 index.\r\n        quiversize = np.dot(invM, [quiversize, 0, 0, 0])[1]\r\n        # Quivers use a fixed 15-degree arrow head, so scale up the length so\r\n        # that the size corresponds to the base. In other words, this constant\r\n        # corresponds to the equation tan(15) = (base / 2) / (arrow length).\r\n        quiversize *= 1.8660254037844388\r\n        eb_quiver_style = {**eb_cap_style,\r\n                           'length': quiversize, 'arrow_length_ratio': 1}\r\n        eb_quiver_style.pop('markersize', None)\r\n\r\n        # loop over x-, y-, and z-direction and draw relevant elements\r\n        for zdir, data, err, lolims, uplims in zip(\r\n                ['x', 'y', 'z'], [x, y, z], [xerr, yerr, zerr],\r\n                [xlolims, ylolims, zlolims], [xuplims, yuplims, zuplims]):\r\n\r\n            dir_vector = art3d.get_dir_vector(zdir)\r\n            i_zdir = i_xyz[zdir]\r\n\r\n            if err is None:\r\n                continue\r\n\r\n            if not np.iterable(err):\r\n                err = [err] * len(data)\r\n\r\n            err = np.atleast_1d(err)\r\n\r\n            # arrays fine here, they are booleans and hence not units\r\n            lolims = np.broadcast_to(lolims, len(data)).astype(bool)\r\n            uplims = np.broadcast_to(uplims, len(data)).astype(bool)\r\n\r\n            # a nested list structure that expands to (xl,xh),(yl,yh),(zl,zh),\r\n            # where x/y/z and l/h correspond to dimensions and low/high\r\n            # positions of errorbars in a dimension we're looping over\r\n            coorderr = [\r\n                _extract_errs(err * dir_vector[i], coord, lolims, uplims)\r\n                for i, coord in enumerate([x, y, z])]\r\n            (xl, xh), (yl, yh), (zl, zh) = coorderr\r\n\r\n            # draws capmarkers - flat caps orthogonal to the error bars\r\n            nolims = ~(lolims | uplims)\r\n            if nolims.any() and capsize > 0:\r\n                lo_caps_xyz = _apply_mask([xl, yl, zl], nolims & everymask)\r\n                hi_caps_xyz = _apply_mask([xh, yh, zh], nolims & everymask)\r\n\r\n                # setting '_' for z-caps and '|' for x- and y-caps;\r\n                # these markers will rotate as the viewing angle changes\r\n                cap_lo = art3d.Line3D(*lo_caps_xyz, ls='',\r\n                                      marker=capmarker[i_zdir],\r\n                                      axlim_clip=axlim_clip,\r\n                                      **eb_cap_style)\r\n                cap_hi = art3d.Line3D(*hi_caps_xyz, ls='',\r\n                                      marker=capmarker[i_zdir],\r\n                                      axlim_clip=axlim_clip,\r\n                                      **eb_cap_style)\r\n                self.add_line(cap_lo)\r\n                self.add_line(cap_hi)\r\n                caplines.append(cap_lo)\r\n                caplines.append(cap_hi)\r\n\r\n            if lolims.any():\r\n                xh0, yh0, zh0 = _apply_mask([xh, yh, zh], lolims & everymask)\r\n                self.quiver(xh0, yh0, zh0, *dir_vector, **eb_quiver_style)\r\n            if uplims.any():\r\n                xl0, yl0, zl0 = _apply_mask([xl, yl, zl], uplims & everymask)\r\n                self.quiver(xl0, yl0, zl0, *-dir_vector, **eb_quiver_style)\r\n\r\n            errline = art3d.Line3DCollection(np.array(coorderr).T,\r\n                                             axlim_clip=axlim_clip,\r\n                                             **eb_lines_style)\r\n            self.add_collection(errline)\r\n            errlines.append(errline)\r\n            coorderrs.append(coorderr)\r\n\r\n        coorderrs = np.array(coorderrs)\r\n\r\n        def _digout_minmax(err_arr, coord_label):\r\n            return (np.nanmin(err_arr[:, i_xyz[coord_label], :, :]),\r\n                    np.nanmax(err_arr[:, i_xyz[coord_label], :, :]))\r\n\r\n        minx, maxx = _digout_minmax(coorderrs, 'x')\r\n        miny, maxy = _digout_minmax(coorderrs, 'y')\r\n        minz, maxz = _digout_minmax(coorderrs, 'z')\r\n        self.auto_scale_xyz((minx, maxx), (miny, maxy), (minz, maxz), had_data)\r\n\r\n        # Adapting errorbar containers for 3d case, assuming z-axis points \"up\"\r\n        errorbar_container = mcontainer.ErrorbarContainer(\r\n            (data_line, tuple(caplines), tuple(errlines)),\r\n            has_xerr=(xerr is not None or yerr is not None),\r\n            has_yerr=(zerr is not None),\r\n            label=label)\r\n        self.containers.append(errorbar_container)\r\n\r\n        return errlines, caplines, limmarks",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def checkpoint_correctness_verification(config_dict,\r\n                                        models,\r\n                                        hidden_dim,\r\n                                        tmpdir,\r\n                                        load_optimizer_states=False,\r\n                                        load_lr_scheduler_states=False,\r\n                                        train_batch=False,\r\n                                        base_optimizers=[None, None],\r\n                                        empty_tag=False,\r\n                                        seq_dataloader=False,\r\n                                        load_module_only=False,\r\n                                        dtype=None):\r\n    if dtype == None:\r\n        dtype = preferred_dtype()\r\n\r\n    ds_model = create_deepspeed_model(config_dict=config_dict, model=models[0], base_optimizer=base_optimizers[0])\r\n\r\n    if seq_dataloader:\r\n        data_loader = sequence_dataloader(model=ds_model,\r\n                                          total_samples=50,\r\n                                          hidden_dim=hidden_dim,\r\n                                          device=ds_model.device,\r\n                                          dtype=dtype)\r\n    else:\r\n        data_loader = random_dataloader(model=ds_model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=ds_model.device,\r\n                                        dtype=dtype)\r\n\r\n    if train_batch:\r\n        ds_model.set_dataloader(data_loader)\r\n        for _, batch in enumerate(data_loader):\r\n            loss = ds_model.train_batch()\r\n    else:\r\n        for _, batch in enumerate(data_loader):\r\n            loss = ds_model(batch[0], batch[1])\r\n            ds_model.backward(loss)\r\n            ds_model.step()\r\n\r\n    # Flush zero stage 3 cache\r\n    ds_model.empty_partition_cache()\r\n\r\n    trained_model = ds_model\r\n\r\n    save_folder = os.path.join(tmpdir, 'saved_checkpoint')\r\n    save_tag = None if empty_tag else '1'\r\n\r\n    trained_model.save_checkpoint(save_folder, tag=save_tag)\r\n\r\n    dist.barrier()\r\n\r\n    for root, _, files in os.walk(save_folder):\r\n        for f in files:\r\n            if \"_expert_\" in f and \"_model_states\" in f:\r\n                expert = torch.load(os.path.join(root, f), weights_only=False)\r\n                needed, storages = 0, {}\r\n                for name, tensor in expert.items():\r\n                    needed += tensor.size().numel()\r\n                    storage = tensor.storage()\r\n                    # some storage can be shared within an expert's checkpoint\r\n                    storages[storage.data_ptr()] = storage.size()\r\n                stored = sum(v for _, v in storages.items())\r\n                assert needed == stored, f\"MoE expert checkpoint uses more storage than required: {f}\"\r\n\r\n    loaded_model = create_deepspeed_model(config_dict=config_dict, model=models[1], base_optimizer=base_optimizers[1])\r\n    assert list(trained_model.parameters())[0].dtype == list(loaded_model.parameters())[0].dtype\r\n\r\n    context = patch.object(loaded_model, \"_get_optimizer_ckpt_name\",\r\n                           wraps=loaded_model._get_optimizer_ckpt_name) if not load_optimizer_states else MagicMock()\r\n    with context as optim_load_state_dict_mock:\r\n        loaded_model.load_checkpoint(save_folder,\r\n                                     tag=save_tag,\r\n                                     load_optimizer_states=load_optimizer_states,\r\n                                     load_lr_scheduler_states=load_lr_scheduler_states,\r\n                                     load_module_only=load_module_only)\r\n        if not load_optimizer_states:\r\n            # should not attempt to get the file name to load it\r\n            optim_load_state_dict_mock.assert_not_called()\r\n\r\n    compare_model_states(trained_model,\r\n                         loaded_model,\r\n                         compare_optimizer=load_optimizer_states,\r\n                         load_module_only=load_module_only)\r\n\r\n    if load_optimizer_states:\r\n        compare_optimizer_states(trained_model, loaded_model, hidden_dim, dtype == torch.float16)\r\n\r\n    if load_lr_scheduler_states:\r\n        compare_lr_scheduler_states(trained_model, loaded_model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_model(self):\r\n        lora_config = LoRAConfig(lora_r=16, lora_alpha=16, base_weight_sharding=2)\r\n        quant_config = None\r\n        hidden_dim = 64\r\n        nlayers = 4\r\n\r\n        with deepspeed.linear.Init(lora_config=lora_config, quant_config=quant_config):\r\n            model = SimpleModel(hidden_dim=hidden_dim, nlayers=nlayers)\r\n\r\n        init_lora(model)\r\n\r\n        model_norms = [model.linears[i].weight.norm().item() for i in range(nlayers)]\r\n\r\n        ds_config = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 1\r\n            }\r\n        }\r\n        model, *_ = deepspeed.initialize(config=ds_config, model=model, model_parameters=model.parameters())\r\n\r\n        engine_norms = [model.module.linears[i].weight.norm().item() for i in range(nlayers)]\r\n\r\n        # Ensure that sharded weights are not broadcast during engine init\r\n        assert engine_norms == model_norms, f\"{dist.get_rank()=} base weight norms are not the same after engine init, {engine_norms=} != {model_norms=}\"\r\n\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.bfloat16)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, ep_size, use_residual):\r\n        if not required_torch_version(min_version=1.8):\r\n            pytest.skip(\"DeepSpeed MoE tests need torch 1.8 or higher to run correctly\")\r\n\r\n        config_dict = {\"train_batch_size\": 8, \"steps_per_print\": 1, \"fp16\": {\"enabled\": True}}\r\n        hidden_dim = 16\r\n\r\n        # E+D -- ep_size = 2\r\n        # E only -- ep_size = 4\r\n        model = SimplePRMoEModel(hidden_dim, ep_size=ep_size, use_residual=use_residual)\r\n        optimizer = torch.optim.AdamW(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict,\r\n                                              model=model,\r\n                                              optimizer=optimizer,\r\n                                              dist_init_required=False)\r\n\r\n        data_loader = sequence_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, ep_size, zero_stage, use_residual):\r\n        if not required_torch_version(min_version=1.8):\r\n            pytest.skip(\"DeepSpeed MoE tests need torch 1.8 or higher to run correctly\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            }\r\n        }\r\n        hidden_dim = 16\r\n\r\n        # E+D -- ep_size = 2\r\n        # E only -- ep_size = 4\r\n        model = SimpleMoEModel(hidden_dim, ep_size=ep_size, use_residual=use_residual)\r\n        param_group = {'params': [p for p in model.parameters()], 'name': 'random-unique-name'}\r\n        params = split_params_into_different_moe_groups_for_optimizer(param_group)\r\n        optimizer = torch.optim.AdamW(params=params)\r\n        model, optimizer, _, _ = deepspeed.initialize(config=config_dict,\r\n                                                      model=model,\r\n                                                      optimizer=optimizer,\r\n                                                      dist_init_required=False)\r\n        #dist_init_required=False -- parameterize to True/False?\r\n\r\n        data_loader = sequence_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        def strict_average_tensor(tensor):\r\n            process_group = optimizer.dp_process_group\r\n            curr_size = 0\r\n            pg_offsets = []\r\n            for i, param_idx, param_id in optimizer.params_in_ipg_bucket:\r\n                param = optimizer.bit16_groups[i][param_idx]\r\n                process_group = optimizer.dp_process_group\r\n                if optimizer.ipg_bucket_has_moe_params:\r\n                    process_group = optimizer.expert_dp_process_group[param.group_name] if is_moe_param(\r\n                        param) else optimizer.dp_process_group\r\n                partition_ids = optimizer.param_to_partition_ids[i][param_id]\r\n                # Get all partition ids + their offsets\r\n                partition_offsets = []\r\n                for partition_id in partition_ids:\r\n                    offset = optimizer.grad_start_offset[i][partition_id][param_id]\r\n                    partition_offsets.append(offset)\r\n                partition_offsets.sort()\r\n                # Calculate rank and offsets for grad slices\r\n                for idx, offset in enumerate(partition_offsets):\r\n                    # Calculate numel for grad slice depending on partition location\r\n                    if idx == len(partition_offsets) - 1:\r\n                        # Last partition_id uses its own offset\r\n                        numel = param.numel() - offset\r\n                    else:\r\n                        # Set numel to next partition's offset\r\n                        numel = partition_offsets[idx + 1] - offset\r\n                    pg_offsets.append((curr_size, process_group))\r\n                    curr_size += numel\r\n\r\n            def strict_narrow(dim, start, length):\r\n                lo, hi = 0, len(pg_offsets) - 1\r\n                while lo < hi:\r\n                    mi = lo + (hi - lo) // 2\r\n                    if pg_offsets[mi][0] >= start:\r\n                        hi = mi\r\n                    else:\r\n                        lo = mi + 1\r\n                curr_slice, reduce_process_group = lo, pg_offsets[lo][1]\r\n                while curr_slice < len(pg_offsets) and start + length > pg_offsets[curr_slice][0]:\r\n                    assert reduce_process_group == pg_offsets[curr_slice][\r\n                        1], \"reduce process_group does not match the parameter's process_group\"\r\n                    curr_slice += 1\r\n                return orig_narrow(dim, start, length)  # real call\r\n\r\n            orig_narrow, tensor.narrow = tensor.narrow, strict_narrow\r\n            type(optimizer).average_tensor(optimizer, tensor)  # real call\r\n            tensor.narrow = orig_narrow\r\n\r\n        if \"average_tensor\" in dir(optimizer):\r\n            optimizer.average_tensor = strict_average_tensor\r\n\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            gc.collect()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage):\r\n\r\n        if not required_torch_version(min_version=1.8):\r\n            pytest.skip(\"DeepSpeed MoE tests need torch 1.8 or higher to run correctly\")\r\n\r\n        def seed_everything(seed=11):\r\n            random.seed(seed)\r\n            torch.manual_seed(seed)\r\n            get_accelerator().manual_seed(seed)\r\n            get_accelerator().manual_seed_all(seed)\r\n            torch.backends.cudnn.deterministic = True\r\n            torch.backends.cudnn.benchmark = False\r\n\r\n        def get_state_dict_ep2(state_dict):\r\n            \"\"\"\r\n            convert state_dict from EP=1 to EP=2\r\n            \"\"\"\r\n            rank = int(deepspeed.comm.get_rank())\r\n            ep_state_dict = dict()\r\n            dst_sub_key = f\"deepspeed_moe.experts.deepspeed_experts.0\"\r\n            src_sub_key = f\"deepspeed_moe.experts.deepspeed_experts.{rank}\"\r\n            for moe_layer in [\"moe_1\", \"moe_2\"]:\r\n                for mlp_in_moe in [0, 1]:\r\n                    dst_key = f\"{moe_layer}.{dst_sub_key}.{mlp_in_moe}\"\r\n                    src_key = f\"{moe_layer}.{src_sub_key}.{mlp_in_moe}\"\r\n                    ep_state_dict[f\"{dst_key}.weight\"] = state_dict[f\"{src_key}.weight\"].detach().clone()\r\n                    ep_state_dict[f\"{dst_key}.bias\"] = state_dict[f\"{src_key}.bias\"].detach().clone()\r\n\r\n            for key in state_dict.keys():\r\n                if \"deepspeed_moe.experts.deepspeed_experts\" not in key:\r\n                    ep_state_dict[key] = state_dict[key].detach().clone()\r\n            return ep_state_dict\r\n\r\n        def get_models(hidden_dim):\r\n            model_ep1 = SimpleMoEModel(hidden_dim=hidden_dim, num_experts=2, ep_size=1, use_rts=False)\r\n            model_ep2 = SimpleMoEModel(hidden_dim=hidden_dim, num_experts=2, ep_size=2, use_rts=False)\r\n\r\n            state_dict_ep1 = model_ep1.state_dict()\r\n            state_dict_ep2 = get_state_dict_ep2(state_dict_ep1)\r\n            model_ep2.load_state_dict(state_dict_ep2)\r\n\r\n            model_ep1, _, _, _ = deepspeed.initialize(config=config_dict, model=model_ep1)\r\n            model_ep2, _, _, _ = deepspeed.initialize(config=config_dict, model=model_ep2)\r\n\r\n            return model_ep1, model_ep2\r\n\r\n        def extract_expert_grad(model, expert_id):\r\n\r\n            def _get_weight_bias(experts):\r\n                return ([deepspeed.utils.safe_get_full_grad(expert[0].weight)\r\n                         for expert in experts][expert_id].detach().clone(),\r\n                        [deepspeed.utils.safe_get_full_grad(expert[0].bias)\r\n                         for expert in experts][expert_id].detach().clone(),\r\n                        [deepspeed.utils.safe_get_full_grad(expert[1].weight)\r\n                         for expert in experts][expert_id].detach().clone(),\r\n                        [deepspeed.utils.safe_get_full_grad(expert[1].bias)\r\n                         for expert in experts][expert_id].detach().clone())\r\n\r\n            return (*_get_weight_bias(model.moe_1.deepspeed_moe.experts.deepspeed_experts),\r\n                    *_get_weight_bias(model.moe_2.deepspeed_moe.experts.deepspeed_experts))\r\n\r\n        seed_everything()\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.1,\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            }\r\n        }\r\n\r\n        hidden_dim = 4\r\n        total_samples = 2\r\n        rank = deepspeed.comm.get_rank()\r\n        model_ep1, model_ep2 = get_models(hidden_dim)\r\n\r\n        data_loader = sequence_dataloader(model=model_ep1,\r\n                                          total_samples=total_samples,\r\n                                          hidden_dim=hidden_dim,\r\n                                          device=model_ep1.device,\r\n                                          dtype=torch.float32)\r\n        expert_weight_grad_ep1 = []\r\n        expert_weight_grad_ep2 = []\r\n        for batch in data_loader:\r\n            loss_ep1 = model_ep1(batch[0], batch[1])\r\n            loss_ep2 = model_ep2(batch[0], batch[1])\r\n\r\n            model_ep1.backward(loss_ep1)\r\n            model_ep2.backward(loss_ep2)\r\n\r\n            expert_weight_grad_ep1.extend(extract_expert_grad(model_ep1, rank))\r\n            expert_weight_grad_ep2.extend(extract_expert_grad(model_ep2, 0))\r\n\r\n            model_ep1.step()\r\n            model_ep2.step()\r\n\r\n        assert len(expert_weight_grad_ep1) == len(expert_weight_grad_ep2)\r\n        for grad_from_ep1, grad_from_ep2 in zip(expert_weight_grad_ep1, expert_weight_grad_ep2):\r\n            assert torch.allclose(grad_from_ep1, grad_from_ep2, atol=0, rtol=1e-4)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage):\r\n        if not required_torch_version(min_version=1.8):\r\n            pytest.skip(\"DeepSpeed MoE tests need torch 1.8 or higher to run correctly\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            }\r\n        }\r\n        # should automatically create moe param groups in deepspeed backend\r\n        hidden_dim = 16\r\n        model = SimpleMoEModel(hidden_dim=hidden_dim, ep_size=1)\r\n        model, optimizer, _, _ = deepspeed.initialize(config=config_dict, model=model)\r\n        data_loader = sequence_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_fixed_linear(self):\r\n        if get_accelerator().device_name() == \"cpu\":\r\n            pytest.skip(\"CPU accelerator does not support this test yet\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"curriculum_learning\": {\r\n                \"enabled\": True,\r\n                \"curriculum_type\": \"seqlen\",\r\n                \"min_difficulty\": 2,\r\n                \"max_difficulty\": 10,\r\n                \"schedule_type\": \"fixed_linear\",\r\n                \"schedule_config\": {\r\n                    \"total_curriculum_step\": 8,\r\n                    \"difficulty_step\": 2\r\n                }\r\n            }\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 0, \"initial_scale_power\": 16}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n        ground_truths = {1: 2, 2: 4, 3: 4, 4: 6, 5: 6, 6: 8, 7: 8, 8: 10, 9: 10, 10: 10}\r\n\r\n        model = Curriculum_SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=20, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss, seqlen = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            if n + 1 in ground_truths:\r\n                true_seqlen = ground_truths[n + 1]\r\n                assert seqlen == true_seqlen, f\"Incorrect curriculum schedule\"",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, train_batch_size, drop_last):\r\n        config_dict = {\"train_batch_size\": train_batch_size, \"dataloader_drop_last\": drop_last, \"steps_per_print\": 1}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.AdamW(params=model.parameters())\r\n        # TODO: no way to set DeepSpeedEngine.deepspeed_io params, need to use\r\n        # pin_memory=False for cuda device\r\n        train_dataset = random_dataset(total_samples=50,\r\n                                       hidden_dim=hidden_dim,\r\n                                       device=torch.device('cpu'),\r\n                                       dtype=torch.float32)\r\n        model, _, training_dataloader, _ = deepspeed.initialize(config=config_dict,\r\n                                                                model=model,\r\n                                                                training_data=train_dataset,\r\n                                                                optimizer=optimizer)\r\n        training_dataloader.num_local_io_workers = 0  # We can't do nested mp.pool\r\n        for n, batch in enumerate(training_dataloader):\r\n            x = batch[0].to(get_accelerator().current_device_name())\r\n            y = batch[1].to(get_accelerator().current_device_name())\r\n            loss = model(x, y)\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_curriculum_learning(self):\r\n        if get_accelerator().device_name() == \"cpu\":\r\n            pytest.skip(\"CPU accelerator does not support this test yet\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"data_efficiency\": {\r\n                \"enabled\": True,\r\n                \"seed\": 1234,\r\n                \"data_sampling\": {\r\n                    \"enabled\": True,\r\n                    \"num_workers\": 0,\r\n                    \"curriculum_learning\": {\r\n                        \"enabled\": True,\r\n                        \"data_cluster_path\": \"/tmp\",\r\n                        \"curriculum_metrics\": {\r\n                            \"dummy_metric\": {\r\n                                \"index_to_sample_path\": \"dummy\",\r\n                                \"index_to_metric_path\": \"dummy\",\r\n                                \"difficulty_type\": \"value\",\r\n                                \"clustering_type\": \"single_cluster\",\r\n                                \"min_difficulty\": 2,\r\n                                \"max_difficulty\": 10,\r\n                                \"schedule_type\": \"fixed_root\",\r\n                                \"schedule_config\": {\r\n                                    \"total_curriculum_step\": 8,\r\n                                    \"difficulty_step\": 2,\r\n                                    \"root_degree\": 1\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 0, \"initial_scale_power\": 16}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        def data_post_process(data, data_sampler_state_dict):\r\n            assert 'dummy_metric' in data_sampler_state_dict['current_difficulties']\r\n            return data\r\n\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim)\r\n        dataset = random_dataset(20, hidden_dim, torch.device('cpu'))\r\n        model, _, data_loader, _ = deepspeed.initialize(config=config_dict,\r\n                                                        model=model,\r\n                                                        training_data=dataset,\r\n                                                        model_parameters=model.parameters(),\r\n                                                        mpu=MPU(1))\r\n        if model.mpu.get_data_parallel_rank() == 0 and not os.path.exists('/tmp'):\r\n            os.makedirs('/tmp')\r\n        model.set_data_post_process_func(data_post_process)\r\n        for n, batch in enumerate(data_loader):\r\n            x = batch[0].to(get_accelerator().current_device_name())\r\n            y = batch[1].to(get_accelerator().current_device_name())\r\n            loss = model(x, y)\r\n            model.backward(loss)\r\n            model.step()\r\n            if n >= 10:\r\n                break",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_fixed_discrete(self):\r\n        if get_accelerator().device_name() == \"cpu\":\r\n            pytest.skip(\"CPU accelerator does not support this test yet\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"curriculum_learning\": {\r\n                \"enabled\": True,\r\n                \"curriculum_type\": \"seqlen\",\r\n                \"min_difficulty\": 1,\r\n                \"max_difficulty\": 5,\r\n                \"schedule_type\": \"fixed_discrete\",\r\n                \"schedule_config\": {\r\n                    \"difficulty\": [1, 2, 3, 4, 5],\r\n                    \"max_step\": [2, 4, 6, 8]\r\n                }\r\n            }\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 0, \"initial_scale_power\": 16}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n        ground_truths = {1: 1, 2: 1, 3: 2, 4: 2, 5: 3, 6: 3, 7: 4, 8: 4}\r\n\r\n        model = Curriculum_SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=20, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss, seqlen = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            true_seqlen = 5\r\n            if n + 1 in ground_truths:\r\n                true_seqlen = ground_truths[n + 1]\r\n            assert seqlen == true_seqlen, f\"Incorrect curriculum schedule\"",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        config_dict = {\r\n            \"train_batch_size\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.001,\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 0\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n            },\r\n            \"flops_profiler\": {\r\n                \"enabled\": True,\r\n                \"step\": 1,\r\n                \"module_depth\": -1,\r\n                \"top_modules\": 3,\r\n            },\r\n        }\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.half)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            if n == 3: break\r\n        assert within_range(model.flops_profiler.flops, 200, tolerance=TOLERANCE)\r\n        assert model.flops_profiler.params == 110",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, scheduler_type, params):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": scheduler_type,\r\n                \"params\": params\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n\r\n        true_lrs = lr_scheduler.get_lr()\r\n        for group, true_lr in zip(model.optimizer.param_groups, true_lrs):\r\n            assert group['lr'] == true_lr, f\"True lr {true_lr}, optimizer lr {group['lr']}\"\r\n\r\n        for n, batch in enumerate(data_loader):\r\n            # get lr before training starts\r\n            lr_scheduler.get_lr()\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr(self, total_num_steps, warmup_num_steps, cos_min_ratio, warmup_min_ratio):\r\n        opt_lr = 0.0015\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": opt_lr\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": WARMUP_COSINE_LR,\r\n                \"params\": {\r\n                    TOTAL_NUM_STEPS: total_num_steps,\r\n                    WARMUP_MIN_RATIO: warmup_min_ratio,\r\n                    WARMUP_NUM_STEPS: warmup_num_steps,\r\n                    COS_MIN_RATIO: cos_min_ratio,\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=max(50, total_num_steps * 3),\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n\r\n        step_lrs = []\r\n        for _, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            step_lrs.extend(lr_scheduler.get_lr())\r\n\r\n        # Verify starting lr\r\n        assert abs(step_lrs[0] - opt_lr * warmup_min_ratio) < 1e-7\r\n\r\n        # Verify peak lr\r\n        assert abs(step_lrs[warmup_num_steps - 1] - opt_lr) < 1e-7\r\n\r\n        # Verify end lr\r\n        assert abs(step_lrs[total_num_steps - 1] - opt_lr * cos_min_ratio) < 1e-7\r\n\r\n        # Verify increasing phase\r\n        _verify_continuous_increase(step_lrs[:warmup_num_steps])\r\n\r\n        # Verify decreasing phase\r\n        _verify_continuous_decrease(step_lrs[warmup_num_steps:total_num_steps])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr(self, min_lr, max_lr, decay_rate, cycle_step_size, decay_step_size):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": ONE_CYCLE,\r\n                \"params\": {\r\n                    CYCLE_MIN_LR: min_lr,\r\n                    CYCLE_MAX_LR: max_lr,\r\n                    DECAY_LR_RATE: decay_rate,\r\n                    CYCLE_FIRST_STEP_SIZE: cycle_step_size,\r\n                    DECAY_STEP_SIZE: decay_step_size\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=max(50, cycle_step_size * 3),\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n\r\n        step_lrs = []\r\n        for _, batch in enumerate(data_loader):\r\n            step_lrs.extend(lr_scheduler.get_lr())\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        # Verify starting lr\r\n        assert step_lrs[0] == min_lr\r\n\r\n        # Verify peak lr\r\n        assert step_lrs[cycle_step_size] == max_lr\r\n\r\n        # Verify increasing phase\r\n        _verify_continuous_increase(step_lrs[:cycle_step_size])\r\n\r\n        # Verify decreasing phase\r\n        _verify_continuous_decrease(step_lrs[cycle_step_size:(cycle_step_size * 2)])\r\n\r\n        # Verify decay phase\r\n        if decay_rate > 0:\r\n            _verify_continuous_decrease(step_lrs[(cycle_step_size * 2):])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr_warmup_schedule(self, warmup_num_steps, warmup_type):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": WARMUP_LR,\r\n                \"params\": {\r\n                    WARMUP_MIN_LR: 0.1,\r\n                    WARMUP_MAX_LR: 0.2,\r\n                    WARMUP_NUM_STEPS: warmup_num_steps,\r\n                    WARMUP_TYPE: warmup_type,\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        schedule_params = config_dict[\"scheduler\"][\"params\"]\r\n        total_num_steps = 2 * warmup_num_steps\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=total_num_steps * 2,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n        step_lrs = []\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            step_lrs.append(lr_scheduler.get_lr())\r\n\r\n        # Verify initial lr\r\n        assert step_lrs[0] == [schedule_params[WARMUP_MIN_LR]]\r\n\r\n        # Verify warmup completion\r\n        warmup_num_steps = schedule_params[WARMUP_NUM_STEPS]\r\n        warmup_max_lr = [schedule_params[WARMUP_MAX_LR]]\r\n        assert step_lrs[warmup_num_steps] == warmup_max_lr\r\n\r\n        # Verify post-warmup completion\r\n        assert all([warmup_max_lr == lr for lr in step_lrs[warmup_num_steps:]])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, scheduler_type, params):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": scheduler_type,\r\n                \"params\": params\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            assert lr_scheduler.get_lr() == model.get_lr()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, min_lr, step_rate, step_size, staircase):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": LR_RANGE_TEST,\r\n                \"params\": {\r\n                    LR_RANGE_TEST_MIN_LR: min_lr,\r\n                    LR_RANGE_TEST_STEP_RATE: step_rate,\r\n                    LR_RANGE_TEST_STEP_SIZE: step_size,\r\n                    LR_RANGE_TEST_STAIRCASE: staircase\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=max(50, step_size * 2),\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n\r\n        step_lrs = []\r\n        for _, batch in enumerate(data_loader):\r\n            step_lrs.extend(lr_scheduler.get_lr())\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        # Verify starting lr\r\n        assert step_lrs[0] == min_lr\r\n\r\n        if staircase:\r\n            # Verify staircase increasing lr\r\n            _verify_staircase_increase(step_lrs, step_size)\r\n        else:\r\n            # Verify continuous increasing lr\r\n            _verify_continuous_increase(step_lrs)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_mom(self, min_mom, max_mom, decay_rate, step_size):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": ONE_CYCLE,\r\n                \"params\": {\r\n                    CYCLE_MIN_LR: 1e-3,\r\n                    CYCLE_MAX_LR: 1e-2,\r\n                    CYCLE_MIN_MOM: min_mom,\r\n                    CYCLE_MAX_MOM: max_mom,\r\n                    DECAY_MOM_RATE: decay_rate,\r\n                    CYCLE_FIRST_STEP_SIZE: step_size,\r\n                    DECAY_STEP_SIZE: step_size\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=max(50, step_size * 3),\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n\r\n        step_moms = []\r\n        for _, batch in enumerate(data_loader):\r\n            step_moms.append(lr_scheduler.get_mom())\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        # Verify starting lr\r\n        assert step_moms[0][0][0] == max_mom\r\n\r\n        # Verify peak lr\r\n        assert step_moms[step_size][0][0] == min_mom\r\n\r\n        # Verify decreasing phase\r\n        _verify_continuous_decrease(step_moms[:step_size])\r\n\r\n        # Verify increasing phase\r\n        _verify_continuous_increase(step_moms[step_size:(step_size * 2)])\r\n\r\n        # Verify decay phase\r\n        if decay_rate > 0:\r\n            _verify_continuous_increase(step_moms[(step_size * 2):])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, tmpdir):\r\n        grad_accumulation_steps = 2\r\n        micro_batch_size = 1\r\n        world_size = self.world_size\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": micro_batch_size,\r\n            \"gradient_accumulation_steps\": grad_accumulation_steps,\r\n            \"train_batch_size\": micro_batch_size * grad_accumulation_steps * world_size,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        hidden_dim = 10\r\n        weight_value = 0.1\r\n\r\n        model = MultiOutputModel(hidden_dim, weight_value)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        total_samples = 4\r\n        data_loader = multi_output_dataloader(model=model,\r\n                                              total_samples=total_samples,\r\n                                              hidden_dim=hidden_dim,\r\n                                              device=model.device,\r\n                                              inputs=[1.0, 2.0],\r\n                                              targets=[1, 2])\r\n        for n, batch in enumerate(data_loader):\r\n            assert len(batch) % 2 == 0, \\\r\n                 f\"multi_output_dataloader failed to return even number of data samples (input+target)\"\r\n\r\n            midpoint = len(batch) // 2\r\n            inputs, targets = batch[:midpoint], batch[midpoint:]\r\n            loss_tuple = model(inputs, targets)\r\n\r\n            expected_loss = torch.tensor(2.302734375, dtype=preferred_dtype(), device=model.device)\r\n            for loss in loss_tuple:\r\n                assert loss.shape == torch.Size([])\r\n                assert loss.item() == approx(expected_loss.item())\r\n\r\n            summed_loss = sum(loss_tuple)\r\n            scaled_loss = model.backward(summed_loss)\r\n            expected_scaled_loss = summed_loss.float() / grad_accumulation_steps\r\n            assert scaled_loss.item() == approx(expected_scaled_loss.item())\r\n\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_multiple_ctxts(self, zero_stage, dtype):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n            },\r\n        }\r\n\r\n        if dtype == torch.bfloat16:\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        elif dtype == torch.float16:\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n\r\n        hidden_dim = 64\r\n        total_samples = 32\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=total_samples,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=dtype)\r\n        dist.barrier()\r\n\r\n        param_list = list(model.parameters())\r\n        first_losses = []\r\n        first_grad_norms = []\r\n        with model.no_sync():\r\n            for _, batch in enumerate(data_loader):\r\n                loss = model(batch[0], batch[1])\r\n                first_losses.append(loss.item())\r\n                model.backward(loss)\r\n                grad_norm = sum([safe_get_full_grad(p).norm() for p in param_list])\r\n                first_grad_norms.append(grad_norm.item())\r\n\r\n        second_losses = []\r\n        second_grad_norms = []\r\n\r\n        model.zero_grad()\r\n        with model.no_sync():\r\n            for _, batch in enumerate(data_loader):\r\n                loss = model(batch[0], batch[1])\r\n                second_losses.append(loss.item())\r\n                model.backward(loss)\r\n                grad_norm = sum([safe_get_full_grad(p).norm() for p in param_list])\r\n                second_grad_norms.append(grad_norm.item())\r\n\r\n        assert len(first_losses) == len(second_losses)\r\n        for x, y in zip(first_losses, second_losses):\r\n            assert x == y\r\n\r\n        assert len(first_grad_norms) == len(second_grad_norms)\r\n        for x, y in zip(first_grad_norms, second_grad_norms):\r\n            assert x == y",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_engine_step(self, zero_stage, dtype):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n            },\r\n        }\r\n\r\n        if dtype == torch.bfloat16:\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        elif dtype == torch.float16:\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n\r\n        hidden_dim = 64\r\n        total_samples = 32\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=total_samples,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=dtype)\r\n        dist.barrier()\r\n\r\n        with model.no_sync():\r\n            for _, batch in enumerate(data_loader):\r\n                loss = model(batch[0], batch[1])\r\n                model.backward(loss)\r\n                with pytest.raises(AssertionError) as assertinfo:\r\n                    model.step()\r\n                assert (\"It is illegal to call Engine.step() inside no_sync context manager\" in str(assertinfo))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_zero_stage(self, zero_stage, dtype):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n            },\r\n        }\r\n\r\n        invalid_cfg = zero_stage > 1\r\n        if dtype == torch.bfloat16:\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        elif dtype == torch.float16:\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n\r\n        hidden_dim = 64\r\n        total_samples = 32\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=total_samples,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=dtype)\r\n        dist.barrier()\r\n\r\n        with pytest.raises(AssertionError) if invalid_cfg else nullcontext() as assertinfo:\r\n            with model.no_sync():\r\n                for _, batch in enumerate(data_loader):\r\n                    loss = model(batch[0], batch[1])\r\n                    model.backward(loss)\r\n        if invalid_cfg:\r\n            assert (\"no_sync context manager is incompatible\" in str(assertinfo))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage=2, use_cpu_offload=False):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": False,\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload\r\n            },\r\n            \"zero_allow_untested_optimizer\": False\r\n        }\r\n\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = SimpleOptimizer(model.parameters())\r\n        with pytest.raises(AssertionError):\r\n            model, optim, _, _ = deepspeed.initialize(config=config_dict,\r\n                                                      model=model,\r\n                                                      optimizer=optimizer,\r\n                                                      model_parameters=model.parameters())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, optimizer, expected_opt_class, zero_offload):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_allow_untested_optimizer\": True,\r\n            \"optimizer\": {\r\n                \"type\": optimizer,\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"zero_optimization\": {\r\n                \"stage\": 2,\r\n                \"cpu_offload\": zero_offload\r\n            }\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim)\r\n        set_base_shapes(model, None)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        ds_optimizer = model.optimizer.optimizer\r\n        assert isinstance(ds_optimizer, expected_opt_class)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr_warmup_decay_schedule(self, warmup_num_steps, warmup_type):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                },\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": WARMUP_DECAY_LR,\r\n                \"params\": {\r\n                    WARMUP_MIN_LR: 0.1,\r\n                    WARMUP_MAX_LR: 0.2,\r\n                    WARMUP_NUM_STEPS: warmup_num_steps,\r\n                    TOTAL_NUM_STEPS: warmup_num_steps * 2,\r\n                    WARMUP_TYPE: warmup_type\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        schedule_params = config_dict[\"scheduler\"][\"params\"]\r\n        total_num_steps = schedule_params[TOTAL_NUM_STEPS]\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=False)\r\n        model, _, _, lr_scheduler = deepspeed.initialize(config=config_dict,\r\n                                                         model=model,\r\n                                                         model_parameters=model.parameters())\r\n\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=total_num_steps * 2,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n        step_lrs = []\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n            step_lrs.append(lr_scheduler.get_lr())\r\n\r\n        # Verify initial lr\r\n        assert step_lrs[0] == [schedule_params[WARMUP_MIN_LR]]\r\n\r\n        # Verify lr at warmup completion\r\n        warmup_num_steps = schedule_params[WARMUP_NUM_STEPS]\r\n        warmup_max_lr = [schedule_params[WARMUP_MAX_LR]]\r\n        assert step_lrs[warmup_num_steps] == warmup_max_lr\r\n\r\n        # Verify decay phase\r\n        previous_lr = warmup_max_lr\r\n        for lr in step_lrs[warmup_num_steps + 1:]:\r\n            assert lr < previous_lr\r\n            previous_lr = lr",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, tmpdir):\r\n        grad_accumulation_steps = 3\r\n        micro_batch_size = 1\r\n        world_size = 1\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": micro_batch_size,\r\n            \"gradient_accumulation_steps\": grad_accumulation_steps,\r\n            \"train_batch_size\": micro_batch_size * grad_accumulation_steps * world_size,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        hidden_dim = 10\r\n        weight_value = 0.1\r\n\r\n        model = MultiOutputModel(hidden_dim, weight_value)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        total_samples = grad_accumulation_steps * micro_batch_size * 2\r\n        data_loader = multi_output_dataloader(model=model,\r\n                                              total_samples=total_samples,\r\n                                              hidden_dim=hidden_dim,\r\n                                              device=model.device,\r\n                                              inputs=[1.0, 2.0, 3.0],\r\n                                              targets=[1, 2, 3])\r\n        for n, batch in enumerate(data_loader):\r\n            assert len(batch) % 2 == 0, \\\r\n                 f\"multi_output_dataloader failed to return even number of data samples (input+target)\"\r\n\r\n            midpoint = len(batch) // 2\r\n            inputs, targets = batch[:midpoint], batch[midpoint:]\r\n            loss_tuple = model(inputs, targets)\r\n            assert len(loss_tuple) == 3\r\n\r\n            expected_loss = torch.tensor(2.302734375, dtype=preferred_dtype(), device=model.device)\r\n\r\n            for loss in loss_tuple:\r\n                assert loss.shape == torch.Size([])\r\n                assert loss.item() == approx(expected_loss.item())\r\n\r\n            summed_loss = sum(loss_tuple)\r\n            scaled_loss = model.backward(summed_loss)\r\n            expected_scaled_loss = summed_loss.float() / grad_accumulation_steps\r\n            assert scaled_loss.item() == approx(expected_scaled_loss.item())\r\n\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage=2, use_cpu_offload=False):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        if zero_stage == 3:\r\n            pytest.skip(\"skip for now\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload,\r\n                \"reduce_bucket_size\": 100,\r\n                \"allgather_bucket_size\": 100\r\n            }\r\n        }\r\n\r\n        hidden_dim = 1\r\n        model = SimpleModel(hidden_dim)\r\n\r\n        # Ensure model has 2 parameters, to cause empty partition with DP=3\r\n        assert len(list(model.parameters())) == 2\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        # Now make sure things work..\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=1,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.bfloat16)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, stage=2):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": stage\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.Adam(model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.bfloat16)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, comp_type, comm_type):\r\n        if comp_type == torch.bfloat16 or comm_type == torch.bfloat16:\r\n            if not bf16_required_version_check():\r\n                pytest.skip(\r\n                    \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n                )\r\n\r\n        if comp_type == torch.float16 or comm_type == torch.float16:\r\n            if not get_accelerator().is_fp16_supported():\r\n                pytest.skip(\"fp16 is not supported\")\r\n\r\n        type_str = {torch.float16: \"fp16\", torch.bfloat16: \"bf16\"}\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": comp_type == torch.float16\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": comp_type == torch.bfloat16\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 2\r\n            },\r\n        }\r\n        if comm_type is not None:\r\n            config_dict[\"communication_data_type\"] = type_str[comm_type]\r\n        else:\r\n            comm_type = comp_type\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.Adam(model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=2,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=comp_type)\r\n\r\n        def custom_reduce(tensor, dst, op=dist.ReduceOp.SUM, group=None, async_op=False):\r\n            assert tensor.dtype == comm_type\r\n            return orig_torch_reduce(tensor, dst, op, group, async_op)\r\n\r\n        orig_torch_reduce = dist.reduce\r\n        dist.reduce = custom_reduce\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n        dist.reduce = orig_torch_reduce",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"zero_optimization\": {\r\n                \"stage\": 2,\r\n                \"contiguous_gradients\": True,\r\n                \"allgather_bucket_size\": 2000000000,\r\n                \"reduce_bucket_size\": 200000000,\r\n                \"overlap_comm\": False,\r\n                \"reduce_scatter\": False\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.bfloat16)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Lamb\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage, use_cpu_offload):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        if zero_stage == 3:\r\n            pytest.skip(\"skip for now\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"initial_scale_power\": 8\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload,\r\n                \"reduce_bucket_size\": 100,\r\n                \"allgather_bucket_size\": 100\r\n            }\r\n        }\r\n        hidden_dim = 1\r\n        model = SimpleModel(hidden_dim)\r\n\r\n        # Ensure model has 2 parameters, to cause empty partition with DP=3\r\n        assert len(list(model.parameters())) == 2\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        # Now make sure things work..\r\n        data_loader = random_dataloader(model=model, total_samples=1, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_adam_basic(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\"train_batch_size\": 2, \"steps_per_print\": 1, \"amp\": {\"enabled\": True}}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.Adam(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lamb_basic(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Lamb\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"amp\": {\r\n                \"enabled\": True,\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_adam_O2(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"amp\": {\r\n                \"enabled\": True,\r\n                \"opt_level\": \"O2\"\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_adam_O2_empty_grad(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"amp\": {\r\n                \"enabled\": True,\r\n                \"opt_level\": \"O2\"\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage, optimizer_constructor):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        client_optimizer = optimizer_constructor(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=client_optimizer)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"zero_optimization\": {\r\n                \"stage\": 2,\r\n                \"contiguous_gradients\": True,\r\n                \"allgather_bucket_size\": 2000000000,\r\n                \"reduce_bucket_size\": 200000000,\r\n                \"overlap_comm\": False,\r\n                \"reduce_scatter\": False\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, adam_type, torch_impl):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"initial_scale_power\": 10\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": adam_type,\r\n                \"torch_adam\": torch_impl,\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        data_loader = random_dataloader(model=model, total_samples=10, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for _, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"initial_scale_power\": 10\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"AdamW\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 3\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        data_loader = random_dataloader(model=model, total_samples=10, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for _, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, stage):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": stage\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.Adam(model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage=2, use_cpu_offload=False):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": \"OneCycle\",\r\n                \"params\": {\r\n                    \"cycle_first_step_size\": 16000,\r\n                    \"cycle_first_stair_count\": 8000,\r\n                    \"decay_step_size\": 16000,\r\n                    \"cycle_min_lr\": 1e-06,\r\n                    \"cycle_max_lr\": 3e-05,\r\n                    \"decay_lr_rate\": 1e-07,\r\n                    \"cycle_min_mom\": 0.85,\r\n                    \"cycle_max_mom\": 0.99,\r\n                    \"decay_mom_rate\": 0.0\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload\r\n            }\r\n        }\r\n\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.bfloat16)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, optimizer_constructor, zero_stage=2):\r\n        if not bf16_required_version_check():\r\n            pytest.skip(\r\n                \" DeepSpeed BFloat16 tests need torch >= 1.10, NCCL >= 2.10.3, CUDA > =11.0 and HW support for BFloat16 to run correctly\"\r\n            )\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        client_optimizer = optimizer_constructor(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=client_optimizer)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": False\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=True)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=50,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage, use_cpu_offload):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        config_dict = {\r\n            \"train_batch_size\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"scheduler\": {\r\n                \"type\": \"OneCycle\",\r\n                \"params\": {\r\n                    \"cycle_first_step_size\": 16000,\r\n                    \"cycle_first_stair_count\": 8000,\r\n                    \"decay_step_size\": 16000,\r\n                    \"cycle_min_lr\": 1e-06,\r\n                    \"cycle_max_lr\": 3e-05,\r\n                    \"decay_lr_rate\": 1e-07,\r\n                    \"cycle_min_mom\": 0.85,\r\n                    \"cycle_max_mom\": 0.99,\r\n                    \"decay_mom_rate\": 0.0\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=10, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, base_datatype):\r\n        skip_on_arch(min_arch=9)\r\n\r\n        def run_zero(stage, model_dtype):\r\n            num_batches = 128\r\n            batch_size = 16\r\n            hidden_dim = 768\r\n            # Have to set seed before model\r\n            torch.random.manual_seed(42)\r\n            enable_fp16 = model_dtype == torch.float16\r\n            enable_bf16 = model_dtype == torch.bfloat16\r\n            # TransformerEngine Model\r\n            model = transformer_engine.Linear(hidden_dim, hidden_dim, bias=True, params_dtype=model_dtype)\r\n\r\n            # Create FP8 recipe. Note: All input args are optional.\r\n            fp8_recipe = recipe.DelayedScaling(fp8_format=recipe.Format.HYBRID,\r\n                                               amax_history_len=16,\r\n                                               amax_compute_algo=\"max\")\r\n            config = {\r\n                \"train_batch_size\": batch_size,\r\n                \"gradient_accumulation_steps\": 1,\r\n                \"optimizer\": {\r\n                    \"type\": \"Adam\",\r\n                    \"params\": {\r\n                        \"lr\": 0.00001\r\n                    }\r\n                },\r\n                \"zero_optimization\": {\r\n                    \"stage\": stage\r\n                },\r\n                \"fp16\": {\r\n                    \"enabled\": enable_fp16,\r\n                    \"loss_scale\": 0.1\r\n                },\r\n                \"bf16\": {\r\n                    \"enabled\": enable_bf16\r\n                }\r\n            }\r\n            # Init DeepSpeed\r\n            model, optimizer, _, _ = deepspeed.initialize(args=None,\r\n                                                          model=model,\r\n                                                          model_parameters=model.parameters(),\r\n                                                          config=config)\r\n\r\n            batches = torch.randn(num_batches, batch_size, hidden_dim, device=model.device, dtype=model_dtype)\r\n            for batch in batches:\r\n                # Enables autocasting for the forward pass\r\n                with transformer_engine.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\r\n                    out = model(batch)\r\n                loss = out.mean()\r\n                model.backward(loss)\r\n                model.step()\r\n            return loss\r\n\r\n        if base_datatype == \"fp16\":\r\n            model_dtype = torch.float16\r\n        elif base_datatype == \"bf16\":\r\n            model_dtype = torch.bfloat16\r\n        else:\r\n            model_dtype = torch.float32\r\n\r\n        # config\r\n        zero_stage = [0, 1, 2, 3]\r\n        losses = []\r\n        for stage in zero_stage:\r\n            loss = run_zero(stage, model_dtype)\r\n            losses.append(loss)\r\n        all_equal = all(torch.allclose(loss, losses[0], 1e-07, 1e-05) for loss in losses)\r\n        assert (all_equal)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\"train_batch_size\": 1, \"steps_per_print\": 1, \"fp16\": {\"enabled\": True}}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.AdamW(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\"train_batch_size\": 1, \"steps_per_print\": 1, \"fp16\": {\"enabled\": True}}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = torch.optim.AdamW(params=model.parameters())\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, optimizer=optimizer)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test__basic(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Lamb\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_zero(stage, model_dtype):\r\n            num_batches = 128\r\n            batch_size = 16\r\n            hidden_dim = 768\r\n            # Have to set seed before model\r\n            torch.random.manual_seed(42)\r\n            enable_fp16 = model_dtype == torch.float16\r\n            enable_bf16 = model_dtype == torch.bfloat16\r\n            # TransformerEngine Model\r\n            model = transformer_engine.Linear(hidden_dim, hidden_dim, bias=True, params_dtype=model_dtype)\r\n\r\n            # Create FP8 recipe. Note: All input args are optional.\r\n            fp8_recipe = recipe.DelayedScaling(fp8_format=recipe.Format.HYBRID,\r\n                                               amax_history_len=16,\r\n                                               amax_compute_algo=\"max\")\r\n            config = {\r\n                \"train_batch_size\": batch_size,\r\n                \"gradient_accumulation_steps\": 1,\r\n                \"optimizer\": {\r\n                    \"type\": \"Adam\",\r\n                    \"params\": {\r\n                        \"lr\": 0.00001\r\n                    }\r\n                },\r\n                \"zero_optimization\": {\r\n                    \"stage\": stage\r\n                },\r\n                \"fp16\": {\r\n                    \"enabled\": enable_fp16,\r\n                    \"loss_scale\": 0.1\r\n                },\r\n                \"bf16\": {\r\n                    \"enabled\": enable_bf16\r\n                }\r\n            }\r\n            # Init DeepSpeed\r\n            model, optimizer, _, _ = deepspeed.initialize(args=None,\r\n                                                          model=model,\r\n                                                          model_parameters=model.parameters(),\r\n                                                          config=config)\r\n\r\n            batches = torch.randn(num_batches, batch_size, hidden_dim, device=model.device, dtype=model_dtype)\r\n            for batch in batches:\r\n                # Enables autocasting for the forward pass\r\n                with transformer_engine.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\r\n                    out = model(batch)\r\n                loss = out.mean()\r\n                model.backward(loss)\r\n                model.step()\r\n            return loss",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage, use_cpu_offload, hidden_dim=4):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 138.\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload\r\n            }\r\n        }\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, optim, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        # Ensure the static scaler is configured.\r\n        assert optim.dynamic_loss_scale == False\r\n        assert optim.loss_scaler.loss_scale == 138.\r\n\r\n        # Now make sure things work..\r\n        data_loader = random_dataloader(model=model, total_samples=10, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_empty_grad(self):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Lamb\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": True\r\n            }\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim, empty_grad=True)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage, use_cpu_offload):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        if use_cpu_offload and not deepspeed.ops.__compatible_ops__[CPUAdamBuilder.NAME]:\r\n            pytest.skip(\"cpu-adam is not compatible\")\r\n\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"cpu_offload\": use_cpu_offload\r\n            },\r\n            \"zero_allow_untested_optimizer\": False,\r\n            \"zero_force_ds_cpu_optimizer\": False\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        optimizer = SimpleOptimizer(model.parameters())\r\n        with pytest.raises(AssertionError):\r\n            model, optim, _, _ = deepspeed.initialize(config=config_dict,\r\n                                                      model=model,\r\n                                                      optimizer=optimizer,\r\n                                                      model_parameters=model.parameters())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def get_model_optimizer():\r\n    torch.manual_seed(0)\r\n    model = Model()\r\n    optimizer = Adam(list(model.linear.parameters()), list(model.emb.parameters()))\r\n    return model, optimizer",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test(self):\r\n        config_dict = {\"train_batch_size\": 2, \"steps_per_print\": 1, \"sparse_gradients\": True}\r\n\r\n        model = Model()\r\n        optimizer = Adam(list(model.linear.parameters()), list(model.emb.parameters()))\r\n        engine, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=config_dict)\r\n        loss = torch.nn.BCEWithLogitsLoss()\r\n        x = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9], dtype=torch.long, device=engine.device)\r\n        offsets = torch.tensor([0, 4], dtype=torch.long, device=engine.device)\r\n        y = torch.tensor([[1.0], [0.0]], device=engine.device)\r\n        res = engine(x, offsets)\r\n        engine.backward(loss(res, y))\r\n        engine.step()\r\n\r\n        results = [engine.all_gather_scalar(i, groups._get_data_parallel_group()) for i in model.emb.parameters()]\r\n        for res in results:\r\n            assert torch.allclose(res[0], res[1])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_overflow(self, tmpdir):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"ZeroOneAdam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01,\r\n                    \"var_freeze_step\": 4,\r\n                    \"var_update_scaler\": 1,\r\n                    \"local_step_scaler\": 1,\r\n                    \"local_step_clipper\": 2,\r\n                    \"cuda_aware\": False,\r\n                    \"comm_backend_name\": get_accelerator().communication_backend_name(),\r\n                },\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 0,\r\n                \"initial_scale_power\": 16\r\n            },\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=100, hidden_dim=hidden_dim, device=model.device)\r\n        save_folder = os.path.join(tmpdir, \"saved_checkpoint\")\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            if dist.get_rank() == 0 and n >= 10:\r\n                loss = loss * 1000000.0\r\n            model.backward(loss)\r\n            dist.barrier()\r\n            model.step()\r\n            dist.barrier()\r\n            model.save_checkpoint(save_folder, tag=None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_overflow(self, tmpdir):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"OneBitAdam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01,\r\n                    \"freeze_step\": 2,\r\n                    \"cuda_aware\": False,\r\n                    \"comm_backend_name\": get_accelerator().communication_backend_name(),\r\n                },\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 0,\r\n                \"initial_scale_power\": 16\r\n            },\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=100, hidden_dim=hidden_dim, device=model.device)\r\n        save_folder = os.path.join(tmpdir, \"saved_checkpoint\")\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            if dist.get_rank() == 0 and n >= 10:\r\n                loss = loss * 1000000.0\r\n            model.backward(loss)\r\n            dist.barrier()\r\n            model.step()\r\n            dist.barrier()\r\n            model.save_checkpoint(save_folder, tag=None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_nvme_checkpointing(self, tmpdir, param_offload_device, optim_offload_device):\r\n        zero_dir, ckpt_dir = os.path.join(tmpdir, \"zero\"), os.path.join(tmpdir, \"checkpoint\")\r\n\r\n        first_stage_steps, second_stage_steps = 2, 2\r\n\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n\r\n        if not deepspeed.ops.__compatible_ops__[AsyncIOBuilder.NAME]:\r\n            pytest.skip('Skip tests since async-io is not compatible')\r\n\r\n        torch.manual_seed(123)\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"initial_scale_power\": 8\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"offload_param\": {\r\n                    \"device\": param_offload_device,\r\n                    \"nvme_path\": str(zero_dir)\r\n                },\r\n                \"offload_optimizer\": {\r\n                    \"device\": optim_offload_device,\r\n                    \"nvme_path\": str(zero_dir)\r\n                },\r\n                \"sub_group_size\": 100,\r\n                \"stage3_max_live_parameters\": 100,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n            },\r\n            \"aio\": {\r\n                \"block_size\": 1048576  # Minimum AIO bytes, anything smaller than this will not be offloaded\r\n            }\r\n        }\r\n\r\n        hidden_dim, nlayers = 2048, 2\r\n        with deepspeed.zero.Init(config_dict_or_path=config_dict):\r\n            model = SimpleModel(hidden_dim, nlayers=nlayers, empty_grad=False)\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        model.empty_partition_cache()\r\n\r\n        assert first_stage_steps > 0\r\n\r\n        data_loader = random_dataloader(model=model,\r\n                                        total_samples=first_stage_steps,\r\n                                        hidden_dim=hidden_dim,\r\n                                        device=model.device,\r\n                                        dtype=torch.float16)\r\n        dist.barrier()\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        dist.barrier()\r\n        model.save_checkpoint(ckpt_dir)\r\n\r\n        if second_stage_steps > 0:\r\n            second_stage_batches = list(\r\n                random_dataloader(model=model,\r\n                                  total_samples=second_stage_steps,\r\n                                  hidden_dim=hidden_dim,\r\n                                  device=model.device,\r\n                                  dtype=torch.float16))\r\n            dist.barrier()\r\n            for n, batch in enumerate(second_stage_batches):\r\n                loss = model(batch[0], batch[1])\r\n                model.backward(loss)\r\n                model.step()\r\n            dist.barrier()\r\n\r\n        final_batch = next(\r\n            iter(\r\n                random_dataloader(model=model,\r\n                                  total_samples=1,\r\n                                  hidden_dim=hidden_dim,\r\n                                  device=model.device,\r\n                                  dtype=torch.float16)))\r\n        dist.barrier()\r\n        loss_before = float(model(final_batch[0], final_batch[1]))\r\n\r\n        # Needed in ZeRO 3. Not doing so can give memory leak\r\n        model.destroy()\r\n\r\n        # TODO: This should be on the engine? There needs to be a better way.\r\n        Init.param_id = 0\r\n\r\n        with deepspeed.zero.Init(config_dict_or_path=config_dict):\r\n            model = SimpleModel(hidden_dim, nlayers=nlayers, empty_grad=False)\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n\r\n        model.load_checkpoint(ckpt_dir)\r\n\r\n        if second_stage_steps > 0:\r\n            dist.barrier()\r\n            for n, batch in enumerate(second_stage_batches):\r\n                loss = model(batch[0], batch[1])\r\n                model.backward(loss)\r\n                model.step()\r\n            dist.barrier()\r\n\r\n        dist.barrier()\r\n        loss_after = float(model(final_batch[0], final_batch[1]))\r\n\r\n        assert loss_before == loss_after",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_overflow(self, tmpdir):\r\n        if not get_accelerator().is_fp16_supported():\r\n            pytest.skip(\"fp16 is not supported\")\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"OneBitLamb\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015,\r\n                    \"weight_decay\": 0.01,\r\n                    \"max_coeff\": 0.3,\r\n                    \"min_coeff\": 0.01,\r\n                    \"freeze_step\": 2,\r\n                    \"cuda_aware\": False,\r\n                    \"comm_backend_name\": get_accelerator().communication_backend_name(),\r\n                    \"coeff_beta\": 0.9,\r\n                    \"factor_max\": 1.0,\r\n                    \"factor_min\": 0.5,\r\n                    \"factor_threshold\": 0.1,\r\n                },\r\n            },\r\n            \"gradient_clipping\": 1.0,\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 0,\r\n                \"initial_scale_power\": 16\r\n            },\r\n        }\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=100, hidden_dim=hidden_dim, device=model.device)\r\n        save_folder = os.path.join(tmpdir, \"saved_checkpoint\")\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            if dist.get_rank() == 0 and n >= 10:\r\n                loss = loss * 1000000.0\r\n            model.backward(loss)\r\n            dist.barrier()\r\n            model.step()\r\n            dist.barrier()\r\n            model.save_checkpoint(save_folder, tag=None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage=2):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 4\r\n\r\n        model = SimpleModel(hidden_dim=hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n\r\n        # get nccl all-gather send buffers alignment factor\r\n        nccl_start_alignment_factor = model.optimizer.nccl_start_alignment_factor\r\n\r\n        parallel_partitioned_bit16_groups = (model.optimizer.parallel_partitioned_bit16_groups\r\n                                             if zero_stage == 2 else model.optimizer.parallel_partitioned_fp16_groups)\r\n        for data_parallel_partitions in parallel_partitioned_bit16_groups:\r\n            for partition_id, partitioned_data in enumerate(data_parallel_partitions):\r\n                # verify that data partition start locations are 4-byte aligned\r\n                assert (partitioned_data.data_ptr() % (2 * nccl_start_alignment_factor) == 0)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, return_type):\r\n        if get_accelerator().device_name() == \"cpu\":\r\n            pytest.skip(\"CPU accelerator does not support this test yet\")\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-4\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 3\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n\r\n        class MyModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim):\r\n                super(MyModel, self).__init__()\r\n                self.l1 = torch.nn.Linear(hidden_dim, hidden_dim)\r\n                self.cel = torch.nn.CrossEntropyLoss()\r\n\r\n            def forward(self, x, y):\r\n                x = self.l1(x)\r\n                loss = self.cel(x, y)\r\n                if return_type == dict:\r\n                    val = {\"a\": x, \"loss\": loss, \"b\": 1, \"c\": None}\r\n                elif return_type == list:\r\n                    val = [x, loss]\r\n                elif return_type == tuple:\r\n                    val = (x, loss)\r\n                else:\r\n                    raise NotImplementedError\r\n                return val\r\n\r\n        with deepspeed.zero.Init():\r\n            model = MyModel(hidden_dim)\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        dist.barrier()\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            if return_type == dict:\r\n                loss = loss[\"loss\"]\r\n            else:\r\n                loss = loss[1]\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage):\r\n        # We verify trhee conditions:\r\n        # 1. global_steps starts at 0\r\n        # 2. All subgroups have the same step count\r\n        # 3. The global step count is the same as the step count of the first subgroup\r\n\r\n        # force all params to be partitioned by forcing threshold=0\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n                \"sub_group_size\": 4,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 4\r\n\r\n        model = SimpleModel(hidden_dim=hidden_dim, nlayers=12)\r\n        model, optimizer, _, _ = deepspeed.initialize(config=config_dict,\r\n                                                      model=model,\r\n                                                      model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        assert model.global_steps == 0\r\n\r\n        for batch in data_loader:\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n\r\n            is_gradient_accumulation_boundary = model.is_gradient_accumulation_boundary()\r\n            model.step()\r\n\r\n            if is_gradient_accumulation_boundary:\r\n                step_counts = []\r\n\r\n                if zero_stage == 3:\r\n                    for sub_group_id, _ in enumerate(optimizer.fp16_groups):\r\n                        fp32_param = optimizer.fp32_partitioned_groups_flat[sub_group_id]\r\n                        state = optimizer.optimizer.state[fp32_param]\r\n                        step_counts.append(state[\"step\"])\r\n                elif zero_stage == 1 or zero_stage == 2:\r\n                    for param_group in optimizer.optimizer.param_groups:\r\n                        for param in param_group[\"params\"]:\r\n                            state = optimizer.optimizer.state[param]\r\n                            step_counts.append(state[\"step\"])\r\n\r\n                assert all(step == step_counts[0] for step in step_counts)\r\n                assert model.global_steps == step_counts[0]",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage):\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-4\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n\r\n        class MyModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim):\r\n                super(MyModel, self).__init__()\r\n                self.l1 = torch.nn.Linear(hidden_dim, hidden_dim)\r\n                self.l2 = torch.nn.Linear(hidden_dim, hidden_dim)\r\n                self.act = torch.nn.ReLU()\r\n                self.cel = torch.nn.CrossEntropyLoss()\r\n\r\n                # freeze one fc\r\n                self.l2.weight.requires_grad = False\r\n                self.l2.bias.requires_grad = False\r\n\r\n            def forward(self, x, y):\r\n                x = self.l1(x)\r\n                x = self.act(x)\r\n                x = self.l2(x)\r\n                loss = self.cel(x, y)\r\n                val = (x, loss)\r\n                return val\r\n\r\n        with deepspeed.zero.Init(config_dict_or_path=config_dict, enabled=zero_stage == 3):\r\n            model = MyModel(hidden_dim)\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        dist.barrier()\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            loss = loss[1]\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, force_ds_optim):\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 1,\r\n                \"offload_optimizer\": {\r\n                    \"device\": \"cpu\"\r\n                }\r\n            },\r\n            \"zero_force_ds_cpu_optimizer\": force_ds_optim,\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n\r\n        optimizer = torch.optim.Adam(model.parameters())\r\n\r\n        if force_ds_optim:\r\n            with pytest.raises(ZeRORuntimeException):\r\n                model, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=config_dict)\r\n        else:\r\n            model, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=config_dict)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_training_partition_cache(self, training):\r\n        hidden_dim = 10\r\n        config_dict = {\r\n            \"train_batch_size\": 2,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_param_persistence_threshold\": hidden_dim,\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        if training:\r\n            config_dict[\"optimizer\"] = {\"type\": \"Adam\"}\r\n\r\n        with deepspeed.zero.Init(config_dict_or_path=config_dict):\r\n            model = SimpleModel(hidden_dim, empty_grad=False)\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, config=config_dict)\r\n\r\n        data_loader = random_dataloader(\r\n            model=model,\r\n            total_samples=6,\r\n            hidden_dim=hidden_dim,\r\n            device=model.device,\r\n        )\r\n\r\n        for _, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            if training:\r\n                model.backward(loss)\r\n                model.step()\r\n\r\n        persist_param_size = sum([p.numel() for p in model.parameters() if p.ds_persist])\r\n\r\n        assert persist_param_size >= sum([p.numel() for p in model.parameters()])\r\n\r\n        model.empty_partition_cache()\r\n        assert sum([p.numel() for p in model.parameters()]) == 0",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, prefetch_ratio, zero_stage=3):\r\n\r\n        hidden_dim = 10\r\n        model = SimpleModel(hidden_dim)\r\n\r\n        prefetch_bucket_size = int(sum([p.numel() for p in model.parameters(recurse=True)]) * prefetch_ratio)\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"stage3_prefetch_bucket_size\": prefetch_bucket_size\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"initial_scale_power\": 8\r\n            }\r\n        }\r\n\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for _ in range(3):\r\n            model.train()\r\n            for batch in data_loader:\r\n                loss = model(batch[0], batch[1])\r\n                model.backward(loss)\r\n                model.step()\r\n\r\n            model.eval()\r\n            with torch.no_grad():\r\n                for batch in data_loader:\r\n                    loss = model(batch[0], batch[1])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_2_param_groups(self, tmpdir, zero_stage, freeze_params):\r\n        # TODO:\r\n        # - need to test with multiple param groups\r\n        # force all params to be partitioned by forcing threshold=0\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_allow_untested_optimizer\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        class MyModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim, n_layers, freeze_params):\r\n                super().__init__()\r\n                self.ll = torch.nn.ModuleList(torch.nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers))\r\n                self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\r\n                if freeze_params:\r\n                    self.ll[0].weight.requires_grad = False\r\n                    self.ll[0].bias.requires_grad = False\r\n\r\n            def forward(self, x, y):\r\n                hidden = x\r\n                for l in self.ll:\r\n                    hidden = l(hidden)\r\n                return self.cross_entropy_loss(hidden, y)\r\n\r\n        hidden_dim = 3\r\n\r\n        world_size = dist.get_world_size()\r\n        n_layers = world_size * 2\r\n        model = MyModel(hidden_dim=hidden_dim, n_layers=n_layers, freeze_params=freeze_params)\r\n\r\n        optim_groups = [\r\n            {\r\n                \"params\": [l.weight for l in model.ll],\r\n                \"weight_decay\": 0.01,\r\n            },\r\n            {\r\n                \"params\": [l.bias for l in model.ll],\r\n                \"weight_decay\": 0.0\r\n            },\r\n        ]\r\n        optim = torch.optim.SGD(optim_groups, lr=0.1)\r\n\r\n        model, _, _, _ = deepspeed.initialize(\r\n            model=model,\r\n            model_parameters=model.parameters(),\r\n            optimizer=optim,\r\n            config=config_dict,\r\n        )\r\n        model.empty_partition_cache()\r\n\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for i, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        model.empty_partition_cache()\r\n        model.save_checkpoint(tmpdir)\r\n\r\n        # make sure all sides saved it\r\n        dist.barrier()\r\n\r\n        # dump_state_dict(model)\r\n\r\n        orig_state_dict = {}\r\n        for name, param in model.module.named_parameters():\r\n            if zero_stage == 3:\r\n                with deepspeed.zero.GatheredParameters(param, modifier_rank=None):\r\n                    orig_state_dict[name] = param.detach().cpu()\r\n            else:\r\n                orig_state_dict[name] = param.detach().cpu()\r\n\r\n        if zero_stage == 3:\r\n            with deepspeed.zero.GatheredParameters(model.parameters(), modifier_rank=None):\r\n                fp32_model = load_state_dict_from_zero_checkpoint(model.module, tmpdir)\r\n                fp32_state_dict = fp32_model.state_dict()\r\n        else:\r\n            fp32_model = load_state_dict_from_zero_checkpoint(model.module, tmpdir)\r\n            fp32_state_dict = fp32_model.state_dict()\r\n\r\n        # dump_state_dict(fp32_model)\r\n\r\n        if dist.get_rank() == 0:\r\n            for name in orig_state_dict.keys():\r\n                # float() workaround for torch<1.6\r\n                assert torch.allclose(orig_state_dict[name].float(), fp32_state_dict[name].float())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_1_param_group(self, tmpdir, zero_stage, freeze_params):\r\n        # XXX: ideally refactor with the 2_param_group test as 75% is the same\r\n        # force all params to be partitioned by forcing threshold=0\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        class MyModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim, n_layers, freeze_params):\r\n                super().__init__()\r\n                # to reproduce https://github.com/microsoft/DeepSpeed/pull/1372 it is important that\r\n                # the number of total elements is uneven:\r\n                # (1) 4 layers of 3*(3+1)=12 elements each, 48 in total\r\n                self.ll = torch.nn.ModuleList(torch.nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers))\r\n                # (2) the following adds 4+1=5 elements\r\n                self.classifier = torch.nn.Linear(4, 1)\r\n                # total 48+5=53 (uneven as desired) elements\r\n                self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\r\n                if freeze_params:\r\n                    self.ll[0].weight.requires_grad = False\r\n                    self.ll[0].bias.requires_grad = False\r\n\r\n            def forward(self, x, y):\r\n                hidden = x\r\n                for l in self.ll:\r\n                    hidden = l(hidden)\r\n                return self.cross_entropy_loss(hidden, y)\r\n\r\n        hidden_dim = 3  # do not change\r\n\r\n        world_size = dist.get_world_size()\r\n        # we want at least 2x layers as there are gpus to trigger round_robin_fp16_groups reshuffle in zero2\r\n        n_layers = world_size * 2\r\n        model = MyModel(hidden_dim=hidden_dim, n_layers=n_layers, freeze_params=freeze_params)\r\n\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        # Flush zero stage 3 cache\r\n        model.empty_partition_cache()\r\n\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for i, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()\r\n\r\n        model.empty_partition_cache()\r\n        model.save_checkpoint(tmpdir)\r\n\r\n        # make sure all sides saved it\r\n        dist.barrier()\r\n\r\n        orig_state_dict = {}\r\n        for name, param in model.module.named_parameters():\r\n            if zero_stage == 3:\r\n                with deepspeed.zero.GatheredParameters(param, modifier_rank=None):\r\n                    orig_state_dict[name] = param.detach().cpu()\r\n            else:\r\n                orig_state_dict[name] = param.detach().cpu()\r\n\r\n        if zero_stage == 3:\r\n            with deepspeed.zero.GatheredParameters(model.parameters(), modifier_rank=None):\r\n                fp32_model = load_state_dict_from_zero_checkpoint(model.module, tmpdir)\r\n                fp32_state_dict = fp32_model.state_dict()\r\n        else:\r\n            fp32_model = load_state_dict_from_zero_checkpoint(model.module, tmpdir)\r\n            fp32_state_dict = fp32_model.state_dict()\r\n\r\n        # dump_state_dict(fp32_model)\r\n\r\n        if dist.get_rank() == 0:\r\n            for name in orig_state_dict.keys():\r\n                # float() workaround for torch<1.6\r\n                assert torch.allclose(orig_state_dict[name].float(), fp32_state_dict[name].float())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, init_context_manager: bool, reduce_scatter: bool, param_sz: int = 8100) -> None:\r\n\r\n        class LargeParamModel(Module):\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.param = Parameter(torch.zeros((param_sz, ), dtype=torch.float32))\r\n\r\n                # only do weight initialization on root rank to\r\n                # make sure we are broadcasting correctly from rank 0\r\n                if dist.get_rank() == 0:\r\n                    partition_sz = math.ceil(self.param.numel() / dist.get_world_size())\r\n                    offset = 0\r\n                    for rank in range(dist.get_world_size()):\r\n                        with torch.no_grad():\r\n                            self.param[offset:offset + partition_sz].fill_(rank)\r\n                        offset += partition_sz\r\n\r\n            def forward(self, x: Tensor) -> Tensor:\r\n                return x * self.param\r\n\r\n        ds_config = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"contiguous_gradients\": True,\r\n                \"overlap_comm\": True,\r\n                \"reduce_scatter\": reduce_scatter,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.0\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            ds_config[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 1.0}\r\n        elif get_accelerator().is_bf16_supported():\r\n            ds_config[\"bf16\"] = {\"enabled\": True}\r\n        with deepspeed.zero.Init(mem_efficient_linear=False, enabled=init_context_manager):\r\n            model = LargeParamModel()\r\n        ds_engine = _ds_initialize_for_param_partitioning_testing(model, ds_config)\r\n\r\n        for train_iter in range(3):  # test multiple iterations to cover prefetching\r\n            activation: Tensor = ds_engine(torch.ones(param_sz, dtype=torch.float16, device=ds_engine.device))\r\n\r\n            partition_sz = math.ceil(param_sz / self.world_size)\r\n            for rank_idx, start_idx in enumerate(range(0, param_sz, partition_sz)):\r\n                activation_from_partition = activation[start_idx:start_idx + partition_sz]\r\n                assert torch.allclose(\r\n                    activation_from_partition,\r\n                    torch.full_like(activation_from_partition, rank_idx),\r\n                )\r\n\r\n            ds_engine.backward(activation.sum())\r\n            ds_engine.allreduce_gradients()\r\n\r\n            avgd_gradients = ds_engine.optimizer.averaged_gradients\r\n            assert set(avgd_gradients.keys()) == {0}, \"should only have one parameter group\"\r\n            (weight_gradient, ) = avgd_gradients[0]\r\n            expected_weight_gradient = (train_iter + 1) * torch.full_like(weight_gradient, 1)\r\n\r\n            assert torch.allclose(weight_gradient, expected_weight_gradient)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test(\r\n        self,\r\n        param_persistence_threshold: int = 0,\r\n        fp16_enabled: bool = False,\r\n        contiguous_gradients: bool = False,\r\n        offload_optimizer: bool = False,\r\n        zero_grad: bool = False,\r\n        prefetching: bool = False,\r\n        reduce_scatter: bool = False,\r\n        model_class: EltwiseMultiplicationTestNetwork_Dict = EltwiseMultiplicationTestNetwork_Dict,\r\n    ) -> None:\r\n        if offload_optimizer and not contiguous_gradients:\r\n            return\r\n\r\n        m = 3\r\n        n = 5\r\n        weights = [Parameter(torch.zeros((m, n), dtype=torch.float32)) for _ in range(3)]\r\n        model = model_class(*weights)\r\n        prefetch_bucket_size = sum([p.numel() for p in model.parameters(recurse=True)])\r\n        cfg = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"stage3_param_persistence_threshold\": param_persistence_threshold,\r\n                \"contiguous_gradients\": contiguous_gradients,\r\n                \"stage3_prefetch_bucket_size\": prefetch_bucket_size if prefetching else 0,\r\n                \"reduce_scatter\": reduce_scatter,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.0\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            cfg[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 1.0}\r\n        elif get_accelerator().is_bf16_supported():\r\n            cfg[\"bf16\"] = {\"enabled\": True}\r\n\r\n        if offload_optimizer:\r\n            cfg[\"zero_optimization\"][\"offload_optimizer\"] = {\r\n                \"device\": \"cpu\",\r\n                \"pin_memory\": True,\r\n            }\r\n\r\n        ds_engine = _ds_initialize_for_param_partitioning_testing(model, cfg)\r\n        for i, weight in enumerate(weights):\r\n            weight.ds_tensor.data = torch.full_like(weight.ds_tensor.data, (i + 1) * (1 + dist.get_rank()))\r\n\r\n        def create_tensor(vals, dtype: torch.dtype = None) -> Tensor:\r\n            return torch.as_tensor(\r\n                vals,\r\n                dtype=dtype or (torch.float16 if fp16_enabled else torch.float32),\r\n                device=ds_engine.device,\r\n            )\r\n\r\n        expected_hidden1 = create_tensor([\r\n            [1, 1, 1, 1, 1],\r\n            [1, 1, 1, 2, 2],\r\n            [2, 2, 2, 2, 2],\r\n        ])\r\n        expected_hidden2 = create_tensor([\r\n            [2, 2, 2, 2, 2],\r\n            [2, 2, 2, 8, 8],\r\n            [8, 8, 8, 8, 8],\r\n        ])\r\n        expected_yhat = create_tensor([[6, 6, 6, 6, 6], [6, 6, 6, 48, 48], [48, 48, 48, 48, 48]])\r\n        expected_loss = create_tensor([\r\n            [5, 5, 5, 5, 5],\r\n            [5, 5, 5, 47, 47],\r\n            [47, 47, 47, 47, 47],\r\n        ])\r\n\r\n        for train_iter in range(3):\r\n            activations = ds_engine(\r\n                x=torch.ones(\r\n                    (m, n),\r\n                    dtype=torch.float16 if fp16_enabled else torch.float32,\r\n                    device=ds_engine.device,\r\n                ),\r\n                y=torch.ones(\r\n                    (m, n),\r\n                    dtype=torch.float16 if fp16_enabled else torch.float32,\r\n                    device=ds_engine.device,\r\n                ),\r\n                use_module_trace=train_iter > 0,\r\n                param_prefetching=prefetching and train_iter > 0,\r\n            )\r\n            # for ease in testing convert outputs to dict.\r\n            activations = model_class.to_dict(activations)\r\n            assert torch.allclose(activations[\"hidden1\"], expected_hidden1)\r\n            assert torch.allclose(activations[\"hidden2\"], expected_hidden2)\r\n            assert torch.allclose(activations[\"y_hat\"], expected_yhat)\r\n            assert torch.allclose(activations[\"loss\"], expected_loss)\r\n\r\n            ds_engine.backward(activations[\"loss\"].sum())\r\n\r\n            # check the gradients\r\n            grad_partitions = ds_engine.optimizer.get_fp32_grad_partitions()\r\n            assert set(grad_partitions.keys()) == {0\r\n                                                   }, f\"should have one parameter group but got {len(grad_partitions)}\"\r\n            assert set(grad_partitions[0].keys()) == {0, 1, 2}\r\n            dloss_wrt_layer1 = grad_partitions[0][0]\r\n            dloss_wrt_layer2 = grad_partitions[0][1]\r\n            dloss_wrt_layer3 = grad_partitions[0][2]\r\n\r\n            assert dloss_wrt_layer1.dtype == torch.float\r\n            assert dloss_wrt_layer2.dtype == torch.float\r\n            assert dloss_wrt_layer3.dtype == torch.float\r\n\r\n            # layer1 = [..., 1, 2, ...]\r\n            # layer2 = [..., 2, 4, ...]\r\n            # layer3 = [..., 3, 6, ...]\r\n            # dloss_wrt_layer3 = hidden2\r\n            # dloss_wrt_layer2 = layer3 * hidden1\r\n            # dloss_wrt_layer1 = layer3 * layer2 * x\r\n\r\n            grad_multiplier = 1 if zero_grad else (train_iter + 1)\r\n            if dist.get_rank() == 0:\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer3.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([2] * 8, torch.float),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer2.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([3 * 1] * 8, torch.float),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer1.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([3 * 2 * 1] * 8, torch.float),\r\n                )\r\n            elif dist.get_rank() == 1:\r\n                # parameters dont split evenly across ranks so rank 1 has a zero-padded\r\n                # partition\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer3.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([8] * 7) + [0], torch.float),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer2.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([6 * 2] * 7) + [0], torch.float),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer1.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([6 * 4 * 1] * 7) + [0], torch.float),\r\n                )\r\n            else:\r\n                raise RuntimeError(\"test has world size of two\")\r\n\r\n            if zero_grad:\r\n                ds_engine.optimizer.zero_grad()\r\n\r\n        # TODO. add testing for this - for now we just call it to make sure it\r\n        # doesn't throw\r\n        ds_engine.optimizer.step()\r\n        # taking an optimizer step invalidates all parameters, make sure everything\r\n        # has been partitioned afterwards\r\n        _assert_partition_status(ds_engine, {ZeroParamStatus.NOT_AVAILABLE})\r\n        assert not math.isclose(ds_engine.optimizer._global_grad_norm, 0.0)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, zero_stage):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 4\r\n\r\n        model = SimpleModel(hidden_dim=hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        run_unbalanced_gradients(model, data_loader)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, allgather_bucket_size, zero_stage=2):\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"allgather_bucket_size\": allgather_bucket_size,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 4\r\n\r\n        model = SimpleModel(hidden_dim=hidden_dim)\r\n        if allgather_bucket_size % 2 == 0:\r\n            model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        else:\r\n            with pytest.raises(AssertionError) as assertinfo:\r\n                model, _, _, _ = deepspeed.initialize(config=config_dict,\r\n                                                      model=model,\r\n                                                      model_parameters=model.parameters())\r\n            assert (\"allgather_bucket_size must be a multiple of nccl_start_alignment_factor\" in str(assertinfo))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n\r\n        class ModelWhereParentInitializesChildWeights(Module):\r\n\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n                self.linear = Linear(12, 1)\r\n\r\n                self.apply(self.__init_weights)\r\n\r\n            def __init_weights(self, module):\r\n                if isinstance(module, Linear):\r\n                    with torch.no_grad():\r\n                        module.weight.fill_(1 + dist.get_rank())\r\n\r\n        ds_cfg = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"contiguous_gradients\": True,\r\n                \"overlap_comm\": True,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.0\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            ds_cfg[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 1.0}\r\n        elif get_accelerator().is_bf16_supported():\r\n            ds_cfg[\"bf16\"] = {\"enabled\": True}\r\n\r\n        with deepspeed.zero.Init(config=ds_cfg, mem_efficient_linear=False, enabled=True):\r\n            model = ModelWhereParentInitializesChildWeights()\r\n\r\n        assert model.linear.weight.ds_tensor.numel() == math.ceil(12 / self.world_size)\r\n        assert torch.allclose(\r\n            model.linear.weight.ds_tensor,\r\n            torch.full_like(model.linear.weight.ds_tensor, 1),\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, mics_enabled, zero_stage=3):\r\n        if mics_enabled and get_accelerator().device_name() == \"cpu\":\r\n            pytest.skip(\"CPU accelerator does not support this test yet\")\r\n        # force all params to be partitioned by forcing threshold=0\r\n        mics_shard_size = -1\r\n        if mics_enabled:\r\n            mics_shard_size = self.world_size\r\n\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 2,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": zero_stage,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n                \"mics_shard_size\": mics_shard_size,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-3\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True, \"initial_scale_power\": 8}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 4\r\n\r\n        class AlbertLikeModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim):\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\r\n                self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\r\n\r\n            def forward(self, x, y):\r\n                # run the same layer multiple times in a loop - to test a stack of forwards, followed by a stack of backwards\r\n                hidden = x\r\n                for i in range(3):\r\n                    hidden = hidden + self.linear(hidden)\r\n                return self.cross_entropy_loss(hidden, y)\r\n\r\n        model = AlbertLikeModel(hidden_dim=hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(config=config_dict, model=model, model_parameters=model.parameters())\r\n        data_loader = random_dataloader(model=model, total_samples=16, hidden_dim=hidden_dim, device=model.device)\r\n\r\n        for i, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self, init_context_manager: bool, param_sz: int = 100, n_layers: int = 100) -> None:\r\n\r\n        class ManyParamModel(Module):\r\n\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n                self.modulelist = ModuleList(\r\n                    EltwiseMultiplicationModule(weight=Parameter(torch.empty((param_sz, ), dtype=torch.float32)))\r\n                    for _ in range(n_layers))\r\n\r\n                for layer_num, module in enumerate(self.modulelist):\r\n                    with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\r\n                        param: Parameter = module.weight\r\n                        partition_sz = math.ceil(param.numel() / dist.get_world_size())\r\n                        offset = 0\r\n                        for rank in range(dist.get_world_size()):\r\n                            with torch.no_grad():\r\n                                param[offset:offset + partition_sz].fill_(2 * layer_num * rank)\r\n                            offset += partition_sz\r\n\r\n            def forward(self, x: Tensor) -> Tensor:\r\n                activations = []\r\n\r\n                for module in self.modulelist:\r\n                    x = module(x)\r\n                    activations.append(x)\r\n\r\n                return activations\r\n\r\n        ds_cfg = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"contiguous_gradients\": True,\r\n                \"overlap_comm\": True,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.0\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            ds_cfg[\"fp16\"] = {\"enabled\": True, \"loss_scale\": 1.0}\r\n        elif get_accelerator().is_bf16_supported():\r\n            ds_cfg[\"bf16\"] = {\"enabled\": True}\r\n\r\n        with deepspeed.zero.Init(config=ds_cfg, mem_efficient_linear=False, enabled=init_context_manager):\r\n            model = ManyParamModel()\r\n\r\n        ds_engine = _ds_initialize_for_param_partitioning_testing(model, ds_cfg)\r\n\r\n        dtype = preferred_dtype()\r\n        for _ in range(3):  # test multiple iterations to cover prefetching\r\n            activations: List[Tensor] = ds_engine(torch.ones((param_sz, ), dtype=dtype, device=ds_engine.device))\r\n            assert len(activations) == n_layers\r\n\r\n            partition_sz = math.ceil(param_sz / self.world_size)\r\n            expected_activations = torch.empty(param_sz, dtype=dtype, device=ds_engine.device)\r\n            for start_idx in range(0, param_sz, partition_sz):\r\n                expected_activations[start_idx:start_idx + partition_sz] = dist.get_rank()\r\n\r\n            for layer_num, activation in enumerate(activations):\r\n                expected_activations *= 2 * layer_num\r\n                assert torch.allclose(activation, expected_activations)\r\n\r\n            # TODO. finish writing this test\r\n            ds_engine.backward(activations[-1].sum())\r\n\r\n            avgd_gradients = ds_engine.optimizer.averaged_gradients\r\n            assert set(avgd_gradients.keys()) == {0}, \"should only have one parameter group\"\r\n            weight_gradients: List[Tensor] = avgd_gradients[0]\r\n\r\n            for layer_num, activation in enumerate(weight_gradients):\r\n                pass",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-4\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 3\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n\r\n        class SubModel(torch.nn.Module):\r\n\r\n            def __init__(self, input_size, output_size, dropout_prob=0.5, device=None):\r\n                super(SubModel, self).__init__()\r\n                self.linear = torch.nn.Linear(input_size, output_size, device=device)\r\n                self.dropout = torch.nn.Dropout(dropout_prob)\r\n                self.module_list = torch.nn.ModuleList([torch.nn.Linear(input_size, output_size, device=device)])\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = self.dropout(x)\r\n                x = self.module_list[0](x)\r\n                return x\r\n\r\n        class MyModel(torch.nn.Module):\r\n\r\n            def __init__(self, hidden_dim):\r\n                super(MyModel, self).__init__()\r\n                self.l1 = skip_init(Linear, hidden_dim, hidden_dim)\r\n                self.l2 = skip_init(SubModel, hidden_dim, hidden_dim)\r\n                self.l3 = torch.nn.Linear(hidden_dim, hidden_dim)\r\n                self.cel = torch.nn.CrossEntropyLoss()\r\n                self.l4 = skip_init(SubModel, hidden_dim, hidden_dim)\r\n\r\n            def forward(self, x, y):\r\n                x = self.l1(x)\r\n                x = self.l2(x)\r\n                x = self.l3(x)\r\n                x = self.l4(x)\r\n                loss = self.cel(x, y)\r\n                val = [x, loss]\r\n                return val\r\n\r\n        with deepspeed.zero.Init(config=config_dict):\r\n            model = MyModel(hidden_dim)\r\n        world_size = dist.get_world_size()\r\n        ds_tensor_numel = math.ceil(hidden_dim * hidden_dim / world_size)\r\n        assert model.l1.weight.ds_tensor.numel() == ds_tensor_numel\r\n        assert model.l2.linear.weight.ds_tensor.numel() == ds_tensor_numel\r\n        assert model.l2.module_list[0].weight.ds_tensor.numel() == ds_tensor_numel\r\n        assert model.l3.weight.ds_tensor.numel() == ds_tensor_numel\r\n        assert model.l4.linear.weight.ds_tensor.numel() == ds_tensor_numel\r\n        assert model.l4.module_list[0].weight.ds_tensor.numel() == ds_tensor_numel\r\n\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        dist.barrier()\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            loss = loss[1]\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(\r\n        self,\r\n        param_persistence_threshold: int,\r\n        contiguous_gradients: bool,\r\n        offload_optimizer: bool,\r\n        zero_grad: bool,\r\n        prefetching: bool,\r\n        reduce_scatter: bool,\r\n        model_class: EltwiseMultiplicationTestNetwork_Dict,\r\n    ) -> None:\r\n        if offload_optimizer and not contiguous_gradients:\r\n            return\r\n\r\n        m = 3\r\n        n = 5\r\n        weights = [Parameter(torch.zeros((m, n), dtype=torch.float32)) for _ in range(3)]\r\n        model = model_class(*weights)\r\n        prefetch_bucket_size = sum([p.numel() for p in model.parameters(recurse=True)])\r\n        cfg = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"stage3_param_persistence_threshold\": param_persistence_threshold,\r\n                \"contiguous_gradients\": contiguous_gradients,\r\n                \"stage3_prefetch_bucket_size\": prefetch_bucket_size if prefetching else 0,\r\n                \"reduce_scatter\": reduce_scatter,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.0\r\n                }\r\n            },\r\n            \"bf16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 1.0,\r\n            },\r\n        }\r\n\r\n        if offload_optimizer:\r\n            cfg[\"zero_optimization\"][\"offload_optimizer\"] = {\r\n                \"device\": \"cpu\",\r\n                \"pin_memory\": True,\r\n            }\r\n\r\n        ds_engine = _ds_initialize_for_param_partitioning_testing(model, cfg)\r\n        for i, weight in enumerate(weights):\r\n            weight.ds_tensor.data = torch.full_like(weight.ds_tensor.data, (i + 1) * (1 + dist.get_rank()))\r\n\r\n        def create_tensor(vals):\r\n            return torch.as_tensor(vals, dtype=torch.bfloat16, device=ds_engine.device)\r\n\r\n        expected_hidden1 = create_tensor([\r\n            [1, 1, 1, 1, 1],\r\n            [1, 1, 1, 2, 2],\r\n            [2, 2, 2, 2, 2],\r\n        ])\r\n        expected_hidden2 = create_tensor([\r\n            [2, 2, 2, 2, 2],\r\n            [2, 2, 2, 8, 8],\r\n            [8, 8, 8, 8, 8],\r\n        ])\r\n        expected_yhat = create_tensor([[6, 6, 6, 6, 6], [6, 6, 6, 48, 48], [48, 48, 48, 48, 48]])\r\n        expected_loss = create_tensor([\r\n            [5, 5, 5, 5, 5],\r\n            [5, 5, 5, 47, 47],\r\n            [47, 47, 47, 47, 47],\r\n        ])\r\n\r\n        for train_iter in range(3):\r\n            _assert_partition_status(ds_engine, {ZeroParamStatus.NOT_AVAILABLE})\r\n            activations = ds_engine(\r\n                x=torch.ones((m, n), dtype=torch.bfloat16, device=ds_engine.device),\r\n                y=torch.ones((m, n), dtype=torch.bfloat16, device=ds_engine.device),\r\n                use_module_trace=train_iter > 0,\r\n                param_prefetching=prefetching and train_iter > 0,\r\n            )\r\n            # for ease in testing convert outputs to dict.\r\n            activations = model_class.to_dict(activations)\r\n            assert torch.allclose(activations[\"hidden1\"], expected_hidden1)\r\n            assert torch.allclose(activations[\"hidden2\"], expected_hidden2)\r\n            assert torch.allclose(activations[\"y_hat\"], expected_yhat)\r\n            assert torch.allclose(activations[\"loss\"], expected_loss)\r\n\r\n            ds_engine.backward(activations[\"loss\"].sum())\r\n            _assert_partition_status(ds_engine, {ZeroParamStatus.NOT_AVAILABLE})\r\n\r\n            # check the gradients\r\n            grad_partitions = ds_engine.optimizer.get_fp32_grad_partitions()\r\n            assert set(grad_partitions.keys()) == {0\r\n                                                   }, f\"should have one parameter group but got {len(grad_partitions)}\"\r\n            assert set(grad_partitions[0].keys()) == {0, 1, 2}\r\n            dloss_wrt_layer1 = grad_partitions[0][0]\r\n            dloss_wrt_layer2 = grad_partitions[0][1]\r\n            dloss_wrt_layer3 = grad_partitions[0][2]\r\n\r\n            # layer1 = [..., 1, 2, ...]\r\n            # layer2 = [..., 2, 4, ...]\r\n            # layer3 = [..., 3, 6, ...]\r\n            # dloss_wrt_layer3 = hidden2\r\n            # dloss_wrt_layer2 = layer3 * hidden1\r\n            # dloss_wrt_layer1 = layer3 * layer2 * x\r\n\r\n            expected_grad_dtype = torch.float32 if offload_optimizer else torch.bfloat16\r\n\r\n            grad_multiplier = 1 if zero_grad else (train_iter + 1)\r\n            if dist.get_rank() == 0:\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer3.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([2] * 8).to(expected_grad_dtype),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer2.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([3 * 1] * 8).to(expected_grad_dtype),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer1.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor([3 * 2 * 1] * 8).to(expected_grad_dtype),\r\n                )\r\n            elif dist.get_rank() == 1:\r\n                # parameters dont split evenly across ranks so rank 1 has a zero-padded\r\n                # partition\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer3.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([8] * 7) + [0]).to(expected_grad_dtype),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer2.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([6 * 2] * 7) + [0]).to(expected_grad_dtype),\r\n                )\r\n                assert torch.allclose(\r\n                    dloss_wrt_layer1.to(get_accelerator().device_name()),\r\n                    grad_multiplier * create_tensor(([6 * 4 * 1] * 7) + [0]).to(expected_grad_dtype),\r\n                )\r\n            else:\r\n                raise RuntimeError(\"test has world size of two\")\r\n\r\n            if zero_grad:\r\n                ds_engine.optimizer.zero_grad()\r\n\r\n        # TODO. add testing for this - for now we just call it to make sure it\r\n        # doesn't throw\r\n        ds_engine.optimizer.step()\r\n        _assert_partition_status(ds_engine, {ZeroParamStatus.NOT_AVAILABLE})",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_gradient_accumulation(self, h_dim: int, n_layers: int, zpg: int) -> None:\r\n        # in this test case, we are testing that hpz should be enabled for the intermediate gradient accumulation steps\r\n        # In this test, we should disable loss_scale\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"gradient_accumulation_steps\": 3,\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_max_reuse_distance\": 0,\r\n                \"zero_hpz_partition_size\": zpg,\r\n                \"contiguous_gradients\": True,\r\n                \"overlap_comm\": True,\r\n            },\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1.\r\n                }\r\n            },\r\n            \"fp16\": {\r\n                \"enabled\": True,\r\n                \"loss_scale\": 0.,\r\n            }\r\n        }\r\n\r\n        model = NNModel(h_dim, n_layers)\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model, total_samples=20, hidden_dim=h_dim, device=model.device)\r\n        dist.barrier()\r\n        if zpg == 1:\r\n            _assert_no_secondary_tensor_group(model)\r\n\r\n        for n, batch in enumerate(data_loader):\r\n            if n == 0 and zpg != 1:\r\n                _assert_secondary_tensor_size(model)\r\n            # here we cannot assert that secondary tensor does not exist because the gradient is likely overflowed as we use random data\r\n            if n > 0 and n % 3 != 0 and zpg != 1:\r\n                # if the previous iteration does not update the model, then the hpz should be enabled\r\n                assert _check_secondary_tensor_existence(model), f\"n={n}\"\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test(self):\r\n        ds_config = {\r\n            \"train_batch_size\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 0.00015\r\n                }\r\n            }\r\n        }\r\n\r\n        class Model(torch.nn.Module):\r\n\r\n            def __init__(self):\r\n                super(Model, self).__init__()\r\n                self.linear = torch.nn.Linear(4, 4)\r\n\r\n            def magic(self):\r\n                return 42\r\n\r\n        with deepspeed.zero.Init():\r\n            model = Model()\r\n            engine, *_ = deepspeed.initialize(model=model, config=ds_config, model_parameters=model.parameters())\r\n        assert engine.magic() == 42",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test(self):\r\n        config_dict = {\r\n            \"train_batch_size\": 4,\r\n            \"gradient_accumulation_steps\": 2,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-4\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 1,\r\n                \"offload_optimizer\": {\r\n                    \"device\": \"cpu\"\r\n                }\r\n            },\r\n        }\r\n        if get_accelerator().is_fp16_supported():\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif get_accelerator().is_bf16_supported():\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n        hidden_dim = 10\r\n\r\n        model = SimpleModel(hidden_dim)\r\n        model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config_dict)\r\n        data_loader = random_dataloader(model=model, total_samples=50, hidden_dim=hidden_dim, device=model.device)\r\n        dist.barrier()\r\n        for n, batch in enumerate(data_loader):\r\n            loss = model(batch[0], batch[1])\r\n            model.backward(loss)\r\n            model.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_finegrained_optimization(self, module_granularity_threshold: int):\r\n        hidden_dim = 128\r\n        num_block = 16\r\n        config_dict = {\r\n            \"train_micro_batch_size_per_gpu\": 1,\r\n            \"steps_per_print\": 1,\r\n            \"optimizer\": {\r\n                \"type\": \"Adam\",\r\n                \"params\": {\r\n                    \"lr\": 1e-6\r\n                }\r\n            },\r\n            \"zero_optimization\": {\r\n                \"stage\": 3,\r\n                \"stage3_prefetch_bucket_size\": hidden_dim**2,\r\n                \"stage3_param_persistence_threshold\": 0,\r\n                \"stage3_max_reuse_distance\": 0,\r\n            }\r\n        }\r\n        if preferred_dtype() is torch.float16:\r\n            config_dict[\"fp16\"] = {\"enabled\": True}\r\n        elif preferred_dtype() is torch.bfloat16:\r\n            config_dict[\"bf16\"] = {\"enabled\": True}\r\n\r\n        def bench_loss_and_time(config):\r\n            warm_up_step = 10\r\n            model = modelWithFineGrainedBlock(hidden_dim, num_block)\r\n            model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config)\r\n            data_loader = random_dataloader(model=model,\r\n                                            total_samples=20,\r\n                                            hidden_dim=hidden_dim,\r\n                                            device=model.device,\r\n                                            dtype=preferred_dtype())\r\n            dist.barrier()\r\n            loss_list = []\r\n\r\n            for i, batch in enumerate(data_loader):\r\n                if i == warm_up_step:\r\n                    dist.barrier()\r\n                    get_accelerator().synchronize()\r\n                    start_time = time.time()\r\n                batch[0].requires_grad = True\r\n                loss = model(batch[0], batch[1])\r\n                loss = loss[1]\r\n                loss_list.append(loss)\r\n                model.backward(loss)\r\n                model.step()\r\n            get_accelerator().synchronize()\r\n            end_time = time.time()\r\n            duration = end_time - start_time\r\n            model.destroy()\r\n            return loss_list, duration\r\n\r\n        baseline_loss_list, baseline_exec_time = bench_loss_and_time(config_dict)\r\n\r\n        config_dict[\"zero_optimization\"][\"stage3_module_granularity_threshold\"] = module_granularity_threshold\r\n        loss, duration = bench_loss_and_time(config_dict)\r\n\r\n        if dist.get_rank() == 0:\r\n            print(f\"baseline exec time:\", baseline_exec_time)\r\n            print(\r\n                f\"finegrained optimziation exec time: {duration},granularity threshold:{module_granularity_threshold} \"\r\n            )\r\n            assert baseline_loss_list == loss, f\"incorrect loss value with threshold:{module_granularity_threshold}\"",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def bench_loss_and_time(config):\r\n            warm_up_step = 10\r\n            model = modelWithFineGrainedBlock(hidden_dim, num_block)\r\n            model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=config)\r\n            data_loader = random_dataloader(model=model,\r\n                                            total_samples=20,\r\n                                            hidden_dim=hidden_dim,\r\n                                            device=model.device,\r\n                                            dtype=preferred_dtype())\r\n            dist.barrier()\r\n            loss_list = []\r\n\r\n            for i, batch in enumerate(data_loader):\r\n                if i == warm_up_step:\r\n                    dist.barrier()\r\n                    get_accelerator().synchronize()\r\n                    start_time = time.time()\r\n                batch[0].requires_grad = True\r\n                loss = model(batch[0], batch[1])\r\n                loss = loss[1]\r\n                loss_list.append(loss)\r\n                model.backward(loss)\r\n                model.step()\r\n            get_accelerator().synchronize()\r\n            end_time = time.time()\r\n            duration = end_time - start_time\r\n            model.destroy()\r\n            return loss_list, duration",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_epoch(model, device, train_loader, optimizer, epoch):\n    loss_fn = torch.nn.CrossEntropyLoss()\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, training_step,\n          lr_scheduler: _LRScheduler, max_steps: int, max_epochs: int):\n    assert max_epochs is not None or max_steps is not None\n    train_loader, test_loader = prepare_dataloader()\n    max_steps = max_steps if max_steps else max_epochs * len(train_loader)\n    max_epochs = max_steps // len(train_loader) + (0 if max_steps % len(train_loader) == 0 else 1)\n    count_steps = 0\n\n    model.train()\n    for epoch in range(max_epochs):\n        for data, target in train_loader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            loss = training_step((data, target), model)\n            loss.backward()\n            optimizer.step()\n            count_steps += 1\n            if count_steps >= max_steps:\n                acc = evaluate(model, test_loader)\n                print(f'[Training Epoch {epoch} / Step {count_steps}] Final Acc: {acc}%')\n                return\n        acc = evaluate(model, test_loader)\n        print(f'[Training Epoch {epoch} / Step {count_steps}] Final Acc: {acc}%')",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Optimizer, training_step: Callable, scheduler: Union[SCHEDULER, None] = None,\n                   max_steps: Union[int, None] = None, max_epochs: Union[int, None] = None):\n    model.train()\n    max_epochs = max_epochs if max_epochs else 1 if max_steps is None else 100\n    current_steps = 0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            optimizer.zero_grad()\n            loss = training_step(batch, model)\n            loss.backward()\n            optimizer.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n        if scheduler is not None:\n            scheduler.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(model: torch.nn.Module, optimizer: Optimizer, training_step: Callable, scheduler: Union[SCHEDULER, None] = None,\n          max_steps: Union[int, None] = None, max_epochs: Union[int, None] = 400):\n    best_top1 = 0\n    max_epochs = max_epochs or (40 if max_steps is None else 400)\n    for epoch in range(max_epochs):\n        print('# Epoch {} #'.format(epoch))\n        model.train()\n        for batch_idx, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n            loss = training_step(batch, model)\n            loss.backward()\n            optimizer.step()\n            if isinstance(scheduler, SCHEDULER):\n                scheduler.step()\n            if batch_idx % 100 == 0:\n                print('{:2.0f}%  Loss {}'.format(100 * batch_idx / len(train_loader), loss.item()))\n\n        adjust_learning_rate(optimizer, epoch)\n        top1 = test(model)\n        if top1 > best_top1:\n            best_top1 = top1\n        print(f\"epoch={epoch}\\tcurrent_acc={top1}\\tbest_acc={best_top1}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_func(model: torch.nn.Module, optimizer: torch.optim.Optimizer, training_step: Callable[[Any, torch.nn.Module], torch.Tensor],\n                  lr_scheduler: torch.optim.lr_scheduler._LRScheduler, max_steps: int, max_epochs: int):\n    # create a train dataloader (and test dataloader if needs)\n    train_dataloader, test_dataloader = prepare_dataloader()\n\n    # deal with training duration, NNI prefers to prioritize the largest number of steps\n    # at least `max_steps` or `max_epochs` will be given\n    assert max_steps is not None or max_epochs is not None\n    total_steps = max_steps if max_steps else max_epochs * len(train_dataloader)\n    total_epochs = total_steps // len(train_dataloader) + (0 if total_steps % len(train_dataloader) == 0 else 1)\n\n    # here is a common training loop\n    current_step = 0\n    for _ in range(total_epochs):\n        for batch in train_dataloader:\n            loss = training_step(batch, model)\n            loss.backward()\n            optimizer.step()\n\n            # if reach the total steps, exit from the training loop\n            current_step = current_step + 1\n            if current_step >= total_steps:\n                return\n\n        # if you are using a epoch-wise scheduler, call it here\n        lr_scheduler.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Union[Optimizer, List[Optimizer]], \\\n                   training_step: _TRAINING_STEP, scheduler: Union[None, SCHEDULER, List[SCHEDULER]] = None,\n                   max_steps: Union[int, None] = None, max_epochs: Union[int, None] = None):\n    model.train()\n    max_epochs = max_epochs or (40 if max_steps is None else 100)\n    current_steps = 0\n    best_acc = 0.0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            if isinstance(optimizer, Optimizer):\n                optimizer.zero_grad()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.zero_grad()\n            loss = training_step(batch, model)\n            assert isinstance(loss, torch.Tensor)\n            loss.backward()\n            if isinstance(optimizer, Optimizer):\n                optimizer.step()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.step()\n            if isinstance(scheduler, SCHEDULER):\n                scheduler.step()\n            if isinstance(scheduler, List) and all(isinstance(_, SCHEDULER) for _ in scheduler):\n                for sch in scheduler:\n                    sch.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n\n        acc = evaluating_model(model)\n        best_acc = max(acc, best_acc)\n        print(f\"epoch={epoch}\\tacc={acc}\\tbest_acc={best_acc}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Optimizer, training_step: Callable, scheduler: SCHEDULER | None = None,\n                   max_steps: int | None = None, max_epochs: int | None = None):\n    model.train()\n    max_epochs = max_epochs if max_epochs else 1 if max_steps is None else 100\n    current_steps = 0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            optimizer.zero_grad()\n            loss = training_step(batch, model)\n            loss.backward()\n            optimizer.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n        if scheduler is not None:\n            scheduler.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Union[Optimizer, List[Optimizer]], \\\n                   training_step: _TRAINING_STEP, scheduler: Union[None, SCHEDULER, List[SCHEDULER]] = None,\n                   max_steps: Union[int, None] = None, max_epochs: Union[int, None] = None):\n    model.train()\n    max_epochs = max_epochs or (10 if max_steps is None else 100)\n    current_steps = 0\n    best_acc = 0.0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            if isinstance(optimizer, Optimizer):\n                optimizer.zero_grad()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.zero_grad()\n            loss = training_step(batch, model)\n            assert isinstance(loss, torch.Tensor)\n            loss.backward()\n            if isinstance(optimizer, Optimizer):\n                optimizer.step()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.step()\n            if isinstance(scheduler, SCHEDULER):\n                scheduler.step()\n            if isinstance(scheduler, List) and all(isinstance(_, SCHEDULER) for _ in scheduler):\n                for sch in scheduler:\n                    sch.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n\n        acc = evaluating_model(model)\n        best_acc = max(acc, best_acc)\n        print(f\"epoch={epoch}\\tacc={acc}\\tbest_acc={best_acc}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Union[Optimizer, List[Optimizer]], \\\n                   training_step: _TRAINING_STEP, scheduler: Union[None, SCHEDULER, List[SCHEDULER]] = None,\n                   max_steps: Union[int, None] = None, max_epochs: Union[int, None] = None):\n    model.train()\n    max_epochs = max_epochs if max_epochs else 10 if max_steps is None else 100\n    current_steps = 0\n    best_acc = 0.0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            if isinstance(optimizer, Optimizer):\n                optimizer.zero_grad()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.zero_grad()\n            loss = training_step(batch, model)\n            assert isinstance(loss, torch.Tensor)\n            loss.backward()\n            if isinstance(optimizer, Optimizer):\n                optimizer.step()\n            elif isinstance(optimizer, List) and all(isinstance(_, Optimizer) for _ in optimizer):\n                for opt in optimizer:\n                    opt.step()\n            if isinstance(scheduler, SCHEDULER):\n                scheduler.step()\n            if isinstance(scheduler, List) and all(isinstance(_, SCHEDULER) for _ in scheduler):\n                for sch in scheduler:\n                    sch.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n\n        acc = evaluating_model(model)\n        best_acc = max(acc, best_acc)\n        print(f\"epoch={epoch}\\tacc={acc}\\tbest_acc={best_acc}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test():\n    url_zip_train = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2'\n    urllib.request.urlretrieve(url_zip_train, filename='train.bz2')\n\n    f_svm = open('train.svm', 'wt')\n    with bz2.open('train.bz2', 'rb') as f_zip:\n        data = f_zip.read()\n        f_svm.write(data.decode('utf-8'))\n    f_svm.close()\n\n\n    X, y = load_svmlight_file('train.svm')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n    pipeline = make_pipeline(FeatureGradientSelector(n_epochs=1, n_features=10), LogisticRegression())\n    # pipeline = make_pipeline(SelectFromModel(ExtraTreesClassifier(n_estimators=50)), LogisticRegression())\n\n    pipeline.fit(X_train, y_train)\n\n    print(\"Pipeline Score: \", pipeline.score(X_train, y_train))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train(logger, config, train_loader, model, optimizer, criterion, epoch, main_proc):\n    meters = AverageMeterGroup()\n    cur_lr = optimizer.param_groups[0][\"lr\"]\n    if main_proc:\n        logger.info(\"Epoch %d LR %.6f\", epoch, cur_lr)\n\n    model.train()\n    for step, (x, y) in enumerate(train_loader):\n        x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n        optimizer.zero_grad()\n        logits, aux_logits = model(x)\n        loss = criterion(logits, y)\n        if config.aux_weight > 0.:\n            loss += config.aux_weight * criterion(aux_logits, y)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n\n        prec1, prec5 = utils.accuracy(logits, y, topk=(1, 5))\n        metrics = {\"prec1\": prec1, \"prec5\": prec5, \"loss\": loss}\n        metrics = utils.reduce_metrics(metrics, config.distributed)\n        meters.update(metrics)\n\n        if main_proc and (step % config.log_frequency == 0 or step + 1 == len(train_loader)):\n            logger.info(\"Epoch [%d/%d] Step [%d/%d]  %s\", epoch + 1, config.epochs, step + 1, len(train_loader), meters)\n\n    if main_proc:\n        logger.info(\"Train: [%d/%d] Final Prec@1 %.4f Prec@5 %.4f\", epoch + 1, config.epochs, meters.prec1.avg, meters.prec5.avg)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_epoch(\n        epoch, model, loader, optimizer, loss_fn, cfg,\n        lr_scheduler=None, saver=None, output_dir='', use_amp=False,\n        model_ema=None, logger=None, writer=None, local_rank=0):\n    batch_time_m = AverageMeter()\n    data_time_m = AverageMeter()\n    losses_m = AverageMeter()\n    prec1_m = AverageMeter()\n    prec5_m = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    num_updates = epoch * len(loader)\n    optimizer.zero_grad()\n    for batch_idx, (input, target) in enumerate(loader):\n        last_batch = batch_idx == last_idx\n        data_time_m.update(time.time() - end)\n\n        input = input.cuda()\n        target = target.cuda()\n        output = model(input)\n\n        loss = loss_fn(output, target)\n\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n\n        if cfg.NUM_GPU > 1:\n            reduced_loss = reduce_tensor(loss.data, cfg.NUM_GPU)\n            prec1 = reduce_tensor(prec1, cfg.NUM_GPU)\n            prec5 = reduce_tensor(prec5, cfg.NUM_GPU)\n        else:\n            reduced_loss = loss.data\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n\n        losses_m.update(reduced_loss.item(), input.size(0))\n        prec1_m.update(prec1.item(), output.size(0))\n        prec5_m.update(prec5.item(), output.size(0))\n\n        if model_ema is not None:\n            model_ema.update(model)\n        num_updates += 1\n\n        batch_time_m.update(time.time() - end)\n        if last_batch or batch_idx % cfg.LOG_INTERVAL == 0:\n            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n            lr = sum(lrl) / len(lrl)\n\n            if local_rank == 0:\n                logger.info(\n                    'Train: {} [{:>4d}/{}] '\n                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f}) '\n                    'Prec@1: {top1.val:>7.4f} ({top1.avg:>7.4f}) '\n                    'Prec@5: {top5.val:>7.4f} ({top5.avg:>7.4f}) '\n                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s '\n                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s) '\n                    'LR: {lr:.3e}'\n                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n                        epoch,\n                        batch_idx,\n                        len(loader),\n                        loss=losses_m,\n                        top1=prec1_m,\n                        top5=prec5_m,\n                        batch_time=batch_time_m,\n                        rate=input.size(0) *\n                        cfg.NUM_GPU /\n                        batch_time_m.val,\n                        rate_avg=input.size(0) *\n                        cfg.NUM_GPU /\n                        batch_time_m.avg,\n                        lr=lr,\n                        data_time=data_time_m))\n\n                writer.add_scalar(\n                    'Loss/train',\n                    prec1_m.avg,\n                    epoch *\n                    len(loader) +\n                    batch_idx)\n                writer.add_scalar(\n                    'Accuracy/train',\n                    prec1_m.avg,\n                    epoch *\n                    len(loader) +\n                    batch_idx)\n                writer.add_scalar(\n                    'Learning_Rate',\n                    optimizer.param_groups[0]['lr'],\n                    epoch * len(loader) + batch_idx)\n\n                if cfg.SAVE_IMAGES and output_dir:\n                    torchvision.utils.save_image(\n                        input, os.path.join(\n                            output_dir, 'train-batch-%d.jpg' %\n                            batch_idx), padding=0, normalize=True)\n\n        if saver is not None and cfg.RECOVERY_INTERVAL and (\n                last_batch or (batch_idx + 1) % cfg.RECOVERY_INTERVAL == 0):\n            saver.save_recovery(\n                model,\n                optimizer,\n                cfg,\n                epoch,\n                model_ema=model_ema,\n                use_amp=use_amp,\n                batch_idx=batch_idx)\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(\n                num_updates=num_updates,\n                metric=losses_m.avg)\n\n        end = time.time()\n        # end for\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    return OrderedDict([('loss', losses_m.avg)])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_epoch(\n    model,\n    auxiliarynet,\n    criterion,\n    train_loader,\n    device,\n    epoch,\n    optimizer,\n    logger,\n):\n    \"\"\"Train one epoch.\"\"\"\n    model.train()\n    auxiliarynet.train()\n\n    batch_time = AverageMeter(\"batch_time\")\n    data_time = AverageMeter(\"data_time\")\n    losses = AverageMeter(\"losses\")\n\n    end = time.time()\n    for i, (img, landmark_gt, angle_gt) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n        img = img.to(device, non_blocking=True)\n        landmark_gt = landmark_gt.to(device, non_blocking=True)\n        angle_gt = angle_gt.to(device, non_blocking=True)\n\n        lands, feats = model(img)\n        landmarks = lands.squeeze()\n        angle = auxiliarynet(feats)\n\n        # task loss\n        weighted_loss, _ = criterion(\n            landmark_gt, angle_gt, angle, landmarks\n        )\n        loss = weighted_loss\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        # measure accuracy and record loss\n        losses.update(np.squeeze(loss.cpu().detach().numpy()), img.size(0))\n\n        if i % 10 == 0:\n            batch_log = (\n                \"Train [{0}][{1}]\\t\"\n                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n                \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n                \"Loss {losses.val:.4f} ({losses.avg:.4f})\".format(\n                    epoch + 1,\n                    i,\n                    batch_time=batch_time,\n                    data_time=data_time,\n                    losses=losses,\n                )\n            )\n            logger.info(batch_log)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def __init__(self, root: str, train: bool = True):\n        filename = 'train.csv' if train else 'eval.csv'\n        if not os.path.exists(os.path.join(root, filename)):\n            download_url(os.path.join('https://storage.googleapis.com/tf-datasets/titanic/', filename), root, filename)\n\n        df = pd.read_csv(os.path.join(root, filename))\n        object_colunmns = df.select_dtypes(include='object').columns.values\n        for idx in df.columns:\n            if idx in object_colunmns:\n                df[idx] = LabelEncoder().fit_transform(df[idx])\n           \n        self.x = df.iloc[:, 1:].values\n        self.y = df.iloc[:, 0].values",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train(config, train_loader, model, optimizer, criterion, epoch):\n    top1 = AverageMeter(\"top1\")\n    top5 = AverageMeter(\"top5\")\n    losses = AverageMeter(\"losses\")\n\n    cur_step = epoch * len(train_loader)\n    cur_lr = optimizer.param_groups[0][\"lr\"]\n    logger.info(\"Epoch %d LR %.6f\", epoch, cur_lr)\n    writer.add_scalar(\"lr\", cur_lr, global_step=cur_step)\n\n    model.train()\n\n    for step, (x, y) in enumerate(train_loader):\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        bs = x.size(0)\n\n        optimizer.zero_grad()\n        logits, aux_logits = model(x)\n        loss = criterion(logits, y)\n        if config.aux_weight > 0.:\n            loss += config.aux_weight * criterion(aux_logits, y)\n        loss.backward()\n        # gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n\n        accuracy = utils.accuracy(logits, y, topk=(1, 5))\n        losses.update(loss.item(), bs)\n        top1.update(accuracy[\"acc1\"], bs)\n        top5.update(accuracy[\"acc5\"], bs)\n        writer.add_scalar(\"loss/train\", loss.item(), global_step=cur_step)\n        writer.add_scalar(\"acc1/train\", accuracy[\"acc1\"], global_step=cur_step)\n        writer.add_scalar(\"acc5/train\", accuracy[\"acc5\"], global_step=cur_step)\n\n        if step % config.log_frequency == 0 or step == len(train_loader) - 1:\n            logger.info(\n                \"Train: [{:3d}/{}] Step {:03d}/{:03d} Loss {losses.avg:.3f} \"\n                \"Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})\".format(\n                    epoch + 1, config.epochs, step, len(train_loader) - 1, losses=losses,\n                    top1=top1, top5=top5))\n\n        cur_step += 1\n\n    logger.info(\"Train: [{:3d}/{}] Final Prec@1 {:.4%}\".format(epoch + 1, config.epochs, top1.avg))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_epoch(model, device, train_loader, optimizer, epoch):\n    loss_fn = torch.nn.CrossEntropyLoss()\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_one_epoch(self, adjust_lr_func, train_log_func, label_smoothing=0.1):\n        batch_time = AverageMeter('batch_time')\n        data_time = AverageMeter('data_time')\n        losses = AverageMeter('losses')\n        top1 = AverageMeter('top1')\n        top5 = AverageMeter('top5')\n        self.model.train()\n        end = time.time()\n        for i, (images, labels) in enumerate(self.train_loader):\n            data_time.update(time.time() - end)\n            new_lr = adjust_lr_func(i)\n            images, labels = images.to(self.device), labels.to(self.device)\n            output = self.model(images)\n            if label_smoothing > 0:\n                loss = cross_entropy_with_label_smoothing(output, labels, label_smoothing)\n            else:\n                loss = self.criterion(output, labels)\n            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n            losses.update(loss, images.size(0))\n            top1.update(acc1[0], images.size(0))\n            top5.update(acc5[0], images.size(0))\n\n            # compute gradient and do SGD step\n            self.model.zero_grad()  # or self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 10 == 0 or i + 1 == len(self.train_loader):\n                batch_log = train_log_func(i, batch_time, data_time, losses, top1, top5, new_lr)\n                print(batch_log)\n        return top1, top5",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _train_epoch(self, epoch, optimizer, arch_train=False):\n        \"\"\"\n        Train one epoch.\n        \"\"\"\n        # switch to train mode\n        self.model.train()\n        self.auxiliarynet.train()\n\n        batch_time = AverageMeter(\"batch_time\")\n        data_time = AverageMeter(\"data_time\")\n        losses = AverageMeter(\"losses\")\n\n        data_loader = self.valid_loader if arch_train else self.train_loader\n        end = time.time()\n        for i, (img, landmark_gt, angle_gt) in enumerate(data_loader):\n            data_time.update(time.time() - end)\n            img = img.to(self.device, non_blocking=True)\n            landmark_gt = landmark_gt.to(self.device, non_blocking=True)\n            angle_gt = angle_gt.to(self.device, non_blocking=True)\n\n            lands, feats = self.model(img)\n            landmarks = lands.squeeze()\n            angle = self.auxiliarynet(feats)\n\n            # task loss\n            weighted_loss, l2_loss = self.criterion(\n                landmark_gt, angle_gt, angle, landmarks\n            )\n            loss = l2_loss if arch_train else weighted_loss\n\n            # hardware-aware loss\n            perf_cost = self._get_perf_cost(requires_grad=True)\n            regu_loss = self.reg_loss(perf_cost)\n            if self.mode.startswith(\"mul\"):\n                loss = loss * regu_loss\n            elif self.mode.startswith(\"add\"):\n                loss = loss + regu_loss\n\n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n            # measure accuracy and record loss\n            losses.update(np.squeeze(loss.cpu().detach().numpy()), img.size(0))\n\n            if i % 10 == 0:\n                batch_log = (\n                    \"Train [{0}][{1}]\\t\"\n                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n                    \"Loss {losses.val:.4f} ({losses.avg:.4f})\".format(\n                        epoch + 1,\n                        i,\n                        batch_time=batch_time,\n                        data_time=data_time,\n                        losses=losses,\n                    )\n                )\n                self.logger.info(batch_log)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(model):\n        model.train()\n        src_mask = generate_square_subsequent_mask(seq_len).to(device)\n        for i in range(0, train_data.size(0) - 1, seq_len):\n            data, target = get_seq(train_data, i)\n            part_len = data.size(0)\n            if part_len != seq_len:\n                src_mask = src_mask[:part_len, :part_len]\n            output = model(data, src_mask)\n            loss = F.cross_entropy(output.view(-1, output.size(-1)), target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_epoch(model, device, train_loader, optimizer, epoch):\n    loss_fn = torch.nn.CrossEntropyLoss()\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def training_model(model: torch.nn.Module, optimizer: Optimizer, training_step: Callable, scheduler: Union[SCHEDULER, None] = None,\n                   max_steps: Union[int, None] = None, max_epochs: Union[int, None] = None):\n    model.train()\n    max_epochs = max_epochs if max_epochs else 1 if max_steps is None else 100\n    current_steps = 0\n\n    # training\n    for epoch in range(max_epochs):\n        print(f'Epoch {epoch} start!')\n        for batch in train_dataloader:\n            optimizer.zero_grad()\n            loss = training_step(batch, model)\n            loss.backward()\n            optimizer.step()\n            current_steps += 1\n            if max_steps and current_steps == max_steps:\n                return\n        if scheduler is not None:\n            scheduler.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def copy_attr_new(from_module: torch.nn.Module, to_module: torch.nn.Module, target: str):\n        *prefix, field = target.split('.')\n        for item in prefix:\n            f = getattr(from_module, item)\n            t = getattr(to_module, item, None)\n            if f is t:\n                return\n\n            if t is None:\n                if isinstance(f, Sequential):\n                    t = Sequential()\n                elif isinstance(f, ModuleList):\n                    t = ModuleList()\n                elif isinstance(f, ModuleDict):\n                    t = ModuleDict()\n                else:\n                    t = torch.nn.Module()\n                if hasattr(f, '_get_name'):\n                    t._get_name = f._get_name\n                to_module.add_module(item, t)\n            from_module, to_module = f, t\n\n        orig = getattr(from_module, field)\n        # If it is a tensor and not a parameter attribute of a module, it should be a named buffer.\n        # So, we register it as a named buffer in the target module.\n        if isinstance(orig, torch.Tensor) and not isinstance(orig, torch.nn.Parameter):\n            to_module.register_buffer(field, orig)\n        else:\n            setattr(to_module, field, orig)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self,\n                 num_labels: int = 1000,\n                 channel_search: bool = False,\n                 affine: bool = True):\n        super().__init__()\n\n        self.num_labels = num_labels\n        self.channel_search = channel_search\n        self.affine = affine\n\n        # the block number in each stage. 4 stages in total. 20 blocks in total.\n        self.stage_repeats = [4, 4, 8, 4]\n\n        # output channels for all stages, including the very first layer and the very last layer\n        self.stage_out_channels = [-1, 16, 64, 160, 320, 640, 1024]\n\n        # building first layer\n        out_channels = self.stage_out_channels[1]\n        self.first_conv = nn.Sequential(\n            nn.Conv2d(3, out_channels, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n        feature_blocks = []\n\n        global_block_idx = 0\n        for stage_idx, num_repeat in enumerate(self.stage_repeats):\n            for block_idx in range(num_repeat):\n                # count global index to give names to choices\n                global_block_idx += 1\n\n                # get ready for input and output\n                in_channels = out_channels\n                out_channels = self.stage_out_channels[stage_idx + 2]\n                stride = 2 if block_idx == 0 else 1\n\n                # mid channels can be searched\n                base_mid_channels = out_channels // 2\n                if self.channel_search:\n                    k_choice_list = [int(base_mid_channels * (.2 * k)) for k in range(1, 9)]\n                    mid_channels = nni.choice(f'channel_{global_block_idx}', k_choice_list)\n                else:\n                    mid_channels = int(base_mid_channels)\n\n                mid_channels = cast(Union[int, MutableExpression[int]], mid_channels)\n\n                choice_block = LayerChoice(dict(\n                    k3=ShuffleNetBlock(in_channels, out_channels, mid_channels=mid_channels, kernel_size=3, stride=stride, affine=affine),\n                    k5=ShuffleNetBlock(in_channels, out_channels, mid_channels=mid_channels, kernel_size=5, stride=stride, affine=affine),\n                    k7=ShuffleNetBlock(in_channels, out_channels, mid_channels=mid_channels, kernel_size=7, stride=stride, affine=affine),\n                    xcep=ShuffleXceptionBlock(in_channels, out_channels, mid_channels=mid_channels, stride=stride, affine=affine)\n                ), label=f'layer_{global_block_idx}')\n                feature_blocks.append(choice_block)\n\n        self.features = nn.Sequential(*feature_blocks)\n\n        # final layers\n        last_conv_channels = self.stage_out_channels[-1]\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(out_channels, last_conv_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(last_conv_channels, affine=affine),\n            nn.ReLU(inplace=True),\n        )\n        self.globalpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Sequential(\n            nn.Linear(last_conv_channels, num_labels, bias=False),\n        )\n\n        self._initialize_weights()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_torchvision_models(model_fn):\n    model = model_fn()\n    model.eval()\n    dummy_inputs = (torch.rand(2, 3, 224, 224), )\n    traced = concrete_trace(model, dummy_inputs, use_operator_patch=True)\n    out_orig = model.forward(*dummy_inputs)\n    out_traced = traced.forward(*dummy_inputs)\n    assert check_equal(out_orig, out_traced), f'{traced.code}'\n    del out_orig, out_traced",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_mmdetection(config_file: str):\n    torch.cuda.empty_cache()\n\n    folder_prefix = os.environ['MMDET_DIR']\n\n    # Specify the path to model config and checkpoint file\n    config = Config.fromfile(folder_prefix + '/configs/' + config_file + '.py')\n\n    # RoIAlign will cause many errors. there are 4 ways to avoid it\n    # 1. add 'mmcv_ops.RoIAlign' to leaf_module when tracing, and set 'config_dict['use_torchvision'] = True' recursively\n    # 2. add 'mmcv_ops.RoIAlign' to leaf_module when tracing, and set 'config_dict['aligned'] = False' recursively\n    # 3. set 'config_dict['use_torchvision'] = True', and set 'config_dict['aligned'] = False' recursively, and\n    #            from torchvision.ops import roi_align as tv_roi_align\n    #            add 'tv_roi_align: (((torchvision.ops, 'roi_align'),), False, None),' to autowrap_leaf_function\n    # 4. set 'config_dict['aligned'] = False' recursively, and add 'from mmcv.ops.roi_align import roi_align as mmcv_roi_align' and\n    #            add 'mmcv_roi_align: ((), False, None),' to autowrap_leaf_function\n    RoIAlign_solution = 3\n\n    def roi_align_setter(config_dict: dict):\n        if 'type' in config_dict:\n            if config_dict['type'] == 'RoIAlign':\n                if RoIAlign_solution in (1, 3):\n                    config_dict['use_torchvision'] = True\n                if RoIAlign_solution in (2, 3, 4):\n                    config_dict['aligned'] = False\n                pass\n            else:\n                for v in config_dict.values():\n                    if isinstance(v, dict):\n                        roi_align_setter(v)\n    roi_align_setter(config._cfg_dict['model'])\n\n    leaf_module_append = ()\n    if RoIAlign_solution in (1, 2, 3):\n        from mmcv import ops as mmcv_ops\n        leaf_module_append = (mmcv_ops.RoIAlign,)\n\n    model = init_detector(config, device=device)\n\n    with torch.no_grad():\n        packed_inputs = demo_mm_inputs()\n        \n        dummy_inputs = model.data_preprocessor(packed_inputs, False)\n        \n        # init run\n        # some models need to be run 2 times before doing trace\n        model.forward(**dummy_inputs)\n        model.forward(**dummy_inputs)\n\n        seed = torch.seed()\n        torch.manual_seed(seed)\n        out_orig_1 = model.forward(**dummy_inputs)\n        torch.manual_seed(seed)\n        out_orig_2 = model.forward(**dummy_inputs)\n        assert check_equal(out_orig_1, out_orig_2), 'check_equal failure for original model'\n        del out_orig_1, out_orig_2\n\n        if config_file == 'pvt/retinanet_pvt-l_fpn_1x_coco':\n            # to support numpy.intc\n            import torch.fx as torch_fx\n            from numpy import intc, int64\n            orig_base_types = torch_fx.proxy.base_types\n            torch_fx.proxy.base_types = (*torch_fx.proxy.base_types, intc, int64)\n\n        traced_model = concrete_trace(model, dummy_inputs,\n                                      use_operator_patch=True,\n                                      forward_function_name='forward',\n                                      autowrap_leaf_function = {\n                                        **ConcreteTracer.default_autowrap_leaf_function,\n                                        all:  ((), False, None),\n                                        min:  ((), False, None),\n                                        max:  ((), False, None),\n                                      }, autowrap_leaf_class = {\n                                        **ConcreteTracer.default_autowrap_leaf_class,\n                                        int:        ((), False),\n                                        reversed:   ((), False),\n                                        torch.Size: ((), False),\n                                      }, leaf_module = (\n                                        *leaf_module_append,\n                                        mmcv_cnn.bricks.wrappers.Conv2d,\n                                        mmcv_cnn.bricks.wrappers.Conv3d,\n                                        mmcv_cnn.bricks.wrappers.ConvTranspose2d,\n                                        mmcv_cnn.bricks.wrappers.ConvTranspose3d,\n                                        mmcv_cnn.bricks.wrappers.Linear,\n                                        mmcv_cnn.bricks.wrappers.MaxPool2d,\n                                        mmcv_cnn.bricks.wrappers.MaxPool3d,\n                                      ), fake_middle_class = (\n                                        mmdet_models.task_modules.prior_generators.AnchorGenerator,\n        ))\n\n        if config_file == 'pvt/retinanet_pvt-l_fpn_1x_coco':\n            torch_fx.proxy.base_types = orig_base_types\n\n        seed = torch.seed()\n        torch.manual_seed(seed)\n        out_orig = model.forward(**dummy_inputs)\n        torch.manual_seed(seed)\n        out_orig_traced = traced_model(**dummy_inputs)\n        assert check_equal(out_orig, out_orig_traced), 'check_equal failure in original inputs'\n        del out_orig, out_orig_traced",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_torchvision_models(model_fn):\n    model = model_fn()\n    model.eval()\n    dummy_inputs = (torch.rand(2, 3, 224, 224), )\n    traced = concrete_trace(model, dummy_inputs, dce=False)\n    traced_dce = concrete_trace(model, dummy_inputs, dce=True)\n    out_orig = model.forward(*dummy_inputs)\n    out_traced = traced.forward(*dummy_inputs)\n    out_traced_dce = traced_dce.forward(*dummy_inputs)\n    assert check_equal(out_orig, out_traced), f'{traced.code}'\n    assert check_equal(out_orig, out_traced_dce), f'{traced_dce.code}'\n    del out_orig, out_traced, out_traced_dce",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_pruner_speedup(model_fn):\n    model = model_fn()\n    dummy_inputs = (torch.rand(2, 3, 224, 224), )\n    \n    config_list = [{\n        'op_types': ['Conv2d'],\n        'sparsity': 0.5\n    }]\n    traced = concrete_trace(model, dummy_inputs, use_operator_patch=True)\n    config_list = auto_set_denpendency_group_ids(traced, config_list)\n    \n    pruner = L1NormPruner(model, config_list)\n    _, masks = pruner.compress()\n    pruner.unwrap_model()\n\n    ModelSpeedup(model, dummy_inputs, masks, graph_module=traced).speedup_model()\n    traced.forward(*dummy_inputs)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_parameters():\n    class Model(ModelSpace):\n        def __init__(self):\n            super().__init__()\n            self.op = DifferentiableMixedLayer(\n                {\n                    'a': MutableLinear(2, 3, bias=False),\n                    'b': MutableLinear(2, 3, bias=True)\n                },\n                nn.Parameter(torch.randn(2)), nn.Softmax(-1), 'abc'\n            )\n\n        def forward(self, x):\n            return self.op(x)\n\n    model = Model()\n    assert len(list(model.parameters())) == 4\n    assert len(list(model.op.arch_parameters())) == 1\n\n    optimizer = torch.optim.SGD(model.parameters(), 0.1)\n    assert len(DartsLightningModule(model).arch_parameters()) == 1\n    optimizer = DartsLightningModule(model).postprocess_weight_optimizers(optimizer)\n    assert len(optimizer.param_groups[0]['params']) == 3",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def simple_evaluation(model, num_batches=20):\n    train_dataset = RandomMnistDataset(1000)\n    train_loader = DataLoader(train_dataset, 64, shuffle=True)\n    valid_dataset = RandomMnistDataset(200)\n    valid_loader = DataLoader(valid_dataset, 64, shuffle=True)\n\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    criterion = nn.CrossEntropyLoss()\n    for _, (x, y) in zip(range(num_batches), train_loader):\n        y_hat = model(x)\n        loss = criterion(y_hat, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    accurate, total = 0, 0\n    for _, (x, y) in zip(range(num_batches), valid_loader):\n        y_hat = model(x)\n        accurate += (y_hat.argmax(1) == y).sum().item()\n        total += y.shape[0]\n    nni.report_final_result(accurate / total)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_feature_names():\r\n    dataset = load_breast_cancer()\r\n    X = dataset.data\r\n    y = dataset.target\r\n    feature_names = [f\"feat{i}\" for i in range(X.shape[1])]\r\n\r\n    check_dmatrix(\r\n        X,\r\n        y,\r\n        feature_names=feature_names,\r\n    )\r\n\r\n    dmatrix = xgb.DMatrix(X, label=y, feature_names=feature_names)\r\n    md_dmatrix = mxgb.DMatrix(\r\n        pd.DataFrame(X), label=pd.Series(y), feature_names=feature_names\r\n    )\r\n\r\n    params = {\r\n        \"objective\": \"binary:logistic\",\r\n        \"eval_metric\": \"mlogloss\",\r\n    }\r\n\r\n    booster = xgb.train(params, dmatrix, num_boost_round=10)\r\n    md_booster = mxgb.train(params, md_dmatrix, num_boost_round=10)\r\n\r\n    predictions = booster.predict(dmatrix)\r\n    modin_predictions = md_booster.predict(md_dmatrix)\r\n\r\n    preds = pandas.DataFrame(predictions).apply(np.round, axis=0)\r\n    modin_preds = modin_predictions.apply(np.round, axis=0)\r\n\r\n    accuracy = accuracy_score(y, preds)\r\n    md_accuracy = accuracy_score(y, modin_preds)\r\n\r\n    np.testing.assert_allclose(accuracy, md_accuracy, atol=0.005, rtol=0.002)\r\n\r\n    # Different feature_names (default) must raise error in this case\r\n    dm = xgb.DMatrix(X)\r\n    md_dm = mxgb.DMatrix(pd.DataFrame(X))\r\n    with pytest.raises(ValueError):\r\n        booster.predict(dm)\r\n    with pytest.raises(ValueError):\r\n        try_cast_to_pandas(md_booster.predict(md_dm))",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _map_predict(booster, part, columns, dmatrix_kwargs={}, **kwargs):\r\n    \"\"\"\r\n    Run prediction on a remote worker.\r\n\r\n    Parameters\r\n    ----------\r\n    booster : xgboost.Booster or ray.ObjectRef\r\n        A trained booster.\r\n    part : pandas.DataFrame or ray.ObjectRef\r\n        Partition of full data used for local prediction.\r\n    columns : list or ray.ObjectRef\r\n        Columns for the result.\r\n    dmatrix_kwargs : dict, optional\r\n        Keyword parameters for ``xgb.DMatrix``.\r\n    **kwargs : dict\r\n        Other parameters are the same as for ``xgboost.Booster.predict``.\r\n\r\n    Returns\r\n    -------\r\n    ray.ObjectRef\r\n        ``ray.ObjectRef`` with partial prediction.\r\n    \"\"\"\r\n    dmatrix = xgb.DMatrix(part, **dmatrix_kwargs)\r\n    prediction = pandas.DataFrame(\r\n        booster.predict(dmatrix, **kwargs),\r\n        index=part.index,\r\n        columns=columns,\r\n    )\r\n    return prediction",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def distplot(a=None, bins=None, hist=True, kde=True, rug=False, fit=None,\r\n             hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None,\r\n             color=None, vertical=False, norm_hist=False, axlabel=None,\r\n             label=None, ax=None, x=None):\r\n    \"\"\"\r\n    DEPRECATED\r\n\r\n    This function has been deprecated and will be removed in seaborn v0.14.0.\r\n    It has been replaced by :func:`histplot` and :func:`displot`, two functions\r\n    with a modern API and many more capabilities.\r\n\r\n    For a guide to updating, please see this notebook:\r\n\r\n    https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\r\n\r\n    \"\"\"\r\n\r\n    if kde and not hist:\r\n        axes_level_suggestion = (\r\n            \"`kdeplot` (an axes-level function for kernel density plots)\"\r\n        )\r\n    else:\r\n        axes_level_suggestion = (\r\n            \"`histplot` (an axes-level function for histograms)\"\r\n        )\r\n\r\n    msg = textwrap.dedent(f\"\"\"\r\n\r\n    `distplot` is a deprecated function and will be removed in seaborn v0.14.0.\r\n\r\n    Please adapt your code to use either `displot` (a figure-level function with\r\n    similar flexibility) or {axes_level_suggestion}.\r\n\r\n    For a guide to updating your code to use the new functions, please see\r\n    https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\r\n    \"\"\")\r\n    warnings.warn(msg, UserWarning, stacklevel=2)\r\n\r\n    if ax is None:\r\n        ax = plt.gca()\r\n\r\n    # Intelligently label the support axis\r\n    label_ax = bool(axlabel)\r\n    if axlabel is None and hasattr(a, \"name\"):\r\n        axlabel = a.name\r\n        if axlabel is not None:\r\n            label_ax = True\r\n\r\n    # Support new-style API\r\n    if x is not None:\r\n        a = x\r\n\r\n    # Make a a 1-d float array\r\n    a = np.asarray(a, float)\r\n    if a.ndim > 1:\r\n        a = a.squeeze()\r\n\r\n    # Drop null values from array\r\n    a = remove_na(a)\r\n\r\n    # Decide if the hist is normed\r\n    norm_hist = norm_hist or kde or (fit is not None)\r\n\r\n    # Handle dictionary defaults\r\n    hist_kws = {} if hist_kws is None else hist_kws.copy()\r\n    kde_kws = {} if kde_kws is None else kde_kws.copy()\r\n    rug_kws = {} if rug_kws is None else rug_kws.copy()\r\n    fit_kws = {} if fit_kws is None else fit_kws.copy()\r\n\r\n    # Get the color from the current color cycle\r\n    if color is None:\r\n        if vertical:\r\n            line, = ax.plot(0, a.mean())\r\n        else:\r\n            line, = ax.plot(a.mean(), 0)\r\n        color = line.get_color()\r\n        line.remove()\r\n\r\n    # Plug the label into the right kwarg dictionary\r\n    if label is not None:\r\n        if hist:\r\n            hist_kws[\"label\"] = label\r\n        elif kde:\r\n            kde_kws[\"label\"] = label\r\n        elif rug:\r\n            rug_kws[\"label\"] = label\r\n        elif fit:\r\n            fit_kws[\"label\"] = label\r\n\r\n    if hist:\r\n        if bins is None:\r\n            bins = min(_freedman_diaconis_bins(a), 50)\r\n        hist_kws.setdefault(\"alpha\", 0.4)\r\n        hist_kws.setdefault(\"density\", norm_hist)\r\n\r\n        orientation = \"horizontal\" if vertical else \"vertical\"\r\n        hist_color = hist_kws.pop(\"color\", color)\r\n        ax.hist(a, bins, orientation=orientation,\r\n                color=hist_color, **hist_kws)\r\n        if hist_color != color:\r\n            hist_kws[\"color\"] = hist_color\r\n\r\n    axis = \"y\" if vertical else \"x\"\r\n\r\n    if kde:\r\n        kde_color = kde_kws.pop(\"color\", color)\r\n        kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\r\n        if kde_color != color:\r\n            kde_kws[\"color\"] = kde_color\r\n\r\n    if rug:\r\n        rug_color = rug_kws.pop(\"color\", color)\r\n        rugplot(**{axis: a}, ax=ax, color=rug_color, **rug_kws)\r\n        if rug_color != color:\r\n            rug_kws[\"color\"] = rug_color\r\n\r\n    if fit is not None:\r\n\r\n        def pdf(x):\r\n            return fit.pdf(x, *params)\r\n\r\n        fit_color = fit_kws.pop(\"color\", \"#282828\")\r\n        gridsize = fit_kws.pop(\"gridsize\", 200)\r\n        cut = fit_kws.pop(\"cut\", 3)\r\n        clip = fit_kws.pop(\"clip\", (-np.inf, np.inf))\r\n        bw = gaussian_kde(a).scotts_factor() * a.std(ddof=1)\r\n        x = _kde_support(a, bw, gridsize, cut, clip)\r\n        params = fit.fit(a)\r\n        y = pdf(x)\r\n        if vertical:\r\n            x, y = y, x\r\n        ax.plot(x, y, color=fit_color, **fit_kws)\r\n        if fit_color != \"#282828\":\r\n            fit_kws[\"color\"] = fit_color\r\n\r\n    if label_ax:\r\n        if vertical:\r\n            ax.set_ylabel(axlabel)\r\n        else:\r\n            ax.set_xlabel(axlabel)\r\n\r\n    return ax",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def plot_univariate_histogram(\r\n        self,\r\n        multiple,\r\n        element,\r\n        fill,\r\n        common_norm,\r\n        common_bins,\r\n        shrink,\r\n        kde,\r\n        kde_kws,\r\n        color,\r\n        legend,\r\n        line_kws,\r\n        estimate_kws,\r\n        **plot_kws,\r\n    ):\r\n\r\n        # -- Default keyword dicts\r\n        kde_kws = {} if kde_kws is None else kde_kws.copy()\r\n        line_kws = {} if line_kws is None else line_kws.copy()\r\n        estimate_kws = {} if estimate_kws is None else estimate_kws.copy()\r\n\r\n        # --  Input checking\r\n        _check_argument(\"multiple\", [\"layer\", \"stack\", \"fill\", \"dodge\"], multiple)\r\n        _check_argument(\"element\", [\"bars\", \"step\", \"poly\"], element)\r\n\r\n        auto_bins_with_weights = (\r\n            \"weights\" in self.variables\r\n            and estimate_kws[\"bins\"] == \"auto\"\r\n            and estimate_kws[\"binwidth\"] is None\r\n            and not estimate_kws[\"discrete\"]\r\n        )\r\n        if auto_bins_with_weights:\r\n            msg = (\r\n                \"`bins` cannot be 'auto' when using weights. \"\r\n                \"Setting `bins=10`, but you will likely want to adjust.\"\r\n            )\r\n            warnings.warn(msg, UserWarning)\r\n            estimate_kws[\"bins\"] = 10\r\n\r\n        # Simplify downstream code if we are not normalizing\r\n        if estimate_kws[\"stat\"] == \"count\":\r\n            common_norm = False\r\n\r\n        orient = self.data_variable\r\n\r\n        # Now initialize the Histogram estimator\r\n        estimator = Hist(**estimate_kws)\r\n        histograms = {}\r\n\r\n        # Do pre-compute housekeeping related to multiple groups\r\n        all_data = self.comp_data.dropna()\r\n        all_weights = all_data.get(\"weights\", None)\r\n\r\n        multiple_histograms = set(self.variables) - {\"x\", \"y\"}\r\n        if multiple_histograms:\r\n            if common_bins:\r\n                bin_kws = estimator._define_bin_params(all_data, orient, None)\r\n        else:\r\n            common_norm = False\r\n\r\n        if common_norm and all_weights is not None:\r\n            whole_weight = all_weights.sum()\r\n        else:\r\n            whole_weight = len(all_data)\r\n\r\n        # Estimate the smoothed kernel densities, for use later\r\n        if kde:\r\n            # TODO alternatively, clip at min/max bins?\r\n            kde_kws.setdefault(\"cut\", 0)\r\n            kde_kws[\"cumulative\"] = estimate_kws[\"cumulative\"]\r\n            densities = self._compute_univariate_density(\r\n                self.data_variable,\r\n                common_norm,\r\n                common_bins,\r\n                kde_kws,\r\n                warn_singular=False,\r\n            )\r\n\r\n        # First pass through the data to compute the histograms\r\n        for sub_vars, sub_data in self.iter_data(\"hue\", from_comp_data=True):\r\n\r\n            # Prepare the relevant data\r\n            key = tuple(sub_vars.items())\r\n            orient = self.data_variable\r\n\r\n            if \"weights\" in self.variables:\r\n                sub_data[\"weight\"] = sub_data.pop(\"weights\")\r\n                part_weight = sub_data[\"weight\"].sum()\r\n            else:\r\n                part_weight = len(sub_data)\r\n\r\n            # Do the histogram computation\r\n            if not (multiple_histograms and common_bins):\r\n                bin_kws = estimator._define_bin_params(sub_data, orient, None)\r\n            res = estimator._normalize(estimator._eval(sub_data, orient, bin_kws))\r\n            heights = res[estimator.stat].to_numpy()\r\n            widths = res[\"space\"].to_numpy()\r\n            edges = res[orient].to_numpy() - widths / 2\r\n\r\n            # Rescale the smoothed curve to match the histogram\r\n            if kde and key in densities:\r\n                density = densities[key]\r\n                if estimator.cumulative:\r\n                    hist_norm = heights.max()\r\n                else:\r\n                    hist_norm = (heights * widths).sum()\r\n                densities[key] *= hist_norm\r\n\r\n            # Convert edges back to original units for plotting\r\n            ax = self._get_axes(sub_vars)\r\n            _, inv = _get_transform_functions(ax, self.data_variable)\r\n            widths = inv(edges + widths) - inv(edges)\r\n            edges = inv(edges)\r\n\r\n            # Pack the histogram data and metadata together\r\n            edges = edges + (1 - shrink) / 2 * widths\r\n            widths *= shrink\r\n            index = pd.MultiIndex.from_arrays([\r\n                pd.Index(edges, name=\"edges\"),\r\n                pd.Index(widths, name=\"widths\"),\r\n            ])\r\n            hist = pd.Series(heights, index=index, name=\"heights\")\r\n\r\n            # Apply scaling to normalize across groups\r\n            if common_norm:\r\n                hist *= part_weight / whole_weight\r\n\r\n            # Store the finalized histogram data for future plotting\r\n            histograms[key] = hist\r\n\r\n        # Modify the histogram and density data to resolve multiple groups\r\n        histograms, baselines = self._resolve_multiple(histograms, multiple)\r\n        if kde:\r\n            densities, _ = self._resolve_multiple(\r\n                densities, None if multiple == \"dodge\" else multiple\r\n            )\r\n\r\n        # Set autoscaling-related meta\r\n        sticky_stat = (0, 1) if multiple == \"fill\" else (0, np.inf)\r\n        if multiple == \"fill\":\r\n            # Filled plots should not have any margins\r\n            bin_vals = histograms.index.to_frame()\r\n            edges = bin_vals[\"edges\"]\r\n            widths = bin_vals[\"widths\"]\r\n            sticky_data = (\r\n                edges.min(),\r\n                edges.max() + widths.loc[edges.idxmax()]\r\n            )\r\n        else:\r\n            sticky_data = []\r\n\r\n        # --- Handle default visual attributes\r\n\r\n        # Note: default linewidth is determined after plotting\r\n\r\n        # Default alpha should depend on other parameters\r\n        if fill:\r\n            # Note: will need to account for other grouping semantics if added\r\n            if \"hue\" in self.variables and multiple == \"layer\":\r\n                default_alpha = .5 if element == \"bars\" else .25\r\n            elif kde:\r\n                default_alpha = .5\r\n            else:\r\n                default_alpha = .75\r\n        else:\r\n            default_alpha = 1\r\n        alpha = plot_kws.pop(\"alpha\", default_alpha)  # TODO make parameter?\r\n\r\n        hist_artists = []\r\n\r\n        # Go back through the dataset and draw the plots\r\n        for sub_vars, _ in self.iter_data(\"hue\", reverse=True):\r\n\r\n            key = tuple(sub_vars.items())\r\n            hist = histograms[key].rename(\"heights\").reset_index()\r\n            bottom = np.asarray(baselines[key])\r\n\r\n            ax = self._get_axes(sub_vars)\r\n\r\n            # Define the matplotlib attributes that depend on semantic mapping\r\n            if \"hue\" in self.variables:\r\n                sub_color = self._hue_map(sub_vars[\"hue\"])\r\n            else:\r\n                sub_color = color\r\n\r\n            artist_kws = self._artist_kws(\r\n                plot_kws, fill, element, multiple, sub_color, alpha\r\n            )\r\n\r\n            if element == \"bars\":\r\n\r\n                # Use matplotlib bar plotting\r\n\r\n                plot_func = ax.bar if self.data_variable == \"x\" else ax.barh\r\n                artists = plot_func(\r\n                    hist[\"edges\"],\r\n                    hist[\"heights\"] - bottom,\r\n                    hist[\"widths\"],\r\n                    bottom,\r\n                    align=\"edge\",\r\n                    **artist_kws,\r\n                )\r\n\r\n                for bar in artists:\r\n                    if self.data_variable == \"x\":\r\n                        bar.sticky_edges.x[:] = sticky_data\r\n                        bar.sticky_edges.y[:] = sticky_stat\r\n                    else:\r\n                        bar.sticky_edges.x[:] = sticky_stat\r\n                        bar.sticky_edges.y[:] = sticky_data\r\n\r\n                hist_artists.extend(artists)\r\n\r\n            else:\r\n\r\n                # Use either fill_between or plot to draw hull of histogram\r\n                if element == \"step\":\r\n\r\n                    final = hist.iloc[-1]\r\n                    x = np.append(hist[\"edges\"], final[\"edges\"] + final[\"widths\"])\r\n                    y = np.append(hist[\"heights\"], final[\"heights\"])\r\n                    b = np.append(bottom, bottom[-1])\r\n\r\n                    if self.data_variable == \"x\":\r\n                        step = \"post\"\r\n                        drawstyle = \"steps-post\"\r\n                    else:\r\n                        step = \"post\"  # fillbetweenx handles mapping internally\r\n                        drawstyle = \"steps-pre\"\r\n\r\n                elif element == \"poly\":\r\n\r\n                    x = hist[\"edges\"] + hist[\"widths\"] / 2\r\n                    y = hist[\"heights\"]\r\n                    b = bottom\r\n\r\n                    step = None\r\n                    drawstyle = None\r\n\r\n                if self.data_variable == \"x\":\r\n                    if fill:\r\n                        artist = ax.fill_between(x, b, y, step=step, **artist_kws)\r\n                    else:\r\n                        artist, = ax.plot(x, y, drawstyle=drawstyle, **artist_kws)\r\n                    artist.sticky_edges.x[:] = sticky_data\r\n                    artist.sticky_edges.y[:] = sticky_stat\r\n                else:\r\n                    if fill:\r\n                        artist = ax.fill_betweenx(x, b, y, step=step, **artist_kws)\r\n                    else:\r\n                        artist, = ax.plot(y, x, drawstyle=drawstyle, **artist_kws)\r\n                    artist.sticky_edges.x[:] = sticky_stat\r\n                    artist.sticky_edges.y[:] = sticky_data\r\n\r\n                hist_artists.append(artist)\r\n\r\n            if kde:\r\n\r\n                # Add in the density curves\r\n\r\n                try:\r\n                    density = densities[key]\r\n                except KeyError:\r\n                    continue\r\n                support = density.index\r\n\r\n                if \"x\" in self.variables:\r\n                    line_args = support, density\r\n                    sticky_x, sticky_y = None, (0, np.inf)\r\n                else:\r\n                    line_args = density, support\r\n                    sticky_x, sticky_y = (0, np.inf), None\r\n\r\n                line_kws[\"color\"] = to_rgba(sub_color, 1)\r\n                line, = ax.plot(\r\n                    *line_args, **line_kws,\r\n                )\r\n\r\n                if sticky_x is not None:\r\n                    line.sticky_edges.x[:] = sticky_x\r\n                if sticky_y is not None:\r\n                    line.sticky_edges.y[:] = sticky_y\r\n\r\n        if element == \"bars\" and \"linewidth\" not in plot_kws:\r\n\r\n            # Now we handle linewidth, which depends on the scaling of the plot\r\n\r\n            # We will base everything on the minimum bin width\r\n            hist_metadata = pd.concat([\r\n                # Use .items for generality over dict or df\r\n                h.index.to_frame() for _, h in histograms.items()\r\n            ]).reset_index(drop=True)\r\n            thin_bar_idx = hist_metadata[\"widths\"].idxmin()\r\n            binwidth = hist_metadata.loc[thin_bar_idx, \"widths\"]\r\n            left_edge = hist_metadata.loc[thin_bar_idx, \"edges\"]\r\n\r\n            # Set initial value\r\n            default_linewidth = math.inf\r\n\r\n            # Loop through subsets based only on facet variables\r\n            for sub_vars, _ in self.iter_data():\r\n\r\n                ax = self._get_axes(sub_vars)\r\n\r\n                # Needed in some cases to get valid transforms.\r\n                # Innocuous in other cases?\r\n                ax.autoscale_view()\r\n\r\n                # Convert binwidth from data coordinates to pixels\r\n                pts_x, pts_y = 72 / ax.figure.dpi * abs(\r\n                    ax.transData.transform([left_edge + binwidth] * 2)\r\n                    - ax.transData.transform([left_edge] * 2)\r\n                )\r\n                if self.data_variable == \"x\":\r\n                    binwidth_points = pts_x\r\n                else:\r\n                    binwidth_points = pts_y\r\n\r\n                # The relative size of the lines depends on the appearance\r\n                # This is a provisional value and may need more tweaking\r\n                default_linewidth = min(.1 * binwidth_points, default_linewidth)\r\n\r\n            # Set the attributes\r\n            for bar in hist_artists:\r\n\r\n                # Don't let the lines get too thick\r\n                max_linewidth = bar.get_linewidth()\r\n                if not fill:\r\n                    max_linewidth *= 1.5\r\n\r\n                linewidth = min(default_linewidth, max_linewidth)\r\n\r\n                # If not filling, don't let lines disappear\r\n                if not fill:\r\n                    min_linewidth = .5\r\n                    linewidth = max(linewidth, min_linewidth)\r\n\r\n                bar.set_linewidth(linewidth)\r\n\r\n        # --- Finalize the plot ----\r\n\r\n        # Axis labels\r\n        ax = self.ax if self.ax is not None else self.facets.axes.flat[0]\r\n        default_x = default_y = \"\"\r\n        if self.data_variable == \"x\":\r\n            default_y = estimator.stat.capitalize()\r\n        if self.data_variable == \"y\":\r\n            default_x = estimator.stat.capitalize()\r\n        self._add_axis_labels(ax, default_x, default_y)\r\n\r\n        # Legend for semantic variables\r\n        if \"hue\" in self.variables and legend:\r\n\r\n            if fill or element == \"bars\":\r\n                artist = partial(mpl.patches.Patch)\r\n            else:\r\n                artist = partial(mpl.lines.Line2D, [], [])\r\n\r\n            ax_obj = self.ax if self.ax is not None else self.facets\r\n            self._add_legend(\r\n                ax_obj, artist, fill, element, multiple, alpha, plot_kws, {},\r\n            )",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def iter_data(\r\n        self, grouping_vars=None, *,\r\n        reverse=False, from_comp_data=False,\r\n        by_facet=True, allow_empty=False, dropna=True,\r\n    ):\r\n        \"\"\"Generator for getting subsets of data defined by semantic variables.\r\n\r\n        Also injects \"col\" and \"row\" into grouping semantics.\r\n\r\n        Parameters\r\n        ----------\r\n        grouping_vars : string or list of strings\r\n            Semantic variables that define the subsets of data.\r\n        reverse : bool\r\n            If True, reverse the order of iteration.\r\n        from_comp_data : bool\r\n            If True, use self.comp_data rather than self.plot_data\r\n        by_facet : bool\r\n            If True, add faceting variables to the set of grouping variables.\r\n        allow_empty : bool\r\n            If True, yield an empty dataframe when no observations exist for\r\n            combinations of grouping variables.\r\n        dropna : bool\r\n            If True, remove rows with missing data.\r\n\r\n        Yields\r\n        ------\r\n        sub_vars : dict\r\n            Keys are semantic names, values are the level of that semantic.\r\n        sub_data : :class:`pandas.DataFrame`\r\n            Subset of ``plot_data`` for this combination of semantic values.\r\n\r\n        \"\"\"\r\n        # TODO should this default to using all (non x/y?) semantics?\r\n        # or define grouping vars somewhere?\r\n        if grouping_vars is None:\r\n            grouping_vars = []\r\n        elif isinstance(grouping_vars, str):\r\n            grouping_vars = [grouping_vars]\r\n        elif isinstance(grouping_vars, tuple):\r\n            grouping_vars = list(grouping_vars)\r\n\r\n        # Always insert faceting variables\r\n        if by_facet:\r\n            facet_vars = {\"col\", \"row\"}\r\n            grouping_vars.extend(\r\n                facet_vars & set(self.variables) - set(grouping_vars)\r\n            )\r\n\r\n        # Reduce to the semantics used in this plot\r\n        grouping_vars = [var for var in grouping_vars if var in self.variables]\r\n\r\n        if from_comp_data:\r\n            data = self.comp_data\r\n        else:\r\n            data = self.plot_data\r\n\r\n        if dropna:\r\n            data = data.dropna()\r\n\r\n        levels = self.var_levels.copy()\r\n        if from_comp_data:\r\n            for axis in {\"x\", \"y\"} & set(grouping_vars):\r\n                converter = self.converters[axis].iloc[0]\r\n                if self.var_types[axis] == \"categorical\":\r\n                    if self._var_ordered[axis]:\r\n                        # If the axis is ordered, then the axes in a possible\r\n                        # facet grid are by definition \"shared\", or there is a\r\n                        # single axis with a unique cat -> idx mapping.\r\n                        # So we can just take the first converter object.\r\n                        levels[axis] = converter.convert_units(levels[axis])\r\n                    else:\r\n                        # Otherwise, the mappings may not be unique, but we can\r\n                        # use the unique set of index values in comp_data.\r\n                        levels[axis] = np.sort(data[axis].unique())\r\n                else:\r\n                    transform = converter.get_transform().transform\r\n                    levels[axis] = transform(converter.convert_units(levels[axis]))\r\n\r\n        if grouping_vars:\r\n\r\n            grouped_data = data.groupby(\r\n                grouping_vars, sort=False, as_index=False, observed=False,\r\n            )\r\n\r\n            grouping_keys = []\r\n            for var in grouping_vars:\r\n                key = levels.get(var)\r\n                grouping_keys.append([] if key is None else key)\r\n\r\n            iter_keys = itertools.product(*grouping_keys)\r\n            if reverse:\r\n                iter_keys = reversed(list(iter_keys))\r\n\r\n            for key in iter_keys:\r\n\r\n                pd_key = (\r\n                    key[0] if len(key) == 1 and _version_predates(pd, \"2.2.0\") else key\r\n                )\r\n                try:\r\n                    data_subset = grouped_data.get_group(pd_key)\r\n                except KeyError:\r\n                    # XXX we are adding this to allow backwards compatibility\r\n                    # with the empty artists that old categorical plots would\r\n                    # add (before 0.12), which we may decide to break, in which\r\n                    # case this option could be removed\r\n                    data_subset = data.loc[[]]\r\n\r\n                if data_subset.empty and not allow_empty:\r\n                    continue\r\n\r\n                sub_vars = dict(zip(grouping_vars, key))\r\n\r\n                yield sub_vars, data_subset.copy()\r\n\r\n        else:\r\n\r\n            yield {}, data.copy()",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def _unscale_coords(\r\n        self, subplots: list[dict], df: DataFrame, orient: str,\r\n    ) -> DataFrame:\r\n        # TODO do we still have numbers in the variable name at this point?\r\n        coord_cols = [c for c in df if re.match(r\"^[xy]\\D*$\", str(c))]\r\n        out_df = (\r\n            df\r\n            .drop(coord_cols, axis=1)\r\n            .reindex(df.columns, axis=1)  # So unscaled columns retain their place\r\n            .copy(deep=False)\r\n        )\r\n\r\n        for view in subplots:\r\n            view_df = self._filter_subplot_data(df, view)\r\n            axes_df = view_df[coord_cols]\r\n            for var, values in axes_df.items():\r\n\r\n                axis = getattr(view[\"ax\"], f\"{str(var)[0]}axis\")\r\n                # TODO see https://github.com/matplotlib/matplotlib/issues/22713\r\n                transform = axis.get_transform().inverted().transform\r\n                inverted = transform(values)\r\n                out_df.loc[values.index, str(var)] = inverted\r\n\r\n        return out_df",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_valid_but_unsupported_language():\r\n    dataset = load_iris()\r\n    x, y = dataset.data, dataset.target\r\n    clf = AdaBoostClassifier()\r\n    clf.fit(X=x, y=y)\r\n    with pytest.raises(exception.NotSupportedYetError):\r\n        Estimator(clf, language='ruby')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_extraction_from_grid_search_cv():\r\n    \"\"\"Test the extraction from an optimizer.\"\"\"\r\n    from sklearn.model_selection import GridSearchCV\r\n\r\n    params = {\r\n        'kernel': ('linear', 'rbf'),\r\n        'C': [1, 10, 100],\r\n        'gamma': [0.001, 0.0001],\r\n    }\r\n    search = GridSearchCV(SVC(), params, cv=2)\r\n\r\n    # Test unfitted optimizer:\r\n    with pytest.raises(ValueError):\r\n        est = Estimator(search)\r\n        assert isinstance(est.estimator, SVC)\r\n\r\n    # Test fitted optimizer:\r\n    search.fit(\r\n        X=[[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],\r\n        y=[1, 2, 3, 1, 2, 3]\r\n    )\r\n    est = Estimator(search)\r\n    assert isinstance(est.estimator, SVC)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_extraction_from_randomized_search_cv():\r\n    \"\"\"Test the extraction from an optimizer.\"\"\"\r\n    from sklearn.model_selection import RandomizedSearchCV\r\n\r\n    params = {\r\n        'kernel': ('linear', 'rbf'),\r\n        'C': [1, 10, 100],\r\n        'gamma': [0.001, 0.0001],\r\n    }\r\n    search = RandomizedSearchCV(SVC(), params, cv=2)\r\n\r\n    # Test unfitted optimizer:\r\n    with pytest.raises(ValueError):\r\n        est = Estimator(search)\r\n        assert isinstance(est.estimator, SVC)\r\n\r\n    # Test fitted optimizer:\r\n    search.fit(\r\n        X=[[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],\r\n        y=[1, 2, 3, 1, 2, 3]\r\n    )\r\n    est = Estimator(search)\r\n    assert isinstance(est.estimator, SVC)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def memory_management(\r\n            self, data_dir, expected_NRMSEs, reservoir_size=100, spectral_radius=1.5, input_scaling=1.5,\r\n            bias_scaling=0.25, connectivity=10.0, washout_length=100, learn_length=100,\r\n            ridge_param_wout=0.01, aperture=1000, places=3, value_test_divider=1.0,\r\n            seed=1, interpolation_rate=20, conceptor_test_length=200, assert_almost_equal=True,\r\n            signal_plot_length=20, loading_method=ecnc.SPESNCell.INPUTS_SIMULATION,\r\n            use_matlab_params=True, dtype=torch.float64, print_debug=False):\r\n        \"\"\"\r\n        Memory management\r\n        \"\"\"\r\n        # Package\r\n        subpackage_dir, this_filename = os.path.split(__file__)\r\n        package_dir = os.path.join(subpackage_dir, \"..\")\r\n        TEST_PATH = os.path.join(package_dir, \"data\", \"tests\", data_dir)\r\n\r\n        # Debug\r\n        debug_mode = Node.DEBUG_TEST_CASE\r\n\r\n        # Random numb. init\r\n        echotorch.utils.random.manual_seed(seed)\r\n\r\n        # Precision decimal\r\n        precision = 1.0 / places\r\n\r\n        # Reservoir parameters\r\n        connectivity = connectivity / reservoir_size\r\n\r\n        # Pattern parameters\r\n        n_patterns = 16\r\n\r\n        # 2. We generate matrices Wstar, Win and Wbias,\r\n        # either from a random number generator or from\r\n        # matlab files.\r\n\r\n        # Load W from matlab file and random init ?\r\n        if use_matlab_params:\r\n            # Load internal weights\r\n            w_generator = mg.matrix_factory.get_generator(\r\n                \"matlab\",\r\n                file_name=os.path.join(TEST_PATH, \"WRaw.mat\"),\r\n                entity_name=\"WRaw\",\r\n                scale=spectral_radius\r\n            )\r\n\r\n            # Load internal weights\r\n            win_generator = mg.matrix_factory.get_generator(\r\n                \"matlab\",\r\n                file_name=os.path.join(TEST_PATH, \"WinRaw.mat\"),\r\n                entity_name=\"WinRaw\",\r\n                scale=input_scaling\r\n            )\r\n\r\n            # Load Wbias from matlab from or init randomly\r\n            wbias_generator = mg.matrix_factory.get_generator(\r\n                \"matlab\",\r\n                file_name=os.path.join(TEST_PATH, \"WbiasRaw.mat\"),\r\n                entity_name=\"WbiasRaw\",\r\n                shape=reservoir_size,\r\n                scale=bias_scaling\r\n            )\r\n        else:\r\n            # Generate internal weights\r\n            w_generator = mg.matrix_factory.get_generator(\r\n                \"normal\",\r\n                mean=0.0,\r\n                std=1.0,\r\n                connectivity=connectivity,\r\n                spectral_radius=spectral_radius\r\n            )\r\n\r\n            # Generate Win\r\n            win_generator = mg.matrix_factory.get_generator(\r\n                \"normal\",\r\n                mean=0.0,\r\n                std=1.0,\r\n                connectivity=1.0,\r\n                scale=input_scaling\r\n            )\r\n\r\n            # Load Wbias from matlab from or init randomly\r\n            wbias_generator = mg.matrix_factory.get_generator(\r\n                \"normal\",\r\n                mean=0.0,\r\n                std=1.0,\r\n                connectivity=1.0,\r\n                scale=bias_scaling\r\n            )\r\n        # end if\r\n\r\n        # 3. We create the different patterns to be loaded\r\n        # into the reservoir and learned by the Conceptors.\r\n        # There is 13 patterns, 3 are repeated (6, 7, 8)\r\n        # to show that it does not increase memory size.\r\n\r\n        # Pattern 1 (sine p=10)\r\n        pattern1_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=10,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 2 (sine p=15)\r\n        pattern2_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=15,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 3 (periodic 4)\r\n        pattern3_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[-0.4564, 0.6712, -2.3953, -2.1594],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 4 (periodic 6)\r\n        pattern4_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[0.5329, 0.9621, 0.1845, 0.5099, 0.3438, 0.7697],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 5 (periodic 7)\r\n        pattern5_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[0.8029, 0.4246, 0.2041, 0.0671, 0.1986, 0.2724, 0.5988],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 6 (sine p=12)\r\n        pattern6_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=12,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 7 (sine p=5)\r\n        pattern7_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=5,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 8 (sine p=6)\r\n        pattern8_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=6,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 9 (periodic 8)\r\n        pattern9_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[0.8731, 0.1282, 0.9582, 0.6832, 0.7420, 0.9829, 0.4161, 0.5316],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 10 (periodic 7)\r\n        pattern10_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[0.6792, 0.5129, 0.2991, 0.1054, 0.2849, 0.7689, 0.6408],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 11 (periodic 3)\r\n        pattern11_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[1.4101, -0.0992, -0.0902],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 12 (sine p=6)\r\n        pattern12_training = etds.SinusoidalTimeseries(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            a=1,\r\n            period=11,\r\n            dtype=dtype\r\n        )\r\n\r\n        # Pattern 13 (periodic 5)\r\n        pattern13_training = etds.PeriodicSignalDataset(\r\n            sample_len=washout_length + learn_length,\r\n            n_samples=1,\r\n            period=[0.9, -0.021439412841318672, 0.0379515995051003, -0.9, 0.06663989939293802],\r\n            dtype=dtype\r\n        )\r\n\r\n        # Composer\r\n        dataset_training = DatasetComposer([\r\n            pattern1_training, pattern2_training, pattern3_training, pattern4_training, pattern5_training,\r\n            pattern1_training,\r\n            pattern2_training, pattern3_training, pattern6_training, pattern7_training, pattern8_training,\r\n            pattern9_training,\r\n            pattern10_training, pattern11_training, pattern12_training, pattern13_training\r\n        ])\r\n\r\n        # Data loader\r\n        patterns_loader = DataLoader(dataset_training, batch_size=1, shuffle=False, num_workers=1)\r\n\r\n        # 4. We create a conceptor set, 16 conceptors,\r\n        # and an incremental conceptor net (IncConceptorNet)\r\n\r\n        # Create a set of conceptors\r\n        conceptors = ecnc.ConceptorSet(input_dim=reservoir_size, debug=debug_mode, test_case=self)\r\n\r\n        # Create sixteen conceptors\r\n        for p in range(n_patterns):\r\n            conceptors.add(\r\n                p,\r\n                ecnc.Conceptor(\r\n                    input_dim=reservoir_size,\r\n                    aperture=aperture,\r\n                    debug=debug_mode,\r\n                    test_case=self,\r\n                    dtype=dtype\r\n                )\r\n            )\r\n        # end for\r\n\r\n        # Create a conceptor network using\r\n        # an incrementing self-predicting ESN which\r\n        # will learn sixteen patterns\r\n        conceptor_net = ecnc.IncConceptorNet(\r\n            input_dim=1,\r\n            hidden_dim=reservoir_size,\r\n            output_dim=1,\r\n            conceptor=conceptors,\r\n            w_generator=w_generator,\r\n            win_generator=win_generator,\r\n            wbias_generator=wbias_generator,\r\n            ridge_param_wout=ridge_param_wout,\r\n            aperture=aperture,\r\n            washout=washout_length,\r\n            fill_left=True,\r\n            loading_method=loading_method,\r\n            debug=debug_mode,\r\n            test_case=self,\r\n            dtype=dtype\r\n        )\r\n\r\n        # 6. For debugging, we add some debug point\r\n        # which will compute the differences between\r\n        # what we want and what we have. You will then be\r\n        # able to check if there is a problem.\r\n\r\n        # If with matlab info\r\n        if use_matlab_params:\r\n            # Load sample matrices\r\n            for i in range(n_patterns):\r\n                # SPESN : Input patterns\r\n                conceptor_net.cell.debug_point(\r\n                    \"u{}\".format(i),\r\n                    torch.reshape(torch.from_numpy(np.load(os.path.join(TEST_PATH, \"u{}.npy\".format(i)))),\r\n                                  shape=(-1, 1)),\r\n                    precision\r\n                )\r\n\r\n                # SPESN : States X\r\n                conceptor_net.cell.debug_point(\r\n                    \"X{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"X{}.npy\".format(i))).T),\r\n                    precision\r\n                )\r\n\r\n                # SPESN : States old\r\n                conceptor_net.cell.debug_point(\r\n                    \"Xold{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"XOld{}.npy\".format(i))).T),\r\n                    precision\r\n                )\r\n\r\n                # SPESN : Td\r\n                if loading_method != ecnc.SPESNCell.INPUTS_RECREATION:\r\n                    conceptor_net.cell.debug_point(\r\n                        \"Td{}\".format(i),\r\n                        torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Td{}.npy\".format(i))).T),\r\n                        precision if i < 13 else precision * 100\r\n                    )\r\n                # end if\r\n\r\n                # SPESN : F\r\n                if i != 15:\r\n                    conceptor_net.cell.debug_point(\r\n                        \"F{}\".format(i),\r\n                        torch.from_numpy(np.load(os.path.join(TEST_PATH, \"F{}.npy\".format(i)))),\r\n                        precision * 10\r\n                    )\r\n                # end if\r\n\r\n                # SPESN : Sold\r\n                conceptor_net.cell.debug_point(\r\n                    \"Sold{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Sold{}.npy\".format(i))).T),\r\n                    precision if i < 15 else precision * 10\r\n                )\r\n\r\n                # SPESN : sTd\r\n                if loading_method != ecnc.SPESNCell.INPUTS_RECREATION:\r\n                    conceptor_net.cell.debug_point(\r\n                        \"sTd{}\".format(i),\r\n                        torch.from_numpy(np.load(os.path.join(TEST_PATH, \"sTd{}.npy\".format(i)))),\r\n                        precision if i < 15 else precision * 100\r\n                    )\r\n                # end if\r\n\r\n                # SPESN : sTs\r\n                conceptor_net.cell.debug_point(\r\n                    \"sTs{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"sTs{}.npy\".format(i)))),\r\n                    precision if i < 9 else precision * 10\r\n                )\r\n\r\n                # SPESN : ridge sTs\r\n                conceptor_net.cell.debug_point(\r\n                    \"ridge_sTs{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"ridge_sTs{}.npy\".format(i)))),\r\n                    precision\r\n                )\r\n\r\n                # SPESN : Dinc\r\n                if i != 15:\r\n                    conceptor_net.cell.debug_point(\r\n                        \"Dinc{}\".format(i),\r\n                        torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Dinc{}.npy\".format(i)))),\r\n                        precision if i < 14 else precision * 100\r\n                    )\r\n                # end if\r\n\r\n                # SPESN : D\r\n                if i != 15:\r\n                    conceptor_net.cell.debug_point(\r\n                        \"D{}\".format(i),\r\n                        torch.from_numpy(np.load(os.path.join(TEST_PATH, \"D{}.npy\".format(i)))),\r\n                        precision if i < 14 else precision * 100\r\n                    )\r\n                # end if\r\n\r\n                # Conceptor : C matrix\r\n                conceptors[i].debug_point(\r\n                    \"C\",\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"C{}.npy\".format(i)))),\r\n                    precision\r\n                )\r\n\r\n                # IncRRCell : Wout Y\r\n                conceptor_net.output.debug_point(\r\n                    \"Y{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Y{}.npy\".format(i))).T),\r\n                    precision\r\n                )\r\n\r\n                # IncRRCell : Wout y\r\n                conceptor_net.output.debug_point(\r\n                    \"y{}\".format(i),\r\n                    torch.reshape(torch.from_numpy(np.load(os.path.join(TEST_PATH, \"u{}.npy\".format(i)))), shape=(-1, 1)),\r\n                    precision\r\n                )\r\n\r\n                # IncRRCell : Wout F\r\n                conceptor_net.output.debug_point(\r\n                    \"F{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wout_F{}.npy\".format(i)))),\r\n                    precision if i < 15 else precision * 10\r\n                )\r\n\r\n                # IncRRCell : Wout S\r\n                conceptor_net.output.debug_point(\r\n                    \"S{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"S{}.npy\".format(i))).T),\r\n                    precision if i < 15 else precision * 10\r\n                )\r\n\r\n                # IncRRCell : Wout sTs\r\n                conceptor_net.output.debug_point(\r\n                    \"sTs{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wout_sTs{}.npy\".format(i)))),\r\n                    precision if i < 9 else precision * 10\r\n                )\r\n\r\n                # IncRRCell : Wout sTy\r\n                conceptor_net.output.debug_point(\r\n                    \"sTy{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"sTy{}.npy\".format(i)))),\r\n                    precision\r\n                )\r\n\r\n                # IncRRCell : Wout ridge sTs\r\n                conceptor_net.output.debug_point(\r\n                    \"ridge_sTs{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wout_ridge_sTs{}.npy\".format(i)))),\r\n                    precision\r\n                )\r\n\r\n                # IncRRCell : Wout inverse of ridge sTs\r\n                if i < 9:\r\n                    inv_sts_precision = precision\r\n                elif i < 15:\r\n                    inv_sts_precision = precision * 10\r\n                else:\r\n                    inv_sts_precision = precision * 100\r\n                # end if\r\n\r\n                conceptor_net.output.debug_point(\r\n                    \"inv_sTs{}\".format(i),\r\n                    torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wout_inv_ridge_sTs{}.npy\".format(i)))),\r\n                    inv_sts_precision\r\n                )\r\n\r\n                # IncRRCell : Wout\r\n                conceptor_net.output.debug_point(\r\n                    \"w_out{}\".format(i),\r\n                    torch.reshape(torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wout{}.npy\".format(i)))), shape=(1, -1)),\r\n                    precision\r\n                )\r\n            # end for\r\n\r\n            # Load test W\r\n            conceptor_net.cell.debug_point(\r\n                \"Wstar\",\r\n                torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wstar.npy\"), allow_pickle=True)),\r\n                precision\r\n            )\r\n\r\n            # Load test Win\r\n            conceptor_net.cell.debug_point(\r\n                \"Win\",\r\n                torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Win.npy\"))),\r\n                precision\r\n            )\r\n\r\n            # Load test Wbias\r\n            conceptor_net.cell.debug_point(\r\n                \"Wbias\",\r\n                torch.from_numpy(np.load(os.path.join(TEST_PATH, \"Wbias.npy\"))),\r\n                precision\r\n            )\r\n        # end if\r\n\r\n        # 7. We incrementally load the patterns into the reservoir\r\n        # and we save the results for plotting and testing.\r\n\r\n        # Save pattern for plotting, last state, quota after each loading\r\n        P_collector = torch.empty(n_patterns, signal_plot_length, dtype=dtype)\r\n        last_states = torch.empty(n_patterns, reservoir_size, dtype=dtype)\r\n        quota_collector = torch.zeros(n_patterns)\r\n\r\n        # Conceptors activated in the loop\r\n        conceptor_net.conceptor_active(True)\r\n\r\n        # For each sample in the dataset\r\n        for p, data in enumerate(patterns_loader):\r\n            # Inputs and labels\r\n            inputs, outputs, labels = data\r\n\r\n            # To Variable\r\n            if dtype == torch.float64:\r\n                inputs, outputs = inputs.double(), outputs.double()\r\n            # end if\r\n\r\n            # Transform to variable\r\n            inputs, outputs = Variable(inputs), Variable(outputs)\r\n\r\n            # Set the conceptor activated in\r\n            # the loop.\r\n            conceptors.set(p)\r\n\r\n            # Feed SPESN with inputs,\r\n            # output learning to recreate the inputs\r\n            # so the inputs are the targets.\r\n            X = conceptor_net(inputs, inputs)\r\n\r\n            # Finalize Conceptor by learning\r\n            # the Conceptor matrix from the\r\n            # neurons activation received in\r\n            # the preceding line.\r\n            conceptors[p].finalize()\r\n\r\n            # We change the aperture of the Conceptor,\r\n            # the Conceptor matrix C is modified.\r\n            conceptors[p].aperture = aperture\r\n\r\n            # We save the patterns to be plotted afterwards,\r\n            # we save the last state to start the generation.\r\n            # we also save the quota of the space used by the\r\n            # patterns currently loaded in the reservoir.\r\n            P_collector[p] = inputs[0, washout_length:washout_length + signal_plot_length, 0]\r\n            last_states[p] = X[0, -1]\r\n            quota_collector[p] = conceptors.quota()\r\n        # end for\r\n\r\n        # 8. We test the system by generating signals,\r\n        # we align these with original patterns and\r\n        # we measure its performances with NRMSE.\r\n\r\n        # We are going to to some pattern\r\n        # generation, so we stop the learning\r\n        # and switch to the evaluation mode.\r\n        conceptor_net.train(False)\r\n\r\n        # Keep aligned NRMSEs to be print\r\n        # at the end.\r\n        NRMSEs_aligned = list()\r\n\r\n        # For each pattern we generate a sample by filtering the neurons\r\n        # activation with the selected Conceptor, we then align the\r\n        # generated sample to the real pattern by testing different\r\n        # phase shift and we save the result.\r\n        for p in range(n_patterns):\r\n            # Set the current conceptor\r\n            # corresponding to the pth pattern.\r\n            conceptors.set(p)\r\n\r\n            # Set last state in training phase as initial\r\n            # state here.\r\n            conceptor_net.cell.set_hidden(last_states[p])\r\n\r\n            # Generate sample, we give a zero input of size\r\n            # batch size x time length x number of inputs.\r\n            # We don't reset the state as we set the initial state\r\n            # just before.\r\n            generated_sample = conceptor_net(torch.zeros(1, conceptor_test_length, 1, dtype=dtype), reset_state=False)\r\n\r\n            # We find the best phase shift by interpolating the original\r\n            # and the generated signal quadratically and trying different\r\n            # shifts. We take the best under the NRMSE evaluation measure.\r\n            _, _, NRMSE_aligned = echotorch.utils.pattern_interpolation(\r\n                P_collector[p],\r\n                generated_sample[0],\r\n                interpolation_rate\r\n            )\r\n\r\n            # Print results ?\r\n            if print_debug:\r\n                print((\"NRMSE aligned : {}\".format(NRMSE_aligned / value_test_divider)))\r\n                print((\"Expected NRMSE : {}\".format(expected_NRMSEs[p] / value_test_divider)))\r\n            # end if\r\n\r\n            # Check NRMSE\r\n            if expected_NRMSEs[p] != np.nan:\r\n                if assert_almost_equal:\r\n                    self.assertAlmostEqual(\r\n                        NRMSE_aligned / value_test_divider,\r\n                        expected_NRMSEs[p] / value_test_divider,\r\n                        places\r\n                    )\r\n                else:\r\n                    self.assertLessEqual(NRMSE_aligned / value_test_divider, expected_NRMSEs[p] / value_test_divider)\r\n                # end if\r\n            # end if\r\n\r\n            # Append to the list\r\n            NRMSEs_aligned.append(str(NRMSE_aligned))\r\n        # end for\r\n\r\n        # Print NRMSEs aligned\r\n        # if in debug mode\r\n        if print_debug:\r\n            print(\"Aligned NRMSEs\")\r\n            print((\",\\n\".join(NRMSEs_aligned)))",
        "labels": [
            "NaN Equivalence Comparison Misused"
        ]
    },
    {
        "code": "def test_simple_row_groupby(by, as_index, col1_category):\r\n    pandas_df = pandas.DataFrame(\r\n        {\r\n            \"col1\": [0, 1, 2, 3],\r\n            \"col2\": [4, 5, np.nan, 7],\r\n            \"col3\": [np.nan, np.nan, 12, 10],\r\n            \"col4\": [17, 13, 16, 15],\r\n            \"col5\": [-4, -5, -6, -7],\r\n        }\r\n    )\r\n\r\n    if col1_category:\r\n        pandas_df = pandas_df.astype({\"col1\": \"category\"})\r\n        # As of pandas 1.4.0 operators like min cause TypeErrors to be raised on unordered\r\n        # categorical columns. We need to specify the categorical column as ordered to bypass this.\r\n        pandas_df[\"col1\"] = pandas_df[\"col1\"].cat.as_ordered()\r\n\r\n    modin_df = from_pandas(pandas_df)\r\n    n = 1\r\n\r\n    def maybe_get_columns(df, by):\r\n        if isinstance(by, list):\r\n            return [o(df) if isinstance(o, GetColumn) else o for o in by]\r\n        else:\r\n            return by\r\n\r\n    modin_groupby = modin_df.groupby(\r\n        by=maybe_get_columns(modin_df, by), as_index=as_index\r\n    )\r\n\r\n    pandas_by = maybe_get_columns(pandas_df, try_cast_to_pandas(by))\r\n    pandas_groupby = pandas_df.groupby(by=pandas_by, as_index=as_index)\r\n\r\n    modin_groupby_equals_pandas(modin_groupby, pandas_groupby)\r\n    eval_ngroups(modin_groupby, pandas_groupby)\r\n    eval_shift(modin_groupby, pandas_groupby)\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.ffill())\r\n    if as_index:\r\n        eval_general(modin_groupby, pandas_groupby, lambda df: df.nth(0))\r\n    else:\r\n        # FIXME: df.groupby(as_index=False).nth() does not produce correct index in Modin,\r\n        #        it should maintain values from df.index, not create a new one or re-order it;\r\n        #        it also produces completely wrong result for multi-column `by` :(\r\n        if not isinstance(pandas_by, list) or len(pandas_by) <= 1:\r\n            eval_general(\r\n                modin_groupby,\r\n                pandas_groupby,\r\n                lambda df: df.nth(0).sort_values(\"col1\").reset_index(drop=True),\r\n            )\r\n\r\n    expected_exception = None\r\n    if col1_category:\r\n        expected_exception = TypeError(\r\n            \"category dtype does not support aggregation 'sem'\"\r\n        )\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.sem(),\r\n        modin_df_almost_equals_pandas,\r\n        expected_exception=expected_exception,\r\n    )\r\n    eval_mean(modin_groupby, pandas_groupby, numeric_only=True)\r\n    eval_any(modin_groupby, pandas_groupby)\r\n    eval_min(modin_groupby, pandas_groupby)\r\n    # FIXME: https://github.com/modin-project/modin/issues/7033\r\n    eval_general(\r\n        modin_groupby, pandas_groupby, lambda df: df.idxmax(), expected_exception=False\r\n    )\r\n    eval_ndim(modin_groupby, pandas_groupby)\r\n    if not check_df_columns_have_nans(modin_df, by):\r\n        # cum* functions produce undefined results for columns with NaNs so we run them only when \"by\" columns contain no NaNs\r\n\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support cumsum operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.cumsum(),\r\n            expected_exception=expected_exception,\r\n        )\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support cummax operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.cummax(),\r\n            expected_exception=expected_exception,\r\n        )\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support cummin operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.cummin(),\r\n            expected_exception=expected_exception,\r\n        )\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support cumprod operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.cumprod(),\r\n            expected_exception=expected_exception,\r\n        )\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support cumcount operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.cumcount(),\r\n            expected_exception=expected_exception,\r\n        )\r\n\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.pct_change(\r\n            periods=2, fill_method=\"bfill\", limit=1, freq=None, axis=1\r\n        ),\r\n        modin_df_almost_equals_pandas,\r\n    )\r\n\r\n    apply_functions = [\r\n        lambda df: df.sum(numeric_only=True),\r\n        lambda df: pandas.Series([1, 2, 3, 4], name=\"result\"),\r\n        min,\r\n    ]\r\n    for func in apply_functions:\r\n        eval_apply(modin_groupby, pandas_groupby, func)\r\n\r\n    eval_dtypes(modin_groupby, pandas_groupby)\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.first())\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.bfill())\r\n    # FIXME: https://github.com/modin-project/modin/issues/7033\r\n    eval_general(\r\n        modin_groupby, pandas_groupby, lambda df: df.idxmin(), expected_exception=False\r\n    )\r\n    expected_exception = None\r\n    if col1_category:\r\n        expected_exception = TypeError(\"category type does not support prod operations\")\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda grp: grp.prod(),\r\n        expected_exception=expected_exception,\r\n    )\r\n\r\n    if as_index:\r\n        eval_std(modin_groupby, pandas_groupby, numeric_only=True)\r\n        eval_var(modin_groupby, pandas_groupby, numeric_only=True)\r\n        eval_skew(modin_groupby, pandas_groupby, numeric_only=True)\r\n\r\n    agg_functions = [\r\n        lambda df: df.sum(),\r\n        \"min\",\r\n        \"max\",\r\n        min,\r\n        sum,\r\n        # Intersection of 'by' and agg cols for TreeReduce impl\r\n        {\"col1\": \"count\", \"col2\": \"count\"},\r\n        # Intersection of 'by' and agg cols for FullAxis impl\r\n        {\"col1\": \"nunique\", \"col2\": \"nunique\"},\r\n    ]\r\n    for func in agg_functions:\r\n        # Pandas raises an exception when 'by' contains categorical key and `as_index=False`\r\n        # because of this bug: https://github.com/pandas-dev/pandas/issues/36698\r\n        # Modin correctly processes the result\r\n        is_pandas_bug_case = not as_index and col1_category and isinstance(func, dict)\r\n        expected_exception = None\r\n        if col1_category:\r\n            # FIXME: https://github.com/modin-project/modin/issues/7033\r\n            expected_exception = False\r\n        if not is_pandas_bug_case:\r\n            eval_general(\r\n                modin_groupby,\r\n                pandas_groupby,\r\n                lambda grp: grp.agg(func),\r\n                expected_exception=expected_exception,\r\n            )\r\n\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.last())\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.rank())\r\n    eval_max(modin_groupby, pandas_groupby)\r\n    eval_len(modin_groupby, pandas_groupby)\r\n    expected_exception = None\r\n    if col1_category:\r\n        expected_exception = TypeError(\"category type does not support sum operations\")\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.sum(),\r\n        expected_exception=expected_exception,\r\n    )\r\n\r\n    eval_ngroup(modin_groupby, pandas_groupby)\r\n    # Pandas raising exception when 'by' contains categorical key and `as_index=False`\r\n    # because of a bug: https://github.com/pandas-dev/pandas/issues/36698\r\n    # Modin correctly processes the result\r\n    if not (col1_category and not as_index):\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.nunique(),\r\n        )\r\n    expected_exception = None\r\n    if col1_category:\r\n        expected_exception = TypeError(\r\n            \"category dtype does not support aggregation 'median'\"\r\n        )\r\n    # TypeError: category type does not support median operations\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.median(),\r\n        modin_df_almost_equals_pandas,\r\n        expected_exception=expected_exception,\r\n    )\r\n\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.head(n))\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.cov(),\r\n        modin_df_almost_equals_pandas,\r\n    )\r\n\r\n    if not check_df_columns_have_nans(modin_df, by):\r\n        # Pandas groupby.transform does not work correctly with NaN values in grouping columns. See Pandas bug 17093.\r\n        transform_functions = [lambda df: df + 4, lambda df: -df - 10]\r\n        for idx, func in enumerate(transform_functions):\r\n            expected_exception = None\r\n            if col1_category:\r\n                if idx == 0:\r\n                    expected_exception = TypeError(\r\n                        \"unsupported operand type(s) for +: 'Categorical' and 'int'\"\r\n                    )\r\n                elif idx == 1:\r\n                    expected_exception = TypeError(\r\n                        \"bad operand type for unary -: 'Categorical'\"\r\n                    )\r\n            eval_general(\r\n                modin_groupby,\r\n                pandas_groupby,\r\n                lambda df: df.transform(func),\r\n                expected_exception=expected_exception,\r\n            )\r\n\r\n    pipe_functions = [lambda dfgb: dfgb.sum()]\r\n    for func in pipe_functions:\r\n        expected_exception = None\r\n        if col1_category:\r\n            expected_exception = TypeError(\r\n                \"category type does not support sum operations\"\r\n            )\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.pipe(func),\r\n            expected_exception=expected_exception,\r\n        )\r\n\r\n    eval_general(\r\n        modin_groupby,\r\n        pandas_groupby,\r\n        lambda df: df.corr(),\r\n        modin_df_almost_equals_pandas,\r\n    )\r\n    eval_fillna(modin_groupby, pandas_groupby)\r\n    eval_count(modin_groupby, pandas_groupby)\r\n    if get_current_execution() != \"BaseOnPython\":\r\n        eval_general(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            lambda df: df.size(),\r\n        )\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.tail(n))\r\n    eval_quantile(modin_groupby, pandas_groupby)\r\n    eval_general(modin_groupby, pandas_groupby, lambda df: df.take([0]))\r\n    if isinstance(by, list) and not any(\r\n        isinstance(o, (pd.Series, pandas.Series)) for o in by\r\n    ):\r\n        # Not yet supported for non-original-column-from-dataframe Series in by:\r\n        eval___getattr__(modin_groupby, pandas_groupby, \"col3\")\r\n        # FIXME: https://github.com/modin-project/modin/issues/7033\r\n        eval___getitem__(\r\n            modin_groupby, pandas_groupby, \"col3\", expected_exception=False\r\n        )\r\n    eval_groups(modin_groupby, pandas_groupby)\r\n    # Intersection of the selection and 'by' columns is not yet supported\r\n    non_by_cols = (\r\n        # Potential selection starts only from the second column, because the first may\r\n        # be categorical in this test, which is not yet supported\r\n        [col for col in pandas_df.columns[1:] if col not in modin_groupby._internal_by]\r\n        if isinstance(by, list)\r\n        else [\"col3\", \"col4\"]\r\n    )\r\n    # FIXME: https://github.com/modin-project/modin/issues/7033\r\n    eval___getitem__(\r\n        modin_groupby, pandas_groupby, non_by_cols, expected_exception=False\r\n    )\r\n    # When GroupBy.__getitem__ meets an intersection of the selection and 'by' columns\r\n    # it throws a warning with the suggested workaround. The following code tests\r\n    # that this workaround works as expected.\r\n    if len(modin_groupby._internal_by) != 0:\r\n        if not isinstance(by, list):\r\n            by = [by]\r\n        by_from_workaround = [\r\n            (\r\n                modin_df[getattr(col, \"name\", col)].copy()\r\n                if (hashable(col) and col in modin_groupby._internal_by)\r\n                or isinstance(col, GetColumn)\r\n                else col\r\n            )\r\n            for col in by\r\n        ]\r\n        # GroupBy result with 'as_index=False' depends on the 'by' origin, since we forcibly changed\r\n        # the origin of 'by' for modin by doing a copy, set 'as_index=True' to compare results.\r\n        modin_groupby = modin_df.groupby(\r\n            maybe_get_columns(modin_df, by_from_workaround), as_index=True\r\n        )\r\n        pandas_groupby = pandas_df.groupby(pandas_by, as_index=True)\r\n        eval___getitem__(\r\n            modin_groupby,\r\n            pandas_groupby,\r\n            list(modin_groupby._internal_by) + non_by_cols[:1],\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_read_parquet_directory_range_index_consistent_metadata(\r\n        self,\r\n        engine,\r\n        filters,\r\n        range_index_start,\r\n        range_index_step,\r\n        range_index_name,\r\n        tmp_path,\r\n    ):\r\n        num_cols = DATASET_SIZE_DICT.get(\r\n            TestDatasetSize.get(), DATASET_SIZE_DICT[\"Small\"]\r\n        )\r\n        df = pandas.DataFrame(\r\n            {f\"col{x + 1}\": np.arange(0, 500) for x in range(num_cols)},\r\n        )\r\n        index = pandas.RangeIndex(\r\n            start=range_index_start,\r\n            stop=range_index_start + (len(df) * range_index_step),\r\n            step=range_index_step,\r\n            name=range_index_name,\r\n        )\r\n        if (\r\n            range_index_start == 0\r\n            and range_index_step == 1\r\n            and range_index_name is None\r\n        ):\r\n            assert df.index.equals(index)\r\n        else:\r\n            df.index = index\r\n\r\n        path = get_unique_filename(extension=None, data_dir=tmp_path)\r\n\r\n        table = pa.Table.from_pandas(df)\r\n        pyarrow.dataset.write_dataset(\r\n            table,\r\n            path,\r\n            format=\"parquet\",\r\n            max_rows_per_group=35,\r\n            max_rows_per_file=100,\r\n        )\r\n\r\n        # There are specific files that PyArrow will try to ignore by default\r\n        # in a parquet directory. One example are files that start with '_'. Our\r\n        # previous implementation tried to read all files in a parquet directory,\r\n        # but we now make use of PyArrow to ensure the directory is valid.\r\n        with open(os.path.join(path, \"_committed_file\"), \"w+\") as f:\r\n            f.write(\"testingtesting\")\r\n\r\n        eval_io(\r\n            fn_name=\"read_parquet\",\r\n            # read_parquet kwargs\r\n            engine=engine,\r\n            path=path,\r\n            filters=filters,\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_optimizer_params(self, tmp_path_dist_ckpt):\r\n        Utils.initialize_model_parallel(1, 1)\r\n        model = Model()\r\n        # Force optimizer state initialization\r\n        for p in model.parameters():\r\n            p.grad = torch.ones_like(p.data)\r\n        optim = Adam(model.parameters())\r\n        optim.step()\r\n\r\n        model_state_dict = model.sharded_state_dict()\r\n        param_map = get_param_id_to_sharded_param_map(\r\n            model_state_dict, optim.param_groups[0]['params']\r\n        )\r\n        optim_state_dict = optim.state_dict()\r\n        optim_state_to_sharding_state(optim_state_dict, param_map, exclude_keys=('step',))\r\n\r\n        optim_sharded_tensors = nested_values(extract_sharded_tensors(optim_state_dict)[0])\r\n        optim_sharded_keys = {sh_ten.key for sh_ten in optim_sharded_tensors}\r\n        assert len(optim_sharded_keys) == 2 * len(model_state_dict)\r\n        assert optim_sharded_keys == set(\r\n            [\r\n                f'optimizer.state.{state_key}.{layer_name}'\r\n                for state_key in ['exp_avg', 'exp_avg_sq']\r\n                for layer_name in model_state_dict\r\n            ]\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_forward(self):\r\n        self.model.cuda()\r\n\r\n        img = torch.zeros((2, 3, 336, 336)).cuda()\r\n\r\n        out = self.model.forward(img)\r\n        assert out.shape == torch.Size([2, 577, 64])",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_inference(self):\r\n        config: TransformerConfig = self.model.config\r\n        micro_batch_size = 2\r\n        inference_params: InferenceParams = InferenceParams(\r\n            max_batch_size=micro_batch_size, max_sequence_length=self.model.max_sequence_length\r\n        )\r\n        prompt_length = self.model.max_sequence_length - 1\r\n\r\n        self.model.cuda()\r\n\r\n        # load-context/first-output-token, step/generate\r\n        for offset in (0, prompt_length):\r\n            if offset == 0:\r\n                sequence_length = prompt_length\r\n            else:\r\n                sequence_length = 1\r\n            inference_params.sequence_len_offset = offset\r\n\r\n            data = list(range(sequence_length))\r\n            input_ids = torch.tensor(data, dtype=torch.int64).repeat((micro_batch_size, 1)).cuda()\r\n            position_ids = (\r\n                torch.tensor(data, dtype=torch.int64).repeat((micro_batch_size, 1)).cuda()\r\n            )\r\n            attention_mask = torch.ones(\r\n                (micro_batch_size, 1, sequence_length, sequence_length), dtype=bool\r\n            ).cuda()\r\n\r\n            logits = self.model.forward(\r\n                input_ids=input_ids,\r\n                position_ids=position_ids,\r\n                attention_mask=attention_mask,\r\n                inference_params=inference_params,\r\n            )\r\n\r\n            assert logits.shape[0] == micro_batch_size\r\n            assert logits.shape[1] == sequence_length\r\n            assert logits.shape[2] == self.model.vocab_size",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_forward(self):\r\n        config: TransformerConfig = self.model.config\r\n        sequence_length = self.model.max_sequence_length\r\n        micro_batch_size = 2\r\n\r\n        self.model.cuda()\r\n\r\n        data = list(range(sequence_length))\r\n        input_ids = torch.tensor(data, dtype=torch.int64).repeat((micro_batch_size, 1)).cuda()\r\n        position_ids = torch.tensor(data, dtype=torch.int64).repeat((micro_batch_size, 1)).cuda()\r\n        attention_mask = torch.ones(\r\n            (micro_batch_size, 1, sequence_length, sequence_length), dtype=bool\r\n        ).cuda()\r\n\r\n        logits = self.model.forward(\r\n            input_ids=input_ids, position_ids=position_ids, attention_mask=attention_mask\r\n        )\r\n\r\n        assert logits.shape[0] == micro_batch_size\r\n        assert logits.shape[1] == sequence_length\r\n        assert logits.shape[2] == self.model.vocab_size",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_forward(self):\r\n        self.model.cuda()\r\n\r\n        # 3 images with 1 tile and 2 images with 2 tiles.\r\n        img = torch.randn((7, 3, 336, 336)).cuda()\r\n\r\n        image_token_index = self.model.image_token_index\r\n        input_ids = torch.randint(0, 2048, (5, 1024)).cuda()\r\n        input_ids[0, 0] = image_token_index  # image before text\r\n        input_ids[1, 100] = image_token_index  # image in between\r\n        input_ids[2, -1] = image_token_index  # image at the end\r\n        # input_ids[3] - no image\r\n        input_ids[4, 50] = image_token_index\r\n        input_ids[4, 150] = image_token_index\r\n\r\n        position_ids = torch.arange(0, 1024, dtype=torch.int).expand(5, 1024).cuda()\r\n\r\n        loss_mask = torch.ones((5, 1024)).cuda()\r\n\r\n        attention_mask = None  # Causal.\r\n\r\n        labels = torch.randint(0, 2048, (5, 1024)).cuda()\r\n        labels[1, 99] = image_token_index\r\n        labels[2, -2] = image_token_index\r\n\r\n        num_image_tiles = torch.tensor([1, 2, 1, 2, 1], dtype=torch.int).cuda()\r\n\r\n        # Try with labels.\r\n        loss, new_loss_mask = self.model.forward(\r\n            img,\r\n            input_ids,\r\n            position_ids,\r\n            attention_mask,\r\n            labels,\r\n            loss_mask,\r\n            num_image_tiles=num_image_tiles,\r\n        )\r\n\r\n        # The maximum sequence length is given by the sample with 2 images in 3 tiles, minus two image token indices, plus other text tokens.\r\n        img_seq_len = 577\r\n        max_seq_len = img_seq_len * 3 - 2 + 1024\r\n        assert loss.shape == new_loss_mask.shape == torch.Size((5, max_seq_len))\r\n\r\n        # Try with labels and PackedSeqParams. Only micro batch size 1 is supported in this mode.\r\n        packed_seq_params = PackedSeqParams(\r\n            qkv_format=\"thd\",\r\n            cu_seqlens_q=torch.tensor(\r\n                [0, 512, 1024, 1600], dtype=torch.int32\r\n            ).cuda(),  # Just example values.\r\n            cu_seqlens_kv=torch.tensor([0, 512, 1024, 1600], dtype=torch.int32).cuda(),\r\n            max_seqlen_q=torch.tensor(1600, dtype=torch.int32).cuda(),\r\n            max_seqlen_kv=torch.tensor(1600, dtype=torch.int32).cuda(),\r\n        )\r\n\r\n        # NOTE: Packing is only supported with BF16. Use BF16 here and switch back to default.\r\n        self.model.to(torch.bfloat16)\r\n        loss, new_loss_mask = self.model.forward(\r\n            img[:1].to(torch.bfloat16),\r\n            input_ids[:1],\r\n            position_ids[:1],\r\n            attention_mask,\r\n            labels[:1],\r\n            loss_mask[:1],\r\n            num_image_tiles=num_image_tiles[:1],\r\n            packed_seq_params=packed_seq_params,\r\n        )\r\n        self.model.to(torch.float32)\r\n\r\n        # 1600 = 577 (img_seq_len) + 1024 (text tokens in the first sample) - 1 (image token).\r\n        assert loss.shape == new_loss_mask.shape == torch.Size((1, 1600))\r\n\r\n        # Try text-only input.\r\n        loss, new_loss_mask = self.model.forward(\r\n            torch.tensor([], dtype=torch.float).cuda(),\r\n            torch.randint(0, 2048, (5, 1024)).cuda(),\r\n            position_ids,\r\n            attention_mask,\r\n            torch.randint(0, 2048, (5, 1024)).cuda(),\r\n            loss_mask,\r\n            num_image_tiles=torch.tensor([], dtype=torch.int).cuda(),\r\n        )\r\n\r\n        assert loss.shape == new_loss_mask.shape == torch.Size((5, 1024))\r\n\r\n        # Try without labels and without inference params.\r\n        logits, _ = self.model.forward(\r\n            img,\r\n            input_ids,\r\n            position_ids,\r\n            attention_mask,\r\n            labels=None,\r\n            loss_mask=None,\r\n            num_image_tiles=num_image_tiles,\r\n        )\r\n        assert logits.shape == torch.Size((5, max_seq_len, 8192))\r\n\r\n        # Try without labels and with inference params.\r\n        inference_params = InferenceParams(5, max_seq_len)\r\n        logits, _ = self.model.forward(\r\n            img,\r\n            input_ids,\r\n            position_ids,\r\n            attention_mask,\r\n            labels=None,\r\n            loss_mask=None,\r\n            num_image_tiles=num_image_tiles,\r\n            inference_params=inference_params,\r\n        )\r\n        assert logits.shape == torch.Size((5, max_seq_len, 8192))\r\n\r\n        # Check KV cache got populated correctly.\r\n        kv_dict = inference_params.key_value_memory_dict\r\n\r\n        assert kv_dict[\"image_tokens_count\"] == 577 * 7\r\n        for layer_no in range(1, 4):  # 3 layers in the model.\r\n            layer_kv = kv_dict[layer_no]\r\n            # Expected shape is [sequence_len, batch_size, num_heads, hidden_size_per_head]\r\n            assert (\r\n                layer_kv[0].shape\r\n                == layer_kv[1].shape\r\n                == torch.Size((max_seq_len, 5, self.language_num_attention_heads, 16))\r\n            )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def rerank(df):\r\n    client = PipelineClient()\r\n    client.connect([\"127.0.0.1:8089\"])\r\n    list_data = []\r\n    for index, row in df.iterrows():\r\n        example = {\"query\": row[\"query_text\"], \"title\": row[\"text\"]}\r\n        list_data.append(example)\r\n    feed = {}\r\n    for i, item in enumerate(list_data):\r\n        feed[str(i)] = str(item)\r\n\r\n    start_time = time.time()\r\n    ret = client.predict(feed_dict=feed)\r\n    end_time = time.time()\r\n    print(\"time to cost :{} seconds\".format(end_time - start_time))\r\n    result = np.array(eval(ret.value[0]))\r\n    df[\"distance\"] = result\r\n    df = df.sort_values(by=[\"distance\"], ascending=False)\r\n    df.to_csv(\"rank_result.csv\", index=False)",
        "labels": [
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def pd_train_some_iters(fake_data, fake_label, max_iter=2):\r\n    paddle_dump_path = \"../weights/paddle_weight.pdparams\"\r\n    config = PDBertConfig()\r\n    model = PDBertForSequenceClassification(config)\r\n    checkpoint = paddle.load(paddle_dump_path)\r\n    model.bert.load_dict(checkpoint)\r\n\r\n    classifier_weights = paddle.load(\"../classifier_weights/paddle_classifier_weights.bin\")\r\n    model.load_dict(classifier_weights)\r\n    model.eval()\r\n    criterion = paddle.nn.CrossEntropyLoss()\r\n    decay_params = [p.name for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"norm\"])]\r\n    optimizer = paddle.optimizer.AdamW(\r\n        learning_rate=3e-5,\r\n        parameters=model.parameters(),\r\n        weight_decay=1e-2,\r\n        epsilon=1e-6,\r\n        apply_decay_param_fun=lambda x: x in decay_params,\r\n    )\r\n    loss_list = []\r\n    for idx in range(max_iter):\r\n        input_ids = paddle.to_tensor(fake_data)\r\n        labels = paddle.to_tensor(fake_label)\r\n\r\n        output = model(input_ids)[0]\r\n        loss = criterion(output, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.clear_grad()\r\n        loss_list.append(loss)\r\n    return loss_list",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def hf_train_some_iters(fake_data, fake_label, max_iter=2):\r\n\r\n    pytorch_dump_path = \"../weights/torch_weight.bin\"\r\n    config = HFBertConfig()\r\n    model = HFBertForSequenceClassification(config)\r\n    checkpoint = torch.load(pytorch_dump_path)\r\n    model.bert.load_state_dict(checkpoint)\r\n    classifier_weights = torch.load(\"../classifier_weights/torch_classifier_weights.bin\")\r\n    model.load_state_dict(classifier_weights, strict=False)\r\n    model.eval()\r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n    optimizer_grouped_parameters = [\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n            \"weight_decay\": 1e-2,\r\n        },\r\n        {\r\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n            \"weight_decay\": 0.0,\r\n        },\r\n    ]\r\n    optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\r\n\r\n    loss_list = []\r\n    for idx in range(max_iter):\r\n        input_ids = torch.from_numpy(fake_data)\r\n        labels = torch.from_numpy(fake_label)\r\n\r\n        output = model(input_ids)[0]\r\n        loss = criterion(output, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n        loss_list.append(loss)\r\n    return loss_list",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_one_epoch(\r\n    model,\r\n    criterion,\r\n    optimizer,\r\n    lr_scheduler,\r\n    data_loader,\r\n    device,\r\n    epoch,\r\n    print_freq,\r\n    scaler=None,\r\n):\r\n    model.train()\r\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\r\n    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value}\"))\r\n    metric_logger.add_meter(\"sentence/s\", utils.SmoothedValue(window_size=10, fmt=\"{value}\"))\r\n\r\n    header = \"Epoch: [{}]\".format(epoch)\r\n    for batch in metric_logger.log_every(data_loader, print_freq, header):\r\n        start_time = time.time()\r\n        batch.to(device)\r\n        labels = batch.pop(\"labels\")\r\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\r\n            logits = model(**batch)[0]\r\n            loss = criterion(logits.reshape(-1, 2), labels.reshape(-1))\r\n\r\n        optimizer.zero_grad()\r\n        if scaler is not None:\r\n            scaler.scale(loss).backward()\r\n            scaler.step(optimizer)\r\n            scaler.update()\r\n        else:\r\n            loss.backward()\r\n            optimizer.step()\r\n        lr_scheduler.step()\r\n        batch_size = batch[\"input_ids\"].shape[0]\r\n        metric_logger.update(loss=loss.item(), lr=lr_scheduler.get_last_lr()[-1])\r\n        metric_logger.meters[\"sentence/s\"].update(batch_size / (time.time() - start_time))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def data_test_ix(request, dirpath):\r\n    i, test_ix = request.param\r\n    fname = os.path.join(dirpath, f\"test_sas7bdat_{i}.csv\")\r\n    df = pd.read_csv(fname)\r\n    epoch = datetime(1960, 1, 1)\r\n    t1 = pd.to_timedelta(df[\"Column4\"], unit=\"D\")\r\n    df[\"Column4\"] = (epoch + t1).astype(\"M8[s]\")\r\n    t2 = pd.to_timedelta(df[\"Column12\"], unit=\"D\")\r\n    df[\"Column12\"] = (epoch + t2).astype(\"M8[s]\")\r\n    for k in range(df.shape[1]):\r\n        col = df.iloc[:, k]\r\n        if col.dtype == np.int64:\r\n            df.isetitem(k, df.iloc[:, k].astype(np.float64))\r\n    return df, test_ix",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_basic_aggregations(dtype):\r\n    data = Series(np.arange(9) // 3, index=np.arange(9), dtype=dtype)\r\n\r\n    index = np.arange(9)\r\n    np.random.default_rng(2).shuffle(index)\r\n    data = data.reindex(index)\r\n\r\n    grouped = data.groupby(lambda x: x // 3, group_keys=False)\r\n\r\n    for k, v in grouped:\r\n        assert len(v) == 3\r\n\r\n    agged = grouped.aggregate(np.mean)\r\n    assert agged[1] == 1\r\n\r\n    expected = grouped.agg(np.mean)\r\n    tm.assert_series_equal(agged, expected)  # shorthand\r\n    tm.assert_series_equal(agged, grouped.mean())\r\n    result = grouped.sum()\r\n    expected = grouped.agg(np.sum)\r\n    if dtype == \"int32\":\r\n        # NumPy's sum returns int64\r\n        expected = expected.astype(\"int32\")\r\n    tm.assert_series_equal(result, expected)\r\n\r\n    expected = grouped.apply(lambda x: x * x.sum())\r\n    transformed = grouped.transform(lambda x: x * x.sum())\r\n    assert transformed[7] == 12\r\n    tm.assert_series_equal(transformed, expected)\r\n\r\n    value_grouped = data.groupby(data)\r\n    result = value_grouped.aggregate(np.mean)\r\n    tm.assert_series_equal(result, agged, check_index_type=False)\r\n\r\n    # complex agg\r\n    agged = grouped.aggregate([np.mean, np.std])\r\n\r\n    msg = r\"nested renamer is not supported\"\r\n    with pytest.raises(pd.errors.SpecificationError, match=msg):\r\n        grouped.aggregate({\"one\": np.mean, \"two\": np.std})\r\n\r\n    # corner cases\r\n    msg = \"Must produce aggregated value\"\r\n    # exception raised is type Exception\r\n    with pytest.raises(Exception, match=msg):\r\n        grouped.aggregate(lambda x: x * 2)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_transform_frame(on):\r\n    # GH#47079\r\n    index = date_range(datetime(2005, 1, 1), datetime(2005, 1, 10), freq=\"D\")\r\n    index.name = \"date\"\r\n    df = DataFrame(\r\n        np.random.default_rng(2).random((10, 2)), columns=list(\"AB\"), index=index\r\n    )\r\n    expected = df.groupby(pd.Grouper(freq=\"20min\")).transform(\"mean\")\r\n    if on == \"date\":\r\n        # Move date to being a column; result will then have a RangeIndex\r\n        expected = expected.reset_index(drop=True)\r\n        df = df.reset_index()\r\n\r\n    r = df.resample(\"20min\", on=on)\r\n    result = r.transform(\"mean\")\r\n    tm.assert_frame_equal(result, expected)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_transform_nuisance_raises(df, using_infer_string):\r\n    # case that goes through _transform_item_by_item\r\n\r\n    df.columns = [\"A\", \"B\", \"B\", \"D\"]\r\n\r\n    # this also tests orderings in transform between\r\n    # series/frame to make sure it's consistent\r\n    grouped = df.groupby(\"A\")\r\n\r\n    gbc = grouped[\"B\"]\r\n    msg = \"Could not convert\"\r\n    if using_infer_string:\r\n        msg = \"Cannot perform reduction 'mean' with string dtype\"\r\n    with pytest.raises(TypeError, match=msg):\r\n        gbc.transform(lambda x: np.mean(x))\r\n\r\n    with pytest.raises(TypeError, match=msg):\r\n        df.groupby(\"A\").transform(lambda x: np.mean(x))",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_transform_invalid_name_raises():\r\n    # GH#27486\r\n    df = DataFrame({\"a\": [0, 1, 1, 2]})\r\n    g = df.groupby([\"a\", \"b\", \"b\", \"c\"])\r\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\r\n        g.transform(\"some_arbitrary_name\")\r\n\r\n    # method exists on the object, but is not a valid transformation/agg\r\n    assert hasattr(g, \"aggregate\")  # make sure the method exists\r\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\r\n        g.transform(\"aggregate\")\r\n\r\n    # Test SeriesGroupBy\r\n    g = df[\"a\"].groupby([\"a\", \"b\", \"b\", \"c\"])\r\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\r\n        g.transform(\"some_arbitrary_name\")",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_idxmin_idxmax_transform_args(how, skipna, numeric_only):\r\n    # GH#55268 - ensure *args are passed through when calling transform\r\n    df = DataFrame({\"a\": [1, 1, 1, 2], \"b\": [3.0, 4.0, np.nan, 6.0], \"c\": list(\"abcd\")})\r\n    gb = df.groupby(\"a\")\r\n    if skipna:\r\n        result = gb.transform(how, skipna, numeric_only)\r\n        expected = gb.transform(how, skipna=skipna, numeric_only=numeric_only)\r\n        tm.assert_frame_equal(result, expected)\r\n    else:\r\n        msg = f\"DataFrameGroupBy.{how} with skipna=False encountered an NA value\"\r\n        with pytest.raises(ValueError, match=msg):\r\n            gb.transform(how, skipna, numeric_only)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_two_classes_cumulative_gain(ploomber_value_error_message):\r\n    X, y = load_iris(return_X_y=True)\r\n    clf = LogisticRegression()\r\n    clf.fit(X, y)\r\n    probas = clf.predict_proba(X)\r\n    with pytest.raises(ValueError, match=ploomber_value_error_message) as e:\r\n        cumulative_gain(y, probas)\r\n    assert \"Cannot calculate Cumulative Gains for data with 3 category/ies\" in str(\r\n        e.value\r\n    )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error_dropping_unknown_column(return_summary):\r\n    df = pd.DataFrame({\"x\": [1, 2, 3]})\r\n\r\n    selector = DataSelector({\"kind\": \"column_drop\", \"names\": [\"y\"]})\r\n\r\n    with pytest.raises(DataSelectorError):\r\n        selector.transform(df, return_summary=return_summary)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_two_classes_lift_curve(ploomber_value_error_message):\r\n    X, y = load_iris(return_X_y=True)\r\n    clf = LogisticRegression()\r\n    clf.fit(X, y)\r\n    probas = clf.predict_proba(X)\r\n    with pytest.raises(ValueError, match=ploomber_value_error_message) as e:\r\n        lift_curve(y, probas)\r\n    assert \"Cannot calculate Lift Curve for data with 3 category/ies\" in str(e.value)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_evaluate_model_none_error(heart_dataset):\r\n    model = RandomForestClassifier()\r\n    data = heart_dataset\r\n    y_test, y_pred, y_score = _get_classifier_model_values(model, data)\r\n\r\n    with pytest.raises(ValueError) as e:\r\n        evaluate_model(None, y_test, y_pred, y_score)\r\n\r\n    assert \"Model is none\" in str(e.value)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_two_classes(ploomber_value_error_message):\r\n    np.random.seed(0)\r\n    X, y = load_iris(return_X_y=True)\r\n    clf = LogisticRegression()\r\n    clf.fit(X, y)\r\n    probas = clf.predict_proba(X)\r\n    with pytest.raises(ValueError, match=ploomber_value_error_message) as e:\r\n        ks_statistic(y, probas)\r\n    assert \"Cannot calculate KS statistic for data with 3 category/ies\" in str(e.value)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def main(replicates: int, force: bool):\r\n    \"\"\"Run the benchmark.\"\"\"\r\n    import pykeen.triples.splitting\r\n\r\n    pykeen.triples.splitting.logger.setLevel(logging.ERROR)\r\n    import pykeen.triples.triples_factory\r\n\r\n    pykeen.triples.triples_factory.logger.setLevel(logging.ERROR)\r\n    import pykeen.utils\r\n\r\n    pykeen.utils.logger.setLevel(logging.ERROR)\r\n\r\n    git_hash = get_git_hash()\r\n    methods = [\"cleanup\", \"coverage\"]\r\n    ratios = [0.8]\r\n\r\n    click.echo(f\"output directory: {SPLITTING_DIRECTORY.as_posix()}\")\r\n    rows = []\r\n    outer_it = tqdm(sorted(dataset_resolver.lookup_dict), desc=\"Dataset\")\r\n    for dataset in outer_it:\r\n        dataset_path = RESULTS_DIRECTORY / f\"{dataset}.tsv\"\r\n        if dataset_path.exists() and not force:\r\n            _log(f\"loading pre-calculated {dataset} from {dataset_path}\")\r\n            df = pd.read_csv(dataset_path, sep=\"\\t\")\r\n            rows.extend(df.values)\r\n            continue\r\n\r\n        _log(f\"loading {dataset}\")\r\n        t = time.time()\r\n        dataset = get_dataset(dataset=dataset)\r\n        dataset_name = dataset.__class__.__name__\r\n        ccl = [\r\n            dataset.training.mapped_triples,\r\n            dataset.testing.mapped_triples,\r\n            dataset.validation.mapped_triples,\r\n        ]\r\n        load_time = time.time() - t\r\n        _log(f\"done loading {dataset_name} after {load_time:.3f} seconds\")\r\n        _log(f\"concatenating {dataset_name}\")\r\n        t = time.time()\r\n        mapped_triples: torch.LongTensor = torch.cat(ccl, dim=0)\r\n        concat_time = time.time() - t\r\n        _log(f\"done concatenating {dataset_name} after {concat_time:.3f} seconds\")\r\n        _log(f\"deleting {dataset_name}\")\r\n        del dataset\r\n        _log(f\"done deleting {dataset_name}\")\r\n\r\n        dataset_rows = []\r\n        inner_it = itt.product(methods, ratios, range(1, 1 + replicates))\r\n        inner_it = tqdm(\r\n            inner_it,\r\n            total=len(methods) * len(ratios) * replicates,\r\n            desc=f\"{dataset_name} ({intword(mapped_triples.shape[0])})\",\r\n        )\r\n        for method, ratio, replicate in inner_it:\r\n            t = time.time()\r\n            results = split(\r\n                mapped_triples,\r\n                ratios=[ratio, (1 - ratio) / 2],\r\n                method=method,\r\n                random_state=replicate,\r\n            )\r\n            split_time = time.time() - t\r\n            dataset_rows.append(\r\n                (\r\n                    git_hash,\r\n                    dataset_name,\r\n                    mapped_triples.shape[0],\r\n                    load_time,\r\n                    concat_time,\r\n                    method,\r\n                    ratio,\r\n                    replicate,\r\n                    split_time,\r\n                    results[0].shape[0],\r\n                    results[1].shape[0],\r\n                    results[2].shape[0],\r\n                )\r\n            )\r\n            del results\r\n\r\n        _log(f\"writing to {dataset_path}\")\r\n        pd.DataFrame(dataset_rows, columns=columns).to_csv(dataset_path, sep=\"\\t\", index=False)\r\n        rows.extend(dataset_rows)\r\n\r\n    df = pd.DataFrame(rows, columns=columns)\r\n    df.to_csv(tsv_path, sep=\"\\t\", index=False)\r\n    _make_1(df, git_hash)\r\n    _make_2(df, git_hash)",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def predict_target(\r\n    model: Model,\r\n    *,\r\n    # exactly one of them is None\r\n    head: Union[None, int, str] = None,\r\n    relation: Union[None, int, str] = None,\r\n    tail: Union[None, int, str] = None,\r\n    #\r\n    triples_factory: Optional[TriplesFactory] = None,\r\n    targets: Union[None, LongTensor, Sequence[Union[int, str]]] = None,\r\n    mode: Optional[InductiveMode] = None,\r\n) -> Predictions:\r\n    \"\"\"Get predictions for the head, relation, and/or tail combination.\r\n\r\n    .. note ::\r\n        Exactly one of `head`, `relation` and `tail` should be None. This is the position\r\n        which will be predicted.\r\n\r\n    :param model:\r\n        A PyKEEN model\r\n\r\n    :param head:\r\n        the head entity, either as ID or as label. If None, predict heads\r\n    :param relation:\r\n        the relation, either as ID or as label. If None, predict relations\r\n    :param tail:\r\n        the tail entity, either as ID or as label. If None, predict tails\r\n\r\n    :param targets:\r\n        restrict prediction to these targets. `None` means no restriction, i.e., scoring all entities/relations.\r\n    :param triples_factory:\r\n        the training triples factory; required if head/relation/tail are given as string, and used to translate the\r\n        label to an ID.\r\n\r\n    :param mode:\r\n        The pass mode, which is None in the transductive setting and one of \"training\",\r\n        \"validation\", or \"testing\" in the inductive setting.\r\n\r\n    :return:\r\n        The predictions, containing either the $k$ highest scoring targets, or all targets if $k$ is `None`.\r\n    \"\"\"\r\n    # TODO: add support for (automatic) slicing\r\n    # note: the models' predict method takes care of setting the model to evaluation mode\r\n\r\n    # get input & target\r\n    target, batch, other_col_ids = _get_input_batch(factory=triples_factory, head=head, relation=relation, tail=tail)\r\n\r\n    # get label-to-id mapping and prediction targets\r\n    labels, ids, targets = _get_targets(\r\n        ids=targets, triples_factory=triples_factory, device=model.device, entity=relation is not None\r\n    )\r\n\r\n    # get scores\r\n    scores = model.predict(batch, full_batch=False, mode=mode, ids=targets, target=target).squeeze(dim=0)\r\n    if ids is None:\r\n        ids = range(len(scores))\r\n\r\n    # note: maybe we want to expose these scores, too?\r\n    if target == LABEL_RELATION and model.use_inverse_triples:\r\n        ids_t = torch.as_tensor(ids)\r\n        non_inv_mask = ~model.relation_inverter.is_inverse(ids_t)\r\n        ids = ids_t[non_inv_mask].tolist()\r\n        scores = scores[non_inv_mask]\r\n\r\n    # create raw dataframe\r\n    data = {f\"{target}_id\": ids, \"score\": scores.tolist()}\r\n    if labels is not None:\r\n        data[f\"{target}_label\"] = labels\r\n    df = pandas.DataFrame(data=data).sort_values(\"score\", ascending=False)\r\n    return TargetPredictions(df=df, factory=triples_factory, target=target, other_columns_fixed_ids=other_col_ids)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def get_relation_pattern_types_df(\r\n    dataset: Dataset,\r\n    *,\r\n    min_support: int = 0,\r\n    min_confidence: float = 0.95,\r\n    drop_confidence: bool = False,\r\n    parts: Optional[Collection[str]] = None,\r\n    force: bool = False,\r\n    add_labels: bool = True,\r\n) -> pd.DataFrame:\r\n    r\"\"\"\r\n    Categorize relations based on patterns from RotatE [sun2019]_.\r\n\r\n    The relation classifications are based upon checking whether the corresponding rules hold with sufficient support\r\n    and confidence. By default, we do not require a minimum support, however, a relatively high confidence.\r\n\r\n    The following four non-exclusive classes for relations are considered:\r\n\r\n    - symmetry\r\n    - anti-symmetry\r\n    - inversion\r\n    - composition\r\n\r\n    This method generally follows the terminology of association rule mining. The patterns are expressed as\r\n\r\n    .. math ::\r\n\r\n        X_1 \\land \\cdot \\land X_k \\implies Y\r\n\r\n    where $X_i$ is of the form $r_i(h_i, t_i)$, and some of the $h_i / t_i$ might re-occur in other atoms.\r\n    The *support* of a pattern is the number of distinct instantiations of all variables for the left hand side.\r\n    The *confidence* is the proportion of these instantiations where the right-hand side is also true.\r\n\r\n    :param dataset:\r\n        The dataset to investigate.\r\n    :param min_support:\r\n        A minimum support for patterns.\r\n    :param min_confidence:\r\n        A minimum confidence for the tested patterns.\r\n    :param drop_confidence:\r\n        Whether to drop the support/confidence information from the result frame, and also drop duplicates.\r\n    :param parts:\r\n        Only use certain parts of the dataset, e.g., train triples. Defaults to using all triples, i.e.\r\n        {\"training\", \"validation\", \"testing}.\r\n    :param force:\r\n        Whether to enforce re-calculation even if a cached version is available.\r\n    :param add_labels:\r\n        Whether to add relation labels (if available).\r\n\r\n    .. warning ::\r\n\r\n        If you intend to use the relation categorization as input to your model, or hyper-parameter selection, do *not*\r\n        include testing triples to avoid leakage!\r\n\r\n    :return:\r\n        A dataframe with columns {\"relation_id\", \"pattern\", \"support\"?, \"confidence\"?}.\r\n    \"\"\"\r\n    # TODO: Merge with _common?\r\n    parts = _normalize_parts(dataset, parts)\r\n    mapped_triples = _get_mapped_triples(dataset, parts)\r\n\r\n    # include hash over triples into cache-file name\r\n    ph = triple_analysis.triple_set_hash(mapped_triples=mapped_triples)[:16]\r\n\r\n    # include part hash into cache-file name\r\n    cache_path = PYKEEN_DATASETS.joinpath(dataset.__class__.__name__.lower(), f\"relation_patterns_{ph}.tsv.xz\")\r\n\r\n    # re-use cached file if possible\r\n    if not cache_path.is_file() or force:\r\n        # select triples\r\n        mapped_triples = torch.cat([dataset.factory_dict[part].mapped_triples for part in parts], dim=0).tolist()\r\n\r\n        df = triple_analysis.relation_pattern_types(mapped_triples=mapped_triples)\r\n\r\n        # save to file\r\n        cache_path.parent.mkdir(exist_ok=True, parents=True)\r\n        df.to_csv(cache_path, sep=\"\\t\", index=False)\r\n        logger.info(f\"Cached {len(df)} relational pattern entries to {cache_path.as_uri()}\")\r\n    else:\r\n        df = pd.read_csv(cache_path, sep=\"\\t\")\r\n        logger.info(f\"Loaded {len(df)} precomputed relational patterns from {cache_path.as_uri()}\")\r\n\r\n    # Prune by support and confidence\r\n    sufficient_support = df[triple_analysis.SUPPORT_COLUMN_NAME] >= min_support\r\n    sufficient_confidence = df[triple_analysis.CONFIDENCE_COLUMN_NAME] >= min_confidence\r\n    df = df[sufficient_support & sufficient_confidence]\r\n\r\n    if drop_confidence:\r\n        df = df[[triple_analysis.RELATION_ID_COLUMN_NAME, triple_analysis.PATTERN_TYPE_COLUMN_NAME]].drop_duplicates()\r\n\r\n    return triple_analysis.add_relation_labels(\r\n        df=df,\r\n        add_labels=add_labels,\r\n        label_to_id=dataset.relation_to_id,\r\n    )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _analyze(\r\n    dataset_name: str,\r\n    dataset: Dataset,\r\n    force: bool,\r\n    countplots: bool,\r\n    directory: Union[None, str, pathlib.Path],\r\n):\r\n    from . import analysis\r\n\r\n    plt, sns = _get_plotting_libraries()\r\n\r\n    # Raise matplotlib level\r\n    logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\r\n\r\n    if directory is None:\r\n        directory = PYKEEN_DATASETS\r\n    else:\r\n        directory = pathlib.Path(directory)\r\n        directory.mkdir(exist_ok=True, parents=True)\r\n\r\n    d = directory.joinpath(dataset_name, \"analysis\")\r\n    d.mkdir(parents=True, exist_ok=True)\r\n\r\n    dfs = {}\r\n    it = tqdm(analysis.__dict__.items(), leave=False, desc=\"Stats\")\r\n    for name, func in it:\r\n        if not name.startswith(\"get\") or not name.endswith(\"df\"):\r\n            continue\r\n        it.set_postfix(func=name)\r\n        key = name[len(\"get_\") : -len(\"_df\")]\r\n        path = d.joinpath(key).with_suffix(\".tsv\")\r\n        if path.exists() and not force:\r\n            df = pd.read_csv(path, sep=\"\\t\")\r\n        else:\r\n            df = func(dataset=dataset)\r\n            df.to_csv(d.joinpath(key).with_suffix(\".tsv\"), sep=\"\\t\", index=False)\r\n        dfs[key] = df\r\n\r\n    fig, ax = plt.subplots(1, 1)\r\n    sns.scatterplot(\r\n        data=dfs[\"relation_injectivity\"],\r\n        x=LABEL_HEAD,\r\n        y=LABEL_TAIL,\r\n        size=\"support\",\r\n        hue=\"support\",\r\n        ax=ax,\r\n    )\r\n    ax.set_title(f'{docdata.get_docdata(dataset.__class__)[\"name\"]} Relation Injectivity')\r\n    fig.tight_layout()\r\n    fig.savefig(d.joinpath(\"relation_injectivity.svg\"))\r\n    plt.close(fig)\r\n\r\n    fig, ax = plt.subplots(1, 1)\r\n    sns.scatterplot(\r\n        data=dfs[\"relation_functionality\"],\r\n        x=\"functionality\",\r\n        y=\"inverse_functionality\",\r\n        ax=ax,\r\n    )\r\n    ax.set_title(f'{docdata.get_docdata(dataset.__class__)[\"name\"]} Relation Functionality')\r\n    fig.tight_layout()\r\n    fig.savefig(d.joinpath(\"relation_functionality.svg\"))\r\n    plt.close(fig)\r\n\r\n    if countplots:\r\n        entity_count_df = (\r\n            dfs[\"entity_count\"].groupby(\"entity_label\").sum().reset_index().sort_values(\"count\", ascending=False)\r\n        )\r\n        fig, ax = plt.subplots(1, 1)\r\n        sns.barplot(data=entity_count_df, y=\"entity_label\", x=\"count\", ax=ax)\r\n        ax.set_ylabel(\"\")\r\n        ax.set_xscale(\"log\")\r\n        fig.tight_layout()\r\n        fig.savefig(d.joinpath(\"entity_counts.svg\"))\r\n        plt.close(fig)\r\n\r\n        relation_count_df = (\r\n            dfs[\"relation_count\"].groupby(\"relation_label\").sum().reset_index().sort_values(\"count\", ascending=False)\r\n        )\r\n        fig, ax = plt.subplots(1, 1)\r\n        sns.barplot(data=relation_count_df, y=\"relation_label\", x=\"count\", ax=ax)\r\n        ax.set_ylabel(\"\")\r\n        ax.set_xscale(\"log\")\r\n        fig.tight_layout()\r\n        fig.savefig(d.joinpath(\"relation_counts.svg\"))\r\n        plt.close(fig)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def expected_metrics(\r\n    dataset_regex: Optional[str],\r\n    max_triples: Optional[int],\r\n    min_triples: Optional[int],\r\n    log_level: str,\r\n    samples: int,\r\n    force: bool,\r\n    output_directory: pathlib.Path,\r\n):\r\n    \"\"\"Compute expected metrics for all datasets (matching the given pattern).\"\"\"\r\n    logging.getLogger(\"pykeen\").setLevel(level=log_level)\r\n    df_data: list[tuple[str, str, str, str, float]] = []\r\n    for dataset_name, dataset_instance in iter_dataset_instances(\r\n        regex_name_filter=dataset_regex, max_triples=max_triples, min_triples=min_triples\r\n    ):\r\n        adjustments_directory = output_directory.joinpath(dataset_name, \"adjustments\")\r\n        adjustments_directory.mkdir(parents=True, exist_ok=True)\r\n        expected_metrics_path = adjustments_directory.joinpath(\"expected_metrics.json\")\r\n        if expected_metrics_path.is_file() and not force:\r\n            expected_metrics_dict = json.loads(expected_metrics_path.read_text())\r\n        else:\r\n            expected_metrics_dict = dict()\r\n            for key, factory in dataset_instance.factory_dict.items():\r\n                if key == \"training\":\r\n                    additional_filter_triples = None\r\n                elif key == \"validation\":\r\n                    additional_filter_triples = dataset_instance.training.mapped_triples\r\n                elif key == \"testing\":\r\n                    additional_filter_triples = [\r\n                        dataset_instance.training.mapped_triples,\r\n                    ]\r\n                    if dataset_instance.validation is None:\r\n                        click.echo(f\"WARNING: {dataset_name} does not have validation triples!\")\r\n                    else:\r\n                        additional_filter_triples.append(dataset_instance.validation.mapped_triples)\r\n                else:\r\n                    raise AssertionError(key)\r\n                df = get_candidate_set_size(\r\n                    mapped_triples=factory.mapped_triples,\r\n                    additional_filter_triples=additional_filter_triples,\r\n                )\r\n                output_path = adjustments_directory.joinpath(f\"{key}_candidates.tsv.gz\")\r\n                df.to_csv(output_path, sep=\"\\t\", index=False)\r\n                tqdm.write(f\"wrote {output_path}\")\r\n\r\n                # expected metrics\r\n                ks = (1, 3, 5, 10) + tuple(\r\n                    10**i for i in range(2, int(math.ceil(math.log(dataset_instance.num_entities))))\r\n                )\r\n                metrics = [\r\n                    ArithmeticMeanRank(),\r\n                    *(HitsAtK(k) for k in ks),\r\n                    InverseHarmonicMeanRank(),\r\n                    # Needs simulation\r\n                    InverseArithmeticMeanRank(),\r\n                    HarmonicMeanRank(),\r\n                    GeometricMeanRank(),\r\n                    InverseGeometricMeanRank(),\r\n                    MedianRank(),\r\n                    InverseMedianRank(),\r\n                ]\r\n                this_metrics: MutableMapping[ExtendedTarget, Mapping[str, float]] = dict()\r\n                for label, sides in SIDE_MAPPING.items():\r\n                    num_candidates = df[[f\"{side}_candidates\" for side in sides]].values.ravel()\r\n                    this_metrics[label] = {\r\n                        metric.key: metric.expected_value(\r\n                            num_candidates=num_candidates,\r\n                            num_samples=samples,\r\n                        )\r\n                        for metric in metrics\r\n                    }\r\n                expected_metrics_dict[key] = this_metrics\r\n            with expected_metrics_path.open(\"w\") as file:\r\n                json.dump(expected_metrics_dict, file, sort_keys=True, indent=4)\r\n            tqdm.write(f\"wrote {expected_metrics_path}\")\r\n\r\n        df_data.extend(\r\n            (dataset_name, metric, side, part, value)\r\n            for part, level1 in expected_metrics_dict.items()\r\n            for side, level2 in level1.items()\r\n            for metric, value in level2.items()\r\n        )\r\n    df = (\r\n        pd.DataFrame(df_data, columns=[\"dataset\", \"metric\", \"side\", \"split\", \"value\"])\r\n        .sort_values(\r\n            by=[\"dataset\", \"metric\", \"side\", \"split\"],\r\n        )\r\n        .reset_index(drop=True)\r\n    )\r\n    results_path = output_directory.joinpath(\"metric_adjustments.tsv.gz\")\r\n    df.to_csv(results_path, sep=\"\\t\", index=False)\r\n    click.secho(f\"wrote to {results_path}\", fg=\"green\")\r\n\r\n    if max_triples is None and min_triples is None and dataset_regex is None:\r\n        try:\r\n            from zenodo_client import update_zenodo\r\n        except ImportError:\r\n            click.secho(\"Unable to import `zenodo_client`. Not uploading results\", fg=\"red\")\r\n        else:\r\n            zenodo_record = \"6331629\"\r\n            # See https://zenodo.org/record/6331629\r\n            rv = update_zenodo(zenodo_record, results_path)\r\n            click.secho(f\"Updated Zenodo record {zenodo_record}: {rv}\", fg=\"green\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def degree(\r\n    dataset_regex: Optional[str],\r\n    min_triples: Optional[int],\r\n    max_triples: Optional[int],\r\n    restrict_split: Optional[str],\r\n    force: bool,\r\n    plot: bool,\r\n    output_root: pathlib.Path,\r\n):\r\n    \"\"\"Analyze degree distributions.\"\"\"\r\n    output_root.mkdir(exist_ok=True, parents=True)\r\n    base_path = output_root.joinpath(\"degree-distributions\")\r\n    path = base_path.with_suffix(suffix=\".tsv.gz\")\r\n    if path.is_file() and not force:\r\n        df = pd.read_csv(path, sep=\"\\t\")\r\n        logger.info(f\"Loaded degree statistics from {path}\")\r\n    else:\r\n        with logging_redirect_tqdm():\r\n            rows = [\r\n                (name, split, factory.num_triples, *row)\r\n                for name, dataset in iter_dataset_instances(\r\n                    regex_name_filter=dataset_regex, min_triples=min_triples, max_triples=max_triples\r\n                )\r\n                for split, factory in dataset.factory_dict.items()\r\n                if (restrict_split is None or split == restrict_split)\r\n                for row in _summarize_degree_distribution(factory=factory)\r\n            ]\r\n        df = pd.DataFrame(\r\n            data=rows,\r\n            columns=[\r\n                \"dataset\",\r\n                \"split\",\r\n                \"num_triples\",\r\n                \"target\",\r\n                \"nobs\",\r\n                \"minmax\",\r\n                \"mean\",\r\n                \"variance\",\r\n                \"skewness\",\r\n                \"kurtosis\",\r\n            ],\r\n        )\r\n        # only save full data\r\n        if dataset_regex is None and min_triples is None and max_triples is None and restrict_split is None:\r\n            df.to_csv(path, sep=\"\\t\", index=False)\r\n            logger.info(f\"Written degree statistics to {path}\")\r\n    if not plot:\r\n        return\r\n    plt, sns = _get_plotting_libraries()\r\n\r\n    # Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)\r\n    df = df.melt(\r\n        id_vars=[\"dataset\", \"split\", \"num_triples\", \"target\"],\r\n        value_vars=[\"mean\", \"variance\", \"skewness\", \"kurtosis\"],\r\n        var_name=\"statistic\",\r\n    )\r\n    grid_1: sns.FacetGrid = sns.relplot(  # type: ignore\r\n        data=df,\r\n        hue=\"dataset\",\r\n        x=\"num_triples\",\r\n        style=None if restrict_split is not None else \"split\",\r\n        col=\"statistic\",\r\n        row=\"target\",\r\n        y=\"value\",\r\n        facet_kws=dict(\r\n            margin_titles=True,\r\n            sharey=\"col\",\r\n        ),\r\n        height=2.5,\r\n        hue_order=sorted(df[\"dataset\"].unique()),\r\n    )\r\n    grid_1.fig.suptitle(\"Dataset Degree Distributions\", x=0.4, y=0.98)\r\n    plt.subplots_adjust(top=0.85)\r\n    sns.move_legend(\r\n        grid_1,\r\n        \"lower center\",\r\n        bbox_to_anchor=(0.45, -0.35),\r\n        ncol=6,\r\n        title=None,\r\n        frameon=False,\r\n    )\r\n    grid_1.tight_layout()\r\n    grid_1.set(xscale=\"log\", yscale=\"log\", xlabel=\"Triples\")\r\n    path = base_path.with_suffix(suffix=\".pdf\")\r\n    grid_1.savefig(path)\r\n    grid_1.savefig(IMG_DIR.joinpath(\"dataset_degree_distributions.svg\"))\r\n    logger.info(f\"Saved plot to {path}\")\r\n\r\n    # Plot: difference between mean head and tail degree\r\n    df_2 = df.loc[df[\"statistic\"] == \"mean\"].pivot(\r\n        index=[\"dataset\", \"split\", \"num_triples\"], columns=\"target\", values=\"value\"\r\n    )\r\n    df_2[\"difference\"] = df_2[\"head\"] - df_2[\"tail\"]\r\n    grid_2: sns.FacetGrid = sns.relplot(  # type: ignore\r\n        data=df_2,\r\n        hue=\"dataset\",\r\n        x=\"num_triples\",\r\n        style=None if restrict_split is not None else \"split\",\r\n        y=\"difference\",\r\n        height=2.5,\r\n        aspect=4,\r\n        hue_order=sorted(df[\"dataset\"].unique()),\r\n    )\r\n    grid_2.fig.suptitle(\"Dataset Mean Degree Imbalance\", x=0.4, y=0.98)\r\n    sns.move_legend(\r\n        grid_2,\r\n        \"lower center\",\r\n        bbox_to_anchor=(0.45, -0.55),\r\n        ncol=6,\r\n        title=None,\r\n        frameon=False,\r\n    )\r\n    grid_2.tight_layout()\r\n    grid_2.set(xscale=\"log\", yscale=\"symlog\", xlabel=\"Triples\")\r\n    path = base_path.with_name(\"degree-imbalance\").with_suffix(suffix=\".pdf\")\r\n    grid_2.savefig(path)\r\n    grid_2.savefig(IMG_DIR.joinpath(\"degree_imbalance.svg\"))\r\n    logger.info(f\"Saved plot to {path}\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def main(force: bool, clip: int, mode):\r\n    \"\"\"Run the inverse stability experiments.\"\"\"\r\n    results_path = INVERSE_STABILITY / \"results.tsv\"\r\n    if results_path.exists() and not force:\r\n        df = pd.read_csv(results_path, sep=\"\\t\")\r\n        df[\"residuals\"] = df[\"forward\"] - df[\"inverse\"]\r\n        df = df[(-clip < df[\"residuals\"]) & (df[\"residuals\"] < clip)]\r\n        g = sns.FacetGrid(df, col=\"model\", row=\"dataset\", hue=\"training_loop\", sharex=False, sharey=False)\r\n        g.map_dataframe(sns.histplot, x=\"residuals\", stat=\"density\")\r\n        g.add_legend()\r\n        g.savefig(INVERSE_STABILITY / \"results_residuals.png\", dpi=300)\r\n\r\n    else:\r\n        outer_dfs = []\r\n        datasets = [\"nations\", \"kinships\"]\r\n        models = [\"rotate\", \"complex\", \"simple\", \"transe\", \"distmult\"]\r\n        training_loops = [\"lcwa\", \"slcwa\"]\r\n        for dataset, model, training_loop in itt.product(datasets, models, training_loops):\r\n            click.secho(f\"{dataset} {model} {training_loop}\", fg=\"cyan\")\r\n            df = run_inverse_stability_workflow(dataset=dataset, model=model, training_loop=training_loop, mode=mode)\r\n            outer_dfs.append(df)\r\n        outer_df = pd.concat(outer_dfs)\r\n        outer_df.to_csv(INVERSE_STABILITY / \"results.tsv\", sep=\"\\t\", index=False)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def save_pipeline_results_to_directory(\r\n    *,\r\n    config: Mapping[str, Any],\r\n    directory: str | pathlib.Path,\r\n    pipeline_results: Iterable[PipelineResult],\r\n    move_to_cpu: bool = False,\r\n    save_metadata: bool = False,\r\n    save_replicates: bool = True,\r\n    save_training: bool = False,\r\n    width: int = 5,\r\n) -> None:\r\n    \"\"\"Save the result set to the directory.\r\n\r\n    :param config: The configuration.\r\n    :param directory: The directory in which the replicates will be saved\r\n    :param pipeline_results: An iterable over results from training and evaluation\r\n    :param move_to_cpu: Should the model be moved back to the CPU? Only relevant if training on GPU.\r\n    :param save_metadata: Should the metadata be saved? Might be redundant in a scenario when you're\r\n        using this function, so defaults to false.\r\n    :param save_replicates: Should the artifacts of the replicates be saved?\r\n    :param save_training: Should the training triples be saved?\r\n    :param width: How many leading zeros should be put in the replicate names?\r\n    \"\"\"\r\n    directory = normalize_path(directory)\r\n    replicates_directory = directory.joinpath(\"replicates\")\r\n    losses_rows = []\r\n\r\n    if move_to_cpu:\r\n        pipeline_results = _iterate_moved(pipeline_results)\r\n\r\n    # metrics accumulates rows for a dataframe for comparison against the original reported results (if any)\r\n    result_comparator = _ResultAccumulator()\r\n    # TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)\r\n    result_comparator.add_original_result(result=config.get(\"results\", {}))\r\n    for i, pipeline_result in enumerate(pipeline_results):\r\n        replicate_directory = replicates_directory.joinpath(f\"replicate-{i:0{width}}\")\r\n        replicate_directory.mkdir(exist_ok=True, parents=True)\r\n        pipeline_result.save_to_directory(\r\n            replicate_directory,\r\n            save_metadata=save_metadata,\r\n            save_replicates=save_replicates,\r\n            save_training=save_training,\r\n        )\r\n        for epoch, loss in enumerate(pipeline_result.losses):\r\n            losses_rows.append((i, epoch, loss))\r\n        result_comparator.parse_from_result(result=pipeline_result)\r\n\r\n    losses_df = pd.DataFrame(losses_rows, columns=[\"Replicate\", \"Epoch\", \"Loss\"])\r\n    losses_df.to_csv(directory.joinpath(\"all_replicates_losses.tsv\"), sep=\"\\t\", index=False)\r\n\r\n    if result_comparator.is_non_empty():\r\n        metric_df = result_comparator.get_df()\r\n        metric_df.to_csv(directory.joinpath(\"all_replicates_metrics.tsv\"), sep=\"\\t\", index=False)\r\n        logger.debug(f\"metric results: {metric_df}\")\r\n\r\n        compare_df = compare_results(metric_df)\r\n        compare_df.to_csv(directory.joinpath(\"comparison.tsv\"), sep=\"\\t\", index=False)\r\n        # summarize\r\n        logger.info(compare_df.to_string())",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _pre_instantiation_hook(self, kwargs: MutableMapping[str, Any]) -> MutableMapping[str, Any]:\r\n        kwargs = super()._pre_instantiation_hook(kwargs)\r\n        generator = torch.manual_seed(seed=42)\r\n        target = kwargs[\"target\"]\r\n        self.df = kwargs[\"df\"] = pandas.DataFrame(\r\n            data={\r\n                f\"{target}_id\": range(self.dataset.num_entities),\r\n                \"score\": torch.rand(size=(self.dataset.num_entities,), generator=generator),\r\n            }\r\n        )\r\n        return kwargs",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _pre_instantiation_hook(self, kwargs: MutableMapping[str, Any]) -> MutableMapping[str, Any]:\r\n        kwargs = super()._pre_instantiation_hook(kwargs)\r\n        # mock prediction data frame\r\n        generator = torch.manual_seed(seed=42)\r\n        data = {\r\n            f\"{label}_id\": torch.randint(max_id, size=(5,), generator=generator).numpy()\r\n            for label, max_id in zip(\r\n                COLUMN_LABELS, [self.dataset.num_entities, self.dataset.num_relations, self.dataset.num_entities]\r\n            )\r\n        }\r\n        data[\"score\"] = torch.rand(size=(5,), generator=generator).numpy()\r\n        self.df = kwargs[\"df\"] = pandas.DataFrame(data=data)\r\n        return kwargs",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def benchmark_process_group(pg, benchmark, use_ddp_for_single_rank=True):\r\n    torch.manual_seed(pg.rank())\r\n    torch.cuda.manual_seed(pg.rank())\r\n\r\n    model = benchmark.create_model()\r\n    data = [(benchmark.generate_inputs(), benchmark.generate_target())]\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = optim.SGD(model.parameters(), 0.001, momentum=0.9, weight_decay=1e-4)\r\n    if use_ddp_for_single_rank or pg.size() > 1:\r\n        model = torch.nn.parallel.DistributedDataParallel(\r\n            model,\r\n            device_ids=[torch.cuda.current_device()],\r\n            broadcast_buffers=False,\r\n            process_group=pg,\r\n            bucket_cap_mb=benchmark.bucket_size,\r\n        )\r\n\r\n    measurements = []\r\n    warmup_iterations = 5\r\n    measured_iterations = 10\r\n    for inputs, target in data * (warmup_iterations + measured_iterations):\r\n        start = time.time()\r\n        output = model(*inputs)\r\n        loss = criterion(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        torch.cuda.synchronize()\r\n        measurements.append(time.time() - start)\r\n\r\n    # Throw away measurements for warmup iterations\r\n    return measurements[warmup_iterations:]",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(runner, args, original_dir=None):\r\n    # Pass the parsed args object to benchmark runner object\r\n    runner.args = args\r\n\r\n    args.filter = args.filter or [r\".\"]\r\n    args.exclude = args.exclude or [r\"^$\"]\r\n    args.exclude_exact = args.exclude_exact or []\r\n\r\n    if args.inductor:\r\n        assert args.backend is None\r\n        args.backend = \"inductor\"\r\n    if args.quantization:\r\n        assert args.backend is None\r\n        args.backend = \"torchao\"\r\n    if args.dynamic_batch_only:\r\n        args.dynamic_shapes = True\r\n        torch._dynamo.config.assume_static_by_default = True\r\n    if args.dynamic_shapes:\r\n        if not args.dynamic_batch_only:\r\n            torch._dynamo.config.assume_static_by_default = False\r\n    if args.propagate_real_tensors:\r\n        # TODO: Separate flag for data dependent\r\n        torch._dynamo.config.capture_scalar_outputs = True\r\n        torch._dynamo.config.capture_dynamic_output_shape_ops = True\r\n        torch._functorch.config.fake_tensor_propagate_real_tensors = True\r\n    if args.specialize_int:\r\n        torch._dynamo.config.specialize_int = True\r\n    if args.ci:\r\n        if args.accuracy:\r\n            # Run fewer iterations when checking accuracy\r\n            args.repeat = min(args.repeat, 2)\r\n\r\n            # Set translation validation on by default on CI accuracy runs.\r\n            torch.fx.experimental._config.translation_validation = True\r\n\r\n    if args.ddp:\r\n        assert args.training, \"DDP benchmark requires --training mode\"\r\n        torch._dynamo.config.optimize_ddp = args.optimize_ddp_mode\r\n        if args.only == \"dlrm\":\r\n            log.error(\r\n                \"DLRM+DDP is unsupported as it requires sharding the embedding layer separately from DDP\"\r\n            )\r\n            return sys.exit(-1)\r\n    if args.accuracy:\r\n        # Use small batch size. We use >1 batch size to ensure we test\r\n        # batch_norm type of operators that work on batch dims.\r\n        # TODO - Go through the failures for batch size = 2\r\n        if args.batch_size is None:\r\n            if runner.suite_name == \"huggingface\":\r\n                args.batch_size = 1\r\n            elif runner.suite_name == \"torchbench\":\r\n                args.batch_size = 4\r\n            else:\r\n                # Larger batch size of TIMM models to have stable batch_norm\r\n                assert runner.suite_name == \"timm_models\"\r\n                args.batch_size = 8\r\n\r\n        # Remove sources of randomness\r\n        if runner.suite_name not in (\"timm_models\", \"huggingface\"):\r\n            # TODO - Using train mode for timm_models and HF models. Move to train mode for Torchbench as well.\r\n            args.use_eval_mode = True\r\n        inductor_config.fallback_random = True\r\n        if args.only is not None and args.only not in {\r\n            \"alexnet\",\r\n            \"Background_Matting\",\r\n            \"pytorch_CycleGAN_and_pix2pix\",\r\n            \"pytorch_unet\",\r\n            \"Super_SloMo\",\r\n            \"vgg16\",\r\n            # https://github.com/pytorch/pytorch/issues/96724\r\n            \"Wav2Vec2ForCTC\",\r\n            \"Wav2Vec2ForPreTraining\",\r\n            \"sam\",\r\n            \"sam_fast\",\r\n            \"resnet50_quantized_qat\",\r\n            \"mobilenet_v2_quantized_qat\",\r\n        }:\r\n            # some of the models do not support use_deterministic_algorithms\r\n            torch.use_deterministic_algorithms(True)\r\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\r\n        torch.backends.cudnn.deterministic = True\r\n        torch.backends.cudnn.allow_tf32 = False\r\n        torch.backends.cudnn.benchmark = False\r\n        torch.backends.cuda.matmul.allow_tf32 = False\r\n\r\n        torch.backends.mkldnn.deterministic = True\r\n\r\n        # Remove randomeness when torch manual seed is called\r\n        patch_torch_manual_seed()\r\n\r\n        # Some models e.g. yolov3 assert batch size on n_gpus\r\n        if \"CUDA_VISIBLE_DEVICES\" not in os.environ and not args.multiprocess:\r\n            args.device_index = \"0\"\r\n\r\n        # Stricter check to disable fallbacks\r\n        args.suppress_errors = False\r\n\r\n    if args.device_index is not None:\r\n        if args.multiprocess:\r\n            print(\"Cannot specify both --device_index and --multiprocess\")\r\n            return sys.exit(-1)\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_index\r\n\r\n    elif args.performance:\r\n        # Ensure that we test on real scenarios\r\n        args.use_eval_mode = False\r\n\r\n    if args.partition_id > args.total_partitions or args.partition_id < 0:\r\n        print(\"Invalid partition id\")\r\n        return sys.exit(-1)\r\n\r\n    if not args.devices:\r\n        if torch.cuda.is_available():\r\n            args.devices = [\"cuda\"]\r\n        else:\r\n            log.warning(\"torch.cuda.is_available() == False, using CPU\")\r\n            args.devices = [\"cpu\"]\r\n\r\n    if args.devices != [\"cpu\"] and (HAS_CUDA or HAS_XPU):\r\n        global synchronize\r\n        synchronize = torch.cuda.synchronize if HAS_CUDA else torch.xpu.synchronize\r\n\r\n    if (\r\n        args.devices == [\"cuda\"]\r\n        and torch.cuda.get_device_properties(0).total_memory < 25 * 2**30\r\n    ):\r\n        # OOM errors on an RTX 3090 with 24gb RAM\r\n        runner.skip_models.update(\r\n            {\r\n                # torchbench\r\n                \"hf_Longformer\",\r\n                \"timm_nfnet\",\r\n                \"timm_efficientdet\",\r\n            }\r\n        )\r\n        if args.training:\r\n            runner.skip_models.add(\"hf_T5\")\r\n\r\n    if args.nnc:\r\n        torch._C._jit_override_can_fuse_on_cpu(True)\r\n        torch._C._jit_override_can_fuse_on_gpu(True)\r\n        torch._C._jit_set_texpr_fuser_enabled(True)\r\n        torch._C._jit_set_nvfuser_enabled(False)\r\n\r\n    if args.threads:\r\n        torch.set_num_threads(args.threads)\r\n\r\n    if args.verbose:\r\n        torch._logging.set_logs(dynamo=logging.DEBUG)\r\n\r\n    if args.print_graph_breaks:\r\n        torch._logging.set_logs(graph_breaks=True)\r\n\r\n    if args.quiet:\r\n        torch._logging.set_logs(dynamo=logging.ERROR)\r\n\r\n    torch._dynamo.config.suppress_errors = args.suppress_errors\r\n\r\n    if args.training:\r\n        runner.model_iter_fn = runner.forward_and_backward_pass\r\n        runner.skip_models.update(runner.skip_not_suitable_for_training_models)\r\n    else:\r\n        runner.model_iter_fn = runner.forward_pass\r\n\r\n    if args.fast:\r\n        runner.skip_models.update(runner.slow_models)\r\n\r\n    if args.devices == [\"cpu\"]:\r\n        runner.skip_models.update(runner.skip_models_for_cpu)\r\n    elif args.devices == [\"cuda\"]:\r\n        runner.skip_models.update(runner.skip_models_for_cuda)\r\n\r\n    if not args.multiprocess:\r\n        runner.skip_models.update(runner.skip_multiprocess_models)\r\n\r\n    if args.freezing:\r\n        if args.devices == [\"cpu\"]:\r\n            runner.skip_models.update(runner.skip_models_for_freezing_cpu)\r\n        elif args.devices == [\"cuda\"]:\r\n            runner.skip_models.update(runner.skip_models_for_freezing_cuda)\r\n\r\n    if args.no_skip:\r\n        runner.skip_models.clear()\r\n\r\n    experiment = null_experiment\r\n    global \\\r\n        current_name, \\\r\n        current_device, \\\r\n        current_batch_size, \\\r\n        current_backend, \\\r\n        current_mode, \\\r\n        current_dtype, \\\r\n        current_quantization, \\\r\n        current_settings, \\\r\n        output_filename, \\\r\n        disable_output, \\\r\n        optimize_ctx, \\\r\n        current_onnx_compiler\r\n    optimize_ctx = contextlib.nullcontext()\r\n\r\n    if args.disable_output:\r\n        disable_output = True\r\n\r\n    if args.overhead:\r\n        optimize_ctx = torch._dynamo.optimize(dummy_fx_compile, nopython=args.nopython)\r\n        experiment = speedup_experiment\r\n        output_filename = \"overheads.csv\"\r\n    elif args.inductor:\r\n        inductor_config.debug = args.verbose\r\n        if args.threads:\r\n            inductor_config.cpp.threads = args.threads\r\n\r\n        optimize_ctx = functools.partial(\r\n            torch.compile,\r\n            backend=\"inductor\",\r\n            fullgraph=args.nopython,\r\n            mode=args.inductor_compile_mode,\r\n        )\r\n        experiment = speedup_experiment\r\n        output_filename = \"inductor.csv\"\r\n    elif args.export:\r\n        optimize_ctx = export\r\n        experiment = speedup_experiment\r\n        output_filename = \"export.csv\"\r\n    elif args.xla:\r\n        (dev,) = args.devices\r\n        os.environ[\"PJRT_DEVICE\"] = {\"cuda\": \"GPU\", \"cpu\": \"CPU\"}[dev]\r\n        torch._dynamo.mark_dynamic = MagicMock()\r\n        experiment = xla\r\n        output_filename = \"xla.csv\"\r\n    elif args.torchscript_onnx:\r\n        optimize_ctx = functools.partial(\r\n            optimize_onnx_ctx,\r\n            args.output_directory or \".\",\r\n            OnnxModelFromTorchScript,\r\n            copy_before_export=args.performance,  # Accuarcy bench already did deepcopy\r\n        )\r\n        experiment = speedup_experiment_onnx\r\n        output_filename = \"torchscript_onnx.csv\"\r\n        current_onnx_compiler = \"torchscript\"\r\n    elif args.torch_onnx_patch:\r\n        optimize_ctx = functools.partial(\r\n            optimize_onnx_ctx,\r\n            args.output_directory or \".\",\r\n            OnnxModelFromTorchScript,\r\n            copy_before_export=args.performance,\r\n            use_experimental_patch=True,\r\n        )\r\n        experiment = speedup_experiment_onnx\r\n        output_filename = \"torch_onnx_patch.csv\"\r\n        current_onnx_compiler = \"dynamo\"\r\n    elif args.dynamo_onnx:\r\n        optimize_ctx = functools.partial(\r\n            optimize_onnx_ctx,\r\n            args.output_directory or \".\",\r\n            OnnxModelFromDynamo,\r\n            dynamic_shapes=args.dynamic_shapes,\r\n            copy_before_export=args.performance,\r\n        )\r\n        experiment = speedup_experiment_onnx\r\n        output_filename = \"dynamo_onnx.csv\"\r\n        current_onnx_compiler = \"dynamo\"\r\n    elif args.dynamo_onnx_aot_inline:\r\n        optimize_ctx = functools.partial(\r\n            optimize_onnx_ctx,\r\n            args.output_directory or \".\",\r\n            OnnxModelFromDynamoAotInline,\r\n            dynamic_shapes=args.dynamic_shapes,\r\n            copy_before_export=args.performance,\r\n        )\r\n        experiment = speedup_experiment_onnx\r\n        output_filename = \"dynamo_onnx_aot_inline.csv\"\r\n        current_onnx_compiler = \"dynamo\"\r\n    elif args.dynamo_onnx_aot_optimize:\r\n        optimize_ctx = functools.partial(\r\n            optimize_onnx_ctx,\r\n            args.output_directory or \".\",\r\n            OnnxModelFromDynamoAotOptimize,\r\n            dynamic_shapes=args.dynamic_shapes,\r\n            copy_before_export=args.performance,\r\n        )\r\n        experiment = speedup_experiment_onnx\r\n        output_filename = \"dynamo_onnx_aot_optimize.csv\"\r\n        current_onnx_compiler = \"dynamo\"\r\n    elif args.speedup_dynamo_ts:\r\n        optimize_ctx = torch._dynamo.optimize(\"ts\", nopython=args.nopython)\r\n        experiment = speedup_experiment\r\n        output_filename = \"speedup_dynamo_ts.csv\"\r\n    elif args.prims_nvfuser:\r\n        optimize_ctx = torch._dynamo.optimize(\"prims_nvfuser\", nopython=args.nopython)\r\n        experiment = speedup_experiment\r\n        backend_str = \"prims_nvfuser\"\r\n        output_filename = f\"accuracy_aot_{backend_str}.csv\"\r\n    elif args.print_fx:\r\n        optimize_ctx = torch._dynamo.optimize(\r\n            print_fx,\r\n            nopython=args.nopython,\r\n        )\r\n    elif args.print_aten_ops:\r\n        optimize_ctx = torch._dynamo.optimize(\r\n            print_aten_ops,\r\n            nopython=args.nopython,\r\n        )\r\n    elif args.nothing:\r\n        optimize_ctx = nothing\r\n        experiment = speedup_experiment\r\n        output_filename = \"nothing.csv\"\r\n    elif args.backend or args.export_aot_inductor:\r\n        if args.export_aot_inductor:\r\n            assert not args.training, \"AOTInductor only supports inference\"\r\n            optimize_ctx = functools.partial(export_aot_inductor)\r\n\r\n            # AOTInductor doesn't support control flow yet\r\n            runner.skip_models.update(runner.skip_models_due_to_control_flow)\r\n        elif args.backend == \"torchao\":\r\n            assert \"cuda\" in args.devices, \"Quantization requires CUDA device.\"\r\n            assert args.bfloat16, \"Quantization requires dtype bfloat16.\"\r\n            try:\r\n                from torchao_backend import setup_baseline, torchao_optimize_ctx\r\n            except ImportError:\r\n                try:\r\n                    from .torchao_backend import setup_baseline, torchao_optimize_ctx\r\n                except ImportError:\r\n                    from userbenchmark.dynamo.dynamobench.torchao_backend import (\r\n                        setup_baseline,\r\n                        torchao_optimize_ctx,\r\n                    )\r\n\r\n            setup_baseline()\r\n            baseline_ctx = functools.partial(\r\n                torch.compile,\r\n                backend=\"inductor\",\r\n                fullgraph=args.nopython,\r\n                mode=args.inductor_compile_mode,\r\n            )\r\n            model_iter_fn = baseline_ctx(runner.model_iter_fn)\r\n\r\n            # needed to avoid error that causes inconsistent timing due to:\r\n            # Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards\r\n            def model_iter_fn_and_mark_step(*args, **kwargs):\r\n                torch.compiler.cudagraph_mark_step_begin()\r\n                model_iter_fn(*args, **kwargs)\r\n\r\n            runner.model_iter_fn = model_iter_fn_and_mark_step\r\n            optimize_ctx = torchao_optimize_ctx(args.quantization)\r\n        else:\r\n            optimize_ctx = torch._dynamo.optimize(args.backend, nopython=args.nopython)\r\n        experiment = (\r\n            speedup_experiment if not args.backend == \"torchao\" else latency_experiment\r\n        )\r\n        if args.accuracy:\r\n            output_filename = f\"accuracy_{args.backend}.csv\"\r\n        elif args.tolerance:\r\n            output_filename = f\"tolerance_{args.backend}.csv\"\r\n        else:\r\n            output_filename = f\"speedup_{args.backend}.csv\"\r\n    elif args.recompile_profiler:\r\n        output_filename = \"recompile_profiler_log.csv\"\r\n        experiment = recompile_profiler_experiment\r\n    else:\r\n        optimize_ctx = torch._dynamo.optimize(\r\n            fx_insert_profiling, nopython=args.nopython\r\n        )\r\n        experiment = coverage_experiment\r\n        output_filename = \"coverage.csv\"\r\n\r\n    if args.inductor or args.backend == \"inductor\" or args.export_aot_inductor:\r\n        inductor_config.triton.cudagraphs = not args.disable_cudagraphs\r\n        inductor_config.triton.persistent_reductions = (\r\n            not args.disable_persistent_reductions\r\n        )\r\n        inductor_config.split_reductions = not args.disable_split_reductions\r\n        inductor_config.triton.divisible_by_16 = not args.disable_divisible_by_16\r\n        if args.inference:\r\n            inductor_config.freezing = args.freezing\r\n        if args.inductor_config:\r\n            for config in args.inductor_config:\r\n                key, value = config.split(\"=\")\r\n                typ = type(inductor_config.__getattr__(key))\r\n                if issubclass(typ, bool):\r\n                    assert value in (\"0\", \"1\", \"True\", \"False\")\r\n                    value = value in (\"1\", \"True\")\r\n                elif issubclass(typ, (str, int, float)):\r\n                    value = typ(value)\r\n                else:\r\n                    raise NotImplementedError(typ)\r\n                inductor_config.__setattr__(key, value)\r\n\r\n    runner.setup_amp()\r\n\r\n    if args.output:\r\n        output_filename = args.output\r\n\r\n    if output_filename:\r\n        if args.output_directory:\r\n            output_filename = os.path.join(args.output_directory, output_filename)\r\n        else:\r\n            output_filename = os.path.join(\r\n                torch._dynamo.config.base_dir, output_filename\r\n            )\r\n\r\n    if args.find_batch_sizes and args.only:\r\n        for device in args.devices:\r\n            batch_size = runner.batch_size_finder(device, args.only)\r\n            print(args.only, batch_size)\r\n            write_outputs(output_filename, [], [args.only, batch_size])\r\n        return\r\n\r\n    args.profile_details = {}\r\n    if args.export_profiler_trace:\r\n        if args.profile_details:\r\n            args.profile_details = {\r\n                \"record_shapes\": True,\r\n                \"profile_memory\": True,\r\n                \"with_stack\": True,\r\n                \"with_modules\": True,\r\n            }\r\n\r\n        if args.profiler_trace_name is None:\r\n            if args.backend:\r\n                args.profiler_trace_name = args.backend\r\n            elif args.inductor:\r\n                args.profiler_trace_name = \"inductor\"\r\n            else:\r\n                args.profiler_trace_name = \"profile\"\r\n        else:\r\n            args.profiler_trace_name = args.profiler_trace_name\r\n\r\n    if args.no_translation_validation:\r\n        # Overwrite 'translation_validation' config, if specified.\r\n        torch.fx.experimental._config.translation_validation = False\r\n\r\n    experiment = functools.partial(experiment, args, runner.model_iter_fn)\r\n\r\n    if args.only and should_diff_branch(args):\r\n        import git\r\n\r\n        repo = git.Repo()\r\n        main_branch = repo.active_branch.name\r\n        try:\r\n            # Adding diff-branch again to the args will override previous value\r\n            call_args = (\r\n                [sys.executable] + sys.argv + [f\"--diff-branch={diff_branch_default}\"]\r\n            )\r\n            # Run for main branch\r\n            subprocess.check_call(call_args + [f\"--tag={main_branch}\"])\r\n            # Run for comparison branch\r\n            repo.git.checkout(args.diff_branch)\r\n            subprocess.check_call(call_args + [f\"--tag={args.diff_branch}\"])\r\n        finally:\r\n            # Go back to main branch\r\n            repo.git.checkout(main_branch)\r\n    elif args.only:\r\n        model_name = args.only\r\n        for device in args.devices:\r\n            batch_size = args.batch_size\r\n            if args.batch_size_file:\r\n                batch_size = read_batch_size_from_file(\r\n                    args, args.batch_size_file, model_name\r\n                )\r\n            if model_specified_by_path(args.only):\r\n                model, example_inputs = load_model_from_path(args.only)\r\n                name = model.__class__.__name__\r\n                model = model.to(device=device)\r\n                example_inputs = tree_map_only(\r\n                    torch.Tensor, lambda x: x.to(device=device), example_inputs\r\n                )\r\n            else:\r\n                name = model_name\r\n                try:\r\n                    with tqdm(desc=\"loading model\"):\r\n                        extra_args = []\r\n                        if hasattr(args, \"rank\") and hasattr(args, \"world_size\"):\r\n                            extra_args += [\r\n                                \"--rank\",\r\n                                str(args.rank),\r\n                                \"--world_size\",\r\n                                str(args.world_size),\r\n                            ]\r\n\r\n                        if args.part:\r\n                            (\r\n                                device,\r\n                                name,\r\n                                model,\r\n                                example_inputs,\r\n                                batch_size,\r\n                            ) = runner.load_model(\r\n                                device,\r\n                                model_name,\r\n                                batch_size=batch_size,\r\n                                part=args.part,\r\n                                extra_args=extra_args,\r\n                            )\r\n                        else:\r\n                            if args.fsdp:\r\n                                # Always load model on cpu for fsdp\r\n                                # When initializing FSDP, we will use the cuda device if args.cuda is set\r\n                                (\r\n                                    _,\r\n                                    name,\r\n                                    model,\r\n                                    example_inputs,\r\n                                    batch_size,\r\n                                ) = runner.load_model(\r\n                                    \"cpu\",\r\n                                    model_name,\r\n                                    batch_size=batch_size,\r\n                                    extra_args=extra_args,\r\n                                )\r\n                            else:\r\n                                (\r\n                                    device,\r\n                                    name,\r\n                                    model,\r\n                                    example_inputs,\r\n                                    batch_size,\r\n                                ) = runner.load_model(\r\n                                    device,\r\n                                    model_name,\r\n                                    batch_size=batch_size,\r\n                                    extra_args=extra_args,\r\n                                )\r\n                except Exception as e:\r\n                    import traceback\r\n\r\n                    mode = \"train\" if args.training else \"eval\"\r\n                    print(f\"{device:4} {mode:5} {name:34} \")\r\n                    print(traceback.format_exc())\r\n                    status = (\r\n                        \"model_fail_to_load\"\r\n                        if isinstance(e, NotImplementedError)\r\n                        else \"eager_fail_to_run\"\r\n                    )\r\n                    write_csv_when_exception(args, name, status, device)\r\n                    # NB: current_name/current_device not set, so pass\r\n                    # explicitly\r\n                    output_signpost(\r\n                        {\"name\": name, \"dev\": device},\r\n                        args,\r\n                        runner.suite_name,\r\n                        error=status,\r\n                    )\r\n                    continue  # bad benchmark implementation\r\n\r\n            if args.trace_on_xla:\r\n                xla_dev = xm.xla_device()\r\n                model = model.to(device=xla_dev)\r\n                example_inputs = tree_map_only(\r\n                    torch.Tensor, lambda x: x.to(device=xla_dev), example_inputs\r\n                )\r\n\r\n            current_name = name\r\n            current_device = device\r\n            current_batch_size = batch_size\r\n            current_backend = args.backend\r\n            current_mode = (\r\n                \"training\" if args.training else \"inference\" if args.inference else \"\"\r\n            )\r\n            if args.float16:\r\n                current_dtype = \"float16\"\r\n            elif args.bfloat16:\r\n                current_dtype = \"bfloat16\"\r\n            elif args.float32:\r\n                current_dtype = \"float32\"\r\n            elif args.amp:\r\n                current_dtype = \"amp\"\r\n            else:\r\n                current_dtype = \"\"\r\n            current_quantization = args.quantization\r\n            # Keep the remaining of the settings\r\n            current_settings = vars(args)\r\n            set_model_name(name)\r\n\r\n            # Look for stuff that looks like batch size, and mark it dynamic.\r\n            # Better integration would integrate directly with benchmark suite\r\n            # but cannot conveniently do this\r\n            # NB: This must be done late enough so that we don't do more\r\n            # conversions on the inputs\r\n            # NB: Assumes only the first batch-y like dimension is the batch\r\n            marked = False\r\n\r\n            def detect_and_mark_batch(t):\r\n                nonlocal marked\r\n                for i, s in enumerate(t.size()):\r\n                    if s == batch_size:\r\n                        torch._dynamo.mark_dynamic(t, i)\r\n                        marked = True\r\n                        break\r\n\r\n            if (\r\n                args.dynamic_batch_only\r\n                and batch_size > 1\r\n                and model_name not in CI_SKIP_DYNAMIC_BATCH_ONLY\r\n            ):\r\n                tree_map_only(torch.Tensor, detect_and_mark_batch, example_inputs)\r\n                assert marked, f\"nothing in example_inputs had a dim with {batch_size}\"\r\n\r\n            if args.log_operator_inputs:\r\n                log_operator_inputs(\r\n                    model, example_inputs, runner.model_iter_fn, name, args\r\n                )\r\n                continue\r\n\r\n            if args.per_process_memory_fraction != 1:\r\n                torch.cuda.set_per_process_memory_fraction(\r\n                    args.per_process_memory_fraction\r\n                )\r\n            if model_name in DO_NOT_CAST_INPUTS:\r\n                model, _ = runner.cast_based_on_args(model, example_inputs)\r\n\r\n            else:\r\n                model, example_inputs = runner.cast_based_on_args(model, example_inputs)\r\n            runner.setup_amp(current_device)\r\n            guard_ctx = contextlib.nullcontext()\r\n            if name in runner.guard_on_nn_module_models:\r\n                guard_ctx = torch._dynamo.config.patch(guard_nn_modules=True)\r\n\r\n            inline_ctx = contextlib.nullcontext()\r\n            if name in runner.inline_inbuilt_nn_modules_models:\r\n                inline_ctx = torch._dynamo.config.patch(inline_inbuilt_nn_modules=True)\r\n\r\n            with guard_ctx:\r\n                with inline_ctx:\r\n                    runner.run_one_model(\r\n                        name,\r\n                        model,\r\n                        example_inputs,\r\n                        optimize_ctx,\r\n                        experiment,\r\n                        explain=args.explain,\r\n                        tag=args.tag,\r\n                    )\r\n        if args.generate_aot_autograd_stats:\r\n            stats_file = output_filename.split(\".csv\")[0] + \"_stats.csv\"\r\n            write_outputs(\r\n                stats_file,\r\n                (\"dev\", \"name\", \"batch_size\", \"total_aot_graphs\", \"ok_aot_graphs\"),\r\n                [\r\n                    current_device,\r\n                    current_name,\r\n                    current_batch_size,\r\n                    *Stats.aot_summary(),\r\n                ],\r\n            )\r\n    else:\r\n        metrics.purge_old_log_files()\r\n        if output_filename and os.path.exists(output_filename):\r\n            os.unlink(output_filename)\r\n        if original_dir:\r\n            os.chdir(original_dir)\r\n        model_names = list(runner.iter_model_names(args))\r\n        nmodels = len(model_names)\r\n        for i, name in enumerate(model_names):\r\n            current_name = name\r\n            if args.progress:\r\n                print(f\"Running model {i + 1}/{nmodels}\", flush=True)\r\n\r\n            try:\r\n                timeout = args.timeout\r\n                if should_diff_branch(args):\r\n                    timeout *= 2\r\n                env = os.environ.copy()\r\n                if args.ci and name in CI_PRESERVE_COMPILE_DEBUG:\r\n                    env[\"TORCH_COMPILE_DEBUG\"] = \"1\"\r\n                subprocess.check_call(\r\n                    [sys.executable] + sys.argv + [f\"--only={name}\"],\r\n                    timeout=timeout,\r\n                    env=env,\r\n                )\r\n            except subprocess.TimeoutExpired:\r\n                write_csv_when_exception(args, name, \"timeout\")\r\n                # NB: device is potentially multiple here, though we should\r\n                # try our best to report in anyway TODO\r\n                output_signpost(\r\n                    {\"name\": name}, args, runner.suite_name, error=\"timeout\"\r\n                )\r\n            except subprocess.CalledProcessError as e:\r\n                print(\"Run failed with return code: \", e.returncode, file=sys.stderr)\r\n                print(\"Output: \", e.output, file=sys.stderr)\r\n                print(\"Error: \", e.stderr, file=sys.stderr)\r\n        print_summary(output_filename, print_dataframe=args.print_dataframe_summary)",
        "labels": [
            "Deterministic Algorithm Option Not Used"
        ]
    },
    {
        "code": "def model_predict(\r\n        self,\r\n        model,\r\n        input_buffer,\r\n        copy_event,\r\n        compute_event,\r\n        copy_sem,\r\n        compute_sem,\r\n        response_list,\r\n        request_time,\r\n    ):\r\n        # copy_sem makes sure copy_event has been recorded in the data copying thread\r\n        copy_sem.acquire()\r\n        self.stream_map[threading.get_native_id()].wait_event(copy_event)\r\n        with torch.cuda.stream(self.stream_map[threading.get_native_id()]):\r\n            with torch.no_grad():\r\n                response_list.append(model(input_buffer))\r\n                compute_event.record()\r\n                compute_sem.release()\r\n        del input_buffer",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):\r\n    \"\"\"\r\n    Measure speedups over eager.\r\n\r\n    Writes to ./speedups.csv\r\n    \"\"\"\r\n    # if args.dynamic_shapes:\r\n    #     return speedup_experiment_ds(args, model_iter_fn, model, example_inputs)\r\n\r\n    timings = np.zeros((args.repeat, 2), np.float64)\r\n    # if we randomize the input, we should also check the result is correct\r\n    should_randomize_input = args.randomize_input\r\n\r\n    import contextlib\r\n\r\n    from torch._inductor.utils import maybe_profile\r\n\r\n    @contextlib.contextmanager\r\n    def maybe_mark_profile(*args, **kwargs):\r\n        prof: torch.profiler.profile = kwargs.pop(\"p\", None)\r\n        mark = kwargs.pop(\"mark\", None)\r\n        if prof:\r\n            with torch.profiler.record_function(mark):\r\n                yield\r\n        else:\r\n            yield\r\n\r\n    times = args.iterations_per_run\r\n\r\n    # Use higher tolerance for XLA since XLA cause numerical unstability when\r\n    # graph size changes\r\n    tolerance = args.xla_tolerance if args.trace_on_xla else 1e-4\r\n    torch._dynamo.config.repro_tolerance = tolerance\r\n\r\n    with maybe_profile(args.export_profiler_trace, **args.profile_details) as p:\r\n        if args.export_aot_inductor:\r\n            frozen_model_iter_fn = export_aot_inductor(model, example_inputs)\r\n        else:\r\n            frozen_model_iter_fn = torch._dynamo.run(model_iter_fn)\r\n\r\n        for rep in trange(args.repeat, desc=\"running benchmark\"):\r\n            inputs = (\r\n                randomize_input(copy.deepcopy(example_inputs))\r\n                if should_randomize_input\r\n                else example_inputs\r\n            )\r\n            # need call mark_step to perform the computation\r\n            # on randomize_input. Otherwise the first call using the\r\n            # inputs will incur high penalty then the next one.\r\n            maybe_mark_step(args)\r\n\r\n            # interleave the runs to handle frequency scaling and load changes\r\n            with maybe_mark_profile(p=p, mark=\"expected\"):\r\n                timings[rep, 0], expected_output = timed(\r\n                    model,\r\n                    model_iter_fn,\r\n                    inputs,\r\n                    return_result=True,\r\n                    times=times,\r\n                    collect_outputs=args.collect_outputs,\r\n                )\r\n\r\n            # call mark_step between the 2 calls to make the comparison fair.\r\n            maybe_mark_step(args)\r\n\r\n            with maybe_mark_profile(p=p, mark=\"actual\"), maybe_enable_compiled_autograd(\r\n                args.compiled_autograd,\r\n                fullgraph=args.nopython,\r\n                dynamic=args.dynamic_shapes,\r\n            ):\r\n                timings[rep, 1], actual_output = timed(\r\n                    model,\r\n                    frozen_model_iter_fn,\r\n                    inputs,\r\n                    return_result=True,\r\n                    times=times,\r\n                    collect_outputs=args.collect_outputs,\r\n                )\r\n\r\n    if args.export_profiler_trace:\r\n        name = args.profiler_trace_name + \"_\" + model.name\r\n        if hasattr(args, \"rank\"):\r\n            name += f\"_rank_{args.rank}\"\r\n        if args.export_perfdoctor and trace_handler:\r\n            trace_handler(name, p)\r\n        else:\r\n            name += \".json\"\r\n            name = os.path.join(torch._dynamo.config.base_dir, name)\r\n            p.export_chrome_trace(name)\r\n\r\n    median = np.median(timings, axis=0)\r\n    speedup = median[0] / median[1]\r\n    if args.dump_raw_metrics:\r\n        np.save(\r\n            f\"{output_filename[:-4]}-raw_timings-{current_name}-{current_device}.npy\",\r\n            timings,\r\n        )\r\n\r\n    first_headers = [\"dev\", \"name\", \"batch_size\"]\r\n    first_fields = [current_device, current_name, current_batch_size]\r\n    if \"tag\" in kwargs:\r\n        first_headers.append(\"tag\")\r\n        first_fields.append(kwargs[\"tag\"])\r\n    headers = first_headers + [\"speedup\", \"abs_latency\"]\r\n    row = first_fields + [float(speedup), median[1] * 1000]\r\n    msg = f\"{speedup:.3f}x\"\r\n    if args.baseline:\r\n        headers.extend(\r\n            [\r\n                \"baseline\",\r\n                \"speedup_vs_baseline\",\r\n            ]\r\n        )\r\n        df = pd.read_csv(args.baseline)\r\n        try:\r\n            baseline_speedup = df[df[\"name\"] == current_name][\"speedup\"].item()\r\n            row.extend([baseline_speedup, speedup / baseline_speedup])\r\n            msg = f\"{baseline_speedup:.3f}x -> {speedup:.3f}x [{speedup / baseline_speedup:.3f}x]\"\r\n        except (KeyError, ZeroDivisionError):\r\n            row.extend(\r\n                [\r\n                    0.0,\r\n                    0.0,\r\n                ]\r\n            )\r\n    if \"compilation_latency\" in kwargs:\r\n        headers += [\r\n            \"compilation_latency\",\r\n            \"compression_ratio\",\r\n            \"eager_peak_mem\",\r\n            \"dynamo_peak_mem\",\r\n        ]\r\n        row.append(kwargs[\"compilation_latency\"])\r\n        row.append(kwargs[\"compression_ratio\"])\r\n        row.append(kwargs[\"eager_peak_mem\"])\r\n        row.append(kwargs[\"dynamo_peak_mem\"])\r\n\r\n    if \"cache_lookup_latency\" in kwargs:\r\n        headers.append(\"cache_lookup_latency\")\r\n        row.append(kwargs[\"cache_lookup_latency\"])\r\n\r\n    if \"dynamo_stats\" in kwargs:\r\n        for k, v in kwargs[\"dynamo_stats\"].items():\r\n            headers.append(k)\r\n            row.append(v)\r\n    write_outputs(\r\n        output_filename,\r\n        headers,\r\n        row,\r\n    )\r\n    c_headers, c_data = torch._dynamo.utils.compile_times(repr=\"csv\", aggregate=True)\r\n    assert (\r\n        output_filename.find(\".csv\") > 0\r\n    ), f\"expected output_filename to be a .csv, but got {output_filename}\"\r\n    write_outputs(\r\n        output_filename[:-4] + \"_compilation_metrics.csv\",\r\n        first_headers + c_headers,\r\n        first_fields + c_data,\r\n    )\r\n\r\n    output_signpost(\r\n        dict(zip(headers, row)),\r\n        args,\r\n        get_suite_from_model_iter_fn(model_iter_fn),\r\n    )\r\n\r\n    return msg",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Chain Indexing"
        ]
    },
    {
        "code": "def train(db, net, device, meta_opt, epoch, log):\r\n    net.train()\r\n    n_train_iter = db.x_train.shape[0] // db.batchsz\r\n\r\n    for batch_idx in range(n_train_iter):\r\n        start_time = time.time()\r\n        # Sample a batch of support and query images and labels.\r\n        x_spt, y_spt, x_qry, y_qry = db.next()\r\n\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n        querysz = x_qry.size(1)\r\n\r\n        # TODO: Maybe pull this out into a separate module so it\r\n        # doesn't have to be duplicated between `train` and `test`?\r\n\r\n        # Initialize the inner optimizer to adapt the parameters to\r\n        # the support set.\r\n        n_inner_iter = 5\r\n        inner_opt = torch.optim.SGD(net.parameters(), lr=1e-1)\r\n\r\n        qry_losses = []\r\n        qry_accs = []\r\n        meta_opt.zero_grad()\r\n        for i in range(task_num):\r\n            with higher.innerloop_ctx(net, inner_opt, copy_initial_weights=False) as (\r\n                fnet,\r\n                diffopt,\r\n            ):\r\n                # Optimize the likelihood of the support set by taking\r\n                # gradient steps w.r.t. the model's parameters.\r\n                # This adapts the model's meta-parameters to the task.\r\n                # higher is able to automatically keep copies of\r\n                # your network's parameters as they are being updated.\r\n                for _ in range(n_inner_iter):\r\n                    spt_logits = fnet(x_spt[i])\r\n                    spt_loss = F.cross_entropy(spt_logits, y_spt[i])\r\n                    diffopt.step(spt_loss)\r\n\r\n                # The final set of adapted parameters will induce some\r\n                # final loss and accuracy on the query dataset.\r\n                # These will be used to update the model's meta-parameters.\r\n                qry_logits = fnet(x_qry[i])\r\n                qry_loss = F.cross_entropy(qry_logits, y_qry[i])\r\n                qry_losses.append(qry_loss.detach())\r\n                qry_acc = (qry_logits.argmax(dim=1) == y_qry[i]).sum().item() / querysz\r\n                qry_accs.append(qry_acc)\r\n\r\n                # print([b.shape for b in fnet[1].buffers()])\r\n\r\n                # Update the model's meta-parameters to optimize the query\r\n                # losses across all of the tasks sampled in this batch.\r\n                # This unrolls through the gradient steps.\r\n                qry_loss.backward()\r\n\r\n        meta_opt.step()\r\n        qry_losses = sum(qry_losses) / task_num\r\n        qry_accs = 100.0 * sum(qry_accs) / task_num\r\n        i = epoch + float(batch_idx) / n_train_iter\r\n        iter_time = time.time() - start_time\r\n        if batch_idx % 4 == 0:\r\n            print(\r\n                f\"[Epoch {i:.2f}] Train Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}\"\r\n            )\r\n\r\n        log.append(\r\n            {\r\n                \"epoch\": i,\r\n                \"loss\": qry_losses,\r\n                \"acc\": qry_accs,\r\n                \"mode\": \"train\",\r\n                \"time\": time.time(),\r\n            }\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train(args, model, train_loader, optimizer, privacy_engine, epoch, device):\r\n    start_time = datetime.now()\r\n\r\n    model.train()\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\n    losses = []\r\n    top1_acc = []\r\n\r\n    for i, (images, target) in enumerate(tqdm(train_loader)):\r\n        images = images.to(device)\r\n        target = target.to(device)\r\n\r\n        # compute output\r\n        output = model(images)\r\n        loss = criterion(output, target)\r\n        preds = np.argmax(output.detach().cpu().numpy(), axis=1)\r\n        labels = target.detach().cpu().numpy()\r\n\r\n        # measure accuracy and record loss\r\n        acc1 = accuracy(preds, labels)\r\n\r\n        losses.append(loss.item())\r\n        top1_acc.append(acc1)\r\n\r\n        # compute gradient and do SGD step\r\n        loss.backward()\r\n\r\n        # make sure we take a step after processing the last mini-batch in the\r\n        # epoch to ensure we start the next epoch with a clean state\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        if i % args.print_freq == 0:\r\n            if not args.disable_dp:\r\n                epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(\r\n                    delta=args.delta,\r\n                    alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\r\n                )\r\n                print(\r\n                    f\"\\tTrain Epoch: {epoch} \\t\"\r\n                    f\"Loss: {np.mean(losses):.6f} \"\r\n                    f\"Acc@1: {np.mean(top1_acc):.6f} \"\r\n                    f\"(ε = {epsilon:.2f}, δ = {args.delta}) for α = {best_alpha}\"\r\n                )\r\n            else:\r\n                print(\r\n                    f\"\\tTrain Epoch: {epoch} \\t\"\r\n                    f\"Loss: {np.mean(losses):.6f} \"\r\n                    f\"Acc@1: {np.mean(top1_acc):.6f} \"\r\n                )\r\n    train_duration = datetime.now() - start_time\r\n    return train_duration",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(db, net, device, meta_opt, epoch, log):\r\n    params, buffers, fnet = net\r\n    n_train_iter = db.x_train.shape[0] // db.batchsz\r\n\r\n    for batch_idx in range(n_train_iter):\r\n        start_time = time.time()\r\n        # Sample a batch of support and query images and labels.\r\n        x_spt, y_spt, x_qry, y_qry = db.next()\r\n\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n        querysz = x_qry.size(1)\r\n\r\n        # TODO: Maybe pull this out into a separate module so it\r\n        # doesn't have to be duplicated between `train` and `test`?\r\n\r\n        # Initialize the inner optimizer to adapt the parameters to\r\n        # the support set.\r\n        n_inner_iter = 5\r\n        # inner_opt = torch.optim.SGD(net.parameters(), lr=1e-1)\r\n\r\n        qry_losses = []\r\n        qry_accs = []\r\n        meta_opt.zero_grad()\r\n        for i in range(task_num):\r\n            # Optimize the likelihood of the support set by taking\r\n            # gradient steps w.r.t. the model's parameters.\r\n            # This adapts the model's meta-parameters to the task.\r\n            new_params = params\r\n            for _ in range(n_inner_iter):\r\n                spt_logits = fnet(new_params, buffers, x_spt[i])\r\n                spt_loss = F.cross_entropy(spt_logits, y_spt[i])\r\n                grads = torch.autograd.grad(spt_loss, new_params, create_graph=True)\r\n                new_params = [p - g * 1e-1 for p, g in zip(new_params, grads)]\r\n\r\n            # The final set of adapted parameters will induce some\r\n            # final loss and accuracy on the query dataset.\r\n            # These will be used to update the model's meta-parameters.\r\n            qry_logits = fnet(new_params, buffers, x_qry[i])\r\n            qry_loss = F.cross_entropy(qry_logits, y_qry[i])\r\n            qry_losses.append(qry_loss.detach())\r\n            qry_acc = (qry_logits.argmax(dim=1) == y_qry[i]).sum().item() / querysz\r\n            qry_accs.append(qry_acc)\r\n\r\n            # Update the model's meta-parameters to optimize the query\r\n            # losses across all of the tasks sampled in this batch.\r\n            # This unrolls through the gradient steps.\r\n            qry_loss.backward()\r\n\r\n        meta_opt.step()\r\n        qry_losses = sum(qry_losses) / task_num\r\n        qry_accs = 100.0 * sum(qry_accs) / task_num\r\n        i = epoch + float(batch_idx) / n_train_iter\r\n        iter_time = time.time() - start_time\r\n        if batch_idx % 4 == 0:\r\n            print(\r\n                f\"[Epoch {i:.2f}] Train Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}\"\r\n            )\r\n\r\n        log.append(\r\n            {\r\n                \"epoch\": i,\r\n                \"loss\": qry_losses,\r\n                \"acc\": qry_accs,\r\n                \"mode\": \"train\",\r\n                \"time\": time.time(),\r\n            }\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test(db, net, device, epoch, log):\r\n    # Crucially in our testing procedure here, we do *not* fine-tune\r\n    # the model during testing for simplicity.\r\n    # Most research papers using MAML for this task do an extra\r\n    # stage of fine-tuning here that should be added if you are\r\n    # adapting this code for research.\r\n    net.train()\r\n    n_test_iter = db.x_test.shape[0] // db.batchsz\r\n\r\n    qry_losses = []\r\n    qry_accs = []\r\n\r\n    for _ in range(n_test_iter):\r\n        x_spt, y_spt, x_qry, y_qry = db.next(\"test\")\r\n\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n\r\n        # TODO: Maybe pull this out into a separate module so it\r\n        # doesn't have to be duplicated between `train` and `test`?\r\n        n_inner_iter = 5\r\n        inner_opt = torch.optim.SGD(net.parameters(), lr=1e-1)\r\n\r\n        for i in range(task_num):\r\n            with higher.innerloop_ctx(net, inner_opt, track_higher_grads=False) as (\r\n                fnet,\r\n                diffopt,\r\n            ):\r\n                # Optimize the likelihood of the support set by taking\r\n                # gradient steps w.r.t. the model's parameters.\r\n                # This adapts the model's meta-parameters to the task.\r\n                for _ in range(n_inner_iter):\r\n                    spt_logits = fnet(x_spt[i])\r\n                    spt_loss = F.cross_entropy(spt_logits, y_spt[i])\r\n                    diffopt.step(spt_loss)\r\n\r\n                # The query loss and acc induced by these parameters.\r\n                qry_logits = fnet(x_qry[i]).detach()\r\n                qry_loss = F.cross_entropy(qry_logits, y_qry[i], reduction=\"none\")\r\n                qry_losses.append(qry_loss.detach())\r\n                qry_accs.append((qry_logits.argmax(dim=1) == y_qry[i]).detach())\r\n\r\n    qry_losses = torch.cat(qry_losses).mean().item()\r\n    qry_accs = 100.0 * torch.cat(qry_accs).float().mean().item()\r\n    print(f\"[Epoch {epoch + 1:.2f}] Test Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f}\")\r\n    log.append(\r\n        {\r\n            \"epoch\": epoch + 1,\r\n            \"loss\": qry_losses,\r\n            \"acc\": qry_accs,\r\n            \"mode\": \"test\",\r\n            \"time\": time.time(),\r\n        }\r\n    )",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test(db, net, device, epoch, log):\r\n    # Crucially in our testing procedure here, we do *not* fine-tune\r\n    # the model during testing for simplicity.\r\n    # Most research papers using MAML for this task do an extra\r\n    # stage of fine-tuning here that should be added if you are\r\n    # adapting this code for research.\r\n    params = dict(net.named_parameters())\r\n    buffers = dict(net.named_buffers())\r\n    n_test_iter = db.x_test.shape[0] // db.batchsz\r\n\r\n    qry_losses = []\r\n    qry_accs = []\r\n\r\n    for batch_idx in range(n_test_iter):\r\n        x_spt, y_spt, x_qry, y_qry = db.next(\"test\")\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n\r\n        # TODO: Maybe pull this out into a separate module so it\r\n        # doesn't have to be duplicated between `train` and `test`?\r\n        n_inner_iter = 5\r\n\r\n        for i in range(task_num):\r\n            new_params = params\r\n            for _ in range(n_inner_iter):\r\n                spt_logits = functional_call(net, (new_params, buffers), x_spt[i])\r\n                spt_loss = F.cross_entropy(spt_logits, y_spt[i])\r\n                grads = torch.autograd.grad(spt_loss, new_params.values())\r\n                new_params = {\r\n                    k: new_params[k] - g * 1e-1 for k, g in zip(new_params, grads)\r\n                }\r\n\r\n            # The query loss and acc induced by these parameters.\r\n            qry_logits = functional_call(net, (new_params, buffers), x_qry[i]).detach()\r\n            qry_loss = F.cross_entropy(qry_logits, y_qry[i], reduction=\"none\")\r\n            qry_losses.append(qry_loss.detach())\r\n            qry_accs.append((qry_logits.argmax(dim=1) == y_qry[i]).detach())\r\n\r\n    qry_losses = torch.cat(qry_losses).mean().item()\r\n    qry_accs = 100.0 * torch.cat(qry_accs).float().mean().item()\r\n    print(f\"[Epoch {epoch + 1:.2f}] Test Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f}\")\r\n    log.append(\r\n        {\r\n            \"epoch\": epoch + 1,\r\n            \"loss\": qry_losses,\r\n            \"acc\": qry_accs,\r\n            \"mode\": \"test\",\r\n            \"time\": time.time(),\r\n        }\r\n    )",
        "labels": [
            "Dataframe Conversion API Misused",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test(db, net, device, epoch, log):\r\n    # Crucially in our testing procedure here, we do *not* fine-tune\r\n    # the model during testing for simplicity.\r\n    # Most research papers using MAML for this task do an extra\r\n    # stage of fine-tuning here that should be added if you are\r\n    # adapting this code for research.\r\n    [params, buffers, fnet] = net\r\n    n_test_iter = db.x_test.shape[0] // db.batchsz\r\n\r\n    qry_losses = []\r\n    qry_accs = []\r\n\r\n    for batch_idx in range(n_test_iter):\r\n        x_spt, y_spt, x_qry, y_qry = db.next(\"test\")\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n\r\n        # TODO: Maybe pull this out into a separate module so it\r\n        # doesn't have to be duplicated between `train` and `test`?\r\n        n_inner_iter = 5\r\n\r\n        for i in range(task_num):\r\n            new_params = params\r\n            for _ in range(n_inner_iter):\r\n                spt_logits = fnet(new_params, buffers, x_spt[i])\r\n                spt_loss = F.cross_entropy(spt_logits, y_spt[i])\r\n                grads = torch.autograd.grad(spt_loss, new_params)\r\n                new_params = [p - g * 1e-1 for p, g in zip(new_params, grads)]\r\n\r\n            # The query loss and acc induced by these parameters.\r\n            qry_logits = fnet(new_params, buffers, x_qry[i]).detach()\r\n            qry_loss = F.cross_entropy(qry_logits, y_qry[i], reduction=\"none\")\r\n            qry_losses.append(qry_loss.detach())\r\n            qry_accs.append((qry_logits.argmax(dim=1) == y_qry[i]).detach())\r\n\r\n    qry_losses = torch.cat(qry_losses).mean().item()\r\n    qry_accs = 100.0 * torch.cat(qry_accs).float().mean().item()\r\n    print(f\"[Epoch {epoch + 1:.2f}] Test Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f}\")\r\n    log.append(\r\n        {\r\n            \"epoch\": epoch + 1,\r\n            \"loss\": qry_losses,\r\n            \"acc\": qry_accs,\r\n            \"mode\": \"test\",\r\n            \"time\": time.time(),\r\n        }\r\n    )",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def train(db, net, device, meta_opt, epoch, log):\r\n    n_train_iter = db.x_train.shape[0] // db.batchsz\r\n\r\n    for batch_idx in range(n_train_iter):\r\n        start_time = time.time()\r\n        # Sample a batch of support and query images and labels.\r\n        x_spt, y_spt, x_qry, y_qry = db.next()\r\n\r\n        task_num, setsz, c_, h, w = x_spt.size()\r\n\r\n        n_inner_iter = 5\r\n        meta_opt.zero_grad()\r\n\r\n        # In parallel, trains one model per task. There is a support (x, y)\r\n        # for each task and a query (x, y) for each task.\r\n        compute_loss_for_task = functools.partial(loss_for_task, net, n_inner_iter)\r\n        qry_losses, qry_accs = vmap(compute_loss_for_task)(x_spt, y_spt, x_qry, y_qry)\r\n\r\n        # Compute the maml loss by summing together the returned losses.\r\n        qry_losses.sum().backward()\r\n\r\n        meta_opt.step()\r\n        qry_losses = qry_losses.detach().sum() / task_num\r\n        qry_accs = 100.0 * qry_accs.sum() / task_num\r\n        i = epoch + float(batch_idx) / n_train_iter\r\n        iter_time = time.time() - start_time\r\n        if batch_idx % 4 == 0:\r\n            print(\r\n                f\"[Epoch {i:.2f}] Train Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}\"\r\n            )\r\n\r\n        log.append(\r\n            {\r\n                \"epoch\": i,\r\n                \"loss\": qry_losses,\r\n                \"acc\": qry_accs,\r\n                \"mode\": \"train\",\r\n                \"time\": time.time(),\r\n            }\r\n        )",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def forward(self, input_batch: CommitClassifierInputs):\r\n        # Encode input title\r\n        title: List[str] = input_batch.title\r\n        model_input = to_tensor(self.transform(title), padding_value=1).to(device)\r\n        title_features = self.encoder(model_input)\r\n        title_embed = title_features[:, 0, :]\r\n        title_embed = self.dropout(title_embed)\r\n        title_embed = self.dense_title(title_embed)\r\n        title_embed = self.activation_fn(title_embed)\r\n        title_embed = self.dropout(title_embed)\r\n        title_embed = self.out_proj_title(title_embed)\r\n\r\n        files: list[str] = input_batch.files\r\n        batch_file_indexes = []\r\n        for file in files:\r\n            paths = [\r\n                truncate_file(Path(file_part), MAX_LEN_FILE)\r\n                for file_part in file.split(\" \")\r\n            ]\r\n            batch_file_indexes.append(\r\n                [\r\n                    self.file_map.get(file, self.file_map[UNKNOWN_TOKEN])\r\n                    for file in paths\r\n                ]\r\n            )\r\n\r\n        flat_indexes = torch.tensor(\r\n            list(chain.from_iterable(batch_file_indexes)),\r\n            dtype=torch.long,\r\n            device=device,\r\n        )\r\n        offsets = [0]\r\n        offsets.extend(len(files) for files in batch_file_indexes[:-1])\r\n        offsets = torch.tensor(offsets, dtype=torch.long, device=device)\r\n        offsets = offsets.cumsum(dim=0)\r\n\r\n        files_embed = self.file_embedding_bag(flat_indexes, offsets)\r\n        files_embed = self.dense_files(files_embed)\r\n        files_embed = self.activation_fn(files_embed)\r\n        files_embed = self.dropout(files_embed)\r\n        files_embed = self.out_proj_files(files_embed)\r\n\r\n        # Add author embedding\r\n        authors: List[str] = input_batch.author\r\n        author_ids = [\r\n            self.author_map.get(author, self.author_map[UNKNOWN_TOKEN])\r\n            for author in authors\r\n        ]\r\n        author_ids = torch.tensor(author_ids).to(device)\r\n        author_embed = self.embedding_table(author_ids)\r\n        author_embed = self.dense_author(author_embed)\r\n        author_embed = self.activation_fn(author_embed)\r\n        author_embed = self.dropout(author_embed)\r\n        author_embed = self.out_proj_author(author_embed)\r\n\r\n        return title_embed + files_embed + author_embed",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_multi_grad_any_hooks(self):\r\n        hook_id = 0\r\n        any_hook_handles: list[RemovableHandle] = []\r\n\r\n        class MultiOutputModule(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin = nn.Linear(3, 3)\r\n\r\n            def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\r\n                z = self.lin(x)\r\n                out = torch.sin(z), torch.cos(z)\r\n                nonlocal hook_id\r\n                z.register_hook(partial(hook, hook_id))\r\n                hook_id += 1\r\n                any_hook_handles.append(\r\n                    torch.autograd.graph.register_multi_grad_hook(\r\n                        out, partial(hook, hook_id), mode=\"any\"\r\n                    )\r\n                )\r\n                hook_id += 1\r\n                return out\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.mod1 = MultiOutputModule()\r\n                self.mod2 = MultiOutputModule()\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                y = self.mod1(x)\r\n                z = y[0] + y[1]\r\n                return self.mod2(z)\r\n\r\n        hook_order: list[int] = []\r\n        hook_count = 0\r\n\r\n        def hook(hook_id: int, *unused):\r\n            nonlocal hook_count\r\n            nonlocal hook_order\r\n            hook_count += 1\r\n            hook_order.append(hook_id)\r\n\r\n        # Any hooks: IDs 1 and 3; regular hooks: IDs 0 and 2\r\n        model = Model()\r\n        inp = torch.randn((2, 3))\r\n        out = model(inp)\r\n        (out[0] + out[1]).sum().backward()\r\n        # Check that the any-hook runs only once and before the regular hook\r\n        # for each module\r\n        self.assertEqual(len(any_hook_handles), 2)\r\n        self.assertEqual(hook_order, [3, 2, 1, 0])\r\n\r\n        hook_id = 0\r\n        hook_order.clear()\r\n        any_hook_handles.clear()\r\n        out = model(inp)\r\n        for handle in any_hook_handles:\r\n            handle.remove()\r\n        (out[0] + out[1]).sum().backward()\r\n        # Check that the any-hook does not run if removed\r\n        self.assertEqual(hook_order, [2, 0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_dataparallel_saved_tensors_hooks(self):\r\n        def pack(x):\r\n            warnings.warn(\"pack\")\r\n            return x\r\n\r\n        _self = self\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                with warnings.catch_warnings(record=True) as w:\r\n                    y = x * x\r\n                    if torch.cuda.device_count() >= 2:\r\n                        # DataParallel is calling the forward in different threads\r\n                        # without progating TLS, so hooks should not be called here\r\n                        _self.assertEqual(len(w), 0)\r\n                    else:\r\n                        # DataParallel only uses one thread\r\n                        # so hooks should be called here\r\n                        _self.assertGreater(len(w), 0)\r\n\r\n        x = torch.ones(5, 5, requires_grad=True)\r\n        model = torch.nn.DataParallel(Model())\r\n\r\n        with torch.autograd.graph.saved_tensors_hooks(pack, lambda x: x):\r\n            model(x)\r\n            with warnings.catch_warnings(record=True) as w:\r\n                y = x * x\r\n                # hooks should be called here\r\n                _self.assertGreater(len(w), 0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_cpp_frontend_module_python_inter_op(self):\r\n        extension = torch.utils.cpp_extension.load(\r\n            name=\"cpp_frontend_extension\",\r\n            sources=\"cpp_extensions/cpp_frontend_extension.cpp\",\r\n            verbose=True,\r\n        )\r\n\r\n        # Create a torch.nn.Module which uses the C++ module as a submodule.\r\n        class M(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.x = torch.nn.Parameter(torch.tensor(1.0))\r\n                self.net = extension.Net(3, 5)\r\n\r\n            def forward(self, input):\r\n                return self.net.forward(input) + self.x\r\n\r\n        net = extension.Net(5, 2)\r\n        net.double()\r\n        net.to(torch.get_default_dtype())\r\n        self.assertEqual(str(net), \"Net\")\r\n\r\n        # Further embed the torch.nn.Module into a Sequential, and also add the\r\n        # C++ module as an element of the Sequential.\r\n        sequential = torch.nn.Sequential(M(), torch.nn.Tanh(), net, torch.nn.Sigmoid())\r\n\r\n        input = torch.randn(2, 3)\r\n        # Try calling the module!\r\n        output = sequential.forward(input)\r\n        # The call operator is bound to forward too.\r\n        self.assertEqual(output, sequential(input))\r\n        self.assertEqual(list(output.shape), [2, 2])\r\n\r\n        # Do changes on the module hierarchy.\r\n        old_dtype = torch.get_default_dtype()\r\n        sequential.to(torch.float64)\r\n        sequential.to(torch.float32)\r\n        sequential.to(old_dtype)\r\n        self.assertEqual(sequential[2].parameters()[0].dtype, old_dtype)\r\n\r\n        # Make sure we can access these methods recursively.\r\n        self.assertEqual(\r\n            len(list(sequential.parameters())), len(net.parameters()) * 2 + 1\r\n        )\r\n        self.assertEqual(\r\n            len(list(sequential.named_parameters())),\r\n            len(net.named_parameters()) * 2 + 1,\r\n        )\r\n        self.assertEqual(len(list(sequential.buffers())), len(net.buffers()) * 2)\r\n        self.assertEqual(len(list(sequential.modules())), 8)\r\n\r\n        # Test clone()\r\n        net2 = net.clone()\r\n        self.assertEqual(len(net.parameters()), len(net2.parameters()))\r\n        self.assertEqual(len(net.buffers()), len(net2.buffers()))\r\n        self.assertEqual(len(net.modules()), len(net2.modules()))\r\n\r\n        # Try differentiating through the whole module.\r\n        for parameter in net.parameters():\r\n            self.assertIsNone(parameter.grad)\r\n        output.sum().backward()\r\n        for parameter in net.parameters():\r\n            self.assertFalse(parameter.grad is None)\r\n            self.assertGreater(parameter.grad.sum(), 0)\r\n\r\n        # Try calling zero_grad()\r\n        net.zero_grad()\r\n        for p in net.parameters():\r\n            assert p.grad is None, \"zero_grad defaults to setting grads to None\"\r\n\r\n        # Test train(), eval(), training (a property)\r\n        self.assertTrue(net.training)\r\n        net.eval()\r\n        self.assertFalse(net.training)\r\n        net.train()\r\n        self.assertTrue(net.training)\r\n        net.eval()\r\n\r\n        # Try calling the additional methods we registered.\r\n        biased_input = torch.randn(4, 5)\r\n        output_before = net.forward(biased_input)\r\n        bias = net.get_bias().clone()\r\n        self.assertEqual(list(bias.shape), [2])\r\n        net.set_bias(bias + 1)\r\n        self.assertEqual(net.get_bias(), bias + 1)\r\n        output_after = net.forward(biased_input)\r\n\r\n        self.assertNotEqual(output_before, output_after)\r\n\r\n        # Try accessing parameters\r\n        self.assertEqual(len(net.parameters()), 2)\r\n        np = net.named_parameters()\r\n        self.assertEqual(len(np), 2)\r\n        self.assertIn(\"fc.weight\", np)\r\n        self.assertIn(\"fc.bias\", np)\r\n\r\n        self.assertEqual(len(net.buffers()), 1)\r\n        nb = net.named_buffers()\r\n        self.assertEqual(len(nb), 1)\r\n        self.assertIn(\"buf\", nb)\r\n        self.assertEqual(nb[0][1], torch.eye(5))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_checkpointing_non_reentrant_autocast(self, device_type):\r\n        for enabled in [True, False]:\r\n\r\n            def foo(x, y, z):\r\n                # torch.mm is on autocast's list of ops that should run in\r\n                # the autocast precision\r\n                x = torch.mm(x, y)\r\n                y = torch.mm(x, z)\r\n                z = torch.mm(z, z)\r\n                expected_dtype = torch.float32 if not enabled else torch.bfloat16\r\n                self.assertEqual(expected_dtype, z.dtype)\r\n                return z\r\n\r\n            x = torch.randn(3, 3, requires_grad=True)\r\n            y = torch.randn(3, 3, requires_grad=True)\r\n            z = torch.randn(3, 3, requires_grad=True)\r\n            if device_type == \"cuda\":\r\n                x = x.cuda()\r\n                y = y.cuda()\r\n                z = z.cuda()\r\n\r\n            with torch.autocast(\r\n                enabled=enabled, device_type=device_type, dtype=torch.bfloat16\r\n            ):\r\n                loss = checkpoint(foo, x, y, z, use_reentrant=False)\r\n                loss = loss.sum()\r\n\r\n            # Without saving + recasting the autocast type, would raise error in autograd\r\n            # about mismatched dtypes.\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_streaming_backwards_multiple_streams(self):\r\n        MultiplyInStream = self._make_multiply_in_stream()\r\n\r\n        class StreamModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.event = torch.cuda.Event()\r\n                self.stream0 = torch.cuda.Stream()\r\n                self.stream1 = torch.cuda.Stream()\r\n\r\n            def forward(self, x, x_first_use_on_ambient):\r\n                if x_first_use_on_ambient:\r\n                    x0 = x.clone()\r\n                self.stream0.wait_stream(torch.cuda.current_stream())\r\n                self.stream1.wait_stream(torch.cuda.current_stream())\r\n                with torch.cuda.stream(self.stream0):\r\n                    if not x_first_use_on_ambient:\r\n                        x0 = x.clone()\r\n                    y0 = MultiplyInStream.apply(x0, 2)\r\n                    self.event.record(stream=torch.cuda.current_stream())\r\n\r\n                with torch.cuda.stream(self.stream1):\r\n                    y1 = MultiplyInStream.apply(x, 3)\r\n                    self.stream1.wait_event(self.event)\r\n                    return y0 + y1\r\n\r\n        stream = torch.cuda.Stream()\r\n\r\n        for x_first_use_on_ambient in (True, False):\r\n            # the out_of_place=False, iters=1 case stresses if proper syncs are inserted\r\n            # when grads are initially None and stolen by backward ops.\r\n            for out_of_place, iters in ((True, 1), (False, 1), (False, 5)):\r\n                with torch.cuda.stream(stream):\r\n                    x = torch.randn(5, 5, device=\"cuda\", requires_grad=True)\r\n                    model = StreamModel().cuda()\r\n                    x.register_hook(\r\n                        lambda grad: self.assertEqual(\r\n                            torch.cuda.current_stream(),\r\n                            stream if x_first_use_on_ambient else model.stream0,\r\n                        )\r\n                    )\r\n                    for p in model.parameters():\r\n                        self.assertTrue(p.grad is None)\r\n                    for _ in range(iters):\r\n                        loss = model(x, x_first_use_on_ambient).sum()\r\n                        if out_of_place:\r\n                            x_grad = torch.autograd.grad((loss,), (x,))[0]\r\n                        else:\r\n                            loss.backward()\r\n                # See \"Stream semantics of backward passes\" on https://pytorch.org/docs/stable/notes/cuda.html\r\n                torch.cuda.current_stream().wait_stream(stream)\r\n\r\n                if out_of_place:\r\n                    self.assertEqual(x_grad, torch.ones_like(x) * 5 * iters)\r\n                else:\r\n                    self.assertEqual(x.grad, torch.ones_like(x) * 5 * iters)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autocast_custom_enabled(self):\r\n        class MyMM(torch.autograd.Function):\r\n            @staticmethod\r\n            @torch.amp.custom_fwd(device_type=\"cuda\")\r\n            def forward(ctx, a, b):\r\n                self.assertTrue(a.dtype is torch.float32)\r\n                self.assertTrue(b.dtype is torch.float32)\r\n                self.assertTrue(torch.is_autocast_enabled())\r\n                ctx.save_for_backward(a, b)\r\n                return a.mm(b)\r\n\r\n            @staticmethod\r\n            @torch.amp.custom_bwd(device_type=\"cuda\")\r\n            def backward(ctx, grad):\r\n                self.assertTrue(torch.is_autocast_enabled())\r\n                a, b = ctx.saved_tensors\r\n                a_grad, b_grad = grad.mm(b.t()), a.t().mm(grad)\r\n                self.assertTrue(a_grad.dtype is dtype and b_grad.dtype is dtype)\r\n                return a_grad, b_grad\r\n\r\n        mymm = MyMM.apply\r\n\r\n        x = torch.randn((8, 8), device=\"cuda\", dtype=torch.float32, requires_grad=True)\r\n        y = torch.randn((8, 8), device=\"cuda\", dtype=torch.float32, requires_grad=True)\r\n\r\n        dtypes = (torch.float16, torch.bfloat16) if TEST_BF16 else (torch.float16,)\r\n        for dtype in dtypes:\r\n            with torch.autocast(device_type=\"cuda\", dtype=dtype):\r\n                output = mymm(x, y)\r\n                self.assertTrue(output.dtype is dtype)\r\n                loss = output.sum()\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autocast_cat_jit(self):\r\n        # Reported at https://github.com/pytorch/pytorch/issues/38958\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self):\r\n                a = torch.randn(1)\r\n                b = torch.randn(1)\r\n                c = torch.cat((a, b), 0)\r\n                d = torch.stack([c, c], 0)\r\n                return d\r\n\r\n        # The JIT here doesn't really matter, we just need to call\r\n        # cat via the boxed API\r\n        model = Model()\r\n        model_jit_script = torch.jit.script(model)\r\n\r\n        with torch.autocast(\"cuda\", enabled=True):\r\n            model()\r\n            model_jit_script()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_graph_make_graphed_callables(\r\n        self, with_amp, cache_enabled, allow_unused_input\r\n    ):\r\n        torch.manual_seed(5)\r\n        torch.cuda.manual_seed(5)\r\n\r\n        N, D_in, H, D_out = 640, 4096, 2048, 1024\r\n\r\n        class MLP1(torch.nn.Module):\r\n            def __init__(self, D_in: int, H: int, D_out: int):\r\n                super().__init__()\r\n                self.net_1 = torch.nn.Sequential(\r\n                    torch.nn.Linear(D_in, H), torch.nn.Dropout(p=0.1)\r\n                ).cuda()\r\n                self.net_2 = torch.nn.Sequential(\r\n                    torch.nn.Linear(H, D_out), torch.nn.Dropout(p=0.2)\r\n                ).cuda()\r\n\r\n            def forward(self, input_dict: dict):\r\n                x = input_dict[\"x\"]\r\n                return self.net_2(self.net_1(x))\r\n\r\n        class MLP2(torch.nn.Module):\r\n            def __init__(self, D_in: int, H: int, D_out: int):\r\n                super().__init__()\r\n                self.net_1 = torch.nn.Sequential(\r\n                    torch.nn.Linear(D_in, H), torch.nn.Dropout(p=0.1)\r\n                ).cuda()\r\n                self.net_2 = torch.nn.Sequential(\r\n                    torch.nn.Linear(H, D_out), torch.nn.Dropout(p=0.2)\r\n                ).cuda()\r\n\r\n            def forward(self, x):\r\n                return self.net_2(self.net_1(x))\r\n\r\n        class ParameterlessModule(torch.nn.Module):\r\n            def forward(self, x):\r\n                idx = (\r\n                    torch.arange(x.size(0), device=x.device)\r\n                    .view(-1, 1)\r\n                    .repeat(1, x.size(1))\r\n                )\r\n                return {\"output\": torch.gather(x, 0, idx)}\r\n\r\n        models = []\r\n        for _ in range(2):\r\n            model_section1 = MLP1(D_in, H, H).cuda()\r\n            model_section2 = MLP2(H, H, D_out).cuda()\r\n            model_section3 = ParameterlessModule().cuda()\r\n            models.append(\r\n                torch.nn.Sequential(model_section1, model_section2, model_section3)\r\n            )\r\n\r\n        model_graphed = models[0]\r\n        model_control = models[1]\r\n\r\n        model_graphed.load_state_dict(model_control.state_dict())\r\n\r\n        opt_graphed = torch.optim.SGD(model_graphed.parameters(), lr=0.1)\r\n        opt_control = torch.optim.SGD(model_control.parameters(), lr=0.1)\r\n\r\n        x = torch.randn(N, D_in, device=\"cuda\")\r\n        h = torch.randn(N, H, device=\"cuda\", requires_grad=True)\r\n        h2 = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\r\n        unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=True)\r\n        y_pred = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn(N, D_out, device=\"cuda\")\r\n\r\n        loss_fn_control = torch.nn.functional.mse_loss\r\n        relu_control = torch.nn.functional.relu\r\n\r\n        # This is a good stress test. It graphs four callables: two Modules and two python functions.\r\n        with torch.amp.autocast(\r\n            device_type=\"cuda\", enabled=with_amp, cache_enabled=cache_enabled\r\n        ):\r\n            (\r\n                model_graphed[0],\r\n                model_graphed[1],\r\n                model_graphed[2],\r\n                relu_graphed,\r\n                loss_fn_graphed,\r\n            ) = torch.cuda.make_graphed_callables(\r\n                (\r\n                    model_graphed[0],\r\n                    model_graphed[1],\r\n                    model_graphed[2],\r\n                    relu_control,\r\n                    loss_fn_control,\r\n                ),\r\n                (\r\n                    ({\"x\": x, \"unused_input\": unused_input},),\r\n                    (h,),\r\n                    (h2,),\r\n                    (y_pred,),\r\n                    (y_pred, y),\r\n                ),\r\n                allow_unused_input=allow_unused_input,\r\n            )\r\n\r\n        real_inputs = [torch.rand_like(x) for _ in range(10)]\r\n        real_targets = [torch.rand_like(y) for _ in range(10)]\r\n\r\n        for m, opt, relu, loss_fn in zip(\r\n            (model_graphed, model_control),\r\n            (opt_graphed, opt_control),\r\n            (relu_graphed, relu_control),\r\n            (loss_fn_graphed, loss_fn_control),\r\n        ):\r\n            # Resets RNC states before iterations for graphed and ungraphed models,\r\n            # so dropout math should be bitwise identical for both.\r\n            torch.manual_seed(5)\r\n            torch.cuda.manual_seed(5)\r\n            for data, target in zip(real_inputs, real_targets):\r\n                opt.zero_grad(set_to_none=True)\r\n                with torch.amp.autocast(\r\n                    device_type=\"cuda\", enabled=with_amp, cache_enabled=cache_enabled\r\n                ):\r\n                    y_pred = m({\"x\": data, \"unused_input\": unused_input})[\"output\"]\r\n                    y_pred = relu(y_pred)\r\n                    loss = loss_fn(y_pred, target)\r\n                    loss.backward()\r\n                opt.step()\r\n\r\n        for p, pc in zip(model_graphed.parameters(), model_control.parameters()):\r\n            self.assertEqual(p, pc)\r\n\r\n        # We graphed the models in training mode. Eval should still run ungraphed.\r\n        model_graphed.eval()\r\n        model_control.eval()\r\n        self.assertEqual(\r\n            model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_graph_make_graphed_callables_same_pool(self):\r\n        torch.manual_seed(5)\r\n        torch.cuda.manual_seed(5)\r\n        models = []\r\n        num_models = 3\r\n        for _ in range(num_models):\r\n            models.append(\r\n                torch.nn.Sequential(\r\n                    torch.nn.Linear(32, 128),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(128, 128),\r\n                ).cuda()\r\n            )\r\n        # we will reuse the same pool for all graph captures\r\n        mempool = torch.cuda.graph_pool_handle()\r\n        graphed_models = []\r\n        for model in models:\r\n            x = torch.randn([64, 32], device=\"cuda\")\r\n            graphed_model = deepcopy(model)\r\n            graphed_model = torch.cuda.make_graphed_callables(\r\n                graphed_model, (x,), pool=mempool\r\n            )\r\n            graphed_models.append(graphed_model)\r\n\r\n        for model, graphed_model in zip(models, graphed_models):\r\n            x = torch.randn([64, 32], device=\"cuda\")\r\n            y = model(x)\r\n            yg = graphed_model(x)\r\n            l = y.norm()\r\n            lg = yg.norm()\r\n            l.backward()\r\n            lg.backward()\r\n\r\n            self.assertEqual(y, yg)\r\n            self.assertEqual(l, lg)\r\n            for p, pg in zip(model.parameters(), graphed_model.parameters()):\r\n                self.assertEqual(p, pg)\r\n                self.assertEqual(p.grad, pg.grad)\r\n                self.assertNotEqual(p.data_ptr(), pg.data_ptr())\r\n                self.assertNotEqual(p.grad.data_ptr(), pg.grad.data_ptr())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_no_grad_copy_sparse(self):\r\n        # create autograd function that saves grad pointer as class static\r\n        class MyFunc(Function):\r\n            static_grad_ptr = None\r\n\r\n            @staticmethod\r\n            def forward(ctx, inp1, inp2):\r\n                return inp1 + inp2\r\n\r\n            @staticmethod\r\n            def backward(ctx, grad):\r\n                MyFunc.static_grad_ptr = grad._values().data_ptr()\r\n                return grad, grad\r\n\r\n        class NonContGradFunc(Function):\r\n            static_grad_ptr = None\r\n\r\n            @staticmethod\r\n            def forward(ctx, inp1, inp2):\r\n                return inp1 + inp2\r\n\r\n            @staticmethod\r\n            def backward(ctx, grad):\r\n                # Create a sparse tensor with non-contigous indices and values\r\n                # and return as grad.\r\n                v = torch.rand(1, 3)\r\n                i = torch.ones(1, 1, dtype=torch.long)\r\n                nv = v.expand(8, 3)\r\n                ni = i.expand(1, 8)\r\n                ngrad = torch.sparse_coo_tensor(ni, nv, (10, 3), dtype=torch.float32)\r\n                NonContGradFunc.static_grad_ptr = ngrad._values().data_ptr()\r\n                return ngrad, ngrad\r\n\r\n        a = torch.randn(10, 3, requires_grad=True)\r\n        b = torch.randn(10, 3, requires_grad=True)\r\n        input = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\r\n        offsets = torch.tensor([0, 4])\r\n        import torch.nn.functional as F\r\n\r\n        # test case that should trigger no copy for one of a,b\r\n        emb_matrix = MyFunc.apply(a, b)\r\n        loss = F.embedding_bag(emb_matrix, input, offsets, sparse=True).sum()\r\n        loss.backward(retain_graph=True)\r\n        p_g = MyFunc.static_grad_ptr\r\n        p_a = a.grad._values().data_ptr()\r\n        p_b = b.grad._values().data_ptr()\r\n        # check a,b uses different grad buffer\r\n        self.assertFalse(p_a == p_b)\r\n        # check one of them is using the computed buffer\r\n        self.assertTrue(p_a == p_g or p_b == p_g)\r\n\r\n        # Run backwards multiple times to ensure accumulation works.\r\n        for i in range(10):\r\n            loss.backward(retain_graph=True)\r\n\r\n        # non-contiguous indices and value, we should trigger a copy.\r\n        a.grad = b.grad = None\r\n        emb_matrix = NonContGradFunc.apply(a, b)\r\n        loss = F.embedding_bag(emb_matrix, input, offsets, sparse=True).sum()\r\n        loss.backward(retain_graph=True)\r\n        p_g = NonContGradFunc.static_grad_ptr\r\n        p_a = a.grad._values().data_ptr()\r\n        p_b = b.grad._values().data_ptr()\r\n        # check a,b uses different grad buffer\r\n        self.assertFalse(p_a == p_b)\r\n        # Verify we cloned both grads.\r\n        self.assertFalse(p_a == p_g)\r\n        self.assertFalse(p_b == p_g)\r\n\r\n        # Run backwards multiple times to ensure accumulation works.\r\n        for i in range(10):\r\n            loss.backward(retain_graph=True)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_post_accumulate_grad_hook_e2e(self):\r\n        def setup_optim_in_bwd(model):\r\n            optims = {}\r\n            handles = []\r\n\r\n            def optim_step_hook(param):\r\n                optims[param].step()\r\n                optims[param].zero_grad()\r\n\r\n            for p in model.parameters():\r\n                optims[p] = torch.optim.Adam([p])\r\n                handles.append(p.register_post_accumulate_grad_hook(optim_step_hook))\r\n\r\n            return handles\r\n\r\n        model = torch.nn.Linear(3, 2)\r\n        input = torch.rand(2, 3)\r\n        handles = setup_optim_in_bwd(model)\r\n\r\n        # make a copy for reference\r\n        model_copy = deepcopy(model)\r\n        optim_copy = torch.optim.Adam(model_copy.parameters())\r\n\r\n        iters = 5\r\n\r\n        for _ in range(iters):\r\n            loss = model(input).sum()\r\n            loss.backward()\r\n\r\n            loss_copy = model_copy(input).sum()\r\n            loss_copy.backward()\r\n            optim_copy.step()\r\n            optim_copy.zero_grad()\r\n\r\n        params_copy = []  # freeze a copy of the params to compare later\r\n        for p_reference, p in zip(model_copy.parameters(), model.parameters()):\r\n            self.assertEqual(p_reference, p)\r\n            params_copy.append(p_reference.detach().clone())\r\n\r\n        # After removing the handle, the model should no longer update.\r\n        for h in handles:\r\n            h.remove()\r\n\r\n        for _ in range(iters):\r\n            loss = model(input).sum()\r\n            loss.backward()\r\n\r\n            loss_copy = model_copy(input).sum()\r\n            loss_copy.backward()\r\n            optim_copy.step()\r\n            optim_copy.zero_grad()\r\n\r\n        for p_static, p_reference, p in zip(\r\n            params_copy, model_copy.parameters(), model.parameters()\r\n        ):\r\n            self.assertEqual(p_static, p)\r\n            self.assertNotEqual(p_reference, p)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_interpreter_default_args(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y=3.14159):\r\n                return x + y\r\n\r\n        model = Model()\r\n        gm = torch.fx.symbolic_trace(model)\r\n\r\n        interp = Interpreter(gm)\r\n        x = torch.randn(5, 3)\r\n        out = interp.run(x)\r\n        torch.testing.assert_close(out, x + 3.14159)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_delete_unused_submodules_leaf(self):\r\n        class SubModule(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(10, 10)\r\n                self.relu = torch.nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.submod = SubModule()\r\n\r\n            def forward(self, x):\r\n                x = self.submod(x)\r\n                return x\r\n\r\n        model = Model()\r\n\r\n        class MyCustomTracer(torch.fx.Tracer):\r\n            def is_leaf_module(self, m: torch.nn.Module, module_qualified_name : str) -> bool:\r\n                return module_qualified_name == \"submod\"\r\n\r\n        inputs = torch.randn(1, 10)\r\n        traced_graph = MyCustomTracer().trace(model)\r\n        gm2 = torch.fx.GraphModule(model, traced_graph)\r\n        gm2.delete_all_unused_submodules()\r\n        torch.testing.assert_close(gm2(inputs), model(inputs))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_interpreter_not_enough_args(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return x + y\r\n\r\n        model = Model()\r\n        gm = torch.fx.symbolic_trace(model)\r\n\r\n        interp = Interpreter(gm)\r\n        x = torch.randn(5, 3)\r\n        with self.assertRaisesRegex(RuntimeError,\r\n                                    'Expected positional argument for parameter y, but one was not passed in'):\r\n            out = interp.run(x)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_dynamo_aot_ts_onednn(self):\r\n        class Seq(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.layers = nn.Sequential(\r\n                    nn.Linear(10, 10),\r\n                    nn.ReLU(),\r\n                    nn.Linear(10, 10),\r\n                    nn.ReLU(),\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.layers(x)\r\n\r\n        mod = Seq()\r\n\r\n        import torch._dynamo\r\n        aot_mod = torch.compile(mod, backend=\"aot_ts\", fullgraph=True)\r\n\r\n        for _ in range(10):\r\n            with torch.jit.fuser(\"fuser3\"):\r\n                loss = aot_mod(torch.rand([10, 10])).sum()\r\n                loss.backward()\r\n\r\n        torch._dynamo.reset()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_conv_base(self, dim):\r\n        conv_module = {1: torch.nn.Conv1d, 2: torch.nn.Conv2d, 3: torch.nn.Conv3d}\r\n        input_shapes = {1: (224,), 2: (224, 224), 3: (55, 55, 55)}\r\n        options = itertools.product([True, False], [True, False], [1, 2], [1, 4])\r\n        for train, bias, dilation, groups in options:\r\n            N = torch.randint(3, 10, (1,)).item()\r\n            M = torch.randint(1, 3, (1,)).item() * groups\r\n            C = torch.randint(1, 3, (1,)).item() * groups\r\n            x_shape = (N, C) + input_shapes[dim]\r\n            x = torch.randn(x_shape, dtype=torch.float32)\r\n            conv = conv_module[dim](in_channels=C,\r\n                                    out_channels=M,\r\n                                    kernel_size=3,\r\n                                    stride=2,\r\n                                    padding=1,\r\n                                    dilation=dilation,\r\n                                    bias=bias,\r\n                                    groups=groups).float()\r\n            x1 = x.clone()\r\n            x2 = x.clone().to_mkldnn()\r\n            if not train:\r\n                mkldnn_conv = mkldnn_utils.to_mkldnn(copy.deepcopy(conv))\r\n            elif train and dim != 1:\r\n                # TODO: enable conv1d training.\r\n                x1.requires_grad_()\r\n                x2.requires_grad_()\r\n                mkldnn_conv = copy.deepcopy(conv)\r\n            with torch.backends.mkldnn.flags(enabled=False):\r\n                y_aten = conv(x1)\r\n                if train and dim != 1:\r\n                    loss1 = y_aten.sum()\r\n                    loss1.backward()\r\n            if not train or (train and dim != 1):\r\n                y_mkldnn = mkldnn_conv(x2).to_dense()\r\n                self.assertEqual(y_aten, y_mkldnn)\r\n            if not train:\r\n                self._test_serialization(mkldnn_conv, (x.to_mkldnn(),))\r\n                self._test_tracing(mkldnn_conv, (x.to_mkldnn(),))\r\n            elif dim != 1:\r\n                loss2 = y_mkldnn.sum()\r\n                loss2.backward()\r\n                self.assertTrue(x2.grad.is_mkldnn)\r\n                self.assertEqual(x1.grad, x2.grad.to_dense())\r\n                self.assertEqual(conv.weight.grad,\r\n                                 mkldnn_conv.weight.grad,\r\n                                 atol=1e-3,\r\n                                 rtol=1e-3)\r\n                if bias:\r\n                    self.assertEqual(conv.bias.grad, mkldnn_conv.bias.grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_batch_norm_train_base(self, dim, channels, input):\r\n        # TODO: support 3d batchnorm training.\r\n        bn_module = {2 : torch.nn.BatchNorm2d}\r\n        # TODO: support none affine.\r\n        options = itertools.product([True], [True, False])\r\n        for affine, track_running_stats in options:\r\n            bn = bn_module[dim](\r\n                num_features=channels,\r\n                affine=affine,\r\n                track_running_stats=track_running_stats).float().train(True)\r\n            mkldnn_bn = copy.deepcopy(bn)\r\n            x1 = input.clone().requires_grad_()\r\n            x2 = input.clone().to_mkldnn().requires_grad_()\r\n            y1 = bn(x1)\r\n            y2 = mkldnn_bn(x2).to_dense()\r\n            loss1 = y1.sum()\r\n            loss2 = y2.sum()\r\n            loss1.backward()\r\n            loss2.backward()\r\n            self.assertEqual(y1, y2)\r\n            self.assertEqual(x1.grad, x2.grad.to_dense())\r\n            self.assertEqual(bn.weight.grad, mkldnn_bn.weight.grad, rtol=1e-3, atol=1e-3)\r\n            if track_running_stats:\r\n                self.assertEqual(bn.running_mean, mkldnn_bn.running_mean)\r\n                self.assertEqual(bn.running_var, mkldnn_bn.running_var, rtol=1e-5, atol=1e-5)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_non_contiguous_tensors(self, device, dtype, module_info, training):\r\n        # Check modules work with non-contiguous tensors\r\n\r\n        module_cls = module_info.module_cls\r\n        module_inputs = module_info.module_inputs_func(module_info, device=device, dtype=dtype,\r\n                                                       requires_grad=True, training=training)\r\n\r\n        def _make_non_contiguous(obj):\r\n            def inner_make_non_contiguous(obj):\r\n                # Scalar tensors can not be made non-contiguous\r\n                if not isinstance(obj, torch.Tensor) or obj.dim() == 0:\r\n                    return obj\r\n\r\n                out = torch.repeat_interleave(obj, 2, dim=-1)\r\n                out = out[..., ::2].detach()\r\n                out.requires_grad = obj.requires_grad\r\n                return out\r\n            return self._traverse_obj(obj, inner_make_non_contiguous)\r\n\r\n        def _can_be_noncontiguous(obj):\r\n            if isinstance(obj, (tuple, list)):\r\n                return any(_can_be_noncontiguous(o) for o in obj)\r\n            elif isinstance(obj, dict):\r\n                return any(_can_be_noncontiguous(o) for o in obj.values())\r\n            # scalar tensors can not be non-contiguous\r\n            return isinstance(obj, torch.Tensor) and obj.dim() != 0\r\n\r\n        for module_input in module_inputs:\r\n            if module_input.forward_input is None:\r\n                continue\r\n\r\n            input_args, input_kwargs = module_input.forward_input.args, module_input.forward_input.kwargs\r\n            if not (_can_be_noncontiguous(input_args) or _can_be_noncontiguous(input_kwargs)):\r\n                continue\r\n\r\n            # === Instantiate the module. ===\r\n            args, kwargs = module_input.constructor_input.args, module_input.constructor_input.kwargs\r\n            m = module_cls(*args, **kwargs)\r\n            m.to(device).to(dtype)\r\n            m.train(training)\r\n\r\n            self._retain_grad((input_args, input_kwargs))\r\n\r\n            # === Forward with default input\r\n            with freeze_rng_state():\r\n                default_output = m(*input_args, **input_kwargs)\r\n                if isinstance(default_output, torch.Tensor):\r\n                    grad_output = default_output.clone().detach_().normal_()\r\n                    default_output.backward(grad_output, retain_graph=True)\r\n                else:\r\n                    grad_output = tuple(self._traverse_obj(o, lambda o: o.clone().detach_().normal_() if o.requires_grad else None)\r\n                                        for o in default_output)\r\n                    flattened_default_output = torch.utils._pytree.tree_leaves(default_output)\r\n                    flattened_grad_output = torch.utils._pytree.tree_leaves(grad_output)\r\n                    for o, g_o in zip(flattened_default_output, flattened_grad_output):\r\n                        if (o.requires_grad):\r\n                            o.backward(g_o, retain_graph=True)\r\n\r\n            default_input_args_grad, default_input_kwargs_grad = deepcopy(self._get_grads((input_args, input_kwargs)))\r\n            default_param_grad = deepcopy([p.grad for p in m.parameters()])\r\n\r\n            # === Construct non-contiguous tensors ===\r\n            nc_input_args, nc_input_kwargs = _make_non_contiguous((input_args, input_kwargs))\r\n            nc_grad_output = _make_non_contiguous(grad_output)\r\n\r\n            # === Compare results with non-contiguous and contiguous tensors ===\r\n            inputs = [(input_args, input_kwargs), (nc_input_args, nc_input_kwargs)]\r\n            grads = [grad_output, nc_grad_output]\r\n\r\n            for (in_args, in_kwargs), g_out in product(inputs, grads):\r\n                g_out_copy = deepcopy(g_out)\r\n                self._zero_grad((in_args, in_kwargs))\r\n                self._zero_grad(m.parameters())\r\n\r\n                with freeze_rng_state():\r\n                    out = m(*in_args, **in_kwargs)\r\n                    if isinstance(out, torch.Tensor):\r\n                        out.backward(g_out_copy, retain_graph=True)\r\n                    else:\r\n                        flattened_out = torch.utils._pytree.tree_leaves(out)\r\n                        flattened_g_out_copy = torch.utils._pytree.tree_leaves(g_out_copy)\r\n                        for o, g_o in zip(flattened_out, flattened_g_out_copy):\r\n                            if o.requires_grad:\r\n                                o.backward(g_o, retain_graph=True)\r\n\r\n                input_args_grad, input_kwargs_grad = self._get_grads((in_args, in_kwargs))\r\n                self.assertEqual(out, default_output)\r\n                self.assertEqual(input_args_grad, default_input_args_grad, atol=1e-4, rtol=0)\r\n                self.assertEqual(input_kwargs_grad, default_input_kwargs_grad, atol=1e-4, rtol=0)\r\n\r\n                param_grad = [p.grad for p in m.parameters()]\r\n                self.assertEqual(param_grad, default_param_grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_attr_module_constants(self):\r\n        class M2(torch.jit.ScriptModule):\r\n            def __init__(self, mod_list):\r\n                super().__init__()\r\n                self.mods = mod_list\r\n\r\n            @torch.jit.script_method\r\n            def forward(self, x):\r\n                return self.mods.forward(x)\r\n\r\n        with torch.jit.optimized_execution(False):\r\n            m = M2(nn.Sequential(nn.ReLU()))\r\n            self.assertExportImportModule(m, (torch.randn(2, 2),))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_ReplicationPad1d_large(self, device):\r\n        shapes = ([2, 65736, 4], [65736, 2, 4])\r\n        pl, pr = 3, 4\r\n        for shape in shapes:\r\n            x = torch.randn(shape, device=device, requires_grad=True)\r\n            model = torch.nn.ReplicationPad1d((pl, pr))\r\n\r\n            # forward\r\n            out = model(x)\r\n            self.assertEqual(out[:, :, pl : -pr], x)\r\n\r\n            left_padding = out[:, :, : pl]\r\n            self.assertEqual(left_padding, x[:, :, :1].expand_as(left_padding))\r\n            right_padding = out[:, :, -pr :]\r\n            self.assertEqual(right_padding, x[:, :, -1:].expand_as(right_padding))\r\n\r\n            # backward\r\n            g = torch.randn_like(out)\r\n            out.backward(g)\r\n            self.assertEqual(x.grad[:, :, 1 : -1], g[:, :, pl + 1 : -pr - 1])\r\n\r\n            self.assertEqual(x.grad[:, :, 0], g[:, :, : pl + 1].sum(-1))\r\n            self.assertEqual(x.grad[:, :, -1], g[:, :, -pr - 1:].sum(-1))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ReplicationPad2d_large(self, device):\r\n        shapes = ([2, 65736, 4, 4], [65736, 2, 4, 4])\r\n        pl, pr, pt, pb = 3, 4, 5, 6\r\n        for shape in shapes:\r\n            x = torch.randn(shape, device=device, requires_grad=True)\r\n            model = torch.nn.ReplicationPad2d((pl, pr, pt, pb))\r\n\r\n            # forward center, edge\r\n            out = model(x)\r\n            self.assertEqual(out[:, :, pt : -pb, pl : -pr], x)\r\n\r\n            left_padding = out[:, :, pt : -pb, : pl]\r\n            self.assertEqual(left_padding, x[:, :, :, :1].expand_as(left_padding))\r\n            right_padding = out[:, :, pt : -pb, -pr :]\r\n            self.assertEqual(right_padding, x[:, :, :, -1:].expand_as(right_padding))\r\n            top_padding = out[:, :, : pt, pl : -pr]\r\n            self.assertEqual(top_padding, x[:, :, :1, :].expand_as(top_padding))\r\n            bottom_padding = out[:, :, -pb : , pl : -pr]\r\n            self.assertEqual(bottom_padding, x[:, :, -1:, :].expand_as(bottom_padding))\r\n\r\n            # forward corner\r\n            tl_padding = out[:, :, : pt + 1, : pl + 1]\r\n            self.assertEqual(tl_padding, x[:, :, :1, :1].expand_as(tl_padding))\r\n            tr_padding = out[:, :, : pt + 1, -pr - 1:]\r\n            self.assertEqual(tr_padding, x[:, :, :1, -1:].expand_as(tr_padding))\r\n            bl_padding = out[:, :, -pb - 1:, : pl + 1]\r\n            self.assertEqual(bl_padding, x[:, :, -1:, :1].expand_as(bl_padding))\r\n            br_padding = out[:, :, -pb - 1:, -pr - 1:]\r\n            self.assertEqual(br_padding, x[:, :, -1:, -1:].expand_as(br_padding))\r\n\r\n            # backward center, edge\r\n            g = torch.randn_like(out)\r\n            out.backward(g)\r\n            self.assertEqual(x.grad[:, :, 1:-1, 1:-1], g[:, :, pt + 1 : -pb - 1, pl + 1 : -pr - 1])\r\n\r\n            self.assertEqual(x.grad[:, :, 1:-1, 0], g[:, :, pt + 1 : -pb - 1, : pl + 1].sum(-1))\r\n            self.assertEqual(x.grad[:, :, 1:-1, -1], g[:, :, pt + 1 : -pb - 1, -pr - 1 :].sum(-1))\r\n            self.assertEqual(x.grad[:, :, 0, 1:-1], g[:, :, : pt + 1, pl + 1 : -pr - 1].sum(-2))\r\n            self.assertEqual(x.grad[:, :, -1, 1:-1], g[:, :, -pb - 1 :, pl + 1 : -pr - 1].sum(-2))\r\n\r\n            # backward corner\r\n            self.assertEqual(x.grad[:, :, 0, 0], g[:, :, : pt + 1, : pl + 1].sum((-2, -1)))\r\n            self.assertEqual(x.grad[:, :, 0, -1], g[:, :, : pt + 1, -pr - 1 :].sum((-2, -1)))\r\n            self.assertEqual(x.grad[:, :, -1, 0], g[:, :, -pb - 1 :, : pl + 1].sum((-2, -1)))\r\n            self.assertEqual(x.grad[:, :, -1, -1], g[:, :, -pb - 1 :, -pr - 1 :].sum((-2, -1)))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ReplicationPad3d_large(self, device):\r\n        shapes = ([1, 65736, 2, 2, 2], [65736, 1, 2, 2, 2])\r\n        pl, pr, pt, pbt, pf, pbk = 3, 4, 5, 6, 7, 8\r\n\r\n        for shape in shapes:\r\n            x = torch.randn(shape, device=device, requires_grad=True)\r\n            model = torch.nn.ReplicationPad3d((pl, pr, pt, pbt, pf, pbk))\r\n\r\n            # forward center\r\n            out = model(x)\r\n            self.assertEqual(out[:, :, pf : -pbk, pt : -pbt, pl : -pr], x)\r\n\r\n            # backward center\r\n            g = torch.randn_like(out)\r\n            out.backward(g)\r\n            self.assertEqual(x.grad[:, :, 1:-1, 1:-1, 1:-1], g[:, :, pf + 1 : -pbk - 1, pt + 1 : -pbt - 1, pl + 1 : -pr - 1])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_named_modules(self):\r\n        class Net(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.l1 = l\r\n                self.l2 = l\r\n                self.param = torch.empty(3, 5)\r\n                self.block = block\r\n        l = nn.Linear(10, 20)\r\n        l1 = nn.Linear(10, 20)\r\n        l2 = nn.Linear(10, 20)\r\n        block = nn.Sequential()\r\n        block.add_module('linear1', l1)\r\n        block.add_module('linear2', l2)\r\n        n = Net()\r\n        s = nn.Sequential(n, n)\r\n        self.assertEqual(list(s.named_modules()), [('', s), ('0', n), ('0.l1', l),\r\n                                                   ('0.block', block), ('0.block.linear1', l1),\r\n                                                   ('0.block.linear2', l2)])\r\n        # test the option to not remove duplicate module instances\r\n        self.assertEqual(list(s.named_modules(remove_duplicate=False)), [\r\n            ('', s), ('0', n), ('0.l1', l), ('0.l2', l),\r\n            ('0.block', block), ('0.block.linear1', l1),\r\n            ('0.block.linear2', l2),\r\n            ('1', n), ('1.l1', l), ('1.l2', l),\r\n            ('1.block', block), ('1.block.linear1', l1),\r\n            ('1.block.linear2', l2)])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_tensor_lr(self, device, dtype, optim_info):\r\n        optim_cls = optim_info.optim_cls\r\n\r\n        # Skip differentiable testing for now, see https://github.com/pytorch/pytorch/issues/116490\r\n        all_optim_inputs = _get_optim_inputs_including_global_cliquey_kwargs(\r\n            device, dtype, optim_info, skip=(\"differentiable\",)\r\n        )\r\n        for optim_input in all_optim_inputs:\r\n            weight = Parameter(torch.randn((10, 5), device=device, dtype=dtype))\r\n            weight_c = weight.detach().clone().requires_grad_(True)\r\n            bias = Parameter(torch.randn((10), device=device, dtype=dtype))\r\n            bias_c = bias.detach().clone().requires_grad_(True)\r\n            inpt = torch.randn(5, device=device, dtype=dtype)\r\n\r\n            kwargs = optim_input.kwargs\r\n            if \"lr\" in kwargs:\r\n                del kwargs[\"lr\"]\r\n\r\n            kwargs[\"lr\"] = 1.0 if optim_info.step_requires_closure else 1e-3\r\n            optimizer_r = optim_cls([weight, bias], **kwargs)\r\n\r\n            try:\r\n                kwargs[\"lr\"] = torch.tensor(kwargs[\"lr\"])\r\n                optimizer = optim_cls([weight_c, bias_c], **kwargs)\r\n            except ValueError as e:\r\n                self.assertRegex(str(e), \".*lr as a Tensor is not supported.*\")\r\n                continue\r\n\r\n            def closure(optim, w, b, i):\r\n                optim.zero_grad()\r\n                loss = (w.mv(i) + b).pow(2).sum()\r\n                loss.backward()\r\n                if optim_info.only_supports_sparse_grads:\r\n                    # For this test, we naively convert the Tensor layout, which we know does\r\n                    # NOT represent the expected use case for optims like SparseAdam!\r\n                    w.grad = w.grad.to_sparse()\r\n                    b.grad = b.grad.to_sparse()\r\n                return loss\r\n\r\n            for _ in range(5):\r\n                if optim_info.step_requires_closure:\r\n                    optimizer_r.step(\r\n                        functools.partial(closure, optimizer_r, weight, bias, inpt)\r\n                    )\r\n                    optimizer.step(\r\n                        functools.partial(closure, optimizer, weight_c, bias_c, inpt)\r\n                    )\r\n                else:\r\n                    closure(optimizer_r, weight, bias, inpt)\r\n                    closure(optimizer, weight_c, bias_c, inpt)\r\n\r\n                self.assertEqual(weight, weight_c)\r\n                self.assertEqual(bias, bias_c)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _compare_between(\r\n        self, inputs, models, optimizers, assert_eq_kwargs=None, assert_step_dtype=None\r\n    ):\r\n        # why 7? iteration 7 is where we start to see differences for RAdam\r\n        # params interacting with the small eps value, because that's right\r\n        # after rho_t becomes greater than 5 in step 6.\r\n        if assert_eq_kwargs is None:\r\n            assert_eq_kwargs = {}\r\n        kIterations = 7\r\n        tracker = TensorTracker(assert_eq_kwargs)\r\n        for i in range(kIterations):\r\n            state, updated_params = [], []\r\n            if not isinstance(inputs, list):\r\n                inputs = [inputs, inputs]\r\n            for input, model, optimizer in zip(inputs, models, optimizers):\r\n                optimizer.zero_grad()\r\n\r\n                if i == 3:\r\n                    # Freeze a layer to test if the step of this layer in 'fused' or 'foreach'\r\n                    # is same as the step in 'forloop'.\r\n                    model[2].requires_grad_(False)\r\n                if i == 5:\r\n                    # Unfreeze the layer after 2 iters.\r\n                    model[2].requires_grad_(True)\r\n\r\n                # Test that step behaves as expected (a no-op) when grads are set to None\r\n                if i != 2:\r\n                    output = model(input)\r\n                    loss = output.sum()\r\n                    loss.backward()\r\n\r\n                optimizer.step()\r\n                state.append(optimizer.state)\r\n                updated_params.append(model.parameters())\r\n\r\n            og_state, new_state = state\r\n            for og_p, new_p in zip(updated_params[0], updated_params[1]):\r\n                tracker.add(og_p)\r\n                tracker.pop_check_set(new_p, self)\r\n\r\n                # check that optimizer states are the same\r\n                og_p_state = og_state[og_p]\r\n                new_p_state = new_state[new_p]\r\n                if assert_step_dtype is not None:\r\n                    if torch.is_tensor(og_p_state.get(\"step\", None)):\r\n                        self.assertEqual(og_p_state[\"step\"].dtype, assert_step_dtype)\r\n                    if torch.is_tensor(new_p_state.get(\"step\", None)):\r\n                        self.assertEqual(new_p_state[\"step\"].dtype, assert_step_dtype)\r\n                for k in og_p_state:\r\n                    tracker.add(og_p_state[k])\r\n                    tracker.pop_check_set(new_p_state[k], self)\r\n\r\n            self.assertTrue(tracker.all_popped())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_can_load_older_state_dict(self, device, dtype, optim_info):\r\n        optim_cls = optim_info.optim_cls\r\n\r\n        # Skip differentiable testing for now, see https://github.com/pytorch/pytorch/issues/116490\r\n        all_optim_inputs = _get_optim_inputs_including_global_cliquey_kwargs(\r\n            device, dtype, optim_info, skip=(\"differentiable\",)\r\n        )\r\n        for optim_input in all_optim_inputs:\r\n            torch.manual_seed(1)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Conv2d(4, 2, 1, stride=2),\r\n                torch.nn.BatchNorm2d(2, eps=1e-05, momentum=0.1),\r\n            )\r\n            model.to(dtype=dtype, device=device)\r\n            input = torch.rand(1, 4, 16, 16, device=device, dtype=dtype)\r\n            optimizer = optim_cls(model.parameters(), **optim_input.kwargs)\r\n\r\n            def fwd_bwd(optim, mod, i):\r\n                optim.zero_grad()\r\n                loss = mod(i).sum()\r\n                loss.backward()\r\n                return loss\r\n\r\n            for _ in range(3):\r\n                if optim_info.step_requires_closure:\r\n                    optimizer.step(functools.partial(fwd_bwd, optimizer, model, input))\r\n                else:\r\n                    fwd_bwd(optimizer, model, input)\r\n                    optimizer.step()\r\n\r\n            # old_state_dict has all new flags del'd\r\n            old_state_dict = deepcopy(optimizer.state_dict())\r\n            old_state_dict_pg = old_state_dict[\"param_groups\"]\r\n            for group in old_state_dict_pg:\r\n                for flag in optim_info.not_og_supported_flags:\r\n                    if flag in group:\r\n                        del group[flag]\r\n\r\n            optimizer.load_state_dict(old_state_dict)\r\n\r\n            # Make sure we can still step\r\n            if optim_info.step_requires_closure:\r\n                optimizer.step(functools.partial(fwd_bwd, optimizer, model, input))\r\n            else:\r\n                fwd_bwd(optimizer, model, input)\r\n                optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_can_load_from_to_named_state_dict(\r\n        self, device, dtype, optim_info, is_named_optim0, is_named_optim1\r\n    ):\r\n        optim_cls = optim_info.optim_cls\r\n\r\n        # Skip differentiable testing for now, see https://github.com/pytorch/pytorch/issues/116490\r\n        all_optim_inputs = _get_optim_inputs_including_global_cliquey_kwargs(\r\n            device, dtype, optim_info, skip=(\"differentiable\",)\r\n        )\r\n        for optim_input in all_optim_inputs:\r\n            torch.manual_seed(1)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Conv2d(4, 2, 1, stride=2),\r\n                torch.nn.BatchNorm2d(2, eps=1e-05, momentum=0.1),\r\n            )\r\n            model.to(dtype=dtype, device=device)\r\n            input = torch.rand(1, 4, 16, 16, device=device, dtype=dtype)\r\n\r\n            def fwd_bwd(optim, mod, i):\r\n                optim.zero_grad()\r\n                loss = mod(i).sum()\r\n                loss.backward()\r\n                return loss\r\n\r\n            # test for parameters, named_parameters, and 2 groups:\r\n            params_to_optimizer = (\r\n                model.named_parameters() if is_named_optim0 else model.parameters()\r\n            )\r\n            optimizer = optim_cls(params_to_optimizer, **optim_input.kwargs)\r\n\r\n            for _ in range(3):\r\n                if optim_info.step_requires_closure:\r\n                    optimizer.step(functools.partial(fwd_bwd, optimizer, model, input))\r\n                else:\r\n                    fwd_bwd(optimizer, model, input)\r\n                    optimizer.step()\r\n\r\n            # old_state_dict has all new flags del'd\r\n            old_state_dict = deepcopy(optimizer.state_dict())\r\n\r\n            params_to_optimizer2 = (\r\n                model.named_parameters() if is_named_optim1 else model.parameters()\r\n            )\r\n            optimizer2 = optim_cls(params_to_optimizer2, **optim_input.kwargs)\r\n            optimizer2.load_state_dict(old_state_dict)\r\n\r\n            # Make sure we can still step\r\n            if optim_info.step_requires_closure:\r\n                optimizer2.step(functools.partial(fwd_bwd, optimizer2, model, input))\r\n            else:\r\n                fwd_bwd(optimizer2, model, input)\r\n                optimizer2.step()\r\n\r\n            # Make sure that param_names are preserved when provided to at least one of the optimizers\r\n            if is_named_optim0 or is_named_optim1:\r\n                self.assertEqual(\r\n                    optimizer2.state_dict()[\"param_groups\"][0][\"param_names\"],\r\n                    [\"0.weight\", \"0.bias\", \"1.weight\", \"1.bias\"],\r\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_mlp_contiguous_relu_compile(backend, dense_input_shape):\r\n        \"\"\"\r\n        Test nn.Linear + .contiguous() + nn.ReLU with SparseSemiStructuredTensor + torch.compile\r\n        We expect:\r\n            (1) The sparse tensor subclass should turn nn.Linear into `aten._structured_sparse_addmm` + `aten.contiguous()`\r\n            (2) Inductor should fuse the .contiguous() call into the relu\r\n        \"\"\"\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = nn.Linear(128, 128)\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = x.contiguous()\r\n                return torch.nn.functional.relu(x)\r\n\r\n        input = torch.rand(dense_input_shape, device=\"cuda\").half()\r\n        model = Model().eval().cuda().half()\r\n        mod_linear = model.linear\r\n        m, n = mod_linear.weight.shape\r\n        mask = torch.Tensor([1, 0, 0, 1]).tile((m, n // 4)).bool().cuda()\r\n        # set masked weight\r\n        mod_linear.weight = nn.Parameter(mod_linear.weight * mask)\r\n\r\n        dense_result = model(input)\r\n        mod_linear.weight = nn.Parameter(SEMI_STRUCTURED_SUPPORTED_BACKENDS[backend].from_dense(mod_linear.weight))\r\n        sparse_result = model(input)\r\n\r\n        model = torch.compile(model, backend=\"inductor\", fullgraph=True)\r\n        sparse_compile_result = model(input)\r\n\r\n        # test that sparse_compile_result and dense_result are numerically close\r\n        torch.testing.assert_close(dense_result, sparse_compile_result, rtol=1e-3, atol=1e-3)\r\n        # assert sparse and sparse_compile have the same strides,\r\n        # as meta registrations may return contiguous tensors when the output is transposed\r\n        # https://github.com/pytorch/pytorch/pull/114477\r\n        assert sparse_result.stride() == sparse_compile_result.stride()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grad_scaling_clipping(self, device):\r\n        device = torch.device(device)\r\n\r\n        def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            max_norm = 0.2  # A reasonable value that actually has an effect, based on printouts of grads\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm * scaler.get_scale())\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()\r\n\r\n        self._run_scaling_case(device.type, run, unskipped=3, skipped=1, atol=1e-5)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_grad_scaling_clipping_separate_unscale(self, device):\r\n        device = torch.device(device)\r\n\r\n        def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            max_norm = 0.2  # A reasonable value that actually has an effect, based on printouts of grads\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.unscale_(optimizer)\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm, error_if_nonfinite=False)\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()\r\n\r\n        self._run_scaling_case(device.type, run, unskipped=3, skipped=1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_grad_scaling_penalty(self, device):\r\n        device = torch.device(device)\r\n\r\n        def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n\r\n                if try_scaling_api:\r\n                    grad_params = torch.autograd.grad(scaler.scale(loss),\r\n                                                      model.parameters(), create_graph=True)\r\n                    inv_scale = 1. / scaler.get_scale()\r\n                    grad_params = [p * inv_scale for p in grad_params]\r\n                else:\r\n                    grad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n\r\n                grad_norm = 0\r\n                for grad in grad_params:\r\n                    grad_norm += grad.pow(2).sum()\r\n                grad_norm = grad_norm.sqrt()\r\n                loss = loss + grad_norm\r\n\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()\r\n\r\n        self._run_scaling_case(device.type, run, unskipped=3, skipped=1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_grad_scaling_accumulation(self, device):\r\n        device = torch.device(device)\r\n\r\n        def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            iters_to_accumulate = 2\r\n            for i, (input, target) in enumerate(data):\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n                loss = loss / iters_to_accumulate\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                else:\r\n                    loss.backward()\r\n                if (i + 1) % iters_to_accumulate == 0:\r\n                    if try_scaling_api:\r\n                        scaler.step(optimizer)\r\n                        scaler.update()\r\n                        optimizer.zero_grad()\r\n                    else:\r\n                        optimizer.step()\r\n                        optimizer.zero_grad()\r\n\r\n        self._run_scaling_case(device.type, run, unskipped=2, skipped=0)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                with torch.autocast(device_type=device, dtype=torch.half, enabled=try_scaling_api):\r\n                    output = model(input)\r\n                    loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        with torch.no_grad():\r\n                            model[1].weight.grad.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                    if try_pickle:\r\n                        scaler = pickle.loads(pickle.dumps(scaler))\r\n                else:\r\n                    loss.backward()\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()\r\n            return scaler",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            max_norm = 0.2  # A reasonable value that actually has an effect, based on printouts of grads\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm * scaler.get_scale())\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            max_norm = 0.2  # A reasonable value that actually has an effect, based on printouts of grads\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.unscale_(optimizer)\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm, error_if_nonfinite=False)\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = loss_fn(output, target)\r\n\r\n                if try_scaling_api:\r\n                    grad_params = torch.autograd.grad(scaler.scale(loss),\r\n                                                      model.parameters(), create_graph=True)\r\n                    inv_scale = 1. / scaler.get_scale()\r\n                    grad_params = [p * inv_scale for p in grad_params]\r\n                else:\r\n                    grad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n\r\n                grad_norm = 0\r\n                for grad in grad_params:\r\n                    grad_norm += grad.pow(2).sum()\r\n                grad_norm = grad_norm.sqrt()\r\n                loss = loss + grad_norm\r\n\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        model[1].weight.grad.data.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_activation_sparsifier(self):\r\n        \"\"\"Simulates the workflow of the activation sparsifier, starting from object creation\r\n        till squash_mask().\r\n        The idea is to check that everything works as expected while in the workflow.\r\n        \"\"\"\r\n\r\n        # defining aggregate, reduce and mask functions\r\n        def agg_fn(x, y):\r\n            return x + y\r\n\r\n        def reduce_fn(x):\r\n            return torch.mean(x, dim=0)\r\n\r\n        def _vanilla_norm_sparsifier(data, sparsity_level):\r\n            r\"\"\"Similar to data norm sparsifier but block_shape = (1,1).\r\n            Simply, flatten the data, sort it and mask out the values less than threshold\r\n            \"\"\"\r\n            data_norm = torch.abs(data).flatten()\r\n            _, sorted_idx = torch.sort(data_norm)\r\n            threshold_idx = round(sparsity_level * len(sorted_idx))\r\n            sorted_idx = sorted_idx[:threshold_idx]\r\n\r\n            mask = torch.ones_like(data_norm)\r\n            mask.scatter_(dim=0, index=sorted_idx, value=0)\r\n            mask = mask.reshape(data.shape)\r\n\r\n            return mask\r\n\r\n        # Creating default function and sparse configs\r\n        # default sparse_config\r\n        sparse_config = {\"sparsity_level\": 0.5}\r\n\r\n        defaults = {\"aggregate_fn\": agg_fn, \"reduce_fn\": reduce_fn}\r\n\r\n        # simulate the workflow\r\n        # STEP 1: make data and activation sparsifier object\r\n        model = Model()  # create model\r\n        activation_sparsifier = ActivationSparsifier(model, **defaults, **sparse_config)\r\n\r\n        # Test Constructor\r\n        self._check_constructor(activation_sparsifier, model, defaults, sparse_config)\r\n\r\n        # STEP 2: Register some layers\r\n        register_layer1_args = {\r\n            \"layer\": model.conv1,\r\n            \"mask_fn\": _vanilla_norm_sparsifier,\r\n        }\r\n        sparse_config_layer1 = {\"sparsity_level\": 0.3}\r\n\r\n        register_layer2_args = {\r\n            \"layer\": model.linear1,\r\n            \"features\": [0, 10, 234],\r\n            \"feature_dim\": 1,\r\n            \"mask_fn\": _vanilla_norm_sparsifier,\r\n        }\r\n        sparse_config_layer2 = {\"sparsity_level\": 0.1}\r\n\r\n        register_layer3_args = {\r\n            \"layer\": model.identity1,\r\n            \"mask_fn\": _vanilla_norm_sparsifier,\r\n        }\r\n        sparse_config_layer3 = {\"sparsity_level\": 0.3}\r\n\r\n        register_layer4_args = {\r\n            \"layer\": model.identity2,\r\n            \"features\": [0, 10, 20],\r\n            \"feature_dim\": 1,\r\n            \"mask_fn\": _vanilla_norm_sparsifier,\r\n        }\r\n        sparse_config_layer4 = {\"sparsity_level\": 0.1}\r\n\r\n        layer_args_list = [\r\n            (register_layer1_args, sparse_config_layer1),\r\n            (register_layer2_args, sparse_config_layer2),\r\n        ]\r\n        layer_args_list += [\r\n            (register_layer3_args, sparse_config_layer3),\r\n            (register_layer4_args, sparse_config_layer4),\r\n        ]\r\n\r\n        # Registering..\r\n        for layer_args in layer_args_list:\r\n            layer_arg, sparse_config_layer = layer_args\r\n            activation_sparsifier.register_layer(**layer_arg, **sparse_config_layer)\r\n\r\n        # check if things are registered correctly\r\n        self._check_register_layer(\r\n            activation_sparsifier, defaults, sparse_config, layer_args_list\r\n        )\r\n\r\n        # check state_dict after registering and before model forward\r\n        self._check_state_dict(activation_sparsifier)\r\n\r\n        # check if forward pre hooks actually work\r\n        # some dummy data\r\n        data_list = []\r\n        num_data_points = 5\r\n        for _ in range(0, num_data_points):\r\n            rand_data = torch.randn(16, 1, 28, 28)\r\n            activation_sparsifier.model(rand_data)\r\n            data_list.append(rand_data)\r\n\r\n        data_agg_actual = self._check_pre_forward_hook(activation_sparsifier, data_list)\r\n        # check state_dict() before step()\r\n        self._check_state_dict(activation_sparsifier)\r\n\r\n        # STEP 3: sparsifier step\r\n        activation_sparsifier.step()\r\n\r\n        # check state_dict() after step() and before squash_mask()\r\n        self._check_state_dict(activation_sparsifier)\r\n\r\n        # self.check_step()\r\n        self._check_step(activation_sparsifier, data_agg_actual)\r\n\r\n        # STEP 4: squash mask\r\n        activation_sparsifier.squash_mask()\r\n\r\n        self._check_squash_mask(activation_sparsifier, data_list[0])\r\n\r\n        # check state_dict() after squash_mask()\r\n        self._check_state_dict(activation_sparsifier)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ptq_sparsify_first(self):\r\n        \"\"\"The expectation is post_training_sparse_quantize function\r\n        1. Takes in a model\r\n        2. Sparsifies the embeddings\r\n        3. Quantize the embeddings\r\n\r\n        This unit test checks that\r\n        1. Embeddings and EmbeddingBags are sparsified to the right sparsity levels\r\n        2. Embeddings and EmbeddingBags are quantized\r\n        3. Linear modules are not quantized\r\n        \"\"\"\r\n        model = Model()\r\n\r\n        sparse_config = {\"sparsity_level\": 0.80, \"sparse_block_shape\": (1, 1)}\r\n        select_embeddings = [model.embbag1, model.emb1]\r\n        post_training_sparse_quantize(\r\n            model,\r\n            data_sparsifier_class=DataNormSparsifier,\r\n            sparsify_first=True,\r\n            select_embeddings=select_embeddings,\r\n            **sparse_config,\r\n        )\r\n\r\n        assert type(model.emb1) == torch.ao.nn.quantized.modules.embedding_ops.Embedding\r\n        assert (\r\n            type(model.embbag1)\r\n            == torch.ao.nn.quantized.modules.embedding_ops.EmbeddingBag\r\n        )\r\n        assert type(model.emb_seq[0] == nn.Embedding)\r\n        assert type(model.emb_seq[1] == nn.EmbeddingBag)\r\n        assert type(model.linear1) == nn.Linear\r\n        assert type(model.linear2) == nn.Linear\r\n\r\n        dequant_emb1 = torch.dequantize(model.emb1.weight())\r\n        dequant_embbag1 = torch.dequantize(model.embbag1.weight())\r\n\r\n        threshold = 1e-2\r\n\r\n        sl_emb1 = (torch.abs(dequant_emb1) < threshold).float().mean()\r\n        sl_embbag1 = (torch.abs(dequant_embbag1) < threshold).float().mean()\r\n\r\n        assert abs(sl_emb1 - 0.80) <= 0.05  # +- 5% leeway\r\n        assert abs(sl_embbag1 - 0.80) <= 0.05",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ptq_quantize_first(self):\r\n        \"\"\"The expectation is post_training_sparse_quantize function\r\n        1. Takes in a model\r\n        2. Quantize the embeddings\r\n        3. Sparsifies the embeddings\r\n\r\n        This unit test checks that\r\n        1. Embeddings and EmbeddingBags are sparsified to the right sparsity levels\r\n        2. Embeddings and EmbeddingBags are quantized\r\n        3. Linear modules are not quantized\r\n        \"\"\"\r\n        model = Model()\r\n\r\n        sparse_config = {\"sparsity_level\": 0.8, \"sparse_block_shape\": (1, 1)}\r\n        post_training_sparse_quantize(\r\n            model, DataNormSparsifier, sparsify_first=False, **sparse_config\r\n        )\r\n\r\n        assert type(model.emb1) == torch.ao.nn.quantized.modules.embedding_ops.Embedding\r\n        assert (\r\n            type(model.embbag1)\r\n            == torch.ao.nn.quantized.modules.embedding_ops.EmbeddingBag\r\n        )\r\n        assert type(\r\n            model.emb_seq[0] == torch.ao.nn.quantized.modules.embedding_ops.Embedding\r\n        )\r\n        assert type(\r\n            model.emb_seq[1] == torch.ao.nn.quantized.modules.embedding_ops.EmbeddingBag\r\n        )\r\n        assert type(model.linear1) == nn.Linear  # not quantized\r\n        assert type(model.linear2) == nn.Linear  # not quantized\r\n\r\n        dequant_emb1 = torch.dequantize(model.emb1.weight())\r\n        dequant_embbag1 = torch.dequantize(model.embbag1.weight())\r\n        dequant_emb_seq_0 = torch.dequantize(model.emb_seq[0].weight())\r\n        dequant_emb_seq_1 = torch.dequantize(model.emb_seq[1].weight())\r\n\r\n        # higher threshold as quantization occurs before sparsity\r\n        threshold = (\r\n            1  # zero points seem to have higher magnitude with sparsity occuring after\r\n        )\r\n\r\n        sl_emb1 = (torch.abs(dequant_emb1) < threshold).float().mean()\r\n        sl_embbag1 = (torch.abs(dequant_embbag1) < threshold).float().mean()\r\n        sl_emb_seq_0 = (torch.abs(dequant_emb_seq_0) < threshold).float().mean()\r\n        sl_emb_seq_1 = (torch.abs(dequant_emb_seq_1) < threshold).float().mean()\r\n\r\n        assert abs(sl_emb1 - 0.80) <= 0.05  # +- 5% leeway\r\n        assert abs(sl_embbag1 - 0.80) <= 0.05  # +- 5% leeway\r\n        assert abs(sl_emb_seq_0 - 0.80) <= 0.05  # +- 5% leeway\r\n        assert abs(sl_emb_seq_1 - 0.80) <= 0.05",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparsity_levels(self):\r\n        nearliness_levels = list(range(-1, 100))\r\n        model = nn.Sequential()\r\n\r\n        p = re.compile(r\"[-\\.\\s]\")\r\n        for nearliness in nearliness_levels:\r\n            sparsifier = NearlyDiagonalSparsifier(nearliness=1)\r\n            layer_name = f\"{nearliness}\"\r\n            layer_name = p.sub(\"_\", layer_name)\r\n\r\n            layer = nn.Linear(32, 32, bias=False)\r\n            layer.weight = nn.Parameter(torch.ones(32, 32))\r\n            width, height = layer.weight.shape\r\n            model.add_module(layer_name, layer)\r\n            config = {\"tensor_fqn\": layer_name + \".weight\", \"nearliness\": nearliness}\r\n\r\n            sparsifier.prepare(model, [config])\r\n            # should raise a ValueError when nearliness arg is illegal\r\n            if (nearliness > 0 and nearliness % 2 == 0) or (\r\n                nearliness // 2 >= min(width, height)\r\n            ):\r\n                with self.assertRaises(ValueError):\r\n                    sparsifier.step()\r\n            else:\r\n                sparsifier.step()\r\n                sparsifier.squash_mask()\r\n                model.eval()\r\n\r\n                layer = getattr(model, layer_name)\r\n                # verify that mask created corresponds to the nearliness\r\n                self._verify_nearliness(layer.weight, nearliness)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparsity_levels(self):\r\n        sparsity_levels = [-1.0, 0.0, 0.5, 1.0, 2.0]\r\n        sparse_block_shapes = [(1, 1), (1, 4), (2, 2), (4, 1)]\r\n        zeros_per_blocks = [0, 1, 2, 3, 4]\r\n\r\n        testcases = itertools.tee(\r\n            itertools.product(sparsity_levels, sparse_block_shapes, zeros_per_blocks)\r\n        )\r\n        # Create a config and model with all the testcases\r\n        model = nn.Sequential()\r\n        sparsifier = WeightNormSparsifier()\r\n\r\n        sparsity_per_layer_config = []\r\n        p = re.compile(r\"[-\\.\\s]\")\r\n        for sl, sbs, zpb in testcases[0]:\r\n            # Make sure the number of zeros is not > values in a block\r\n            if zpb > sbs[0] * sbs[1]:\r\n                continue\r\n            layer_name = f\"{sl}_{sbs}_{zpb}\"\r\n            layer_name = p.sub(\"_\", layer_name)\r\n\r\n            layer = nn.Linear(12, 12, bias=False)\r\n            layer.weight = nn.Parameter(torch.ones(12, 12))\r\n            model.add_module(layer_name, layer)\r\n            config = {\r\n                \"tensor_fqn\": layer_name + \".weight\",\r\n                \"sparsity_level\": sl,\r\n                \"sparse_block_shape\": sbs,\r\n                \"zeros_per_block\": zpb,\r\n            }\r\n            sparsity_per_layer_config.append(config)\r\n\r\n        sparsifier.prepare(model, sparsity_per_layer_config)\r\n        sparsifier.step()\r\n        sparsifier.squash_mask()\r\n        model.eval()\r\n\r\n        for sl, sbs, zpb in testcases[1]:\r\n            if zpb > sbs[0] * sbs[1]:\r\n                continue\r\n            layer_name = f\"{sl}_{sbs}_{zpb}\"\r\n            layer_name = p.sub(\"_\", layer_name)\r\n            layer = getattr(model, layer_name)\r\n\r\n            # Level of sparsity is achieved\r\n            sparse_mask = (layer.weight == 0).float()\r\n            if zpb == 0:\r\n                assert sparse_mask.mean() == 0\r\n            else:\r\n                # Ratio of individual zeros in the tensor\r\n                true_sl = min(max(sl, 0.0), 1.0)\r\n                true_sl = true_sl * zpb / sbs[0] / sbs[1]\r\n                assert sparse_mask.mean() == true_sl",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def main():\r\n    data = torch.randn(10, 50).cuda()\r\n    model = Model().cuda()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\r\n    for _ in range(10):\r\n        optimizer.zero_grad()\r\n        loss = model(data)\r\n        loss.backward()\r\n        optimizer.step()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def run(optimizer_name, iterations, sample_every):\r\n    torch.manual_seed(0)\r\n    model = torch.nn.Sequential(\r\n        torch.nn.Linear(2, 3),\r\n        torch.nn.Sigmoid(),\r\n        torch.nn.Linear(3, 1),\r\n        torch.nn.Sigmoid(),\r\n    )\r\n    model = model.to(torch.float64).apply(weight_init)\r\n\r\n    optimizer = OPTIMIZERS[optimizer_name](model.parameters())\r\n\r\n    input = torch.tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=torch.float64)\r\n\r\n    values = []\r\n    for i in range(iterations):\r\n        optimizer.zero_grad()\r\n\r\n        output = model.forward(input)\r\n        loss = output.sum()\r\n        loss.backward()\r\n\r\n        def closure():\r\n            return torch.tensor([10.0])\r\n\r\n        optimizer.step(closure)\r\n\r\n        if i % sample_every == 0:\r\n            values.append(\r\n                [p.clone().flatten().data.numpy() for p in model.parameters()]\r\n            )\r\n\r\n    return values",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def setup(self):\r\n        class Model(torch.jit.ScriptModule):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.dropout = torch.nn.Dropout(0.1)\r\n\r\n            @torch.jit.script_method\r\n            def forward(self, x):\r\n                x = self.dropout(x)\r\n                return x\r\n\r\n        model = Model()\r\n        model = model.train()\r\n        model.save(self.path)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def setUp(self):\r\n        # Load the library containing the custom backend.\r\n        self.library_path = get_custom_backend_library_path()\r\n        torch.ops.load_library(self.library_path)\r\n        # Create an instance of the test Module and lower it for\r\n        # the custom backend.\r\n        self.model = to_custom_backend(torch.jit.script(Model()))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def setup(self):\r\n        if not torch.cuda.is_available():\r\n            return\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self):\r\n                s = torch.cuda.Stream()\r\n                a = torch.rand(3, 4, device=\"cuda\")\r\n                b = torch.rand(3, 4, device=\"cuda\")\r\n\r\n                with torch.cuda.stream(s):\r\n                    is_stream_s = (\r\n                        torch.cuda.current_stream(s.device_index()).id() == s.id()\r\n                    )\r\n                    c = torch.cat((a, b), 0).to(\"cuda\")\r\n                s.synchronize()\r\n                return is_stream_s, a, b, c\r\n\r\n        model = Model()\r\n\r\n        # Script the model and save\r\n        script_model = torch.jit.script(model)\r\n        torch.jit.save(script_model, self.path)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_calling_custom_op_inside_script_module(self):\r\n        model = Model()\r\n        output = model.forward(torch.ones(5))\r\n        self.assertTrue(output.allclose(torch.ones(5) + 1))",
        "labels": [
            "PyTorch Call Method Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def main():\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Serialize a script module with custom ops\"\r\n    )\r\n    parser.add_argument(\"--export-script-module-to\", required=True)\r\n    options = parser.parse_args()\r\n\r\n    torch.ops.load_library(get_custom_op_library_path())\r\n\r\n    model = Model()\r\n    model.save(options.export_script_module_to)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_saving_and_loading_script_module_with_custom_op(self):\r\n        model = Model()\r\n        # Ideally we would like to not have to manually delete the file, but NamedTemporaryFile\r\n        # opens the file, and it cannot be opened multiple times in Windows. To support Windows,\r\n        # close the file after creation and try to remove it manually.\r\n        file = tempfile.NamedTemporaryFile(delete=False)\r\n        try:\r\n            file.close()\r\n            model.save(file.name)\r\n            loaded = torch.jit.load(file.name)\r\n        finally:\r\n            os.unlink(file.name)\r\n\r\n        output = loaded.forward(torch.ones(5))\r\n        self.assertTrue(output.allclose(torch.ones(5) + 1))",
        "labels": [
            "PyTorch Call Method Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ignored_output(self):\r\n        \"\"\"\r\n        Test that the output of a model can be ignored and that there is no\r\n        implicit requirement that `backward` gets called.\r\n        \"\"\"\r\n        process_group = self._get_process_group()\r\n\r\n        class IgnoredOutput(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        model = DistributedDataParallel(\r\n            IgnoredOutput().float(),\r\n            process_group=process_group,\r\n        )\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])\r\n\r\n        # Run a few iterations where we ignore the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            del output\r\n\r\n        # Run a few iterations where we use the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ignored_output_with_unused_parameters(self):\r\n        \"\"\"\r\n        Test that the output of a model can be ignored and that there is no\r\n        implicit requirement that `backward` gets called, if not all model\r\n        parameters participated in computing the model output.\r\n        \"\"\"\r\n        process_group = self._get_process_group()\r\n\r\n        class IgnoredOutputWithUnusedParameters(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.fc3 = nn.Linear(4, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        model = DistributedDataParallel(\r\n            IgnoredOutputWithUnusedParameters().float(),\r\n            process_group=process_group,\r\n            find_unused_parameters=True,\r\n        )\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])\r\n\r\n        # Run a few iterations where we ignore the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            del output\r\n\r\n        # Run a few iterations where we use the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_save_load_checkpoint(self):\r\n        dist.init_process_group(\r\n            \"gloo\",\r\n            init_method=f\"file://{self.file_name}\",\r\n            world_size=self.world_size,\r\n            rank=self.rank,\r\n        )\r\n\r\n        class TestModel(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        def train_loop(model, optimizer, iterations):\r\n            for _ in range(iterations):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = criterion(output, target)\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n        device_id = gpus_for_rank(self.world_size)[self.rank][0]\r\n\r\n        model_withload = TestModel().float().to(device_id)\r\n        model_withoutload = TestModel().float().to(device_id)\r\n\r\n        ddp_withload = DistributedDataParallel(\r\n            model_withload,\r\n            device_ids=[device_id],\r\n        )\r\n        ddp_withoutload = DistributedDataParallel(\r\n            model_withoutload,\r\n            device_ids=[device_id],\r\n        )\r\n\r\n        # ensure that all the three models start with the same set of parameters. By default they are randomized on construction\r\n        for p in ddp_withload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n        for p in model_withload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n        for p in ddp_withoutload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n\r\n        optimizer_withload = torch.optim.SGD(ddp_withload.parameters(), lr=0.001)\r\n        optimizer_non_ddp_withload = torch.optim.SGD(\r\n            model_withload.parameters(), lr=0.001\r\n        )\r\n        optimizer_withoutload = torch.optim.SGD(ddp_withoutload.parameters(), lr=0.001)\r\n\r\n        input = torch.rand([batch_size, 2], dtype=torch.float).to(device_id)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)]).to(\r\n            device_id\r\n        )\r\n\r\n        # run the model for 6 iterations, with a checkpoint in the middle\r\n        train_loop(ddp_withload, optimizer_withload, 3)\r\n\r\n        # zero out parameters of both DDP and non-DDP models and reload them from the DDP state dict\r\n        checkpoint_path = tempfile.gettempdir() + \"/model.checkpoint\"\r\n        if self.rank == 0:\r\n            torch.save(ddp_withload.state_dict(), checkpoint_path)\r\n\r\n        dist.barrier()\r\n        map_location = {\"cuda:0\": f\"cuda:{self.rank:d}\"}\r\n        ddp_state_dict = torch.load(checkpoint_path, map_location=map_location)\r\n\r\n        for model in [ddp_withload, model_withload]:\r\n            for p in model.parameters():\r\n                with torch.no_grad():\r\n                    p.zero_()\r\n        ddp_withload.load_state_dict(ddp_state_dict)\r\n        # the non-DDP model needs to first remove the prefix of \"module.\" from the DDP state dict\r\n        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\r\n            ddp_state_dict, \"module.\"\r\n        )\r\n        model_withload.load_state_dict(ddp_state_dict)\r\n\r\n        train_loop(ddp_withload, optimizer_withload, 3)\r\n        train_loop(model_withload, optimizer_non_ddp_withload, 3)\r\n\r\n        # re-run the model with the same inputs for 6 iterations with no checkpoint\r\n        train_loop(ddp_withoutload, optimizer_withoutload, 6)\r\n\r\n        for p_withload, p_withoutload, p_non_ddp_withload in zip(\r\n            ddp_withload.parameters(),\r\n            ddp_withoutload.parameters(),\r\n            model_withload.parameters(),\r\n        ):\r\n            self.assertEqual(p_withload, p_withoutload)\r\n            self.assertEqual(p_non_ddp_withload, p_withoutload)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ignored_output(self):\r\n        \"\"\"\r\n        Test that the output of a model can be ignored and that there is no\r\n        implicit requirement that `backward` gets called.\r\n        \"\"\"\r\n        process_group = self._get_process_group()\r\n\r\n        class IgnoredOutput(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        model = DistributedDataParallel(\r\n            IgnoredOutput().float(),\r\n            process_group=process_group,\r\n        )\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])\r\n\r\n        # Run a few iterations where we ignore the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            del output\r\n\r\n        # Run a few iterations where we use the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ignored_output_with_unused_parameters(self):\r\n        \"\"\"\r\n        Test that the output of a model can be ignored and that there is no\r\n        implicit requirement that `backward` gets called, if not all model\r\n        parameters participated in computing the model output.\r\n        \"\"\"\r\n        process_group = self._get_process_group()\r\n\r\n        class IgnoredOutputWithUnusedParameters(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.fc3 = nn.Linear(4, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        model = DistributedDataParallel(\r\n            IgnoredOutputWithUnusedParameters().float(),\r\n            process_group=process_group,\r\n            find_unused_parameters=True,\r\n        )\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])\r\n\r\n        # Run a few iterations where we ignore the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            del output\r\n\r\n        # Run a few iterations where we use the output.\r\n        for _ in range(4):\r\n            output = model(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_save_load_checkpoint(self):\r\n        dist.init_process_group(\r\n            \"ucc\",\r\n            init_method=f\"file://{self.file_name}\",\r\n            world_size=self.world_size,\r\n            rank=self.rank,\r\n        )\r\n\r\n        class TestModel(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        def train_loop(model, optimizer, iterations):\r\n            for _ in range(iterations):\r\n                optimizer.zero_grad()\r\n                output = model(input)\r\n                loss = criterion(output, target)\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n        device_id = gpus_for_rank(self.world_size)[self.rank][0]\r\n\r\n        model_withload = TestModel().float().to(device_id)\r\n        model_withoutload = TestModel().float().to(device_id)\r\n\r\n        ddp_withload = DistributedDataParallel(\r\n            model_withload,\r\n            device_ids=[device_id],\r\n        )\r\n        ddp_withoutload = DistributedDataParallel(\r\n            model_withoutload,\r\n            device_ids=[device_id],\r\n        )\r\n\r\n        # ensure that all the three models start with the same set of parameters. By default they are randomized on construction\r\n        for p in ddp_withload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n        for p in model_withload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n        for p in ddp_withoutload.parameters():\r\n            with torch.no_grad():\r\n                p.zero_()\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n\r\n        optimizer_withload = torch.optim.SGD(ddp_withload.parameters(), lr=0.001)\r\n        optimizer_non_ddp_withload = torch.optim.SGD(\r\n            model_withload.parameters(), lr=0.001\r\n        )\r\n        optimizer_withoutload = torch.optim.SGD(ddp_withoutload.parameters(), lr=0.001)\r\n\r\n        input = torch.rand([batch_size, 2], dtype=torch.float).to(device_id)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)]).to(\r\n            device_id\r\n        )\r\n\r\n        # run the model for 6 iterations, with a checkpoint in the middle\r\n        train_loop(ddp_withload, optimizer_withload, 3)\r\n\r\n        # zero out parameters of both DDP and non-DDP models and reload them from the DDP state dict\r\n        checkpoint_path = tempfile.gettempdir() + \"/model.checkpoint\"\r\n        if self.rank == 0:\r\n            torch.save(ddp_withload.state_dict(), checkpoint_path)\r\n\r\n        dist.barrier()\r\n        map_location = {\"cuda:0\": f\"cuda:{self.rank:d}\"}\r\n        ddp_state_dict = torch.load(checkpoint_path, map_location=map_location)\r\n\r\n        for model in [ddp_withload, model_withload]:\r\n            for p in model.parameters():\r\n                with torch.no_grad():\r\n                    p.zero_()\r\n        ddp_withload.load_state_dict(ddp_state_dict)\r\n        # the non-DDP model needs to first remove the prefix of \"module.\" from the DDP state dict\r\n        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\r\n            ddp_state_dict, \"module.\"\r\n        )\r\n        model_withload.load_state_dict(ddp_state_dict)\r\n\r\n        train_loop(ddp_withload, optimizer_withload, 3)\r\n        train_loop(model_withload, optimizer_non_ddp_withload, 3)\r\n\r\n        # re-run the model with the same inputs for 6 iterations with no checkpoint\r\n        train_loop(ddp_withoutload, optimizer_withoutload, 6)\r\n\r\n        for p_withload, p_withoutload, p_non_ddp_withload in zip(\r\n            ddp_withload.parameters(),\r\n            ddp_withoutload.parameters(),\r\n            model_withload.parameters(),\r\n        ):\r\n            self.assertEqual(p_withload, p_withoutload)\r\n            self.assertEqual(p_non_ddp_withload, p_withoutload)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autocast(self):\r\n        class Model(torch.nn.Linear):\r\n            def __init__(self) -> None:\r\n                super().__init__(8, 8)\r\n\r\n            @torch.autocast(device_type=\"cuda\")\r\n            def forward(self, input):\r\n                return super().forward(input)\r\n\r\n        model = dp.DataParallel(Model().cuda().to(dtype=torch.float32))\r\n        input = torch.randn((8, 8), dtype=torch.float32, device=\"cuda\")\r\n        self.assertTrue(model(input).dtype is torch.float16)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_data_parallel_model_no_refcycles(self):\r\n        # Python 2.7 will create reference cycles with the following\r\n        # Module on multiple GPUs, but Python 3 shouldn't unless\r\n        # there are refcycles on the PyTorch side (or the defined module)\r\n        import gc\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = nn.Linear(1, 1)\r\n\r\n            def forward(self, x):\r\n                return self.linear(x)\r\n\r\n        gc.collect()\r\n        model = nn.DataParallel(Model().cuda())\r\n        data = torch.randn(1, device=\"cuda\")\r\n        model(data)\r\n\r\n        refcycles = gc.collect()\r\n        self.assertEqual(refcycles, 0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ddp_optimizer_inductor_strides_dont_specialize(self):\r\n        class Model(nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.fc_0 = nn.Linear(768, 768)\r\n                self.fc_1 = nn.Linear(768, 768)\r\n\r\n            def forward(self, x):\r\n                x = self.fc_0(x)\r\n                x = self.fc_1(x)\r\n                return x\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        inp = torch.randn((16, 18, 768))\r\n        inp2 = torch.randn((16, 20, 768))\r\n\r\n        torch._dynamo.mark_dynamic(inp, 1)\r\n        torch._dynamo.mark_dynamic(inp2, 1)\r\n\r\n        torch._dynamo.utils.clear_compilation_metrics()\r\n        torch._dynamo.reset()\r\n        try:\r\n            DDP._active_ddp_module = model\r\n            opt_model = torch.compile(model)\r\n            self.assertEqual(0, len(torch._dynamo.utils.get_compilation_metrics()))\r\n            opt_model(inp)\r\n            compile_count_before = len(torch._dynamo.utils.get_compilation_metrics())\r\n            opt_model(inp2)\r\n            compile_count_after = len(torch._dynamo.utils.get_compilation_metrics())\r\n            # no recompiles\r\n            self.assertEqual(compile_count_before, compile_count_after)\r\n        finally:\r\n            DDP._active_ddp_module = None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unbacked_symbol_splitting_direct(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.weight1 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight2 = nn.Parameter(torch.randn(512, 512))\r\n\r\n            def forward(self, x, y):\r\n                u0, _ = y.tolist()\r\n                x = torch.cat([x, x])\r\n                y = x @ self.weight1\r\n                z = (x + y @ self.weight2) * u0\r\n                return z\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        opt_model = torch.compile(dynamic=True)(model)\r\n        opt_model(torch.randn(20, 512), torch.tensor([12, 13]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unbacked_symbol_splitting_indirect(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.weight1 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight2 = nn.Parameter(torch.randn(512, 512))\r\n\r\n            def forward(self, x, y):\r\n                u0, _ = y.tolist()\r\n                a = torch.ones(u0)\r\n                x = torch.cat([x, x])\r\n                y = x @ self.weight1\r\n                z = (x + y @ self.weight2) * a.sum()\r\n                return z\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        opt_model = torch.compile(dynamic=True)(model)\r\n        opt_model(torch.randn(20, 512), torch.tensor([12, 13]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unbacked_symbol_splitting_torture_multi(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.weight1 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight2 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight3 = nn.Parameter(torch.randn(512, 512))\r\n\r\n            def forward(self, x, y):\r\n                # partition one (contains the u0 def)\r\n                u0, _ = y.tolist()\r\n                x = torch.cat([x, x])\r\n                y1 = x @ self.weight1\r\n                # partition two (contains the variable)\r\n                y2 = y1 @ self.weight2\r\n                a = torch.ones(u0)\r\n                # partition three\r\n                z = (x + y2 @ self.weight3) * a.sum()\r\n                return z\r\n\r\n        model = Model()\r\n        model = FakeDDP(model, bucket_cap_mb=1)\r\n\r\n        opt_model = torch.compile(dynamic=True)(model)\r\n        opt_model(torch.randn(20, 512), torch.tensor([12, 13]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unbacked_symbol_splitting_no_binding(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.weight1 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight2 = nn.Parameter(torch.randn(512, 512))\r\n\r\n            def forward(self, x, y):\r\n                nz = y.nonzero()\r\n                x = torch.cat([x, x])\r\n                y = x @ self.weight1\r\n                z = (x + y @ self.weight2) * (nz + 1).sum()\r\n                return z\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        opt_model = torch.compile(dynamic=True)(model)\r\n        opt_model(torch.randn(20, 512), torch.tensor([0.0, 12.0, 0.0, 11.0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_call_method_forward(self):\r\n        class Model(nn.Module):\r\n            def __init__(\r\n                self,\r\n            ):\r\n                super().__init__()\r\n                layers = []\r\n                for _ in range(2):\r\n                    layer = nn.ModuleList(\r\n                        [\r\n                            nn.LayerNorm(96),\r\n                            nn.MultiheadAttention(\r\n                                embed_dim=96, num_heads=4, batch_first=True\r\n                            ),\r\n                        ]\r\n                    )\r\n                    layers.append(layer)\r\n                self.layers = nn.ModuleList(layers)\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                # x: [Batch, Freq, Time, Feature]\r\n                B, F, T, H = x.shape\r\n                for m in self.layers:\r\n                    x = x.reshape(B * F, T, H)\r\n                    x = m[0](x)\r\n                    x, _ = m[1].forward(x, x, x)\r\n                    x = x.reshape(B, F, T, H)\r\n                return x\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n        opt_model = torch.compile(model)\r\n        opt_model(torch.randn(2, 129, 100, 96))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_hf_bert_fsdp(self):\r\n        def apply_fsdp(model, wrap_policy):\r\n            model = FSDP(\r\n                copy.deepcopy(model), auto_wrap_policy=wrap_policy, use_orig_params=True\r\n            )\r\n            return model\r\n\r\n        with _dynamo_dist_per_rank_init(self.rank, self.world_size):\r\n            for wrap_policy, test_instance in (\r\n                (None, \"FSDP without recursive wrapping\"),\r\n            ):\r\n                print(f\"Running hf_bert test for {test_instance}\")\r\n                model, inputs = get_hf_bert(self.rank)\r\n                reset_rng_state()\r\n                eager_model = apply_fsdp(model, wrap_policy)\r\n                correct_outputs = eager_model(**inputs)\r\n                correct_loss = correct_outputs.loss\r\n                correct_loss.backward()\r\n\r\n                reset_rng_state()\r\n                opt_model = apply_fsdp(model, wrap_policy)\r\n                opt_model = torch.compile(opt_model, backend=\"inductor\")\r\n                opt_outputs = opt_model(**inputs)\r\n                opt_loss = opt_outputs.loss\r\n                opt_loss.backward()\r\n\r\n                inputs_flat = [inputs[k] for k in inputs]\r\n                correct_results = collect_results(\r\n                    eager_model, correct_outputs.logits, correct_loss, inputs_flat\r\n                )\r\n                opt_results = collect_results(\r\n                    opt_model, opt_outputs.logits, opt_loss, inputs_flat\r\n                )\r\n                self.assertTrue(same(correct_results, opt_results))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_hf_bert_fsdp_activation_checkpointing(self):\r\n        from transformers.models.bert.modeling_bert import BertLayer\r\n\r\n        with _dynamo_dist_per_rank_init(self.rank, self.world_size):\r\n            for wrap_policy, test_instance in (\r\n                (\r\n                    functools.partial(\r\n                        transformer_auto_wrap_policy, transformer_layer_cls=(BertLayer,)\r\n                    ),\r\n                    \"FSDP with recursive wrapping BertLayer instances\",\r\n                ),\r\n            ):\r\n                print(\r\n                    f\"Running hf_bert_activation_checkpointing test for {test_instance}\"\r\n                )\r\n                model, inputs = get_hf_bert(self.rank)\r\n                check_fn = lambda submodule: isinstance(  # noqa: E731\r\n                    submodule, BertLayer\r\n                )\r\n                reset_rng_state()\r\n                eager_model = apply_fsdp_with_checkpointing(\r\n                    model, wrap_policy, check_fn\r\n                )\r\n                correct_outputs = eager_model(**inputs)\r\n                correct_loss = correct_outputs.loss\r\n                correct_loss.backward()\r\n\r\n                reset_rng_state()\r\n                opt_model = apply_fsdp_with_checkpointing(model, wrap_policy, check_fn)\r\n                opt_model = torch.compile(opt_model, backend=\"inductor\")\r\n                opt_outputs = opt_model(**inputs)\r\n                opt_loss = opt_outputs.loss\r\n                opt_loss.backward()\r\n\r\n                inputs_flat = [inputs[k] for k in inputs]\r\n                correct_results = collect_results(\r\n                    eager_model, correct_outputs.logits, correct_loss, inputs_flat\r\n                )\r\n                opt_results = collect_results(\r\n                    opt_model, opt_outputs.logits, opt_loss, inputs_flat\r\n                )\r\n                self.assertTrue(same(correct_results, opt_results))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                # x: [Batch, Freq, Time, Feature]\r\n                B, F, T, H = x.shape\r\n                for m in self.layers:\r\n                    x = x.reshape(B * F, T, H)\r\n                    x = m[0](x)\r\n                    x, _ = m[1].forward(x, x, x)\r\n                    x = x.reshape(B, F, T, H)\r\n                return x",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_fsdp_tp_fake_e2e(self):\r\n        world_size = 4\r\n        tp_size = 2\r\n\r\n        store = dist.HashStore()\r\n        dist.init_process_group(\r\n            backend=\"fake\", rank=0, world_size=world_size, store=store\r\n        )\r\n\r\n        device_mesh = DeviceMesh(\"cuda\", torch.arange(0, world_size).view(-1, tp_size))\r\n        device_mesh = init_device_mesh(\r\n            \"cuda\", (world_size // tp_size, tp_size), mesh_dim_names=[\"dp\", \"tp\"]\r\n        )\r\n\r\n        sequence_parallelize_plan = {\r\n            \"net1\": ColwiseParallel(input_layouts=Shard(0)),\r\n            \"net2\": RowwiseParallel(output_layouts=Shard(0)),\r\n        }\r\n        pairwise_parallelize_plan = {\r\n            \"net1\": ColwiseParallel(),\r\n            \"net2\": RowwiseParallel(),\r\n        }\r\n        for parallel_plan in [sequence_parallelize_plan, pairwise_parallelize_plan]:\r\n            my_module = parallelize_module(\r\n                MLPModule(device=\"cuda\"),\r\n                device_mesh[\"tp\"],\r\n                parallel_plan,\r\n            )\r\n\r\n            sharded_module = FSDP(\r\n                my_module, use_orig_params=True, device_mesh=device_mesh[\"dp\"]\r\n            )\r\n            optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\r\n\r\n            for i in range(10):\r\n                dp_rank = dist.get_rank()\r\n                torch.manual_seed(i + dp_rank)\r\n                input = torch.randn(20, 10).cuda(dist.get_rank())\r\n                x = sharded_module(input)\r\n                loss = x.sum()\r\n                loss.backward()\r\n                optim.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_issue90375(self):\r\n        class Model(nn.Module):\r\n            def forward(self):\r\n                return torch.randn(3) * torch.randn(3)\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        opt_model = torch.compile(model, backend=\"aot_eager\")\r\n        opt_model()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_symbol_splitting(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.weight1 = nn.Parameter(torch.randn(512, 512))\r\n                self.weight2 = nn.Parameter(torch.randn(512, 512))\r\n\r\n            def forward(self, x):\r\n                x = torch.cat([x, x])\r\n                y = x @ self.weight1\r\n                z = x + y @ self.weight2\r\n                return z\r\n\r\n        model = Model()\r\n        model = FakeDDP(model)\r\n\r\n        opt_model = torch.compile(dynamic=True)(model)\r\n        opt_model(torch.randn(20, 512))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_allgather_output_buffer_reuse(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self, *args, **kwargs) -> None:\r\n                super().__init__(*args, **kwargs)\r\n                self.emb = torch.nn.Embedding(4, 4)\r\n\r\n            def forward(self, x, world_size, tag, ranks, group_size):\r\n                y = self.emb(x)\r\n                last_dim = y.dim() - 1\r\n                res = _functional_collectives.all_gather_tensor(y, 0, ranks, tag)\r\n                out = torch.cat(torch.chunk(res, world_size, dim=0), dim=last_dim)\r\n                return out\r\n\r\n        with _dynamo_dist_per_rank_init(self.rank, self.world_size):\r\n            model = Model().cuda()\r\n            model_compiled = torch.compile(model)\r\n            inp = torch.tensor([[2, 1, 3, 0]], dtype=torch.long, device=\"cuda\")\r\n            out = model_compiled(inp, self.world_size, **self.get_world_trs())\r\n            correct = model(inp, self.world_size, **self.get_world_trs())\r\n            self.assertTrue(same(out, correct))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_allgather_contiguous_input(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self, *args, **kwargs) -> None:\r\n                super().__init__(*args, **kwargs)\r\n                self.emb = torch.nn.Embedding(4, 4)\r\n\r\n            def forward(self, x, world_size, tag, ranks, group_size):\r\n                y = self.emb(x)\r\n                last_dim = y.dim() - 1\r\n                y = y.transpose_(0, last_dim).contiguous()\r\n                _functional_collectives.all_gather_tensor(y, 0, ranks, tag)\r\n                out = y.transpose_(0, last_dim).contiguous()\r\n                return out\r\n\r\n        with _dynamo_dist_per_rank_init(self.rank, self.world_size):\r\n            model = Model().cuda()\r\n            model_compiled = torch.compile(model)\r\n            inp = torch.tensor([[2, 1, 3, 0]], dtype=torch.long, device=\"cuda\")\r\n            out = model_compiled(inp, self.world_size, **self.get_world_trs())\r\n            correct = model(inp, self.world_size, **self.get_world_trs())\r\n            self.assertTrue(same(out, correct))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _train(model, optim, train_steps=1):\r\n    torch.manual_seed(0)\r\n    loss = None\r\n\r\n    train_state = TestTrainState()\r\n\r\n    for _ in range(train_steps):\r\n        loss = model(model.get_input()).sum()\r\n        loss.backward()\r\n\r\n        # We usually sync the loss across dp ranks in real training.\r\n        # This is just simulating for testing purpose.\r\n        train_state.step += 1\r\n        train_state.current_loss = torch.rand(1).item()\r\n        train_state.losses.append(train_state.current_loss)\r\n\r\n        optim.step()\r\n        optim.zero_grad()\r\n\r\n    return loss, train_state",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def finetune(self, pretrain_dir: str, finetune_dir: str) -> None:\r\n        device_mesh = init_device_mesh(self.device_type, (self.world_size,))\r\n\r\n        model = FineTuningModel().cuda()\r\n        # TODO: make the parallelism more complicated, e.g., using 2D + DDP.\r\n        model = FSDP(model, use_orig_params=True, device_mesh=device_mesh)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n\r\n        # Simulate that the fine tuning restart after 3 iterations\r\n        for i in range(2):\r\n            # Load pretrain submodules checkpoint\r\n            pretrain_state_dict = get_model_state_dict(\r\n                model,\r\n                submodules={model.pretrain},\r\n                options=StateDictOptions(keep_submodule_prefixes=False),\r\n            )\r\n            dist_cp.load(\r\n                {\"model\": pretrain_state_dict},\r\n                storage_reader=dist_cp.FileSystemReader(pretrain_dir),\r\n            )\r\n            set_model_state_dict(\r\n                model,\r\n                model_state_dict={model.pretrain: pretrain_state_dict},\r\n                options=StateDictOptions(strict=False),\r\n            )\r\n\r\n            try:\r\n                # Load training submodules checkpoint\r\n                model_state_dict, optim_state_dict = get_state_dict(\r\n                    model,\r\n                    optimizers=optim,\r\n                    options=StateDictOptions(ignore_frozen_params=True),\r\n                )\r\n                dist_cp.load_state_dict(\r\n                    {\"model\": model_state_dict, \"optim\": optim_state_dict},\r\n                    storage_reader=dist_cp.FileSystemReader(pretrain_dir),\r\n                )\r\n                set_state_dict(\r\n                    model,\r\n                    optimizers=optim,\r\n                    model_state_dict=model_state_dict,\r\n                    optim_state_dict=optim_state_dict,\r\n                    options=StateDictOptions(strict=False),\r\n                )\r\n            except KeyError:\r\n                # If this is the first round of the fine tuning, then nothing is saved.\r\n                # If this is the restart of the fine tuning, then checkpoint should exit.\r\n                self.assertEqual(i, 0)\r\n\r\n            # Training\r\n            for _ in range(3):\r\n                batch = torch.rand(32, DIM, device=\"cuda\")\r\n                loss = model(batch).sum()\r\n                loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n\r\n            # Save state_dict\r\n            model_state_dict, optim_state_dict = get_state_dict(\r\n                model,\r\n                optimizers=optim,\r\n                options=StateDictOptions(ignore_frozen_params=True),\r\n            )\r\n            saved_state_dict = {\"model\": model_state_dict, \"optim\": optim_state_dict}\r\n            dist_cp.save(\r\n                state_dict=saved_state_dict,\r\n                storage_writer=dist_cp.FileSystemWriter(finetune_dir),\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def pretrain(self, pretrain_dir: str) -> None:\r\n        device_mesh = init_device_mesh(self.device_type, (self.world_size,))\r\n\r\n        model = PreTrainedModel().cuda()\r\n        model = FSDP(model, device_mesh=device_mesh)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n\r\n        # Training\r\n        for _ in range(3):\r\n            batch = torch.rand(32, DIM, device=\"cuda\")\r\n            loss = model(batch).sum()\r\n            loss.backward()\r\n            optim.step()\r\n            optim.zero_grad()\r\n\r\n        # Save state_dict\r\n        model_state_dict, optim_state_dict = get_state_dict(model, optimizers=optim)\r\n        saved_state_dict = {\"model\": model_state_dict, \"optim\": optim_state_dict}\r\n        dist_cp.save(\r\n            state_dict=saved_state_dict,\r\n            storage_writer=dist_cp.FileSystemWriter(pretrain_dir),\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_apply_activation_checkpointing(self):\r\n        \"\"\"\r\n        Ensures that `apply_activation_checkpointing` can be used\r\n        to swap modules for their checkpoint-wrapped counterparts given\r\n        a model.\r\n        \"\"\"\r\n\r\n        class LinearWithBatchNorm(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin = nn.Linear(10, 10)\r\n                self.bn = nn.BatchNorm1d(10)\r\n                self.nested_linear = nn.Sequential(nn.Linear(10, 10))\r\n\r\n            def forward(self, x):\r\n                return self.bn(self.nested_linear(self.lin(x)))\r\n\r\n        class MyModel(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.seq = nn.Sequential(\r\n                    LinearWithBatchNorm(), LinearWithBatchNorm(), LinearWithBatchNorm()\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.seq(x)\r\n\r\n        def check_fn(l):\r\n            return isinstance(l, nn.Linear)\r\n\r\n        n_linear = None\r\n\r\n        for i, wrapper in enumerate(\r\n            [\r\n                partial(checkpoint_wrapper, checkpoint_impl=CheckpointImpl.REENTRANT),\r\n                partial(\r\n                    checkpoint_wrapper, checkpoint_impl=CheckpointImpl.NO_REENTRANT\r\n                ),\r\n                offload_wrapper,\r\n            ]\r\n        ):\r\n            model = MyModel()\r\n            if n_linear is None:\r\n                n_linear = sum(\r\n                    1 if isinstance(x, nn.Linear) else 0 for x in model.modules()\r\n                )\r\n\r\n            with self.subTest(wrapper=wrapper):\r\n                if i != 0:\r\n                    apply_activation_checkpointing(\r\n                        model, checkpoint_wrapper_fn=wrapper, check_fn=check_fn\r\n                    )\r\n                else:\r\n                    apply_activation_checkpointing(\r\n                        model,\r\n                        checkpoint_wrapper_fn=wrapper,\r\n                        auto_wrap_policy=ModuleWrapPolicy({nn.Linear}),\r\n                    )\r\n                n_linear_wrapped = sum(\r\n                    1 if isinstance(x, nn.Linear) else 0 for x in model.modules()\r\n                )\r\n                n_checkpointed = sum(\r\n                    1 if isinstance(x, (CheckpointWrapper, OffloadWrapper)) else 0\r\n                    for x in model.modules()\r\n                )\r\n                self.assertEqual(n_checkpointed, n_linear_wrapped)\r\n                self.assertEqual(n_linear, n_linear_wrapped)\r\n                for j in range(3):\r\n                    self.assertTrue(\r\n                        isinstance(\r\n                            model.seq[j].lin, (CheckpointWrapper, OffloadWrapper)\r\n                        )\r\n                    )\r\n                    self.assertTrue(\r\n                        isinstance(\r\n                            model.seq[j].nested_linear[0],\r\n                            (CheckpointWrapper, OffloadWrapper),\r\n                        )\r\n                    )\r\n\r\n                inp = torch.randn(4, 10, requires_grad=True)\r\n                for i in range(6):\r\n                    # Kwarg input\r\n                    loss = model(x=inp).sum()\r\n                    self.assertTrue(loss.requires_grad)\r\n                    loss.backward()\r\n                    # ensure checkpointed part of model has gradients\r\n                    for j in range(3):\r\n                        weight_lin = model.seq[j].lin._checkpoint_wrapped_module.weight\r\n                        bias_lin = model.seq[j].lin._checkpoint_wrapped_module.bias\r\n                        weight_nested_lin = (\r\n                            model.seq[j]\r\n                            .nested_linear[0]\r\n                            ._checkpoint_wrapped_module.weight\r\n                        )\r\n                        bias_nested_lin = (\r\n                            model.seq[j]\r\n                            .nested_linear[0]\r\n                            ._checkpoint_wrapped_module.bias\r\n                        )\r\n                        for param in [\r\n                            weight_lin,\r\n                            bias_lin,\r\n                            weight_nested_lin,\r\n                            bias_nested_lin,\r\n                        ]:\r\n                            self.assertTrue(param.requires_grad)\r\n                            self.assertFalse(param.grad is None)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_checkpoint_fsdp_wrapping(\r\n        self,\r\n        cpu_offload: CPUOffload,\r\n        offload_activations: bool,\r\n        use_orig_params: bool,\r\n    ):\r\n        # Test checkpoint(FSDP(layer1), FSDP(layer2), ....)\r\n        if offload_activations:\r\n            wrapper_to_use = offload_wrapper\r\n        else:\r\n            wrapper_to_use = checkpoint_wrapper\r\n        fsdp_kwargs = {\"cpu_offload\": cpu_offload, \"use_orig_params\": use_orig_params}\r\n        ckpt_sequential_wrapped_fsdp = wrapper_to_use(\r\n            TestFSDPCheckpoint.SequentialModule(\r\n                wrap_fsdp=True,\r\n                **fsdp_kwargs,\r\n            ),\r\n        )\r\n        # Test FSDP(checkpoint(layer1)), FSDP(checkpoint(layer2)), ....\r\n        inner_ckpt = TestFSDPCheckpoint.SequentialModule(\r\n            checkpoint_layer=True,\r\n            offload_activations=offload_activations,\r\n            wrap_fsdp=True,\r\n            **fsdp_kwargs,\r\n        )\r\n        baseline = TestFSDPCheckpoint.SequentialModule(\r\n            wrap_fsdp=True,\r\n            **fsdp_kwargs,\r\n        )\r\n        # note that reentrant-based checkpointing requires inputs to have grad\r\n        # flag set.\r\n        inp = torch.randn(10, 3, device=device_type.type, requires_grad=True)\r\n        global _save_on_cpu_called\r\n        models = [ckpt_sequential_wrapped_fsdp, inner_ckpt, baseline]\r\n        with patch_save_on_cpu(get_patched_save_on_cpu()):\r\n            for i in range(2):\r\n                losses = []\r\n                outputs = []\r\n                for m in models:\r\n                    check_offload = m != baseline and i == 0 and offload_activations\r\n                    if check_offload:\r\n                        self.assertFalse(_save_on_cpu_called)\r\n                    out = m(inp)\r\n                    if check_offload:\r\n                        self.assertTrue(_save_on_cpu_called)\r\n                        _save_on_cpu_called = False\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n                    losses.append(loss)\r\n                    outputs.append(out)\r\n                self._verify_parity(losses, outputs, models)\r\n        dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_basic_checkpoint_end_to_end(\r\n        self,\r\n        cpu_offload: CPUOffload,\r\n        offload_activations: bool,\r\n        use_orig_params: bool,\r\n    ):\r\n        fsdp_kwargs = {\"cpu_offload\": cpu_offload, \"use_orig_params\": use_orig_params}\r\n        global _save_on_cpu_called\r\n        with patch_save_on_cpu(get_patched_save_on_cpu()):\r\n            seq = TestFSDPCheckpoint.SequentialModule().to(device_type.type)\r\n            # Runs FSDP with no checkpointing\r\n            fsdp_only_seq = FSDP(deepcopy(seq), **fsdp_kwargs)\r\n            # Runs checkpoint-wrapped FSDP\r\n            if offload_activations:\r\n                wrapper_to_use = offload_wrapper\r\n            else:\r\n                wrapper_to_use = checkpoint_wrapper\r\n            checkpointed_fsdp = wrapper_to_use(\r\n                FSDP(deepcopy(seq), **fsdp_kwargs),\r\n            )\r\n            # Runs FSDP-wrapped checkpointed module\r\n            fsdp_wrapped_checkpoint = FSDP(\r\n                wrapper_to_use(deepcopy(seq)),\r\n                **fsdp_kwargs,\r\n            )\r\n            # Runs FSDP with manual calls to checkpoint.\r\n            fsdp_call_checkpoint = FSDP(deepcopy(seq), **fsdp_kwargs)\r\n            # note that reentrant-based checkpointing requires inputs to have grad\r\n            # flag set.\r\n            inp = torch.randn(10, 3, device=device_type.type, requires_grad=True)\r\n            models = [\r\n                fsdp_only_seq,\r\n                checkpointed_fsdp,\r\n                fsdp_wrapped_checkpoint,\r\n                fsdp_call_checkpoint,\r\n            ]\r\n            # Ensure _save_on_cpu is not yet called\r\n            self.assertFalse(_save_on_cpu_called)\r\n            for i in range(6):\r\n                losses = []\r\n                outputs = []\r\n                for m in models:\r\n                    check_offload = (\r\n                        m != fsdp_only_seq and i == 0 and offload_activations\r\n                    )\r\n                    if m == fsdp_call_checkpoint:\r\n                        # _save_on_cpu should not be called yet\r\n                        self.assertFalse(_save_on_cpu_called)\r\n                        offload_ctx = (\r\n                            get_patched_save_on_cpu()(pin_memory=True)\r\n                            if offload_activations\r\n                            else contextlib.nullcontext()\r\n                        )\r\n                        with offload_ctx:\r\n                            out = checkpoint(m, inp, use_reentrant=True)\r\n                    else:\r\n                        # _save_on_cpu should not be called yet\r\n                        self.assertFalse(_save_on_cpu_called)\r\n                        out = m(inp)\r\n                    if check_offload:\r\n                        self.assertTrue(_save_on_cpu_called)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n                    losses.append(loss)\r\n                    outputs.append(out)\r\n                    _save_on_cpu_called = False\r\n                self._verify_parity(losses, outputs, models)\r\n        dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_non_root(self, device):\r\n        \"\"\"\r\n        Tests that calling ``clip_grad_norm_()`` on a non-root FSDP instance\r\n        raises an error.\r\n        \"\"\"\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin1 = nn.Linear(5, 5)\r\n                self.lin2 = nn.Linear(5, 5)\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                return self.lin2(self.lin1(x))\r\n\r\n        model = Model().to(device_type.type)\r\n        model.lin2 = FSDP(model.lin2)\r\n        fsdp_model = FSDP(model)\r\n        # fsdp_model(torch.randn((2, 5), device=torch.device(self.device_type))).sum().backward()\r\n        fsdp_model(torch.randn((2, 5), device=device_type)).sum().backward()\r\n        error_regex = \"should only be called on the root FSDP instance\"\r\n        with self.assertRaisesRegex(RuntimeError, error_regex):\r\n            fsdp_model.lin2.clip_grad_norm_(max_norm=2)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_ddp_parity(\r\n        self,\r\n        device,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int],\r\n        sharding_strategy: Union[ShardingStrategy, str],\r\n        use_orig_params: bool,\r\n        offload_params: bool,\r\n    ):\r\n        local_model = TransformerWithSharedParams.init(\r\n            self.process_group,\r\n            FSDPInitMode.NO_FSDP,\r\n            DEVICEInitMode.DEVICE_BEFORE,\r\n            deterministic=True,\r\n        )\r\n        ddp_model = DDP(local_model, device_ids=[device_type])\r\n        fsdp_kwargs = {\r\n            \"cpu_offload\": CPUOffload(offload_params=offload_params),\r\n            \"use_orig_params\": use_orig_params,\r\n            \"device_id\": device_type.type,\r\n        }\r\n        if sharding_strategy == \"mixed_strategy\":\r\n            fsdp_model = TransformerWithSharedParams.init(\r\n                self.process_group,\r\n                FSDPInitMode.NO_FSDP,\r\n                DEVICEInitMode.DEVICE_BEFORE,\r\n                deterministic=True,\r\n            )\r\n            # Apply `NO_SHARD` to the encoder\r\n            fsdp_model.transformer.encoder = FSDP(\r\n                fsdp_model.transformer.encoder,\r\n                sharding_strategy=ShardingStrategy.NO_SHARD,\r\n                **fsdp_kwargs,\r\n            )\r\n            # Apply `FULL_SHARD` to the decoder\r\n            fsdp_model.transformer.decoder = FSDP(\r\n                fsdp_model.transformer.decoder,\r\n                sharding_strategy=ShardingStrategy.FULL_SHARD,\r\n                **fsdp_kwargs,\r\n            )\r\n            # TODO: FSDP's `clip_grad_norm_()` is not a static method, so we\r\n            # must make the root module an FSDP instance\r\n            fsdp_model = FSDP(\r\n                fsdp_model, sharding_strategy=ShardingStrategy.FULL_SHARD, **fsdp_kwargs\r\n            )\r\n        else:\r\n            fsdp_kwargs.update(\r\n                {\r\n                    \"sharding_strategy\": sharding_strategy,\r\n                    \"auto_wrap_policy\": ModuleWrapPolicy(\r\n                        {\r\n                            TransformerEncoderLayer,\r\n                            TransformerDecoderLayer,\r\n                        }\r\n                    ),\r\n                }\r\n            )\r\n            fsdp_model = TransformerWithSharedParams.init(\r\n                self.process_group,\r\n                FSDPInitMode.RECURSIVE,\r\n                DEVICEInitMode.DEVICE_BEFORE,\r\n                deterministic=True,\r\n                fsdp_kwargs=fsdp_kwargs,\r\n            )\r\n        LR = 1e-2\r\n        ddp_optim = torch.optim.Adam(ddp_model.parameters(), lr=LR)\r\n        fsdp_optim = torch.optim.Adam(fsdp_model.parameters(), lr=LR)\r\n        device = torch.device(self.device_type)\r\n        LARGE_FACTOR = 100\r\n        inp = ddp_model.module.get_input(device)\r\n        for model in (ddp_model, fsdp_model):\r\n            out = model(*inp)\r\n            if isinstance(model, (DDP, FSDP)):\r\n                loss = model.module.get_loss(inp, out)\r\n            else:\r\n                loss = model.get_loss(inp, out)\r\n            loss.backward()\r\n        # Multiply gradients by a large factor to ensure that gradients will\r\n        # actually be clipped\r\n        for param in itertools.chain(ddp_model.parameters(), fsdp_model.parameters()):\r\n            if (\r\n                param.grad is not None\r\n            ):  # gradients may be `None` for `use_orig_params=True`\r\n                param.grad *= LARGE_FACTOR\r\n        orig_ddp_grads = [\r\n            param.grad.detach().clone() for param in ddp_model.parameters()\r\n        ]\r\n        orig_fsdp_grads = [\r\n            param.grad.detach().clone() if param.grad is not None else None\r\n            for param in fsdp_model.parameters()\r\n        ]\r\n        ddp_total_norm = torch.nn.utils.clip_grad_norm_(\r\n            ddp_model.parameters(),\r\n            max_norm=max_norm,\r\n            norm_type=norm_type,\r\n        )\r\n        fsdp_total_norm = fsdp_model.clip_grad_norm_(\r\n            max_norm=max_norm, norm_type=norm_type\r\n        )\r\n        self.assertEqual(ddp_total_norm, fsdp_total_norm)\r\n        # Check that the gradients were modified by `clip_grad_norm_()`\r\n        for param, orig_grad in zip(ddp_model.parameters(), orig_ddp_grads):\r\n            assert not torch.equal(param.grad, orig_grad)\r\n        for param, orig_grad in zip(fsdp_model.parameters(), orig_fsdp_grads):\r\n            if param.grad is None:\r\n                self.assertEqual(param.grad, orig_grad)  # `None`\r\n            else:\r\n                assert not torch.equal(param.grad, orig_grad)\r\n        # Run an optimizer step to ensure gradients matched after clipping\r\n        ddp_optim.step()\r\n        fsdp_optim.step()\r\n        with FSDP.summon_full_params(fsdp_model):\r\n            for (n1, p1), (n2, p2) in zip(\r\n                ddp_model.module.named_parameters(),\r\n                fsdp_model.named_parameters(),\r\n            ):\r\n                self.assertEqual(n1, n2)\r\n                self.assertEqual(p1, p2)\r\n        if offload_params:\r\n            # TODO: Gradient computation on CPU and GPU differ slightly causing\r\n            # drift unrelated to `clip_grad_norm_()`.\r\n            # https://github.com/pytorch/pytorch/issues/89133\r\n            return\r\n        # Run a few more iterations\r\n        # TODO: We cannot run too many iterations, or else there is drift:\r\n        # https://github.com/pytorch/pytorch/issues/89136\r\n        for i in range(3):\r\n            set_to_none = i % 2 == 0  # exercise both\r\n            ddp_optim.zero_grad(set_to_none=set_to_none)\r\n            fsdp_optim.zero_grad(set_to_none=set_to_none)\r\n            inp = ddp_model.module.get_input(device)\r\n            for model in (ddp_model, fsdp_model):\r\n                out = model(*inp)\r\n                out.sum().backward()\r\n            ddp_total_norm = torch.nn.utils.clip_grad_norm_(\r\n                ddp_model.parameters(),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n            )\r\n            fsdp_total_norm = fsdp_model.clip_grad_norm_(\r\n                max_norm=max_norm, norm_type=norm_type\r\n            )\r\n            self.assertEqual(ddp_total_norm, fsdp_total_norm)\r\n            ddp_optim.step()\r\n            fsdp_optim.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_unshard_params_as_tensors(\r\n        self,\r\n        sharding_strategy: ShardingStrategy,\r\n        use_orig_params: bool,\r\n        forward_prefetch: bool,\r\n        backward_prefetch: Optional[BackwardPrefetch],\r\n    ):\r\n        orig_use_unsharded_views = FlatParamHandle._use_unsharded_views\r\n\r\n        def _use_unsharded_views_assert_as_tensors(\r\n            self: FlatParamHandle, as_params: bool\r\n        ) -> None:\r\n            _p_assert(\r\n                not as_params, \"Expects to use Tensor views but using parameter views\"\r\n            )\r\n            return orig_use_unsharded_views(self, as_params)\r\n\r\n        fsdp_kwargs = {\r\n            \"sharding_strategy\": sharding_strategy,\r\n            \"use_orig_params\": use_orig_params,\r\n            \"forward_prefetch\": forward_prefetch,\r\n            \"backward_prefetch\": backward_prefetch,\r\n            \"auto_wrap_policy\": ModuleWrapPolicy({nn.Linear}),\r\n            \"device_id\": device_type,\r\n        }\r\n        # Define a model with enough FSDP instances to exercise prefetching\r\n        NUM_LINEARS = 5\r\n        model = nn.Sequential(\r\n            *[nn.Linear(3, 3, device=device_type) for _ in range(NUM_LINEARS)]\r\n        )\r\n        fsdp_model = FSDP(model, **fsdp_kwargs)\r\n        self.assertEqual(len(list(FSDP.fsdp_modules(fsdp_model))), NUM_LINEARS + 1)\r\n        for _ in range(3):\r\n            inp = torch.randn((2, 3), device=device_type)\r\n            with self._patch_use_unsharded_views(\r\n                _use_unsharded_views_assert_as_tensors\r\n            ):\r\n                loss = fsdp_model(inp).sum()\r\n                loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_parity_with_non_frozen_fsdp(\r\n        self,\r\n        device_id,\r\n        sharding_strategy: ShardingStrategy,\r\n        use_orig_params: bool,\r\n        offload_params: bool,\r\n        mixed_precision: MixedPrecision,\r\n        backward_prefetch: BackwardPrefetch,\r\n    ):\r\n        torch.manual_seed(42)\r\n        model = ModelUnusedInput(freeze=True).to(device_type)\r\n        torch.manual_seed(42)\r\n        ref_model = ModelUnusedInput(freeze=False).to(device_type)\r\n        fsdp_kwargs = {\r\n            \"device_id\": device_type,\r\n            \"auto_wrap_policy\": ModuleWrapPolicy({LinearUnusedInput}),\r\n            \"sharding_strategy\": sharding_strategy,\r\n            \"use_orig_params\": use_orig_params,\r\n            \"cpu_offload\": CPUOffload(offload_params=offload_params),\r\n            \"mixed_precision\": mixed_precision,\r\n            \"backward_prefetch\": backward_prefetch,\r\n        }\r\n        model = FSDP(model, **fsdp_kwargs)\r\n        ref_model = FSDP(ref_model, **fsdp_kwargs)\r\n        model_optim = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n        ref_model_optim = torch.optim.Adam(\r\n            [\r\n                param\r\n                for name, param in ref_model.named_parameters()\r\n                if not name.startswith(\"_fsdp_wrapped_module.layer1_frozen\")\r\n            ],\r\n            lr=1e-2,\r\n        )\r\n        torch.manual_seed(self.rank + 1)\r\n        losses = []\r\n        for _ in range(6):\r\n            frozen_input = torch.randn((4, 4), device=device_type, requires_grad=False)\r\n            for _model, _optim in ((model, model_optim), (ref_model, ref_model_optim)):\r\n                loss = _model(frozen_input, frozen_input).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                _optim.step()\r\n                _optim.zero_grad()\r\n            self.assertEqual(losses[0], losses[1])\r\n            losses.clear()\r\n        with FSDP.summon_full_params(model):\r\n            with FSDP.summon_full_params(ref_model):\r\n                for param, ref_param in zip(model.parameters(), ref_model.parameters()):\r\n                    self.assertEqual(param, ref_param)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_parity_with_ddp(\r\n        self,\r\n        sharding_strategy: ShardingStrategy,\r\n        use_orig_params: bool,\r\n    ):\r\n        seq = self._init_seq_module(device_type)\r\n        policy = ModuleWrapPolicy({nn.Linear})\r\n        fsdp_kwargs = {\"device_id\": device_type}\r\n        fsdp_seq = FSDP(\r\n            copy.deepcopy(seq),\r\n            auto_wrap_policy=policy,\r\n            sharding_strategy=sharding_strategy,\r\n            use_orig_params=use_orig_params,\r\n            **fsdp_kwargs,\r\n        )\r\n        ddp_seq = DDP(copy.deepcopy(seq), device_ids=[device_type])\r\n        fsdp_optim = torch.optim.Adam(fsdp_seq.parameters(), lr=1e-2)\r\n        ddp_optim = torch.optim.Adam(ddp_seq.parameters(), lr=1e-2)\r\n        torch.manual_seed(self.rank + 1)\r\n        losses = []\r\n        for _ in range(6):\r\n            inp = torch.randn((8, 5), device=device_type.type)\r\n            for seq, optim in ((fsdp_seq, fsdp_optim), (ddp_seq, ddp_optim)):\r\n                loss = seq(inp).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n            if TEST_CUDA:\r\n                torch.testing.assert_close(losses[0], losses[1])\r\n            else:\r\n                torch.testing.assert_close(losses[0], losses[1], atol=1e-03, rtol=1e-03)\r\n            losses.clear()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_hooks_multi_traversal(\r\n        self,\r\n        sharding_strategy: ShardingStrategy,\r\n        use_orig_params: bool,\r\n        inp_requires_grad: bool,\r\n        forward_prefetch: bool,\r\n    ):\r\n        seq = self._init_multi_traversal_module(device_type.type)\r\n        policy = ModuleWrapPolicy({nn.Linear})\r\n        fsdp_kwargs = {\"device_id\": device_type}\r\n        fsdp_seq = FSDP(\r\n            copy.deepcopy(seq),\r\n            auto_wrap_policy=policy,\r\n            sharding_strategy=sharding_strategy,\r\n            use_orig_params=use_orig_params,\r\n            forward_prefetch=forward_prefetch,\r\n            **fsdp_kwargs,\r\n        )\r\n        ddp_seq = DDP(copy.deepcopy(seq), device_ids=[device_type])\r\n        fsdp_optim = torch.optim.Adam(fsdp_seq.parameters(), lr=1e-2)\r\n        ddp_optim = torch.optim.Adam(ddp_seq.parameters(), lr=1e-2)\r\n        torch.manual_seed(self.rank + 1)\r\n        losses = []\r\n        for _ in range(6):\r\n            inp = torch.randn(\r\n                (8, 5), device=device_type, requires_grad=inp_requires_grad\r\n            )\r\n            for seq, optim in ((fsdp_seq, fsdp_optim), (ddp_seq, ddp_optim)):\r\n                loss = seq(inp).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n            torch.testing.assert_close(losses[0], losses[1])\r\n            losses.clear()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_symbolic_tracing_outputs(self):\r\n        \"\"\"\r\n        Tests running ``tracer.trace()`` inside ``patch_tracer()`` by checking\r\n        the saved data structures.\r\n        \"\"\"\r\n        model = Model()\r\n        tracer = torch.fx.Tracer()\r\n        orig_call_module = tracer.call_module\r\n        orig_create_proxy = tracer.create_proxy\r\n        exec_order_tracer = _ExecOrderTracer()\r\n        with exec_order_tracer.patch_tracer(tracer=tracer, root_module=model):\r\n            concrete_args = {\"run_all_layers\": True}\r\n            tracer.trace(model, concrete_args)\r\n        # Check that the tracer methods are unchanged after exiting the context\r\n        self.assertEqual(orig_call_module, tracer.call_module)\r\n        self.assertEqual(orig_create_proxy, tracer.create_proxy)\r\n        # Check `module_forward_order`\r\n        correct_module_forward_order = [\r\n            model,\r\n            model.layer0,\r\n            model.relu,\r\n            model.layer2,\r\n            model.layer2[0],\r\n            model.layer2[1],\r\n            model.layer2[2],\r\n            model.relu,\r\n            model.layer1,\r\n            model.relu,\r\n            model.layer0,\r\n            model.relu,\r\n        ]\r\n        exec_info = exec_order_tracer.exec_info\r\n        self.assertEqual(exec_info.module_forward_order, correct_module_forward_order)\r\n        # Check `module_to_param_usage_infos`\r\n        self.assertEqual(\r\n            exec_info.module_to_param_usage_infos[model],\r\n            [\r\n                (model.layer0, list(model.layer0.named_parameters())),\r\n                (model.layer2, list(model.layer2.named_parameters())),\r\n                (model, [(\"weight1\", model.weight1)]),\r\n                (model.layer1, list(model.layer1.named_parameters())),\r\n                (model, [(\"weight2\", model.weight2)]),\r\n                (model.layer0, list(model.layer0.named_parameters())),\r\n            ],\r\n        )\r\n        self.assertEqual(\r\n            exec_info.module_to_param_usage_infos[model.layer0],\r\n            [(model.layer0, list(model.layer0.named_parameters()))],\r\n        )\r\n        self.assertEqual(\r\n            exec_info.module_to_param_usage_infos[model.layer1],\r\n            [(model.layer1, list(model.layer1.named_parameters()))],\r\n        )\r\n        self.assertEqual(\r\n            exec_info.module_to_param_usage_infos[model.layer2],\r\n            [\r\n                (model.layer2[0], list(model.layer2[0].named_parameters())),\r\n                (model.layer2[2], list(model.layer2[2].named_parameters())),\r\n            ],\r\n        )\r\n        self.assertEqual(exec_info.module_to_param_usage_infos[model.relu], [])\r\n        # Check `param_forward_order`\r\n        correct_param_order = [\r\n            model.layer0.weight,\r\n            model.layer0.bias,\r\n            model.layer2[0].weight,\r\n            model.layer2[2].weight,\r\n            model.weight1,\r\n            model.layer1.weight,\r\n            model.weight2,\r\n        ]\r\n        self.assertEqual(exec_info.param_forward_order, correct_param_order)\r\n        # Check `visited_params`\r\n        self.assertEqual(\r\n            len(exec_info.visited_params), len(exec_info.param_forward_order)\r\n        )\r\n        self.assertEqual(exec_info.visited_params, set(exec_info.param_forward_order))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_grad_acc(\r\n        self,\r\n        batch_dim: int,\r\n        configs: list[_GradAccConfig],\r\n        cpu_offload: CPUOffload,\r\n        backward_prefetch: Optional[BackwardPrefetch],\r\n        sharding_strategy: ShardingStrategy,\r\n        use_orig_params: bool,\r\n    ):\r\n        \"\"\"\r\n        Tests gradient accumulation by comparing a run that trains sequentially\r\n        through some batches while accumulating gradients with a run that\r\n        trains on the concatenation of those batches in a single iteration.\r\n\r\n        The last iteration always synchronizes gradients regardless of what is\r\n        specified by the last element of ``configs``.\r\n\r\n        Arguments:\r\n            batch_dim (int): Batch dimension in the input tensor to be passed\r\n                into the model for the forward pass.\r\n            configs (List[_GradAccConfig]): :class:`list` of configurations\r\n                specifying how gradients are accumulated; for example, a list\r\n                corresponding to [(False, 2), (True, 2), (False, 2)] indicates\r\n                to accumulate over 2 + 2 + 2 = 6 total iterations, where the\r\n                first two do not use ``no_sync()``, the middle two do use\r\n                ``no_sync()``, and the final two again do not use\r\n                ``no_sync()``.\r\n            cpu_offload (CPUOffload): Configures CPU offloading.\r\n            backward_prefetch (Optional[BackwardPrefetch]): Specifies at which\r\n                point to prefetch the next layer's full parameters during the\r\n                backward pass, if at all.\r\n        \"\"\"\r\n        # Initialize the FSDP model and optimizer\r\n        fsdp_kwargs = {\r\n            \"cpu_offload\": cpu_offload,\r\n            \"backward_prefetch\": backward_prefetch,\r\n            \"sharding_strategy\": sharding_strategy,\r\n            \"use_orig_params\": use_orig_params,\r\n        }\r\n        fsdp_model: FSDP = TransformerWithSharedParams.init(\r\n            self.process_group,\r\n            FSDPInitMode.RECURSIVE,\r\n            DEVICEInitMode.DEVICE_BEFORE,\r\n            fsdp_kwargs,\r\n            deterministic=True,\r\n            add_bn=False,  # disable BN since the test uses varying batch sizes\r\n        )\r\n        device = torch.device(\"cuda\")\r\n        optim = torch.optim.SGD(\r\n            fsdp_model.parameters(),\r\n            lr=0.01,\r\n            momentum=0.9,\r\n        )\r\n\r\n        # Generate the sequence of batches, each containing the same data\r\n        # but permuted\r\n        def permute_tensor(x: torch.Tensor):\r\n            return x.view(-1)[torch.randperm(x.numel())].view_as(x)\r\n\r\n        batch: tuple[torch.Tensor, ...] = fsdp_model.module.get_input(device)\r\n        batches: list[tuple[torch.Tensor, ...]] = [batch]\r\n        num_iters_to_acc = sum(config.num_iters for config in configs)\r\n        for _ in range(num_iters_to_acc - 1):\r\n            batches.append(tuple(permute_tensor(t) for t in batch))\r\n        for batch1, batch2 in itertools.combinations(batches, r=2):\r\n            for t1, t2 in zip(batch1, batch2):\r\n                assert not torch.all(\r\n                    t1 == t2\r\n                ), \"Check the test to make sure that batches are distinct\"\r\n\r\n        # Concatenate the batches along the given batch dimension\r\n        concat_batch: tuple[torch.Tensor, ...] = tuple(\r\n            torch.cat(ts, dim=batch_dim) for ts in zip(*batches)\r\n        )\r\n\r\n        # Establish reference gradients using the concatenated batch\r\n        fsdp_model.zero_grad()\r\n        output = fsdp_model(*concat_batch)\r\n        ref_loss = fsdp_model.module.get_loss(concat_batch, output)\r\n        ref_loss.backward()\r\n        ref_grads = [\r\n            p.grad.detach().clone()\r\n            for p in fsdp_model.parameters()\r\n            if p.grad is not None\r\n        ]\r\n\r\n        # Compute and accumulate the gradients\r\n        fsdp_model.zero_grad()\r\n        losses = []\r\n        batch_idx = 0\r\n        for config in configs:\r\n            sync_context = (\r\n                fsdp_model.no_sync() if config.use_no_sync else contextlib.nullcontext()\r\n            )\r\n            with sync_context:\r\n                for _ in range(config.num_iters):\r\n                    if batch_idx == num_iters_to_acc - 1:\r\n                        break  # always sync on the last iteration\r\n                    batch = batches[batch_idx]\r\n                    batch_idx += 1\r\n                    output = fsdp_model(*batch)\r\n                    loss = fsdp_model.module.get_loss(batch, output)\r\n                    loss.backward()\r\n                    losses.append(loss)\r\n        output = fsdp_model(*batches[-1])\r\n        loss = fsdp_model.module.get_loss(batches[-1], output)\r\n        loss.backward()\r\n        losses.append(loss)\r\n        acc_loss = sum(losses)\r\n        acc_grads = [\r\n            p.grad.detach().clone()\r\n            for p in fsdp_model.parameters()\r\n            if p.grad is not None\r\n        ]\r\n\r\n        # Compare the losses and gradients\r\n        torch.testing.assert_close(ref_loss, acc_loss)\r\n        self.assertEqual(len(ref_grads), len(acc_grads))\r\n        for ref_grad, acc_grad in zip(ref_grads, acc_grads):\r\n            self.assertEqual(ref_grad.device, acc_grad.device)\r\n            self.assertEqual(ref_grad.size(), acc_grad.size())\r\n            self.assertEqual(ref_grad.dtype, acc_grad.dtype)\r\n            torch.testing.assert_close(ref_grad, acc_grad)\r\n\r\n        # Check that the optimizer step does not error\r\n        optim.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_fsdp_hybrid_shard_parity(\r\n        self, hsdp_sharding_strategy: ShardingStrategy, use_orig_params: bool\r\n    ):\r\n        fsdp_model = self._init_fsdp_model(use_orig_params)\r\n        global_pg = dist.distributed_c10d._get_default_group()\r\n        hsdp_pgs = _init_intra_and_inter_node_groups(global_pg, 2)\r\n        hsdp_model = self._init_hsdp_model(\r\n            hsdp_sharding_strategy,\r\n            ShardingStrategyMode.ALL_HYBRID_SHARD,\r\n            use_orig_params,\r\n            hsdp_process_groups=hsdp_pgs,\r\n        )\r\n        assert (\r\n            hsdp_model._inter_node_pg.size() > 1\r\n        ), \"HSDP model initialized without replication\"\r\n        fsdp_optim = torch.optim.Adam(fsdp_model.parameters(), lr=1e-2)\r\n        hsdp_optim = torch.optim.Adam(hsdp_model.parameters(), lr=1e-2)\r\n        torch.manual_seed(global_pg.rank() + 1)\r\n        for _ in range(5):\r\n            inp = fsdp_model.module.get_input(torch.device(\"cuda\"))\r\n            losses: list[torch.Tensor] = []\r\n            for model, optim in ((fsdp_model, fsdp_optim), (hsdp_model, hsdp_optim)):\r\n                optim.zero_grad()\r\n                loss = model(*inp).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                optim.step()\r\n            self.assertEqual(losses[0], losses[1])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ignored_modules_not_under_wrapped_root(self, ignore_modules: bool):\r\n        model = Model().cuda()\r\n        ignored_modules = list(model.layer1.children())[1:]\r\n\r\n        ignore_kwargs = (\r\n            {\"ignored_modules\": ignored_modules}\r\n            if ignore_modules\r\n            else {\r\n                \"ignored_states\": {p for m in ignored_modules for p in m.parameters()}\r\n            }\r\n        )\r\n\r\n        wrap_cls = FSDP\r\n\r\n        model.layer1 = wrap_cls(\r\n            model.layer1,\r\n            **ignore_kwargs,\r\n        )\r\n        model.layer3 = wrap_cls(\r\n            model.layer3,\r\n            # the ignored modules/parameters contains submodule under model.layer1, which\r\n            # is out of the local root model.layer3.\r\n            **ignore_kwargs,\r\n        )\r\n\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n        self._train_model(model, optim, 3)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_ignored_modules_nested(self, use_orig_params: bool, ignore_modules: bool):\r\n        # Initialize an FSDP-wrapped nested model that first wraps the nested\r\n        # sequential's second linear layer (`layer1[1]`) and then wraps the\r\n        # overall model while ignoring the nested sequential (`layer1`)\r\n        model = Model().cuda()\r\n        fsdp_fn = functools.partial(FSDP, use_orig_params=use_orig_params)\r\n        model.layer1[1] = fsdp_fn(model.layer1[1])\r\n        if ignore_modules:\r\n            wrapped_model = fsdp_fn(model, ignored_modules=[model.layer1])\r\n        else:\r\n            wrapped_model = fsdp_fn(\r\n                model, ignored_states=list(model.layer1.parameters())\r\n            )\r\n        # Check that the wrapped model's flattened parameter does not include\r\n        # the ignored nested sequential's parameters\r\n        nonwrapped_model = Model()\r\n        total_numel = sum(p.numel() for p in nonwrapped_model.parameters())\r\n        ignored_numel = sum(p.numel() for p in nonwrapped_model.layer1.parameters())\r\n        nonignored_numel = total_numel - ignored_numel\r\n        with FSDP.summon_full_params(wrapped_model):\r\n            flat_param = wrapped_model.params[0]\r\n            flat_param_numel = flat_param.numel()\r\n            if use_orig_params:\r\n                # Subtract the numel contributed from alignment padding\r\n                padding_numel = sum(\r\n                    numel\r\n                    for (numel, is_padding) in zip(\r\n                        flat_param._numels_with_padding, flat_param._is_padding_mask\r\n                    )\r\n                    if is_padding\r\n                )\r\n                flat_param_numel -= padding_numel\r\n                self.assertEqual(flat_param_numel, nonignored_numel)\r\n            self.assertEqual(flat_param_numel, nonignored_numel)\r\n        # Check that we can run a few iterations\r\n        optim = torch.optim.Adam(wrapped_model.parameters(), lr=1e-3)\r\n        self._train_model(wrapped_model, optim, 3)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_ignored_states_auto_wrap(self, policy, ignore_bias: bool):\r\n        model = Model().cuda()\r\n        ignored_states = [model.layer1[1].weight]\r\n        if ignore_bias:\r\n            ignored_states.append(model.layer1[1].bias)\r\n        # Construct 2 flat parameters: one for `layer1` and one for the model\r\n        fsdp_model = FSDP(\r\n            model,\r\n            # Use `False` to avoid complexity of intra-flat-parameter padding\r\n            use_orig_params=False,\r\n            auto_wrap_policy=policy,\r\n            ignored_states=ignored_states,\r\n        )\r\n        ref_model = Model()\r\n        expected_layer1_unsharded_numel = (\r\n            sum(p.numel() for p in ref_model.layer1.parameters())\r\n            - ref_model.layer1[1].weight.numel()\r\n        )\r\n        if ignore_bias:\r\n            expected_layer1_unsharded_numel -= ref_model.layer1[1].bias.numel()\r\n        expected_model_unsharded_numel = sum(\r\n            p.numel() for p in ref_model.parameters()\r\n        ) - sum(p.numel() for p in ref_model.layer1.parameters())\r\n        expected_layer1_sharded_numel = math.ceil(\r\n            expected_layer1_unsharded_numel / self.world_size\r\n        )\r\n        expected_model_sharded_numel = math.ceil(\r\n            expected_model_unsharded_numel / self.world_size\r\n        )\r\n        self.assertLessEqual(\r\n            fsdp_model.layer1.module._flat_param.numel(), expected_layer1_sharded_numel\r\n        )\r\n        self.assertLessEqual(\r\n            fsdp_model.module._flat_param.numel(), expected_model_sharded_numel\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _dist_train(self, with_checkpoint, expected, model_hidden_dim, iterations):\r\n        gpu_id = self.rank\r\n        batch = torch.randn(size=(2, 3, 224, 224)).cuda()\r\n\r\n        model = create_model(\r\n            with_fsdp=True,\r\n            with_checkpoint=with_checkpoint,\r\n            model_hidden_dim=model_hidden_dim,\r\n        )\r\n        model = model.cuda()\r\n        model = FSDP(model)\r\n\r\n        # We enable momentum so that after the first iteration, the optimizer state is added\r\n        # to the total memory used.\r\n        criterion = nn.MSELoss()\r\n        optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\r\n\r\n        results = {}  # results of memory stats\r\n        for iteration in range(iterations):\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: start\")\r\n\r\n            out = model(batch)\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: after fwd\")\r\n\r\n            out = sum(o.sum() for o in out[0])\r\n            fake_loss = criterion(out, torch.tensor(0.0).cuda())\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: after loss\")\r\n\r\n            fake_loss.backward()\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: after bwd\")\r\n\r\n            optimizer.step()\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: after step\")\r\n\r\n            # It is important to use `set_to_none` below, not optimizer.zero_grad() to reclaim memory.\r\n            model.zero_grad(set_to_none=True)\r\n            get_cur_mem(gpu_id, results, f\"iter {iteration}: done\")\r\n\r\n        def cmp(results, expected):\r\n            ret = \"\"\r\n            self.assertEqual(results.keys(), expected.keys())\r\n            for k, v in results.items():\r\n                exp = expected[k]\r\n                if abs(exp - v) > 1:  # allow 1MB rounding differences\r\n                    ret += f\"{k}: got {v}, expected {exp}\\n\"\r\n            return ret\r\n\r\n        output = cmp(results, expected)\r\n        self.assertEqual(output, \"\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_container_copy(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = nn.Linear(4, 5)\r\n\r\n            def forward(self, input):\r\n                return self.linear(input)\r\n\r\n        input = torch.randn(2, 4)\r\n\r\n        model = Model()\r\n        model_cp = deepcopy(model)\r\n        self.assertEqual(model(input).data, model_cp(input).data)\r\n\r\n        model_cp.linear.weight.data[:] = 2\r\n        self.assertNotEqual(model(input).data, model_cp(input).data)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_input_type(self, device, input_cls):\r\n        \"\"\"Test FSDP with input being a list or a dict, only single GPU.\"\"\"\r\n\r\n        class Model(Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.layer = Linear(4, 4)\r\n\r\n            def forward(self, input):\r\n                if isinstance(input, list):\r\n                    input = input[0]\r\n                else:\r\n                    assert isinstance(input, dict), input\r\n                    input = input[\"in\"]\r\n                return self.layer(input)\r\n\r\n        fsdp_kwargs = {\r\n            \"device_id\": device,\r\n        }\r\n        model = FSDP(Model().to(device), **fsdp_kwargs)\r\n        optim = SGD(model.parameters(), lr=0.1)\r\n        for _ in range(5):\r\n            in_data = torch.rand(64, 4).to(device)\r\n            in_data.requires_grad = True\r\n            if input_cls is list:\r\n                in_data = [in_data]\r\n            else:\r\n                self.assertTrue(input_cls is dict)\r\n                in_data = {\"in\": in_data}\r\n            out = model(in_data)\r\n            out.sum().backward()\r\n            optim.step()\r\n            optim.zero_grad()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_meta_device_with_mixed_precision(self):\r\n        \"\"\"\r\n        Tests meta device initialization with a ``param_init_fn`` when\r\n        specifying mixed precision with ``param_dtype=torch.float32``.\r\n        \"\"\"\r\n\r\n        class FakeLinear(nn.Module):\r\n            def __init__(\r\n                self, in_dim: int, out_dim: int, device: Union[torch.device, str]\r\n            ) -> None:\r\n                super().__init__()\r\n                self.weight = nn.Parameter(\r\n                    torch.randn((in_dim, out_dim), device=device)\r\n                )\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                return x @ self.weight\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin1 = nn.Linear(5, 5, device=\"meta\")\r\n                self.lin2 = FakeLinear(5, 5, device=\"meta\")\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                return self.lin2(self.relu(self.lin1(x)))\r\n\r\n            def _module_init_fn(self, module: nn.Module):\r\n                if isinstance(module, nn.Linear):\r\n                    torch.nn.init.normal_(module.weight, mean=0.0, std=0.1)\r\n                    if module.bias is not None:\r\n                        torch.nn.init.zeros_(module.bias)\r\n\r\n        def _param_init_fn(module: nn.Module) -> None:\r\n            # TODO: `module.to_empty()` is not generally correct for meta\r\n            # device initialization.\r\n            # https://github.com/pytorch/pytorch/issues/90465\r\n            module.to_empty(device=torch.device(\"cuda\"))\r\n            module.apply(model._module_init_fn)\r\n\r\n        model = Model()\r\n        # Wrap `lin1` and the top level `model` to create nested FSDP instances\r\n        # where each instance has parameters\r\n        FSDP(\r\n            model,\r\n            auto_wrap_policy=ModuleWrapPolicy({nn.Linear}),\r\n            mixed_precision=MixedPrecision(\r\n                param_dtype=torch.float32, reduce_dtype=torch.float16\r\n            ),\r\n            param_init_fn=_param_init_fn,\r\n            device_id=torch.cuda.current_device(),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fsdp_zero2_eval_with_prefetch(self):\r\n        # Test FSDP validation with SHARD_GRAD_OP and forward_prefetch\r\n\r\n        class Mnist(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.conv1 = nn.Conv2d(1, 32, 3, 1)\r\n                self.conv2 = nn.Conv2d(32, 64, 3, 1)\r\n                self.dropout1 = nn.Dropout(0.25)\r\n                self.dropout2 = nn.Dropout(0.5)\r\n                self.fc1 = nn.Linear(9216, 128)\r\n                self.fc2 = nn.Linear(128, 10)\r\n                self.ln = nn.LayerNorm(9216)\r\n\r\n            def forward(self, x, y):\r\n                x = self.conv1(x)\r\n                x = torch.nn.functional.relu(x)\r\n                x = self.conv2(x)\r\n                x = torch.nn.functional.relu(x)\r\n                x = torch.nn.functional.max_pool2d(x, 2)\r\n                x = self.dropout1(x)\r\n                x = torch.flatten(x, 1)\r\n                x = self.ln(x)\r\n                x = self.fc1(x)\r\n                x = torch.nn.functional.relu(x)\r\n                x = self.dropout2(x)\r\n                x = self.fc2(x)\r\n                output = torch.nn.functional.log_softmax(x, dim=1)\r\n                loss = torch.nn.functional.cross_entropy(output, y)\r\n                return loss\r\n\r\n        model = Mnist().cuda()\r\n        model1 = Mnist().cuda()\r\n        model1.load_state_dict(model.state_dict())\r\n        fsdp_model = FSDP(\r\n            model,\r\n            sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,\r\n            forward_prefetch=True,\r\n            use_orig_params=True,\r\n            auto_wrap_policy=ModuleWrapPolicy([nn.Linear, nn.Conv2d]),\r\n        )\r\n        ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n            model1,\r\n        )\r\n\r\n        fsdp_opt = torch.optim.SGD(fsdp_model.parameters(), lr=1e-4)\r\n        ddp_opt = torch.optim.SGD(ddp_model.parameters(), lr=1e-4)\r\n\r\n        seed = self.rank + 20231010\r\n        torch.manual_seed(seed)\r\n        torch.cuda.manual_seed(seed)\r\n\r\n        losses = []\r\n        grads = []\r\n        for i in range(5):\r\n            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\r\n            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\r\n            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\r\n                seed = self.rank + i\r\n                torch.manual_seed(seed)\r\n                torch.cuda.manual_seed(seed)\r\n                loss = model(x, y).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                opt.step()\r\n                grads.append(x.grad)\r\n                opt.zero_grad()\r\n            assert torch.allclose(losses[0], losses[1])\r\n            assert torch.allclose(grads[0], grads[1])\r\n            losses.clear()\r\n            grads.clear()\r\n\r\n        with torch.no_grad():\r\n            fsdp_model.eval()\r\n            ddp_model.eval()\r\n            for _ in range(5):\r\n                x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\r\n                y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\r\n                fsdp_loss = fsdp_model(x, y)\r\n                ddp_loss = ddp_model(x, y)\r\n                assert torch.allclose(fsdp_loss, ddp_loss)\r\n\r\n        fsdp_model.train()\r\n        ddp_model.train()\r\n        for i in range(5):\r\n            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\r\n            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\r\n            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\r\n                seed = self.rank + i\r\n                torch.manual_seed(seed)\r\n                torch.cuda.manual_seed(seed)\r\n                loss = model(x, y).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                opt.step()\r\n                grads.append(x.grad)\r\n                opt.zero_grad()\r\n            assert torch.allclose(losses[0], losses[1])\r\n            assert torch.allclose(grads[0], grads[1])\r\n            losses.clear()\r\n            grads.clear()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_fsdp_cpu_training(self):\r\n        \"\"\"Tests FSDP training on CPU.\"\"\"\r\n        gloo_pg = dist.new_group(backend=\"gloo\")\r\n        for ss in [  # noqa: F841\r\n            ShardingStrategy.NO_SHARD,\r\n            ShardingStrategy.FULL_SHARD,\r\n            ShardingStrategy.SHARD_GRAD_OP,\r\n            ShardingStrategy.HYBRID_SHARD,\r\n            ShardingStrategy._HYBRID_SHARD_ZERO2,\r\n        ]:\r\n            torch.manual_seed(42)\r\n            model = MyModel()\r\n            ref_model = DDP(deepcopy(model), process_group=gloo_pg)\r\n            model = FSDP(\r\n                model,\r\n                auto_wrap_policy=always_wrap_policy,\r\n                process_group=gloo_pg,\r\n                device_id=torch.device(\"cpu\"),\r\n            )\r\n            ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\r\n            optim = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n            torch.manual_seed(42 + self.rank)\r\n            inp = torch.randn(2, 2)\r\n            for _ in range(10):\r\n                losses = []\r\n                for _model, _optim in ((ref_model, ref_optim), (model, optim)):\r\n                    loss = _model(inp, inp).sum()\r\n                    losses.append(loss)\r\n                    loss.backward()\r\n                    _optim.step()\r\n                    _optim.zero_grad()\r\n                self.assertEqual(losses[0], losses[1])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_fsdp_not_all_outputs_used_in_loss(\r\n        self, sharding_strategy: ShardingStrategy\r\n    ):\r\n        class MyModule(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin1 = nn.Linear(4, 4)\r\n                self.lin2 = nn.Linear(4, 4)\r\n\r\n            def forward(self, x):\r\n                a = self.lin1(x)\r\n                b = self.lin2(x)\r\n                return (a, b)\r\n\r\n        def _check_resharded(fsdp_module):\r\n            handle = fsdp_module._handle\r\n            if not handle:\r\n                return\r\n            param = handle.flat_param\r\n            if handle.uses_sharded_strategy:\r\n                full_param = param._full_param_padded\r\n                self.assertEqual(full_param.storage().size(), 0)\r\n\r\n            self.assertEqual(param.data_ptr(), param._local_shard.data_ptr())\r\n\r\n        def _check_equal(local, fsdp):\r\n            with FSDP.summon_full_params(fsdp):\r\n                for p1, p2 in zip(fsdp.parameters(), local.parameters()):\r\n                    torch.testing.assert_close(p1, p2)\r\n\r\n        fsdp_ctor = functools.partial(FSDP, sharding_strategy=sharding_strategy)\r\n        m = MyModule().cuda()\r\n        m_local = deepcopy(m)\r\n        local_m = m_local\r\n        prev_params = [p.clone() for p in m_local.parameters()]\r\n\r\n        m.lin1 = fsdp_ctor(m.lin1)\r\n        m = fsdp_ctor(m)\r\n        _check_equal(m_local, m)\r\n\r\n        opt = torch.optim.SGD(m.parameters(), lr=1e-3)\r\n        opt_local = torch.optim.SGD(local_m.parameters(), lr=1e-3)\r\n\r\n        for i in range(6):\r\n            t = torch.ones(4, device=\"cuda\")\r\n            a, b = m(t)\r\n            local_a, local_b = local_m(t)\r\n            if i < 2:\r\n                # use both params in loss computation. Later,\r\n                # b will go unused and we check grads are the\r\n                # same as local training.\r\n                loss = (a @ b).sum()\r\n                loss_local = (local_a @ local_b).sum()\r\n            else:\r\n                loss = a.sum()\r\n                loss_local = local_a.sum()\r\n\r\n            loss.backward()\r\n            loss_local.backward()\r\n            _check_resharded(m)\r\n            opt.step()\r\n            opt_local.step()\r\n            _check_equal(m_local, m)\r\n            # Ensure at least some change from previous params, otherwise\r\n            # above check would be vacuously true.\r\n            self.assertTrue(\r\n                any(\r\n                    not torch.equal(p1, p2)\r\n                    for p1, p2 in zip(prev_params, m_local.parameters())\r\n                )\r\n            )\r\n            prev_params = [p.clone() for p in local_m.parameters()]\r\n            opt.zero_grad()\r\n            opt_local.zero_grad()\r\n\r\n        dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_fsdp_module_no_compute_grad(self, use_second_layer, sharding_strategy):\r\n        # When use_second_layer=True, b is involved in forward computation but does\r\n        # not receive grad in backward. Otherwise, b is not involved in forward\r\n        # computation.\r\n\r\n        class MyModel(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.a = nn.Linear(10, 10)\r\n                self.b = nn.Linear(10, 10)\r\n\r\n            def forward(self, x, y):\r\n                out1 = self.a(x)\r\n                if use_second_layer:\r\n                    out2 = self.b(y)\r\n                    return out1, out2\r\n                else:\r\n                    return out1\r\n\r\n        fsdp = FSDP(\r\n            MyModel().cuda(),\r\n            sharding_strategy=sharding_strategy,\r\n            auto_wrap_policy=always_wrap_policy,\r\n        )\r\n        x = torch.randn(10, 10, device=\"cuda\")\r\n        y = torch.randn(10, 10, device=\"cuda\")\r\n        for _ in range(4):\r\n            if use_second_layer:\r\n                a, _ = fsdp(x, y)\r\n            else:\r\n                a = fsdp(x, y)\r\n            loss = a.sum()\r\n            loss.backward()\r\n\r\n            # self.a receives grad, self.b does not\r\n            a_grad = fsdp.module.a._handle.flat_param.grad\r\n            b_grad = fsdp.module.b._handle.flat_param.grad\r\n            self.assertIsNotNone(a_grad)\r\n            self.assertIsNone(b_grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _run_test_mixed_precision_e2e(\r\n        self,\r\n        mp_config,\r\n        cpu_offload,\r\n        backward_prefetch,\r\n        forward_prefetch,\r\n        full_precision_param_dtype,\r\n        sharding_strategy,\r\n        enable_sharded_grad_scaler,\r\n    ):\r\n        torch.cuda.set_device(self.rank)\r\n        fsdp_models = [\r\n            self._get_simple_model(\r\n                param_dtype=full_precision_param_dtype,\r\n                sharding_strategy=sharding_strategy,\r\n                cpu_offload=cpu_offload,\r\n                mixed_precision=mp_config,\r\n                backward_prefetch=backward_prefetch,\r\n                forward_prefetch=forward_prefetch,\r\n            ),\r\n            self._get_simple_nested_model(\r\n                param_dtype=full_precision_param_dtype,\r\n                run_checks=True,\r\n                sharding_strategy=sharding_strategy,\r\n                cpu_offload=cpu_offload,\r\n                mixed_precision=mp_config,\r\n                backward_prefetch=backward_prefetch,\r\n                forward_prefetch=forward_prefetch,\r\n            ),\r\n        ]\r\n        for model in fsdp_models:\r\n            if not cpu_offload.offload_params:\r\n                model.cuda()\r\n\r\n            # Patch reduce_scatter to add validation for mixed precision types.\r\n            orig_reduce_scatter = dist.reduce_scatter_tensor\r\n            test_reduce_scatter = partial(\r\n                self._reduce_scatter_validate_mp,\r\n                orig_reduce_scatter,\r\n                mp_config,\r\n                True,\r\n            )\r\n            with patch_reduce_scatter(test_reduce_scatter, full_precision_param_dtype):\r\n                scaler = ShardedGradScaler(enabled=enable_sharded_grad_scaler)\r\n                optim = torch.optim.Adam(model.parameters())\r\n\r\n                for _ in range(3):\r\n                    inp = torch.randn(\r\n                        3, 10, device=\"cuda\", dtype=full_precision_param_dtype\r\n                    )\r\n                    # Forward pass of LinearMixedPrecision check casting of\r\n                    # inputs, params, buffers.\r\n                    act, *_ = model(\r\n                        (inp, self, model, mp_config, full_precision_param_dtype)\r\n                    )\r\n                    # Buffers should be casted.\r\n                    for buf in model.buffers():\r\n                        if mp_config.buffer_dtype is not None:\r\n                            self.assertEqual(buf.dtype, mp_config.buffer_dtype)\r\n                        else:\r\n                            self.assertEqual(buf.dtype, _BUFFER_ORIG_DTYPE)\r\n                    # p._mp_shard should be freed.\r\n                    if mp_config.param_dtype is not None:\r\n                        self._validate_mp_shard_freed(model)\r\n                    else:\r\n                        # We never should have allocated an _mp_shard.\r\n                        self._validate_no_mp_shard(model)\r\n\r\n                    loss = act.sum()\r\n                    loss = scaler.scale(loss)\r\n                    if mp_config.param_dtype is not None:\r\n                        self.assertEqual(loss.dtype, mp_config.param_dtype)\r\n                    else:\r\n                        self.assertEqual(loss.dtype, full_precision_param_dtype)\r\n                    # Will run patched reduce scatter that validates mixed_precision\r\n                    # types in backward.\r\n                    loss.backward()\r\n                    # Buffers stay casted even after backwards.\r\n                    for buf in model.buffers():\r\n                        if mp_config.buffer_dtype is not None:\r\n                            self.assertEqual(buf.dtype, mp_config.buffer_dtype)\r\n                        else:\r\n                            self.assertEqual(buf.dtype, _BUFFER_ORIG_DTYPE)\r\n                    # p._mp_shard should be freed.\r\n                    if mp_config.param_dtype is not None:\r\n                        self._validate_mp_shard_freed(model)\r\n                    else:\r\n                        self._validate_no_mp_shard(model)\r\n\r\n                    # Ensure params and grads are in full precision,\r\n                    # as after fwd/backward we maintain full precision shards.\r\n                    for param in model.parameters():\r\n                        self.assertEqual(param.dtype, full_precision_param_dtype)\r\n                        if param.grad is not None:\r\n                            self.assertEqual(\r\n                                param.grad.dtype, full_precision_param_dtype\r\n                            )\r\n\r\n                    # Unscale the gradients and step\r\n                    scaler.step(optim)\r\n                    # Update the scale factor\r\n                    scaler.update()\r\n\r\n                    # Summon full params should be in full precision\r\n                    with model.summon_full_params(model):\r\n                        # It is not expected for summon_full_params to allocate\r\n                        # a mixed precision shard.\r\n                        if mp_config.param_dtype is not None:\r\n                            self._validate_mp_shard_freed(model)\r\n                        else:\r\n                            self._validate_no_mp_shard(model)\r\n                        params = list(model.parameters())\r\n                        for p in params:\r\n                            self.assertEqual(p.dtype, full_precision_param_dtype)\r\n\r\n                        # Note that buffers are cast only once and only restored\r\n                        # to the original buffer dtype in state_dict, so\r\n                        # summon_full_params is not expected to restore buffer\r\n                        # types to their original.\r\n                        named_buffers = dict(model.named_buffers())\r\n                        for v in named_buffers.values():\r\n                            if mp_config.buffer_dtype is not None:\r\n                                self.assertEqual(v.dtype, mp_config.buffer_dtype)\r\n                            else:\r\n                                self.assertEqual(v.dtype, _BUFFER_ORIG_DTYPE)\r\n\r\n                    # state_dict should be in full precision\r\n                    state_dict = {k: v.clone() for k, v in model.state_dict().items()}\r\n                    for name, tensor in state_dict.items():\r\n                        # Parameters and buffers are checkpointed in their\r\n                        # original dtypes, which may be different.\r\n                        if name in named_buffers.keys():\r\n                            self.assertEqual(tensor.dtype, _BUFFER_ORIG_DTYPE)\r\n                        else:\r\n                            self.assertEqual(\r\n                                tensor.dtype,\r\n                                full_precision_param_dtype,\r\n                                f\"{name}: {tensor.dtype} vs {full_precision_param_dtype}\",\r\n                            )\r\n\r\n                    # After state_dict, buffer's dtype should have been restored\r\n                    # to the mixed precision one.\r\n                    for buf in model.buffers():\r\n                        if mp_config.buffer_dtype is not None:\r\n                            self.assertEqual(buf.dtype, mp_config.buffer_dtype)\r\n                        else:\r\n                            self.assertEqual(buf.dtype, _BUFFER_ORIG_DTYPE)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_failure_recovery(self):\r\n        process_group = self._get_process_group()\r\n\r\n        # need to create a separate file for the recovered FileStore, because\r\n        # the original one will be deleted when destructing the first FileStore.\r\n        recovery_filename = self.file_name + \"_recovery\"\r\n\r\n        if self.rank == 0:\r\n            # the file will be deleted by the recovered FileStore\r\n            open(recovery_filename, \"w\").close()\r\n\r\n        # not necessary to run barrier here, as DDP will synchronize\r\n\r\n        class TestModel(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.fc1 = nn.Linear(2, 10, bias=False)\r\n                self.fc2 = nn.Linear(10, 4, bias=False)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.relu(self.fc1(x))\r\n                x = self.relu(self.fc2(x))\r\n                return F.softmax(x, dim=1)\r\n\r\n        device_id = gpus_for_rank(self.world_size)[self.rank][0]\r\n        model = TestModel().float().to(device_id)\r\n        ddp = DistributedDataParallel(\r\n            model,\r\n            device_ids=[device_id],\r\n            process_group=process_group,\r\n        )\r\n\r\n        batch_size = 4\r\n        criterion = nn.CrossEntropyLoss()\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)]).to(\r\n            device_id\r\n        )\r\n\r\n        for _ in range(6):\r\n            output = ddp(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()\r\n\r\n        del ddp\r\n        c10d.destroy_process_group(process_group)\r\n\r\n        store = c10d.FileStore(recovery_filename, self.world_size)\r\n        c10d.init_process_group(\r\n            \"nccl\", store=store, rank=self.rank, world_size=self.world_size\r\n        )\r\n        process_group = c10d.distributed_c10d._get_default_group()\r\n        ddp = DistributedDataParallel(\r\n            model,\r\n            device_ids=[device_id],\r\n            process_group=process_group,\r\n        )\r\n\r\n        input = torch.rand([batch_size, 2], dtype=torch.float)\r\n        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)]).to(\r\n            device_id\r\n        )\r\n        for _ in range(6):\r\n            output = ddp(input)\r\n            loss = criterion(output, target)\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_save_load_without_0th_param_state(self, state_dict_type: StateDictType):\r\n        \"\"\"\r\n        Tests saving and loading an optim state dict for Adam optimizer (i.e.\r\n        any optimizer with a \"step\" key in its state) when the first parameter\r\n        does not have optimizer state (e.g. unused or frozen).\r\n        \"\"\"\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin1 = nn.Linear(5, 5)\r\n                self.lin2 = nn.Linear(5, 5)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                # Do not use `lin1`, which is the parameter passed to the\r\n                # optimizer and the one checked for \"step\" state to see if it\r\n                # is tensor or float\r\n                return self.relu(self.lin2(x))\r\n\r\n        model = Model().cuda()\r\n        model.lin1 = FSDP(model.lin1)\r\n        model.lin2 = FSDP(model.lin2)\r\n        fsdp_model = FSDP(model)\r\n        optim = torch.optim.Adam(\r\n            fsdp_model.parameters(), lr=1e-2\r\n        )  # or any optimizer with \"step\"\r\n\r\n        # Run an iteration to construct optimizer state\r\n        device = torch.device(\"cuda\")\r\n        inp = torch.randn((2, 5), device=device)\r\n        loss = fsdp_model(inp).sum()\r\n        loss.backward()\r\n        optim.step()\r\n\r\n        # Check that save and load does not error\r\n        if state_dict_type == StateDictType.FULL_STATE_DICT:\r\n            fsdp_osd = FSDP.full_optim_state_dict(fsdp_model, optim, rank0_only=False)\r\n            flattened_osd = FSDP.shard_full_optim_state_dict(fsdp_osd, fsdp_model)\r\n        elif state_dict_type == StateDictType.SHARDED_STATE_DICT:\r\n            fsdp_osd = FSDP.sharded_optim_state_dict(fsdp_model, optim)\r\n            flattened_osd = FSDP.flatten_sharded_optim_state_dict(\r\n                fsdp_osd, fsdp_model, optim\r\n            )\r\n        optim.load_state_dict(flattened_osd)\r\n        # `__setstate__()` will check the 0th parameter to see if \"step\" is\r\n        # represented as a tensor or float, so it is imperative that its state\r\n        # is non-empty.\r\n\r\n        # Run an iteration as a sanity check\r\n        inp = torch.randn((2, 5), device=device)\r\n        loss = fsdp_model(inp).sum()\r\n        loss.backward()\r\n        optim.step()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_compatible_with_trec(self):\r\n        class DenseModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.net1 = nn.Sequential(nn.Linear(8, 16), nn.ReLU())\r\n                self.net2 = nn.Sequential(nn.Linear(16, 32), nn.ReLU())\r\n                self.net3 = nn.Linear(32, 64)\r\n                self.net4 = nn.Sequential(nn.ReLU(), nn.Linear(64, 8))\r\n\r\n            def forward(self, x):\r\n                return self.net4(self.net3(self.net2(self.net1(x))))\r\n\r\n        class FakeMPModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                torch.manual_seed(0)\r\n                self.dense = FSDP(DenseModel().cuda(), use_orig_params=True)\r\n                if dist.get_rank() == 0:\r\n                    self.sparse0 = nn.Sequential(nn.Linear(8, 8), nn.ReLU())\r\n                else:\r\n                    self.sparse1 = nn.Sequential(nn.Linear(8, 8), nn.ReLU())\r\n\r\n            def forward(self, x):\r\n                if dist.get_rank() == 0:\r\n                    sparse = self.sparse0(x)\r\n                else:\r\n                    sparse = self.sparse1(x)\r\n                dist.all_reduce(sparse)\r\n                return self.dense(sparse)\r\n\r\n        models = [FakeMPModel().cuda(), FakeMPModel().cuda()]\r\n        optims = [\r\n            torch.optim.Adam(models[0].parameters(), lr=1e-2),\r\n            _NamedOptimizer(\r\n                models[1].named_parameters(),\r\n                torch.optim.Adam,\r\n                [{\"params\": models[1].parameters()}],\r\n                models[1],\r\n                lr=1e-2,\r\n            ),\r\n        ]\r\n        state_dicts = []\r\n\r\n        # Train one batch and see if optim_state_dict are the same.\r\n        batch = torch.rand(5, 8, device=torch.device(\"cuda\"))\r\n        for model, optim in zip(models, optims):\r\n            # Eagerly initialize the states\r\n            for param in model.parameters():\r\n                if param.requires_grad:\r\n                    t = torch.zeros_like(param)\r\n                    param.grad = torch.autograd.Variable(t)\r\n            optim.step()\r\n            loss = model(batch).sum()\r\n            loss.backward()\r\n            optim.step()\r\n            state_dicts.append(deepcopy(FSDP.optim_state_dict(model, optim)))\r\n\r\n        self._check_same_param_groups(\r\n            state_dicts[0], state_dicts[1], check_same_param_keys=False\r\n        )\r\n        self._check_same_state(\r\n            state_dicts[0], state_dicts[1], check_same_param_keys=True\r\n        )\r\n\r\n        # Make optim1 has a different state.\r\n        for _ in range(5):\r\n            batch = torch.rand(5, 8).cuda()\r\n            loss = models[1](batch).sum()\r\n            loss.backward()\r\n            optims[1].step()\r\n\r\n        # Load the state back to see if load_optim_state_dict works.\r\n        state_dict_to_load = FSDP.optim_state_dict_to_load(\r\n            models[1], optims[1], state_dicts[1], is_named_optimizer=True\r\n        )\r\n        optims[1].load_state_dict(state_dict_to_load)\r\n        state_dicts[1] = FSDP.optim_state_dict(models[1], optims[1])\r\n\r\n        self._check_same_param_groups(\r\n            state_dicts[0], state_dicts[1], check_same_param_keys=False\r\n        )\r\n        self._check_same_state(\r\n            state_dicts[0], state_dicts[1], check_same_param_keys=True\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_no_grad(self):\r\n        model = TestDummyModel(no_grad=True).cuda()\r\n        fsdp_model = FSDP(deepcopy(model), use_orig_params=True)\r\n        fsdp_optim = torch.optim.Adam(fsdp_model.parameters(), lr=1e-2)\r\n\r\n        for i in range(5):\r\n            if i % 2 == 1:\r\n                fsdp_model.net1[0].weight.requires_grad = True\r\n                fsdp_model.net1[0].bias.requires_grad = True\r\n            else:\r\n                fsdp_model.net1[0].weight.requires_grad = False\r\n                fsdp_model.net1[0].bias.requires_grad = False\r\n            batch = fsdp_model.get_input()\r\n            loss = fsdp_model(batch).sum()\r\n            loss.backward()\r\n            fsdp_optim.step()\r\n            orig_state_dict = deepcopy(fsdp_optim.state_dict())\r\n            FSDP.optim_state_dict(fsdp_model, fsdp_optim)\r\n            FSDP.optim_state_dict_to_load(\r\n                fsdp_model,\r\n                fsdp_optim,\r\n                FSDP.optim_state_dict(fsdp_model, fsdp_optim),\r\n                load_directly=True,\r\n            )\r\n\r\n            self._check_same_state(\r\n                fsdp_optim.state_dict(),\r\n                orig_state_dict,\r\n                check_same_param_keys=True,\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _dist_train(self):\r\n        rank = self.rank\r\n        world_size = self.world_size\r\n        # Save the original torch.distributed.all_gather_into_tensor function since we will\r\n        # patch it to include an artificial delay.\r\n        orig_all_gather = torch.distributed.all_gather_into_tensor\r\n\r\n        def run(compute_cycles, all_gather_cycles):\r\n            has_params = all_gather_cycles > 0\r\n            model = _create_model(compute_cycles, has_params)\r\n\r\n            # Get the input and sets the input's requires_grad to True because\r\n            # we have a fake compute in the forward pass.\r\n            batch = torch.rand(1).cuda()\r\n            batch.requires_grad = True\r\n\r\n            # Run one dummy iteration to trigger the execution order validation\r\n            # all-gathers\r\n            out = model(batch)\r\n            out.backward()\r\n            model.zero_grad(set_to_none=True)\r\n\r\n            # We run 20 iterations but only collect timing data from the minimal 10\r\n            # data points because nondeterministic system events can disturb the timing.\r\n            cpu_iter = Min10()\r\n            cpu_wait = Min10()\r\n            gpu_compute = Min10()\r\n            gpu_total = Min10()\r\n            for _ in range(20):\r\n                # Get two events for measuring the overall time.\r\n                e1 = Event(enable_timing=True)\r\n                e2 = Event(enable_timing=True)\r\n\r\n                cpu_start = time.process_time()\r\n\r\n                all_gather_called = False\r\n\r\n                def _delayed_all_gather(*args, **kwargs):\r\n                    nonlocal all_gather_called\r\n                    all_gather_called = True\r\n                    torch.cuda._sleep(all_gather_cycles)\r\n                    assert orig_all_gather\r\n                    return orig_all_gather(*args, **kwargs)\r\n\r\n                # forward pass\r\n                #\r\n                # Even though both e1 & e2 are on the compute stream, since\r\n                # compute depends on all_gather, e2-e1 includes all_gather time.\r\n                e1.record()\r\n                with patch(\r\n                    \"torch.distributed.all_gather_into_tensor\", _delayed_all_gather\r\n                ):\r\n                    out = model(batch)\r\n                    if has_params and world_size > 1:\r\n                        self.assertTrue(all_gather_called)\r\n                    else:\r\n                        self.assertFalse(all_gather_called)\r\n                e2.record()\r\n\r\n                # backward pass\r\n                out.backward()\r\n                model.zero_grad(set_to_none=True)\r\n\r\n                cpu_iter_time = time.process_time() - cpu_start\r\n\r\n                # wait for gpu\r\n                out.item()\r\n                cpu_wait_for_gpu_time = time.process_time() - cpu_start - cpu_iter_time\r\n\r\n                # get sum of the compute time\r\n                times = []\r\n                for mod in model.modules():\r\n                    if not isinstance(mod, Layer):\r\n                        continue\r\n                    times.append(mod.get_time())\r\n\r\n                # get gpu compute + all_gather time\r\n                overall_gpu_time = e1.elapsed_time(e2)\r\n\r\n                cpu_iter.add(cpu_iter_time)\r\n                cpu_wait.add(cpu_wait_for_gpu_time)\r\n                gpu_compute.add(sum(times))\r\n                gpu_total.add(overall_gpu_time)\r\n\r\n            del model\r\n            return {\r\n                \"cpu_iter\": cpu_iter.avg(),\r\n                \"cpu_wait\": cpu_wait.avg(),\r\n                \"gpu_compute\": gpu_compute.avg(),\r\n                \"gpu_total\": gpu_total.avg(),\r\n            }\r\n\r\n        sleep_cycles = int(100 * get_cycles_per_ms())\r\n\r\n        e1 = run(0, 0)  # no compute, no all-gather\r\n        e2 = run(0, sleep_cycles)  # no compute, only all-gather\r\n        e3 = run(sleep_cycles, 0)  # only compute, no all-gather\r\n        e4 = run(sleep_cycles, sleep_cycles)  # both compute and all-gather\r\n        debug_string = f\"\\nrank{rank}:\\n  e1: {e1}\\n  e2: {e2}\\n  e3: {e3}\\n  e4: {e4}\"\r\n        print(debug_string)\r\n\r\n        # Check the cpu/gpu timing. CPU should run ahead of GPU. Therefore, cpu-gpu\r\n        # wait should be long, except when there is no real work on GPU.\r\n        #\r\n        # If the assertions fail below, we likely have a cpu-gpu wait in the forward/backward pass.\r\n        # e4[\"cpu_iter\"] may not be short as cpu may take some time to queue both compute and all-gather.\r\n        short = [\r\n            e1[\"cpu_iter\"],\r\n            e2[\"cpu_iter\"],\r\n            e3[\"cpu_iter\"],\r\n            e1[\"cpu_wait\"],\r\n        ]\r\n        long = [e3[\"cpu_wait\"], e4[\"cpu_wait\"]]\r\n        if world_size == 1:\r\n            short.append(e2[\"cpu_wait\"])  # all gather should not be happening.\r\n        else:\r\n            long.append(\r\n                e2[\"cpu_wait\"]\r\n            )  # all gather should happen and prolong the cpu-gpu wait.\r\n        for s in short:\r\n            for l in long:\r\n                # 10X longer is a safe margin, since the GPU work timing is around 100X more\r\n                # of that of the CPU.\r\n                self.assertTrue(s * 10 < l)\r\n\r\n        # Check the GPU timing.\r\n        short = [e1[\"gpu_compute\"], e1[\"gpu_total\"], e2[\"gpu_compute\"]]\r\n        long = [e3[\"gpu_compute\"], e3[\"gpu_total\"], e4[\"gpu_compute\"], e4[\"gpu_total\"]]\r\n        if world_size == 1:\r\n            short.append(e2[\"gpu_total\"])  # all gather should not be happening.\r\n        else:\r\n            long.append(\r\n                e2[\"gpu_total\"]\r\n            )  # all gather should happen and prolong the cpu-gpu wait.\r\n        for s in short:\r\n            for l in long:\r\n                # 10X longer is a safe margin, since the time is around 100X longer\r\n                # when there is work on GPU vs. no work.\r\n                self.assertTrue(s * 10 < l)\r\n\r\n        # Check the GPU overlapping when there is all-gather.\r\n        if world_size > 1:\r\n            compute_only = e3[\"gpu_compute\"]\r\n            all_gather_only = e2[\"gpu_total\"]\r\n            both = e4[\"gpu_total\"]\r\n            self.assertTrue(compute_only + all_gather_only > 1.1 * both)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(compute_cycles, all_gather_cycles):\r\n            has_params = all_gather_cycles > 0\r\n            model = _create_model(compute_cycles, has_params)\r\n\r\n            # Get the input and sets the input's requires_grad to True because\r\n            # we have a fake compute in the forward pass.\r\n            batch = torch.rand(1).cuda()\r\n            batch.requires_grad = True\r\n\r\n            # Run one dummy iteration to trigger the execution order validation\r\n            # all-gathers\r\n            out = model(batch)\r\n            out.backward()\r\n            model.zero_grad(set_to_none=True)\r\n\r\n            # We run 20 iterations but only collect timing data from the minimal 10\r\n            # data points because nondeterministic system events can disturb the timing.\r\n            cpu_iter = Min10()\r\n            cpu_wait = Min10()\r\n            gpu_compute = Min10()\r\n            gpu_total = Min10()\r\n            for _ in range(20):\r\n                # Get two events for measuring the overall time.\r\n                e1 = Event(enable_timing=True)\r\n                e2 = Event(enable_timing=True)\r\n\r\n                cpu_start = time.process_time()\r\n\r\n                all_gather_called = False\r\n\r\n                def _delayed_all_gather(*args, **kwargs):\r\n                    nonlocal all_gather_called\r\n                    all_gather_called = True\r\n                    torch.cuda._sleep(all_gather_cycles)\r\n                    assert orig_all_gather\r\n                    return orig_all_gather(*args, **kwargs)\r\n\r\n                # forward pass\r\n                #\r\n                # Even though both e1 & e2 are on the compute stream, since\r\n                # compute depends on all_gather, e2-e1 includes all_gather time.\r\n                e1.record()\r\n                with patch(\r\n                    \"torch.distributed.all_gather_into_tensor\", _delayed_all_gather\r\n                ):\r\n                    out = model(batch)\r\n                    if has_params and world_size > 1:\r\n                        self.assertTrue(all_gather_called)\r\n                    else:\r\n                        self.assertFalse(all_gather_called)\r\n                e2.record()\r\n\r\n                # backward pass\r\n                out.backward()\r\n                model.zero_grad(set_to_none=True)\r\n\r\n                cpu_iter_time = time.process_time() - cpu_start\r\n\r\n                # wait for gpu\r\n                out.item()\r\n                cpu_wait_for_gpu_time = time.process_time() - cpu_start - cpu_iter_time\r\n\r\n                # get sum of the compute time\r\n                times = []\r\n                for mod in model.modules():\r\n                    if not isinstance(mod, Layer):\r\n                        continue\r\n                    times.append(mod.get_time())\r\n\r\n                # get gpu compute + all_gather time\r\n                overall_gpu_time = e1.elapsed_time(e2)\r\n\r\n                cpu_iter.add(cpu_iter_time)\r\n                cpu_wait.add(cpu_wait_for_gpu_time)\r\n                gpu_compute.add(sum(times))\r\n                gpu_total.add(overall_gpu_time)\r\n\r\n            del model\r\n            return {\r\n                \"cpu_iter\": cpu_iter.avg(),\r\n                \"cpu_wait\": cpu_wait.avg(),\r\n                \"gpu_compute\": gpu_compute.avg(),\r\n                \"gpu_total\": gpu_total.avg(),\r\n            }",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_sharded_grad_scaler_found_inf(\r\n        self,\r\n        use_orig_params: bool,\r\n        cpu_offload: CPUOffload,\r\n    ):\r\n        model, optim, ref_model, ref_optim = self._build_model_and_optim(\r\n            cpu_offload=cpu_offload,\r\n            use_orig_params=use_orig_params,\r\n        )\r\n        grad_scaler = ShardedGradScaler(init_scale=2.0)\r\n        ref_grad_scaler = torch.amp.GradScaler(device=\"cuda\", init_scale=2.0)\r\n        scaled_losses: list[torch.Tensor] = []\r\n        device = torch.device(\"cuda\")\r\n        torch.manual_seed(42 + self.rank + 1)\r\n\r\n        for iter in range(10):\r\n            for _model, _optim, _grad_scaler in (\r\n                (ref_model, ref_optim, ref_grad_scaler),\r\n                (model, optim, grad_scaler),\r\n            ):\r\n                module = _model.module\r\n                inp = module.get_input(device)\r\n                _optim.zero_grad()\r\n                output = _model(*inp)\r\n                loss = module.get_loss(inp, output)\r\n                scaled_loss = _grad_scaler.scale(loss)\r\n                scaled_losses.append(scaled_loss)\r\n                scaled_loss.backward()\r\n                orig_params = [\r\n                    param.detach().clone()\r\n                    for param in _model.parameters()\r\n                    if param.grad is not None\r\n                ]\r\n                should_find_inf = iter % 2 == 0\r\n                if should_find_inf and (\r\n                    _model is ref_model or (_model is model and self.rank == 0)\r\n                ):\r\n                    # other ranks should find infs from rank 0\r\n                    # after collectives\r\n                    for param in _model.parameters():\r\n                        if param.grad is None:\r\n                            continue\r\n                        param.grad.fill_(float(\"inf\"))\r\n                        break\r\n                _grad_scaler.step(_optim)\r\n                orig_scale = _grad_scaler.get_scale()\r\n                _grad_scaler.update()\r\n                if should_find_inf:\r\n                    self.assertEqual(\r\n                        _grad_scaler.get_scale(),\r\n                        orig_scale * _grad_scaler.get_backoff_factor(),\r\n                        (\r\n                            f\"rank: {self.rank} iter: {iter} expect origin scale {orig_scale} \"\r\n                            f\"to be backed off by {_grad_scaler.get_backoff_factor()} \"\r\n                            f\"but got {_grad_scaler.get_scale()}\"\r\n                        ),\r\n                    )\r\n                else:\r\n                    self.assertEqual(\r\n                        _grad_scaler.get_scale(),\r\n                        orig_scale,\r\n                        (\r\n                            f\"rank: {self.rank} iter: {iter} expect same scale {orig_scale} \"\r\n                            f\"but got {_grad_scaler.get_scale()}\"\r\n                        ),\r\n                    )\r\n                for param, orig_param in zip(\r\n                    [param for param in _model.parameters() if param.grad is not None],\r\n                    orig_params,\r\n                ):\r\n                    if should_find_inf:\r\n                        self.assertEqual(\r\n                            param,\r\n                            orig_param,\r\n                            (\r\n                                f\"rank: {self.rank} iter: {iter} expect the same params before \"\r\n                                f\"and after optim.step but got {param} vs {orig_param}\"\r\n                            ),\r\n                        )\r\n                    else:\r\n                        self.assertNotEqual(\r\n                            param,\r\n                            orig_param,\r\n                            (\r\n                                f\"rank: {self.rank} iter: {iter} expect the updated params after \"\r\n                                f\"optim.step but got {param} vs {orig_param}\"\r\n                            ),\r\n                        )\r\n            self.assertEqual(\r\n                scaled_losses[0],\r\n                scaled_losses[1],\r\n                f\"iter: {iter} {scaled_losses[0]} vs {scaled_losses[1]}\",\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_local_state_dict_with_empty_ranks(self):\r\n        class Model(Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.my_tensor = torch.full((1,), 3.1415926)\r\n                self.my_parameter = nn.Parameter(self.my_tensor)\r\n\r\n            def forward(self, x):\r\n                return self.my_parameter\r\n\r\n        model = FSDP(Model().cuda())\r\n        with FSDP.state_dict_type(model, StateDictType.LOCAL_STATE_DICT):\r\n            out = model(None)\r\n            out.backward()\r\n\r\n            state_dict = deepcopy(model.state_dict())\r\n            with torch.no_grad():\r\n                with FSDP.summon_full_params(model):\r\n                    self.assertEqual(model.my_parameter.item(), 3.1415926)\r\n                    model.my_parameter.copy_(torch.full((1,), 1.75).cuda())\r\n                    self.assertEqual(model.my_parameter.item(), 1.75)\r\n            model.load_state_dict(state_dict)\r\n            with FSDP.summon_full_params(model):\r\n                self.assertEqual(model.my_parameter.item(), 3.1415926)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_save_and_load_after_forward_state_dict(\r\n        self, state_dict_type, mixed_precision, state_dict_rank0_and_offload\r\n    ):\r\n        \"\"\"\r\n        Test that saving after some training results in params being updated as\r\n        expected.\r\n        \"\"\"\r\n        if state_dict_rank0_and_offload and state_dict_type != \"state_dict\":\r\n            return\r\n        torch.cuda.set_device(self.rank)\r\n        mixed_precision = (\r\n            MixedPrecision(\r\n                param_dtype=torch.float16,\r\n                reduce_dtype=torch.float16,\r\n                buffer_dtype=torch.float16,\r\n            )\r\n            if mixed_precision\r\n            else None\r\n        )\r\n        model = self._get_simple_nested_model(mixed_precision=mixed_precision)\r\n        optim = torch.optim.SGD(model.parameters(), lr=0.1)\r\n        initial_params = get_full_params(model)\r\n        for _ in range(6):\r\n            inp = torch.randn(1, 10, device=torch.cuda.current_device())\r\n            output = model(*inp)\r\n            loss = output.sum()\r\n            expected_dtype = torch.float32 if mixed_precision is None else torch.float16\r\n            self.assertEqual(expected_dtype, loss.dtype)\r\n            loss.backward()\r\n            optim.step()\r\n\r\n        trained_params = get_full_params(model)\r\n        # Ensure some training occurred\r\n        self.assertNotEqual(initial_params, trained_params)\r\n        # Save a copy of the state_dict\r\n        fsd_mgr = self._get_state_dict_mgr(\r\n            model, state_dict_type, state_dict_rank0_and_offload\r\n        )\r\n        with fsd_mgr:\r\n            state_dict = model.state_dict()\r\n            if state_dict_type == \"state_dict\":\r\n                state_dict = {k: v.clone() for k, v in state_dict.items()}\r\n            else:\r\n                for sharded_tensor in state_dict.values():\r\n                    shard = sharded_tensor._local_shards[0]\r\n                    shard.tensor = shard.tensor.clone().detach_()\r\n        self._validate_state_dict_contents(\r\n            model, state_dict, state_dict_rank0_and_offload\r\n        )\r\n        _zero_model(model)\r\n\r\n        # Ensure checkpointed params have the full param dtype\r\n        for tensor in state_dict.values():\r\n            self.assertEqual(tensor.dtype, torch.float32)\r\n\r\n        # Load state_dict into zeroed model\r\n        if state_dict_rank0_and_offload:\r\n            state_dict = self._broadcast_state_dict(state_dict)\r\n\r\n        with FSDP.state_dict_type(model, STATE_DICT_MAPPING[state_dict_type]):\r\n            model.load_state_dict(state_dict, strict=True)\r\n        loaded_params = get_full_params(model)\r\n        self.assertEqual(loaded_params, trained_params)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_access_params_after_forward(\r\n        self,\r\n        sharding_strategy: ShardingStrategy,\r\n    ):\r\n        # NOTE: This test needs to be changed if the FSDP sharding algorithm\r\n        # changes. It is still valuable until such a change to sanity check the\r\n        # `use_orig_params=True` implementation.\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                torch.manual_seed(42)\r\n                # 5 * 5 = 25 numel -> pad to 26 -> 13 on each rank\r\n                self.lin1 = nn.Linear(5, 5, bias=False)\r\n                # 5 * 7 + (1) + 7 = 43 numel -> pad to 44 -> 22 on each rank,\r\n                # where the (1) is from intra-`FlatParameter` alignment padding\r\n                # 22 of weight on rank 0; 13 of weight, 1 alignment padding,\r\n                # and 7 of bias on rank 1\r\n                self.lin2 = nn.Linear(5, 7)\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                z = self.lin1(x)\r\n                z = nn.functional.relu(z)\r\n                z = self.lin2(z)\r\n                return z\r\n\r\n            def get_input(self, device: torch.device) -> tuple[torch.Tensor, ...]:\r\n                return (torch.randn((2, 5)).to(device),)\r\n\r\n            def get_loss(self, inp, out):\r\n                return out.sum()\r\n\r\n        def check_parameter_parity(\r\n            ddp_model: DDP, fsdp_model: FSDP, between_fwd_and_bwd: bool\r\n        ):\r\n            assert self.rank in (\r\n                0,\r\n                1,\r\n            ), f\"Expects world size of 2 but got {self.world_size}\"\r\n            for (n1, p1), (n2, p2) in zip(\r\n                ddp_model.module.named_parameters(),\r\n                fsdp_model.named_parameters(),\r\n            ):\r\n                self.assertEqual(n1, clean_tensor_name(n2))\r\n                if sharding_strategy == ShardingStrategy.NO_SHARD:\r\n                    # For `NO_SHARD`, do nothing since the original parameters\r\n                    # are unflattened\r\n                    pass\r\n                elif (\r\n                    between_fwd_and_bwd\r\n                    and sharding_strategy in NO_RESHARD_AFTER_FORWARD_STRATEGIES\r\n                ):\r\n                    # For no reshard after forward strategies, do nothing since\r\n                    # FSDP did not use sharded views after forward\r\n                    pass\r\n                # Otherwise, case on the parameter (see the model definition)\r\n                elif n1 == \"lin1.weight\":\r\n                    if self.rank == 0:\r\n                        p1 = p1.flatten()[:13]\r\n                    elif self.rank == 1:\r\n                        p1 = p1.flatten()[13:]\r\n                elif n1 == \"lin2.weight\":\r\n                    if self.rank == 0:\r\n                        p1 = p1.flatten()[:22]\r\n                    elif self.rank == 1:\r\n                        p1 = p1.flatten()[22:]\r\n                elif n1 == \"lin2.bias\":\r\n                    if self.rank == 0:\r\n                        p1 = torch.empty(0, device=p1.device)\r\n                    elif self.rank == 1:\r\n                        p1 = p1.flatten()\r\n                torch.testing.assert_close(p1, p2)\r\n\r\n        ddp_model = DDP(Model().cuda(), device_ids=[self.rank])\r\n        fsdp_model = FSDP(\r\n            Model().cuda(),\r\n            sharding_strategy=sharding_strategy,\r\n            auto_wrap_policy=always_wrap_policy,\r\n            use_orig_params=True,\r\n        )\r\n        LR = 1e-2\r\n        ddp_optim = torch.optim.Adam(ddp_model.parameters(), lr=LR)\r\n        fsdp_optim = torch.optim.Adam(fsdp_model.parameters(), lr=LR)\r\n        device = torch.device(\"cuda\")\r\n\r\n        inp = fsdp_model.get_input(device)\r\n        ddp_out = ddp_model(*inp)\r\n        fsdp_out = fsdp_model(*inp)\r\n        check_parameter_parity(ddp_model, fsdp_model, True)\r\n\r\n        ddp_loss = ddp_model.module.get_loss(inp, ddp_out)\r\n        fsdp_loss = fsdp_model.get_loss(inp, fsdp_out)\r\n        ddp_loss.backward()\r\n        fsdp_loss.backward()\r\n        ddp_optim.step()\r\n        fsdp_optim.step()\r\n        check_parameter_parity(ddp_model, fsdp_model, False)\r\n\r\n        inp = fsdp_model.get_input(device)\r\n        ddp_out = ddp_model(*inp)\r\n        fsdp_out = fsdp_model(*inp)\r\n        check_parameter_parity(ddp_model, fsdp_model, True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_named_parameters_in_forward(self):\r\n        \"\"\"\r\n        Tests that calling ``named_parameters()`` during forward returns FQNs\r\n        and ``Tensor`` s corresponding to the original parameters.\r\n        \"\"\"\r\n        param_shapes = [None, None]\r\n        assert_equal_fn = self.assertEqual\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.lin = nn.Linear(5, 5)\r\n\r\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n                nonlocal param_shapes\r\n                # Allow for FSDP prefixes\r\n                param_names = [\r\n                    clean_tensor_name(tup[0]) for tup in self.named_parameters()\r\n                ]\r\n                params = [tup[1] for tup in self.named_parameters()]\r\n                assert (\r\n                    param_shapes[0] is not None and param_shapes[1] is not None\r\n                ), \"`param_sizes` should be set\"\r\n                assert_equal_fn(\r\n                    param_names,\r\n                    [\r\n                        \"lin.weight\",\r\n                        \"lin.bias\",\r\n                    ],\r\n                )\r\n                assert_equal_fn(params[0].shape, param_shapes[0])\r\n                assert_equal_fn(params[1].shape, param_shapes[1])\r\n                return self.lin(x)\r\n\r\n        model = Model().cuda()\r\n        # Save the *unsharded* original parameter shapes and check the shapes\r\n        # match in the forward pass\r\n        param_shapes[0] = model.lin.weight.shape\r\n        param_shapes[1] = model.lin.bias.shape\r\n        fsdp_model = FSDP(model, use_orig_params=True)\r\n        inp = torch.randn((2, 5), device=torch.device(\"cuda\"))\r\n        fsdp_model(inp)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_no_sync_correctness(self, sharding_strategy: ShardingStrategy):\r\n        model = nn.Linear(7, 1, bias=False, device=\"cuda\")\r\n        fsdp_kwargs = {\r\n            \"sharding_strategy\": sharding_strategy,\r\n        }\r\n        model_use_flat_params = FSDP(\r\n            copy.deepcopy(model), use_orig_params=False, **fsdp_kwargs\r\n        )\r\n        model_use_orig_params = FSDP(model, use_orig_params=True, **fsdp_kwargs)\r\n        optim_use_flat_params = torch.optim.AdamW(\r\n            model_use_flat_params.parameters(), foreach=True\r\n        )\r\n        optim_use_orig_params = torch.optim.AdamW(\r\n            model_use_orig_params.parameters(), foreach=True\r\n        )\r\n\r\n        def _check_param_grad_parity(\r\n            _baseline_model: nn.Module,\r\n            _test_model: nn.Module,\r\n        ):\r\n            \"\"\"\r\n            This assumes that the model is ``nn.Linear(7, 1, bias=False)``\r\n            (i.e. with a single 1D weight parameter) to be able to directly\r\n            compare the baseline and test models. On rank 1, the baseline\r\n            includes 1 element of padding.\r\n            \"\"\"\r\n            self.assertEqual(len(list(_baseline_model.parameters())), 1)\r\n            self.assertEqual(len(list(_test_model.parameters())), 1)\r\n            for flat_param, orig_param in zip(\r\n                _baseline_model.parameters(), _test_model.parameters()\r\n            ):\r\n                # Baseline is permitted to have padding\r\n                self.assertGreaterEqual(flat_param.numel(), orig_param.numel())\r\n                unpadded_param_numel = orig_param.numel()\r\n                # For `NO_SHARD`, `use_orig_params=True` presents unflattened\r\n                # parameters, while `False` presents flattened ones\r\n                torch.testing.assert_close(\r\n                    flat_param[:unpadded_param_numel], orig_param.flatten()\r\n                )\r\n                # Gradient numel is different if right after `no_sync()` since\r\n                # the gradient is unsharded, while the parameter is sharded\r\n                unpadded_grad_numel = orig_param.grad.numel()\r\n                # For `use_orig_params=False`, the unsharded gradient is\r\n                # flattened, while for `True`, it is unflattened\r\n                torch.testing.assert_close(\r\n                    flat_param.grad[:unpadded_grad_numel].reshape(\r\n                        orig_param.grad.shape\r\n                    ),\r\n                    orig_param.grad,\r\n                )\r\n\r\n        inp = torch.randn((2, 7), device=\"cuda\")\r\n        grad = torch.randn((2, 1), device=\"cuda\")\r\n\r\n        # Compute some reference gradients using one forward/backward\r\n        out_use_flat_params = model_use_flat_params(inp)\r\n        out_use_orig_params = model_use_orig_params(inp)\r\n        torch.testing.assert_close(out_use_flat_params, out_use_orig_params)\r\n        out_use_flat_params.backward(grad)\r\n        out_use_orig_params.backward(grad)\r\n        _check_param_grad_parity(model_use_flat_params, model_use_orig_params)\r\n        ref_grads_use_flat_params = [\r\n            param.grad.detach().clone() for param in model_use_flat_params.parameters()\r\n        ]\r\n        ref_grads_use_orig_params = [\r\n            param.grad.detach().clone()\r\n            for param in model_use_orig_params.parameters()\r\n            if param.grad is not None\r\n        ]\r\n\r\n        # Run a forward/backward in `no_sync()`\r\n        optim_use_flat_params.zero_grad(set_to_none=True)\r\n        optim_use_orig_params.zero_grad(set_to_none=True)\r\n        for model in (model_use_flat_params, model_use_orig_params):\r\n            with model.no_sync():\r\n                out = model(inp)\r\n                out.backward(grad)\r\n        _check_param_grad_parity(model_use_flat_params, model_use_orig_params)\r\n\r\n        # Run a forward/backward outside `no_sync()`\r\n        for model in (model_use_flat_params, model_use_orig_params):\r\n            out = model(inp)\r\n            out.backward(grad)\r\n        _check_param_grad_parity(model_use_flat_params, model_use_orig_params)\r\n\r\n        # Check that, since we accumulated gradients across 2 iterations, that\r\n        # the new gradients are 2x the reference gradients\r\n        grads_use_flat_params = [\r\n            param.grad.detach().clone() for param in model_use_flat_params.parameters()\r\n        ]\r\n        grads_use_orig_params = [\r\n            param.grad.detach().clone()\r\n            for param in model_use_orig_params.parameters()\r\n            if param.grad is not None\r\n        ]\r\n        for grad, ref_grad in zip(grads_use_flat_params, ref_grads_use_flat_params):\r\n            torch.testing.assert_close(grad, 2 * ref_grad)\r\n        for grad, ref_grad in zip(grads_use_orig_params, ref_grads_use_orig_params):\r\n            torch.testing.assert_close(grad, 2 * ref_grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_fsdp_compile(\r\n        self, sharding_strategy: ShardingStrategy, skip_fsdp_guards: bool\r\n    ):\r\n        torch._dynamo.config.skip_fsdp_guards = skip_fsdp_guards\r\n        fsdp_kwargs = {\r\n            \"auto_wrap_policy\": ModuleWrapPolicy(\r\n                {\r\n                    TransformerEncoderLayer,\r\n                    TransformerDecoderLayer,\r\n                }\r\n            ),\r\n            \"use_orig_params\": True,\r\n            \"sharding_strategy\": sharding_strategy,\r\n            \"backward_prefetch\": BackwardPrefetch.BACKWARD_PRE,\r\n            \"cpu_offload\": CPUOffload(False),\r\n        }\r\n        base_model = TransformerWithSharedParams.init(\r\n            self.process_group,\r\n            FSDPInitMode.NO_FSDP,\r\n            DEVICEInitMode.DEVICE_BEFORE,\r\n            deterministic=True,\r\n        )\r\n        ref_model = FSDP(copy.deepcopy(base_model), self.process_group, **fsdp_kwargs)\r\n        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\r\n        model = FSDP(copy.deepcopy(base_model), self.process_group, **fsdp_kwargs)\r\n        model = torch.compile(model)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n        for _ in range(10):\r\n            losses = []\r\n            inp = ref_model.get_input(torch.device(\"cuda\"))\r\n            for _model, _optim in ((ref_model, ref_optim), (model, optim)):\r\n                _optim.zero_grad()\r\n                loss = _model(*inp).sum()\r\n                losses.append(loss)\r\n                loss.backward()\r\n                _optim.step()\r\n            self.assertEqual(losses[0], losses[1])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_ddp_zero_overlap(\r\n        self,\r\n        device,\r\n        hook_constructor,\r\n        gradient_as_bucket_view,\r\n        static_graph,\r\n        **kwargs,\r\n    ):\r\n        SGD_LR = 0.01\r\n        SGD_MOMENTUM = 0.9\r\n        SGD_WEIGHT_DECAY = 0.001\r\n        NUM_INPUTS = 5\r\n        torch.manual_seed(0)\r\n        torch.cuda.manual_seed(0)\r\n\r\n        rank = self.rank\r\n        is_gpu = device.type == \"cuda\"\r\n        if is_gpu:\r\n            torch.cuda.set_device(device)\r\n        models_to_test = [\r\n            (\r\n                torch.nn.Sequential(\r\n                    torch.nn.Linear(1000, 2000),\r\n                    torch.nn.Linear(2000, 500),\r\n                ),\r\n                [torch.randn(1, 1000).to(device) for _ in range(NUM_INPUTS)],\r\n            )\r\n        ]\r\n        if HAS_TORCHVISION:\r\n            models_to_test.append(\r\n                (\r\n                    torchvision.models.resnet50(),\r\n                    [torch.randn(1, 3, 3, 1000).to(device) for _ in range(NUM_INPUTS)],\r\n                )\r\n            )\r\n        for model, inputs in models_to_test:\r\n            # Enable determinism in cudnn operators\r\n            with torch.backends.cudnn.flags(\r\n                enabled=True, deterministic=True, benchmark=False\r\n            ):\r\n                device_ids = [rank] if is_gpu else None\r\n                # Set up the DDP model overlapping with ZeRO\r\n                ddp_model_overlap = DDP(\r\n                    copy.deepcopy(model).to(device),\r\n                    device_ids=device_ids,\r\n                    gradient_as_bucket_view=gradient_as_bucket_view,\r\n                )\r\n                if static_graph:\r\n                    ddp_model_overlap._set_static_graph()\r\n                zero_optim = ZeroRedundancyOptimizer(\r\n                    ddp_model_overlap.parameters(),\r\n                    optimizer_class=torch.optim.SGD,\r\n                    overlap_with_ddp=True,\r\n                    lr=SGD_LR,\r\n                    momentum=SGD_MOMENTUM,\r\n                    weight_decay=SGD_WEIGHT_DECAY,\r\n                )\r\n                ddp_model_overlap.register_comm_hook(\r\n                    None,\r\n                    hook_constructor(\r\n                        allreduce_hook,\r\n                        ddp_model_overlap,\r\n                        zero_optim,\r\n                        **kwargs,\r\n                    ),\r\n                )\r\n\r\n                # Set up the DDP model with local optimizer\r\n                ddp_model_local = DDP(\r\n                    copy.deepcopy(model).to(device),\r\n                    device_ids=device_ids,\r\n                    gradient_as_bucket_view=gradient_as_bucket_view,\r\n                )\r\n                if static_graph:\r\n                    ddp_model_local._set_static_graph()\r\n                local_optim = torch.optim.SGD(\r\n                    ddp_model_local.parameters(),\r\n                    lr=SGD_LR,\r\n                    momentum=SGD_MOMENTUM,\r\n                    weight_decay=SGD_WEIGHT_DECAY,\r\n                )\r\n\r\n                # Check that the parameters match initially\r\n                for p1, p2 in zip(\r\n                    ddp_model_overlap.parameters(), ddp_model_local.parameters()\r\n                ):\r\n                    self.assertEqual(p1, p2)\r\n\r\n                # Save the parameters to ensure they were updated\r\n                init_params_overlap = copy.deepcopy(\r\n                    list(ddp_model_overlap.parameters())\r\n                )\r\n\r\n                # Ensure that this test runs independently\r\n                dist.barrier()\r\n\r\n                # Run the DDP model overlapping with ZeRO\r\n                # NOTE: Overlapping currently requires 2 or 3 warmup iterations\r\n                # to ensure DDP buckets have been rebuilt (depending on the\r\n                # value of `static_graph`)\r\n                num_warmup_inputs = 2 if not static_graph else 3\r\n                for input in inputs[:num_warmup_inputs]:\r\n                    output = ddp_model_overlap(input)\r\n                    loss = output.sum()\r\n                    loss.backward()\r\n                for input in inputs:\r\n                    zero_optim.zero_grad()\r\n                    output = ddp_model_overlap(input)\r\n                    loss = output.sum()\r\n                    loss.backward()\r\n\r\n                # Run the DDP model with local optimizer\r\n                for input in inputs:\r\n                    local_optim.zero_grad()\r\n                    output = ddp_model_local(input)\r\n                    loss = output.sum()\r\n                    loss.backward()\r\n                    local_optim.step()\r\n                dist.barrier()\r\n\r\n                # Check that the parameters are equal\r\n                for p1, p2 in zip(\r\n                    ddp_model_overlap.parameters(), ddp_model_local.parameters()\r\n                ):\r\n                    self.assertEqual(p1, p2)\r\n\r\n                # Check that the parameters were updated\r\n                self.assertNotEqual(\r\n                    init_params_overlap,\r\n                    list(ddp_model_overlap.parameters()),\r\n                )\r\n\r\n                # Ensure that this test runs independently\r\n                dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def check(optimizer):\r\n            for _ in range(EPOCHS):\r\n                target = torch.rand((BATCH_SIZE, OUTPUT_DIM), device=device)\r\n                inputs = torch.rand((BATCH_SIZE, INPUT_DIM), device=device)\r\n\r\n                def closure():\r\n                    optimizer.zero_grad()\r\n                    output = model(inputs)\r\n                    loss = loss_fn(output, target)\r\n                    loss /= self.world_size\r\n                    loss.backward()\r\n                    dist.all_reduce(loss, group=process_group)\r\n                    return loss\r\n\r\n                _ = optimizer.step(closure=closure)\r\n\r\n                # Check that the parameters match across ranks after a step\r\n                for pg in optimizer.param_groups:\r\n                    for p in pg[\"params\"]:\r\n                        receptacle = (\r\n                            [p.clone() for _ in subgroup_ranks]\r\n                            if self.rank == REFERENCE_RANK\r\n                            else []\r\n                        )\r\n                        dist.gather(\r\n                            p,\r\n                            receptacle,\r\n                            dst=REFERENCE_RANK,\r\n                            group=process_group,\r\n                        )\r\n                        if self.rank == REFERENCE_RANK:\r\n                            reference_param = receptacle[0]\r\n                            for param in receptacle[1:]:\r\n                                torch.testing.assert_close(\r\n                                    reference_param,\r\n                                    param,\r\n                                    msg=\"Models differ between ranks\",\r\n                                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr_scheduler(self):\r\n        \"\"\"Check that a normal PyTorch ``lr_scheduler`` is usable with\r\n        ZeroRedundancyOptimizer.\"\"\"\r\n        self.dist_init(self.rank)\r\n        NUM_ITERS = 5\r\n        LR = 0.01\r\n        x = torch.tensor([1.0], device=self.device, requires_grad=True)\r\n        x2 = torch.tensor([1.0], device=self.device, requires_grad=True)\r\n        o = ZeroRedundancyOptimizer([x], optimizer_class=SGD, lr=LR)\r\n        o2 = torch.optim.SGD([x2], lr=LR)\r\n        s = torch.optim.lr_scheduler.StepLR(o, 1)\r\n        s2 = torch.optim.lr_scheduler.StepLR(o2, 1)\r\n        for _ in range(NUM_ITERS):\r\n            x.backward()\r\n            o.zero_grad()\r\n            o.step()\r\n            s.step()\r\n            x2.backward()\r\n            o2.zero_grad()\r\n            o2.step()\r\n            s2.step()\r\n            self.assertEqual(x, x2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_lr_scheduler(self):\r\n        \"\"\"Check that a normal PyTorch ``lr_scheduler`` is usable with\r\n        ZeroRedundancyOptimizer.\"\"\"\r\n        self.dist_init(self.rank)\r\n        x = torch.tensor([1.0], device=self.device, requires_grad=True)\r\n        x2 = torch.tensor([1.0], device=self.device, requires_grad=True)\r\n        o = ZeroRedundancyOptimizer([x], optimizer_class=SGD, lr=0.01)\r\n        o2 = torch.optim.SGD([x2], lr=0.01)\r\n        s = torch.optim.lr_scheduler.StepLR(o, 1)\r\n        s2 = torch.optim.lr_scheduler.StepLR(o2, 1)\r\n        for _ in range(5):\r\n            x.backward()\r\n            o.zero_grad()\r\n            o.step()\r\n            s.step()\r\n            x2.backward()\r\n            o2.zero_grad()\r\n            o2.step()\r\n            s2.step()\r\n            self.assertEqual(x, x2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_stage_backward_weight_multiple_iters(self):\r\n        # MLP as a stage module\r\n        mod = MLPModule(d_hid)\r\n        inputs = []\r\n        for _ in range(10):\r\n            x = torch.randn(batch_size, d_hid)\r\n            inputs.append(x)\r\n            # As in a pipeline stage, the inputs to this stage requires gradients\r\n            x.requires_grad_(True)\r\n\r\n        target = torch.randn(batch_size, d_hid)\r\n        loss_fn = torch.nn.MSELoss(reduction=\"sum\")\r\n\r\n        # Make a copy\r\n        ref_mod = copy.deepcopy(mod)\r\n        ref_inputs = []\r\n        for x in inputs:\r\n            ref_inputs.append(x.detach().requires_grad_(x.requires_grad))\r\n        ref_target = target.detach()\r\n\r\n        # Forward, then backward of loss with respect to inputs\r\n        for x in inputs:\r\n            out = mod(x)\r\n            loss = loss_fn(out, target)\r\n            _dinputs, param_groups = stage_backward_input(\r\n                stage_outputs_or_loss=(loss,),\r\n                output_grads=None,\r\n                input_values=[x],\r\n                weights=mod.parameters(),\r\n            )\r\n\r\n            # backward of loss with respect to weights\r\n            stage_backward_weight(mod.parameters(), param_groups)\r\n\r\n        # Run reference\r\n        for ref_x in ref_inputs:\r\n            ref_out = ref_mod(ref_x)\r\n            ref_loss = loss_fn(ref_out, ref_target)\r\n            ref_loss.backward()\r\n\r\n        # Every rank checks gradients\r\n        for name, p in mod.named_parameters():\r\n            ref_p = ref_mod.get_parameter(name)\r\n            try:\r\n                torch.testing.assert_close(p.grad, ref_p.grad)\r\n            except AssertionError:\r\n                print(f\"Gradient test failed for {name}: {p.grad} vs {ref_p.grad}\")\r\n                raise",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_step_with_closure(self):\r\n        \"\"\"Check that ZeroRedundancyOptimizer properly exposes the\r\n        ``step(closure)`` interface.\"\"\"\r\n        self.dist_init(self.rank, world_size=self.world_size)\r\n\r\n        with self.context:\r\n            for bucket_view in [False, True]:\r\n                x_val = self.rank + 1\r\n                weight = 1.0\r\n                bias = 2.0\r\n                error = 1.0\r\n                target = torch.tensor(\r\n                    [x_val * weight + bias + error],\r\n                    device=self.device,\r\n                )\r\n                loss_fn = torch.nn.L1Loss()\r\n\r\n                x = torch.tensor([float(x_val)], device=self.device)\r\n                m = torch.nn.Linear(1, 1)\r\n                m.weight.data = torch.tensor([[weight]])\r\n                m.bias.data = torch.tensor([bias])\r\n                m.to(self.device)\r\n\r\n                o = ZeroRedundancyOptimizer(\r\n                    m.parameters(),\r\n                    optimizer_class=SGD,\r\n                    parameters_as_bucket_view=bucket_view,\r\n                    lr=0.1,\r\n                )\r\n\r\n                y = m(x)\r\n                y.backward(x)\r\n                for p in m.parameters():\r\n                    dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)\r\n                    p.grad.data /= self.world_size\r\n\r\n                def closure():\r\n                    o.zero_grad()\r\n                    output = m(x)\r\n                    loss = loss_fn(output, target)\r\n                    loss.backward()\r\n                    return loss\r\n\r\n                loss = o.step(closure=closure)\r\n\r\n                self.assertEqual(loss, torch.tensor(error))\r\n                self.assertEqual(m.weight, torch.tensor([[1.1]]))\r\n                self.assertEqual(m.bias, torch.tensor([2.1]))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_multiple_param_groups(self):\r\n        \"\"\"\r\n        Check parity between constructing ZeRO with multiple parameter groups\r\n        upfront versus adding parameter groups to ZeRO after construction\r\n        versus a non-sharded optimizer.\r\n        \"\"\"\r\n        self.dist_init(self.rank)\r\n        BATCH_SIZE, NUM_ITERS = 8, 3\r\n        INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM = 5, 10, 5\r\n        WD, LR = 0.01, 0.01\r\n        model1 = torch.nn.Sequential(\r\n            torch.nn.Linear(INPUT_DIM, HIDDEN_DIM),\r\n            torch.nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\r\n            torch.nn.Linear(HIDDEN_DIM, OUTPUT_DIM),\r\n        )\r\n        model2 = copy.deepcopy(model1)\r\n        model3 = copy.deepcopy(model1)\r\n        model1 = model1.to(self.device)\r\n        model2 = model2.to(self.device)\r\n        model3 = model3.to(self.device)\r\n        inputs = [\r\n            torch.randn(BATCH_SIZE, INPUT_DIM).to(self.device) for _ in range(NUM_ITERS)\r\n        ]\r\n        # Construct `optim1` with both parameter groups upfront\r\n        optim1 = ZeroRedundancyOptimizer(\r\n            [\r\n                {\"params\": [l.weight for l in model1], \"weight_decay\": 0.0},\r\n                {\"params\": [l.bias for l in model1], \"weight_decay\": WD},\r\n            ],\r\n            optimizer_class=AdamW,\r\n            lr=LR,\r\n        )\r\n        # Construct `optim2` by adding the second parameter after\r\n        optim2 = ZeroRedundancyOptimizer(\r\n            [l.weight for l in model2],\r\n            optimizer_class=AdamW,\r\n            lr=LR,\r\n            weight_decay=0.0,\r\n        )\r\n        optim2.add_param_group({\"params\": [l.bias for l in model2], \"weight_decay\": WD})\r\n        # Construct `optim3` as a non-sharded optimizer\r\n        optim3 = AdamW(\r\n            [\r\n                {\"params\": [l.weight for l in model3], \"weight_decay\": 0.0},\r\n                {\"params\": [l.bias for l in model3], \"weight_decay\": WD},\r\n            ],\r\n            lr=LR,\r\n        )\r\n        # Check parity over a few iterations\r\n        for input in inputs:\r\n            for model, optim in (\r\n                (model1, optim1),\r\n                (model2, optim2),\r\n                (model3, optim3),\r\n            ):\r\n                optim.zero_grad()\r\n                out = model(input)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                optim.step()\r\n            for layer1, layer2, layer3 in zip(model1, model2, model3):\r\n                torch.testing.assert_close(layer1.weight, layer2.weight)\r\n                torch.testing.assert_close(layer1.weight, layer3.weight)\r\n                torch.testing.assert_close(layer1.bias, layer2.bias)\r\n                torch.testing.assert_close(layer1.bias, layer3.bias)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_nll(self):\r\n        device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\r\n        shard_spec = [Replicate()]\r\n\r\n        pred_list = torch.rand(ITER_TIME, 1024, 10)\r\n        target_list = torch.randint(0, 10, (ITER_TIME, 1024), dtype=torch.long)\r\n\r\n        criterion = torch.nn.CrossEntropyLoss()\r\n\r\n        for i in range(ITER_TIME):\r\n            pred = pred_list[i].to(self.device_type).requires_grad_()\r\n            target = target_list[i].to(self.device_type)\r\n\r\n            # nll with dtensor\r\n            pred_dtensor = distribute_tensor(pred, device_mesh, shard_spec)\r\n            target_dtensor = distribute_tensor(target, device_mesh, shard_spec)\r\n            loss = criterion(pred_dtensor, target_dtensor)\r\n            loss.backward()\r\n\r\n            # nll with plain tensor\r\n            loss_gt = criterion(pred, target)\r\n            loss_gt.backward()\r\n\r\n            loss_diff_abs = loss.to_local() - loss_gt\r\n            loss_diff_rel = loss_diff_abs / (torch.abs(loss_gt) + 1e-8)\r\n            loss_mse_abs = torch.mean(loss_diff_abs * loss_diff_abs).item()\r\n            loss_mse_rel = torch.mean(loss_diff_rel * loss_diff_rel).item()\r\n\r\n            grad_diff_abs = pred_dtensor.grad.to_local() - pred.grad\r\n            grad_diff_rel = grad_diff_abs / (torch.abs(pred.grad) + 1e-8)\r\n            grad_mse_abs = torch.mean(grad_diff_abs * grad_diff_abs).item()\r\n            grad_mse_rel = torch.mean(grad_diff_rel * grad_diff_rel).item()\r\n\r\n            self.assertTrue(\r\n                loss_mse_abs <= 1e-6,\r\n                f\"Too large absolute mse for loss, expected less equal 1e-6, got {loss_mse_abs}\",\r\n            )\r\n            self.assertTrue(\r\n                loss_mse_rel <= 1e-6,\r\n                f\"Too large relative mse for loss, expected less equal 1e-6, got {loss_mse_rel}\",\r\n            )\r\n            self.assertTrue(\r\n                grad_mse_abs <= 1e-6,\r\n                f\"Too large absolute mse for gradient, expected less equal 1e-6, got {grad_mse_abs}\",\r\n            )\r\n            self.assertTrue(\r\n                grad_mse_rel <= 1e-6,\r\n                f\"Too large relative mse for gradient, expected less equal 1e-6, got {grad_mse_rel}\",\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_checkpoint_kwargs(self):\r\n        class MyModel(torch.nn.Module):\r\n            def __init__(self, raise_exp: bool, change_shape_in_recomp: bool):\r\n                super().__init__()\r\n                self.fwd_count = 0\r\n                self.raise_exp = raise_exp\r\n                self.change_shape_in_recomp = change_shape_in_recomp\r\n                self.a = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                if self.raise_exp and self.fwd_count == 0:\r\n                    raise RuntimeError(\"foo\")\r\n                if self.raise_exp and self.fwd_count == 1:\r\n                    raise RuntimeError(\"bar\")\r\n                if self.change_shape_in_recomp and self.fwd_count == 1:\r\n                    x.relu_()\r\n                random_tensor = torch.randn(1, 2)\r\n                x = self.a(x + random_tensor)\r\n                self.fwd_count += 1\r\n                return x\r\n\r\n        m = MyModel(True, False)\r\n        m0, m1, m2, m3 = (deepcopy(m) for _ in range(4))\r\n\r\n        # composable checkpoint does not support use_reentrant=True\r\n        with self.assertRaisesRegex(\r\n            NotImplementedError,\r\n            \"use_reentrant=True is not supported in composable checkpoint. \"\r\n            \"Please use torch.utils.checkpoint.checkpoint instead.\",\r\n        ):\r\n            checkpoint(m, use_reentrant=True)\r\n\r\n        # check giving an unsupported kwarg\r\n        with self.assertRaisesRegex(ValueError, \"Unexpected keyword arguments: foo\"):\r\n            checkpoint(m0, foo=\"bar\")\r\n\r\n        handled_fwd_exp = False\r\n        handled_recomp_exp = False\r\n\r\n        @contextmanager\r\n        def fwd_ctx(mod: MyModel):\r\n            try:\r\n                mod.raise_exp = False\r\n                yield\r\n            finally:\r\n                nonlocal handled_fwd_exp\r\n                handled_fwd_exp = True\r\n                mod.raise_exp = True\r\n\r\n        @contextmanager\r\n        def recomp_ctx(mod: MyModel):\r\n            try:\r\n                mod.raise_exp = False\r\n                yield\r\n            finally:\r\n                nonlocal handled_recomp_exp\r\n                handled_recomp_exp = True\r\n                mod.raise_exp = True\r\n\r\n        # Test different context functions\r\n        x = torch.randn(1, 2, requires_grad=True)\r\n        checkpoint(\r\n            m1, context_fn=lambda: (partial(fwd_ctx, m1)(), partial(recomp_ctx, m1)())\r\n        )\r\n        m1(x.clone()).sum().backward()\r\n        self.assertEqual((handled_fwd_exp, handled_recomp_exp), (True, True))\r\n\r\n        checkpoint(m2, context_fn=lambda: (nullcontext(), partial(recomp_ctx, m2)()))\r\n        with self.assertRaisesRegex(RuntimeError, \"foo\"):\r\n            m2(x.clone())\r\n\r\n        handled_fwd_exp = False  # Reset flag\r\n        checkpoint(m3, context_fn=lambda: (partial(fwd_ctx, m3)(), nullcontext()))\r\n        with self.assertRaisesRegex(RuntimeError, \"bar\"):\r\n            m3(x.clone()).sum().backward()\r\n        self.assertEqual(handled_fwd_exp, True)\r\n\r\n        # Test determinism check failure\r\n        m4 = MyModel(False, True)\r\n        m5 = deepcopy(m4)\r\n        # Determinism check should not throw an error,\r\n        # but autograd should throw a RuntimeError\r\n        checkpoint(m4, determinism_check=\"none\")\r\n        with self.assertRaises(RuntimeError):\r\n            m4(x.clone()).sum().backward()\r\n\r\n        # Determinism check should throw a CheckpointError\r\n        checkpoint(m5, determinism_check=\"default\")\r\n        with self.assertRaises(CheckpointError):\r\n            m5(x.clone()).sum().backward()\r\n\r\n        # Test preserving random state\r\n        m6 = MyModel(False, False)\r\n        m7, m8 = (deepcopy(m6) for _ in range(2))\r\n        checkpoint(m7, preserve_rng_state=False)\r\n        checkpoint(m8, preserve_rng_state=True)\r\n\r\n        for mi in (m6, m7, m8):\r\n            torch.manual_seed(42)\r\n            loss = mi(x.clone()).sum()\r\n            torch.manual_seed(41)\r\n            loss.backward()\r\n        # check that m6 and m7 have at least one different grad\r\n        self.assertNotEqual(\r\n            (p1.grad for p1 in m6.parameters()), (p2.grad for p2 in m7.parameters())\r\n        )\r\n        # check that m6 and m8 have identical grads\r\n        for p1, p2 in zip(m6.parameters(), m8.parameters()):\r\n            self.assertEqual(p1.grad, p2.grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_compile(\r\n        self,\r\n        *,\r\n        use_gpu: bool,\r\n        no_sync: bool,\r\n        setup_func: Optional[Callable] = None,\r\n        no_inductor: bool = False,\r\n        no_compile_forward: bool = False,\r\n        checkpoint: bool = False,\r\n    ):\r\n        backend = \"nccl\" if use_gpu else \"gloo\"\r\n        dist.init_process_group(\r\n            backend=backend,\r\n            rank=self.rank,\r\n            world_size=self.world_size,\r\n            store=dist.FileStore(self.file_name, self.world_size),\r\n        )\r\n        if use_gpu:\r\n            torch.cuda.set_device(f\"cuda:{self.rank}\")\r\n            device = torch.device(\"cuda\")\r\n        else:\r\n            device = torch.device(\"cpu\")\r\n\r\n        torch._dynamo.config.optimize_ddp = (\r\n            \"python_reducer_without_compiled_forward\"\r\n            if no_compile_forward\r\n            else \"python_reducer\"\r\n        )\r\n        torch.manual_seed(123)\r\n        model = Net(checkpoint=checkpoint).to(device)\r\n        input = torch.randn([1, DIM], device=device)\r\n\r\n        compiled_replicate_model = replicate(deepcopy(model))\r\n        if not no_compile_forward:\r\n            compiled_replicate_model = torch.compile(\r\n                compiled_replicate_model, fullgraph=False\r\n            )\r\n        compiled_replicate_optim = torch.optim.Adam(\r\n            compiled_replicate_model.parameters()\r\n        )\r\n        compiled_ddp_model = DDP(deepcopy(model))\r\n        if not no_compile_forward:\r\n            compiled_ddp_model = torch.compile(compiled_ddp_model, fullgraph=True)\r\n        compiled_ddp_optim = torch.optim.Adam(compiled_ddp_model.parameters())\r\n        model = replicate(model)\r\n        optim = torch.optim.Adam(model.parameters())\r\n\r\n        if setup_func:\r\n            setup_func(model, compiled_replicate_model, compiled_ddp_model)\r\n\r\n        models = [model, compiled_replicate_model, compiled_ddp_model]\r\n        optims = [optim, compiled_replicate_optim, compiled_ddp_optim]\r\n        sync_contexts = [\r\n            contextlib.nullcontext(),\r\n            contextlib.nullcontext(),\r\n            compiled_ddp_model.no_sync(),\r\n        ]\r\n\r\n        # Run multiple iterations so that we could test no_sync\r\n        for i in range(2):\r\n            # Setting a different random seed so that if the allreduces are not\r\n            # executed correctly, the gradients won't be correct compared to the\r\n            # eager DDP.\r\n            torch.manual_seed(123 + self.rank + i)\r\n            input = torch.randn([1, DIM], device=device)\r\n\r\n            for model_idx in range(3):\r\n                if no_sync and i % 2 == 0:\r\n                    context = sync_contexts[model_idx]\r\n                    if model_idx <= 1:\r\n                        models[model_idx].set_requires_gradient_sync(False)\r\n                else:\r\n                    context = contextlib.nullcontext()\r\n                    if model_idx <= 1:\r\n                        models[model_idx].set_requires_gradient_sync(True)\r\n                context = contextlib.nullcontext()\r\n\r\n                with context:\r\n                    bwd_context = (\r\n                        contextlib.nullcontext()\r\n                        if model_idx == 0\r\n                        else compiled_autograd._enable(compiler_fn(no_inductor))\r\n                    )\r\n                    with bwd_context:\r\n                        loss = models[model_idx](input).sum()\r\n                        loss.backward()\r\n\r\n            if not no_sync or i % 2 == 1:\r\n                for p1, p2, p3 in zip(\r\n                    model.parameters(),\r\n                    compiled_replicate_model.parameters(),\r\n                    compiled_ddp_model.parameters(),\r\n                ):\r\n                    self.assertEqual(p1.grad, p2.grad)\r\n                    self.assertEqual(p1.grad, p3.grad)\r\n                for optim in optims:\r\n                    optim.step()\r\n                    optim.zero_grad()\r\n\r\n        self.assertEqual(\r\n            tuple(model.parameters()), tuple(compiled_replicate_model.parameters())\r\n        )\r\n        self.assertEqual(\r\n            tuple(model.parameters()), tuple(compiled_ddp_model.parameters())\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_unused_forward_output(self, reshard_after_forward: Union[bool, int]):\r\n        torch.manual_seed(42)\r\n        local_batch_size = 2\r\n        global_batch_size, dim = (self.world_size * local_batch_size, 24)\r\n        model = DoubleLinear(dim=dim, use_second_linear=True)\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        fully_shard(model.lin1, reshard_after_forward=reshard_after_forward)\r\n        fully_shard(model, reshard_after_forward=reshard_after_forward)\r\n        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n\r\n        torch.manual_seed(1)  # same on all ranks\r\n        for iter_idx in range(10):\r\n            # Use all forward outputs in the loss/backward for the first half\r\n            # of the iterations and only the 1st forward output for the rest\r\n            global_inp = torch.rand((global_batch_size, dim), device=\"cuda\")\r\n            local_inp = global_inp[\r\n                self.rank * local_batch_size : (self.rank + 1) * local_batch_size\r\n            ].detach()\r\n            out1, out2 = model(local_inp)\r\n            loss = (out1 * out2).sum() if iter_idx < 3 else out1.sum()\r\n            loss.backward()\r\n            optim.step()\r\n            ref_out1, ref_out2 = ref_model(global_inp)\r\n            ref_loss = (ref_out1 * ref_out2).sum() if iter_idx < 3 else ref_out1.sum()\r\n            ref_loss.backward()\r\n            self._reduce_1d_partial_grads(ref_model)\r\n            ref_optim.step()\r\n            dist.all_reduce(loss)  # partial -> replicated\r\n            self.assertEqual(loss, ref_loss)\r\n            optim.zero_grad(set_to_none=(iter_idx % 2))\r\n            ref_optim.zero_grad(set_to_none=(iter_idx % 2))\r\n            check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_post_acc_grad_hook_optim_parity(self):\r\n        \"\"\"\r\n        Tests parity of running the optimizer via the post-accumulate-grad\r\n        hook vs. normally.\r\n        \"\"\"\r\n        torch.manual_seed(42)\r\n        model_args = ModelArgs(dropout_p=0.0)\r\n        model = Transformer(model_args)\r\n\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        for module in itertools.chain(ref_model.layers, [ref_model]):\r\n            fully_shard(module)\r\n        optim_kwargs = {\"lr\": 1e-2, \"foreach\": False}\r\n        ref_optim = torch.optim.AdamW(ref_model.parameters(), **optim_kwargs)\r\n        lr_scheduler_kwargs = {\"step_size\": 5}\r\n        ref_lr_scheduler = torch.optim.lr_scheduler.StepLR(\r\n            ref_optim, **lr_scheduler_kwargs\r\n        )\r\n\r\n        for module in itertools.chain(model.layers, [model]):\r\n            fully_shard(module)\r\n        param_to_optim = {}\r\n        param_to_lr_scheduler = {}\r\n        for param in model.parameters():\r\n            param_to_optim[param] = torch.optim.AdamW([param], **optim_kwargs)\r\n            param_to_lr_scheduler[param] = torch.optim.lr_scheduler.StepLR(\r\n                param_to_optim[param], **lr_scheduler_kwargs\r\n            )\r\n\r\n        def optim_hook(param: nn.Parameter) -> None:\r\n            param_to_optim[param].step()\r\n            param_to_optim[param].zero_grad()\r\n            param_to_lr_scheduler[param].step()\r\n\r\n        for param in model.parameters():\r\n            param.register_post_accumulate_grad_hook(optim_hook)\r\n\r\n        torch.manual_seed(42 + self.rank)\r\n        inp = torch.randint(0, model_args.vocab_size, (2, 16), device=\"cuda\")\r\n        for _ in range(10):\r\n            ref_loss = ref_model(inp).sum()\r\n            ref_loss.backward()\r\n            ref_optim.step()\r\n            ref_optim.zero_grad()\r\n            ref_lr_scheduler.step()\r\n            loss = model(inp).sum()\r\n            loss.backward()\r\n            self.assertTrue(torch.equal(ref_loss, loss))\r\n            for ref_param, param in zip(ref_model.parameters(), model.parameters()):\r\n                self.assertTrue(torch.equal(ref_param, param))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_set_reduce_scatter_divide_factor(self, divide_factor: float):\r\n        torch.manual_seed(42)\r\n        model_args = ModelArgs(dropout_p=0.0, weight_tying=False)\r\n        model = Transformer(model_args)\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        ref_optim = torch.optim.AdamW(ref_model.parameters(), lr=1e-2)\r\n        for module in model.modules():\r\n            if isinstance(module, TransformerBlock):\r\n                fully_shard(module, reshard_after_forward=False)\r\n        model = fully_shard(model, reshard_after_forward=False)\r\n        optim = torch.optim.AdamW(model.parameters(), lr=1e-2)\r\n        model.set_reduce_scatter_divide_factor(divide_factor)\r\n\r\n        torch.manual_seed(42 + self.rank)\r\n        inp = torch.randint(0, model_args.vocab_size, (2, 16), device=\"cuda\")\r\n\r\n        for _ in range(10):\r\n            ref_loss = ref_model(inp).sum()\r\n            ref_loss.backward()\r\n            for param in ref_model.parameters():\r\n                param.grad.mul_(1.0 / divide_factor)\r\n                dist.all_reduce(param.grad)\r\n            loss = model(inp).sum()\r\n            loss.backward()\r\n            ref_optim.step()\r\n            optim.step()\r\n            ref_optim.zero_grad()\r\n            optim.zero_grad()\r\n            self.assertEqual(ref_loss, loss)\r\n            check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_compute_dtype(\r\n        self,\r\n        param_dtype: torch.dtype,\r\n        reshard_after_forward: Union[bool, int],\r\n        use_shard_placement_fn: bool,\r\n    ):\r\n        ref_model, ref_optim, model, optim = self._init_models_and_optims(\r\n            reshard_after_forward,\r\n            param_dtype=param_dtype,\r\n            reduce_dtype=None,\r\n            use_shard_placement_fn=use_shard_placement_fn,\r\n        )\r\n        ref_model_bf16 = copy.deepcopy(ref_model).to(param_dtype)\r\n        orig_reduce_scatter = dist.reduce_scatter_tensor\r\n\r\n        def assert_fn(output: torch.Tensor):\r\n            self.assertEqual(output.dtype, param_dtype)\r\n\r\n        reduce_scatter = functools.partial(\r\n            reduce_scatter_with_assert, self, orig_reduce_scatter, assert_fn\r\n        )\r\n        predivide_factor, postdivide_factor = _get_gradient_divide_factors(\r\n            self.process_group, all_reduce_group=None, reduce_dtype=param_dtype\r\n        )\r\n\r\n        torch.manual_seed(42 + self.rank + 1)\r\n        inp = torch.randn((4, 16), device=\"cuda\", dtype=param_dtype)\r\n        for iter_idx in range(10):\r\n            optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            fsdp_loss = model(inp).sum()\r\n            with patch_reduce_scatter(reduce_scatter):\r\n                fsdp_loss.backward()\r\n            optim.step()\r\n\r\n            ref_optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            ref_loss = ref_model_bf16(inp.to(param_dtype)).sum()\r\n            ref_loss.backward()\r\n            for param in ref_model_bf16.parameters():\r\n                # Use reduce-scatter -> all-gather as all-reduce because for\r\n                # world size >=4, NCCL all-reduce shows numeric differences\r\n                # compared with NCCL reduce-scatter\r\n                if predivide_factor is not None and predivide_factor > 1:\r\n                    param.grad.div_(predivide_factor)\r\n                elif predivide_factor is None:\r\n                    param.grad.div_(self.world_size)\r\n                output = torch.zeros_like(torch.chunk(param.grad, self.world_size)[0])\r\n                dist.reduce_scatter_tensor(output, param.grad)\r\n                dist.all_gather_into_tensor(param.grad, output)\r\n                if postdivide_factor is not None and postdivide_factor > 1:\r\n                    param.grad.div_(postdivide_factor)\r\n            for param_fp32, param_bf16 in zip(\r\n                ref_model.parameters(), ref_model_bf16.parameters()\r\n            ):\r\n                param_fp32.grad = param_bf16.grad.to(param_fp32.dtype)\r\n                param_bf16.grad = None\r\n            ref_optim.step()  # fp32 optimizer step\r\n            for param_fp32, param_bf16 in zip(\r\n                ref_model.parameters(), ref_model_bf16.parameters()\r\n            ):\r\n                param_bf16.detach().copy_(param_fp32)\r\n\r\n            self.assertEqual(fsdp_loss, ref_loss)\r\n            check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_reduce_dtype_fp32_reduce(\r\n        self, reshard_after_forward: Union[bool, int], use_shard_placement_fn: bool\r\n    ):\r\n        if (\r\n            self.world_size > 2\r\n            and isinstance(reshard_after_forward, int)\r\n            and use_shard_placement_fn\r\n        ):\r\n            return\r\n        param_dtype, reduce_dtype = torch.bfloat16, torch.float32\r\n        ref_model, ref_optim, model, optim = self._init_models_and_optims(\r\n            reshard_after_forward,\r\n            param_dtype=param_dtype,\r\n            reduce_dtype=reduce_dtype,\r\n            use_shard_placement_fn=use_shard_placement_fn,\r\n        )\r\n        ref_model_bf16 = copy.deepcopy(ref_model).to(param_dtype)\r\n        orig_reduce_scatter = dist.reduce_scatter_tensor\r\n\r\n        def assert_fn(output: torch.Tensor):\r\n            self.assertEqual(output.dtype, reduce_dtype)\r\n\r\n        reduce_scatter = functools.partial(\r\n            reduce_scatter_with_assert, self, orig_reduce_scatter, assert_fn\r\n        )\r\n        torch.manual_seed(42 + self.rank + 1)\r\n        inp = torch.randn((4, 16), device=\"cuda\", dtype=param_dtype)\r\n        for iter_idx in range(10):\r\n            optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            fsdp_loss = model(inp).sum()\r\n            with patch_reduce_scatter(reduce_scatter):\r\n                fsdp_loss.backward()\r\n            optim.step()\r\n\r\n            ref_optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            ref_loss = ref_model_bf16(inp.to(param_dtype)).sum()\r\n            ref_loss.backward()\r\n            for param in ref_model_bf16.parameters():\r\n                param.grad.data = param.grad.to(torch.float32)\r\n                dist.all_reduce(param.grad)  # fp32 reduction\r\n                param.grad.div_(self.world_size)\r\n            for param_fp32, param_bf16 in zip(\r\n                ref_model.parameters(), ref_model_bf16.parameters()\r\n            ):\r\n                param_fp32.grad = param_bf16.grad\r\n                param_bf16.grad = None\r\n            ref_optim.step()  # fp32 optimizer step\r\n            for param_fp32, param_bf16 in zip(\r\n                ref_model.parameters(), ref_model_bf16.parameters()\r\n            ):\r\n                param_bf16.detach().copy_(param_fp32)\r\n\r\n            self.assertEqual(fsdp_loss, ref_loss)\r\n            check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_reduce_dtype_bf16_reduce(\r\n        self, reshard_after_forward: Union[bool, int], use_shard_placement_fn: bool\r\n    ):\r\n        param_dtype, reduce_dtype = torch.float32, torch.bfloat16\r\n        ref_model, ref_optim, model, optim = self._init_models_and_optims(\r\n            reshard_after_forward,\r\n            param_dtype=param_dtype,\r\n            reduce_dtype=reduce_dtype,\r\n            use_shard_placement_fn=use_shard_placement_fn,\r\n        )\r\n        group = dist.distributed_c10d._get_default_group()\r\n        orig_reduce_scatter = dist.reduce_scatter_tensor\r\n\r\n        def assert_fn(output: torch.Tensor):\r\n            self.assertEqual(output.dtype, reduce_dtype)\r\n\r\n        reduce_scatter = functools.partial(\r\n            reduce_scatter_with_assert, self, orig_reduce_scatter, assert_fn\r\n        )\r\n        torch.manual_seed(42 + self.rank + 1)\r\n        inp = torch.randn((4, 16), device=\"cuda\", dtype=param_dtype)\r\n        for iter_idx in range(10):\r\n            optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            fsdp_loss = model(inp).sum()\r\n            with patch_reduce_scatter(reduce_scatter):\r\n                fsdp_loss.backward()\r\n            optim.step()\r\n\r\n            ref_optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\r\n            ref_loss = ref_model(inp).sum()\r\n            ref_loss.backward()\r\n            for param in ref_model.parameters():\r\n                param_grad = param.grad.to(reduce_dtype)\r\n                # Use reduce-scatter -> all-gather to implement all-reduce\r\n                # since for world size >2, bf16 all-reduce and reduce-scatter\r\n                # have numeric differences\r\n                sharded_grad = funcol.reduce_scatter_tensor(\r\n                    param_grad, scatter_dim=0, reduceOp=\"avg\", group=group\r\n                )  # bf16 reduction\r\n                param.grad = funcol.all_gather_tensor(\r\n                    sharded_grad, gather_dim=0, group=group\r\n                ).to(\r\n                    param.dtype\r\n                )  # upcast to fp32\r\n            ref_optim.step()  # fp32 optimizer step\r\n\r\n            self.assertEqual(fsdp_loss, ref_loss)\r\n            check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run_train_steps(num_iters: int, use_post_optim_event: bool):\r\n            for _ in range(num_iters):\r\n                optim.zero_grad()\r\n                with patch_all_gather(delayed_all_gather):\r\n                    loss = model(inp).sum()\r\n                loss.backward()\r\n                with implicit_replication():\r\n                    optim.step()\r\n                if use_post_optim_event:\r\n                    post_optim_event = torch.cuda.current_stream().record_event()\r\n                    model[1].set_post_optim_event(post_optim_event)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_train_parity_shard_placement_fn_shard_largest_dim(self):\r\n        torch.manual_seed(42)\r\n        model_args = ModelArgs(n_layers=3, dropout_p=0.0)\r\n        model = Transformer(model_args)\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        ref_optim = torch.optim.AdamW(ref_model.parameters(), lr=1e-2)\r\n\r\n        def shard_placement_fn(param: nn.Parameter) -> Optional[Shard]:\r\n            largest_dim = -1\r\n            largest_dim_size = -1\r\n            for dim, dim_size in enumerate(param.shape):\r\n                if dim_size > largest_dim_size:\r\n                    largest_dim = dim\r\n                    largest_dim_size = dim_size\r\n            return Shard(largest_dim)\r\n\r\n        for layer in model.layers:\r\n            fully_shard(layer, shard_placement_fn=shard_placement_fn)\r\n        fully_shard(model, shard_placement_fn=shard_placement_fn)\r\n        optim = torch.optim.AdamW(model.parameters(), lr=1e-2)\r\n\r\n        for param, ref_param in zip(model.parameters(), ref_model.parameters()):\r\n            full_param = param.full_tensor()\r\n            self.assertEqual(full_param, ref_param)\r\n\r\n        torch.manual_seed(42 + self.rank)\r\n        inp = torch.randint(0, model_args.vocab_size, (2, 16), device=\"cuda\")\r\n        for iter_idx in range(5):\r\n            ref_loss = ref_model(inp).sum()\r\n            loss = model(inp).sum()\r\n            self.assertEqual(ref_loss, loss)\r\n\r\n            ref_loss.backward()\r\n            loss.backward()\r\n            for param in ref_model.parameters():\r\n                if param.grad is not None:\r\n                    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)\r\n\r\n            ref_optim.step()\r\n            optim.step()\r\n            ref_optim.zero_grad()\r\n            optim.zero_grad()\r\n\r\n        for param, ref_param in zip(model.parameters(), ref_model.parameters()):\r\n            full_param = param.full_tensor()\r\n            self.assertEqual(full_param, ref_param)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_register_fsdp_forward_method(self):\r\n        \"\"\"Based on https://github.com/pytorch/pytorch/issues/109385\"\"\"\r\n\r\n        class VisionTransformer(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.patch_proj = nn.Conv2d(3, 1024, kernel_size=14, stride=14)\r\n\r\n            def forward_features(self, imgs: torch.Tensor) -> torch.Tensor:\r\n                return self.patch_proj(imgs).flatten(2).transpose(1, 2)\r\n\r\n            def forward(self, imgs: torch.Tensor) -> torch.Tensor:\r\n                return self.forward_features(imgs).sum(dim=1)\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.vit, self.projector = VisionTransformer(), nn.Linear(1024, 256)\r\n\r\n            def forward(self, imgs: torch.Tensor) -> torch.Tensor:\r\n                # Run `vit.forward_features`, which is not `forward`!\r\n                patch_embeddings = self.vit.forward_features(imgs)\r\n                return self.projector(patch_embeddings)\r\n\r\n        torch.manual_seed(42)\r\n        model = Model()\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        fully_shard(model.vit)\r\n        fully_shard(model.projector)\r\n        fully_shard(model)\r\n        register_fsdp_forward_method(model.vit, \"forward_features\")\r\n\r\n        torch.manual_seed(42 + self.rank + 1)\r\n        inp = torch.randn(4, 3, 224, 224, device=\"cuda\")\r\n        ref_loss = ref_model(inp).sum()\r\n        loss = model(inp).sum()\r\n        self.assertEqual(ref_loss, loss)\r\n        ref_loss.backward()\r\n        loss.backward()\r\n        for param in ref_model.parameters():\r\n            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)\r\n        check_sharded_parity(self, ref_model, model)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_train_parity_2d_transformer(self, use_shard_placement_fn: bool):\r\n        torch.manual_seed(42)\r\n        model_args = ModelArgs(n_layers=3, dropout_p=0.0)\r\n        model = Transformer(model_args)\r\n        ref_model = copy.deepcopy(model).cuda()\r\n        ref_optim = torch.optim.AdamW(ref_model.parameters(), lr=1e-2)\r\n\r\n        dp_size, tp_size = self.world_size // 2, 2\r\n        global_mesh = init_device_mesh(\r\n            \"cuda\", (dp_size, tp_size), mesh_dim_names=(\"dp\", \"tp\")\r\n        )\r\n        model = Transformer.parallelize(model, global_mesh[\"tp\"], use_seq_parallel=True)\r\n\r\n        def _shard_placement_fn(param: nn.Parameter) -> Optional[Shard]:\r\n            if isinstance(param, DTensor):\r\n                for placement in param.placements:\r\n                    if isinstance(placement, Shard):\r\n                        shard_dim = param.ndim - 1 - placement.dim\r\n                        assert shard_dim >= 0, f\"{param.shape}\"\r\n                        return Shard(shard_dim)\r\n            return Shard(0)\r\n\r\n        shard_placement_fn = _shard_placement_fn if use_shard_placement_fn else None\r\n        for layer in model.layers:\r\n            fully_shard(\r\n                layer, mesh=global_mesh[\"dp\"], shard_placement_fn=shard_placement_fn\r\n            )\r\n        fully_shard(\r\n            model, mesh=global_mesh[\"dp\"], shard_placement_fn=shard_placement_fn\r\n        )\r\n        optim = torch.optim.AdamW(model.parameters(), lr=1e-2)\r\n\r\n        for param, ref_param in zip(model.parameters(), ref_model.parameters()):\r\n            full_param = param.full_tensor()\r\n            self.assertEqual(full_param, ref_param)\r\n\r\n        torch.manual_seed(42 + global_mesh.get_local_rank(\"dp\"))\r\n        inp = torch.randint(0, model_args.vocab_size, (2, 16), device=\"cuda\")\r\n        for iter_idx in range(5):\r\n            ref_loss = ref_model(inp).sum()\r\n            loss = model(inp).sum()\r\n            self.assertEqual(ref_loss, loss)\r\n            ref_loss.backward()\r\n            loss.backward()\r\n            for param in ref_model.parameters():\r\n                if param.grad is not None:\r\n                    dist.all_reduce(\r\n                        param.grad,\r\n                        group=global_mesh.get_group(\"dp\"),\r\n                        op=dist.ReduceOp.AVG,\r\n                    )\r\n\r\n            # Specially check the TP placement for `pos_embeddings.weight` and\r\n            # its which since the grad naturally has replicate placement,\r\n            # requiring FSDP to redistribute it to shard placement before FSDP\r\n            # runs its reduce-scatter\r\n            self.assertIsInstance(model.pos_embeddings.weight.placements[1], Shard)\r\n            self.assertIsInstance(model.pos_embeddings.weight.grad.placements[1], Shard)\r\n            for ref_param, (param_name, param) in zip(\r\n                ref_model.parameters(), model.named_parameters()\r\n            ):\r\n                full_grad = param.grad.full_tensor()\r\n                self.assertEqual(ref_param.grad, full_grad)\r\n\r\n            ref_optim.step()\r\n            optim.step()\r\n            ref_optim.zero_grad()\r\n            optim.zero_grad()\r\n\r\n        for param, ref_param in zip(model.parameters(), ref_model.parameters()):\r\n            full_param = param.full_tensor()\r\n            self.assertEqual(full_param, ref_param)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_tracker_multi_group(\r\n        self,\r\n        reshard_after_forward: Union[bool, int],\r\n        offload_policy: OffloadPolicy,\r\n        mp_policy: MixedPrecisionPolicy,\r\n    ):\r\n        debug = False\r\n        dev = torch.device(torch.cuda.current_device())\r\n        _init_cublas_workspace(dev)\r\n        gc.collect()\r\n        _reset_mem_stats(dev)\r\n        mem_stats = torch.cuda.memory_stats(dev)\r\n        pre_cuda_active = mem_stats[\"active_bytes.all.current\"]\r\n        torch.manual_seed(42)\r\n        lin_dim, bsz = 2048, 8192\r\n        with torch.device(dev):\r\n            model = nn.Sequential(*[MLP(dim=lin_dim, device=dev) for _ in range(4)])\r\n        mesh = init_device_mesh(\"cuda\", (self.world_size,))\r\n        fully_shard_fn = functools.partial(\r\n            fully_shard,\r\n            mesh=mesh,\r\n            reshard_after_forward=reshard_after_forward,\r\n            offload_policy=offload_policy,\r\n            mp_policy=mp_policy,\r\n        )\r\n        for mlp in model:\r\n            fully_shard_fn(mlp)\r\n        fully_shard_fn(model)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n        inp = torch.randn((bsz, lin_dim), device=dev)\r\n        fmt = FSDPMemTracker(model, optim)\r\n        fmt.track_inputs((inp,))\r\n        with fmt:\r\n            for iter_idx in range(2):\r\n                loss = model(inp).sum()\r\n                loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n                if iter_idx == 0:\r\n                    fmt.reset_mod_stats()\r\n        mem_stats = torch.cuda.memory_stats()\r\n        tracker_max = fmt.get_tracker_snapshot(\"peak\")[dev][\"Total\"]\r\n        cuda_max = mem_stats[\"active_bytes.all.peak\"] - pre_cuda_active\r\n        accuracy = tracker_max / cuda_max\r\n        if self.rank == 0 and debug:\r\n            print(f\"Accuracy: {accuracy} Tracker Max:{tracker_max} CUDA Max:{cuda_max}\")\r\n        self.assertAlmostEqual(\r\n            accuracy,\r\n            1.0,\r\n            delta=0.1,\r\n            msg=f\"Tracker Max:{tracker_max} CUDA Max:{cuda_max}\",\r\n        )\r\n        del model\r\n        del inp\r\n        del optim",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_tracker_non_root_forward_backward(self):\r\n        \"\"\"\r\n        Tests tracker accracy when running forward/backward through a non-root.\r\n        \"\"\"\r\n        debug = False\r\n        dev = torch.device(torch.cuda.current_device())\r\n        _init_cublas_workspace(dev)\r\n        gc.collect()\r\n        _reset_mem_stats(dev)\r\n        mem_stats = torch.cuda.memory_stats(dev)\r\n        pre_cuda_active = mem_stats[\"active_bytes.all.current\"]\r\n        torch.manual_seed(42)\r\n        lin_dim, bsz = 2048, 8\r\n        model = nn.Sequential(*[MLP(lin_dim, dev) for _ in range(3)])\r\n        for mlp in model:\r\n            fully_shard(mlp)\r\n        fully_shard(model)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\r\n        torch.manual_seed(42 + self.rank)\r\n        inp = torch.randn((bsz, lin_dim), device=dev)\r\n        fmt = FSDPMemTracker(model, optim)\r\n        fmt.track_inputs((inp,))\r\n        with fmt:\r\n            for iter_idx in range(2):\r\n                nonroot_loss = model[0](inp).sum()\r\n                nonroot_loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n                if iter_idx == 0:\r\n                    fmt.reset_mod_stats()\r\n        mem_stats = torch.cuda.memory_stats()\r\n        tracker_max = fmt.get_tracker_snapshot(\"peak\")[dev][\"Total\"]\r\n        cuda_max = mem_stats[\"active_bytes.all.peak\"] - pre_cuda_active\r\n        accuracy = tracker_max / cuda_max\r\n        if self.rank == 0 and debug:\r\n            print(f\"Accuracy: {accuracy} Tracker Max:{tracker_max} CUDA Max:{cuda_max}\")\r\n        self.assertAlmostEqual(\r\n            accuracy,\r\n            1.0,\r\n            delta=0.1,\r\n            msg=f\"Tracker Max:{tracker_max} CUDA Max:{cuda_max}\",\r\n        )\r\n        del inp\r\n        del model\r\n        del optim",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_tracker_with_activation_checkpointing(\r\n        self, reshard_after_forward: Union[bool, int], checkpoint_impl: str\r\n    ):\r\n        assert checkpoint_impl in (\"composable\", \"wrapper\")\r\n        debug = False\r\n        dev = torch.device(torch.cuda.current_device())\r\n        _init_cublas_workspace(dev)\r\n        gc.collect()\r\n        _reset_mem_stats(dev)\r\n        mem_stats = torch.cuda.memory_stats(dev)\r\n        pre_cuda_active = mem_stats[\"active_bytes.all.current\"]\r\n        torch.manual_seed(42)\r\n        vocab_size = 8192\r\n        bsz, seq_len = 16, 512\r\n        with torch.device(dev):\r\n            model_args = ModelArgs(\r\n                n_layers=4,\r\n                n_heads=4,\r\n                vocab_size=vocab_size,\r\n                max_seq_len=seq_len,\r\n                dropout_p=0.1,\r\n            )\r\n            model = Transformer(model_args)\r\n        foreach = False\r\n        fully_shard_fn = functools.partial(\r\n            fully_shard,\r\n            reshard_after_forward=reshard_after_forward,\r\n        )\r\n        if checkpoint_impl == \"wrapper\":\r\n            apply_activation_checkpointing(\r\n                model, check_fn=lambda m: isinstance(m, TransformerBlock)\r\n            )\r\n            for module in model.modules():\r\n                # Apply to `CheckpointWrapper`, which wraps `TransformerBlock`\r\n                if isinstance(module, CheckpointWrapper):\r\n                    fully_shard_fn(module)\r\n        else:\r\n            for module in model.modules():\r\n                if isinstance(module, TransformerBlock):\r\n                    if checkpoint_impl == \"composable\":\r\n                        checkpoint(module)\r\n                    fully_shard_fn(module)\r\n        fully_shard_fn(model)\r\n        optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=foreach)\r\n\r\n        torch.manual_seed(42 + self.rank)\r\n        inp = torch.randint(0, vocab_size, (bsz, seq_len), device=dev)\r\n        fmt = FSDPMemTracker(model, optim)\r\n        fmt.track_inputs((inp,))\r\n        with fmt:\r\n            for iter_idx in range(2):\r\n                loss = model(inp).sum()\r\n                loss.backward()\r\n                optim.step()\r\n                optim.zero_grad()\r\n                if iter_idx == 0:\r\n                    fmt.reset_mod_stats()\r\n        mem_stats = torch.cuda.memory_stats()\r\n        tracker_max = fmt.get_tracker_snapshot(\"peak\")[dev][\"Total\"]\r\n        cuda_max = mem_stats[\"active_bytes.all.peak\"] - pre_cuda_active\r\n        accuracy = tracker_max / cuda_max\r\n        if self.rank == 0 and debug:\r\n            print(f\"Accuracy: {accuracy} Tracker Max:{tracker_max} CUDA Max:{cuda_max}\")\r\n        self.assertAlmostEqual(\r\n            accuracy,\r\n            1.0,\r\n            delta=0.1,\r\n            msg=f\"Tracker Max:{tracker_max} CUDA Max:{cuda_max}\",\r\n        )\r\n        del inp\r\n        del model\r\n        del optim",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_aot_sequence_nr(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.conv1 = torch.nn.Conv2d(\r\n                    in_channels=16,\r\n                    out_channels=16,\r\n                    kernel_size=(1, 1),\r\n                    stride=1,\r\n                    padding=\"same\",\r\n                    bias=True,\r\n                )\r\n                self.bn1 = torch.nn.BatchNorm2d(num_features=16)\r\n                self.relu1 = torch.nn.ReLU()\r\n                self.fc1 = torch.nn.Linear(in_features=1638400, out_features=1)\r\n                self.loss_fn = torch.nn.L1Loss()\r\n\r\n            def forward(self, x, target):\r\n                y = x\r\n                x = self.conv1(x)\r\n                x = self.bn1(x)\r\n                x = self.relu1(x)\r\n                x = x + y\r\n                x = torch.flatten(x)\r\n                x = self.fc1(x)\r\n                output = self.loss_fn(x, target)\r\n\r\n                return (output,)\r\n\r\n        mod = Model()\r\n        mod.train()\r\n        x = torch.rand(100, 16, 32, 32, requires_grad=True)\r\n        target = torch.rand(1)\r\n\r\n        # Use dynamo export to get the fx graph module\r\n        g_mod, _ = torch._dynamo.export(mod, x, target)\r\n\r\n        def _prepare_model_args():\r\n            named_parameters = dict(g_mod.named_parameters(remove_duplicate=False))\r\n            named_buffers = dict(g_mod.named_buffers(remove_duplicate=False))\r\n            params_and_buffers = {\r\n                **dict(named_parameters),\r\n                **dict(named_buffers),\r\n            }\r\n            params_and_buffers_flat, params_spec = pytree.tree_flatten(\r\n                params_and_buffers\r\n            )\r\n            params_len = len(params_and_buffers_flat)\r\n            functional_call = create_functional_call(g_mod, params_spec, params_len)\r\n            return params_and_buffers_flat, functional_call\r\n\r\n        full_args, fn_to_trace = _prepare_model_args()\r\n        param_and_buf_len = len(full_args)\r\n        full_args.extend([x, target])\r\n\r\n        # aot_export requires a graph mod input of fwd graph\r\n        # returns the full fwd/bwd graph in graph mod format\r\n        with torch.enable_grad(), fx_traceback.preserve_node_meta():\r\n            fx_g, _, _, _ = _aot_export_function(\r\n                fn_to_trace,\r\n                full_args,\r\n                decompositions=None,\r\n                num_params_buffers=param_and_buf_len,\r\n                no_tangents=True,\r\n            )\r\n\r\n        # Walk all the nodes in fx graph.\r\n        # Write the resulting ops to a table\r\n        min_seq_nr = -1\r\n        seq_table = \"SeqNr|OrigAten|SrcFn|FwdSrcFn\\n\"\r\n        for node in fx_g.graph.nodes:\r\n            if \"call_\" in node.op and \"getitem\" not in str(node.target):\r\n                seq_nr = node.meta.get(\"seq_nr\", -1)\r\n                if seq_nr < 0:\r\n                    continue\r\n                if min_seq_nr < 0:\r\n                    min_seq_nr = seq_nr\r\n                source_fn_stack = node.meta.get(\"source_fn_stack\", [])\r\n                orig_aten = node.meta.get(\"original_aten\", \"\")\r\n                mod_name = \"\"\r\n                if len(source_fn_stack) > 0:\r\n                    mod_name = source_fn_stack[-1][0]\r\n                # Make all seq_nr relative so it starts at 0\r\n                seq_nr = seq_nr - min_seq_nr\r\n                # For backward nodes, also test that metadata from the corresponding\r\n                # forward node is copied over.\r\n                fwd_source_fn_stack = node.meta.get(\"fwd_source_fn_stack\", [])\r\n                fwd_mod_name = \"\"\r\n                if len(fwd_source_fn_stack):\r\n                    fwd_mod_name = fwd_source_fn_stack[-1][0]\r\n                seq_table = (\r\n                    seq_table + f\"{seq_nr}|{orig_aten}|{mod_name}|{fwd_mod_name}\\n\"\r\n                )\r\n\r\n        self.maxDiff = None\r\n        self.assertExpectedInline(\r\n            seq_table,\r\n            dedent(\r\n                \"\"\"\\\r\nSeqNr|OrigAten|SrcFn|FwdSrcFn\r\n0|aten.convolution.default|l__self___conv1|\r\n0|aten.add.Tensor|l__self___bn1|\r\n1|aten._native_batch_norm_legit_functional.default|l__self___bn1|\r\n2|aten.relu.default|l__self___relu1|\r\n2|aten.detach.default|l__self___relu1|\r\n2|aten.detach.default|l__self___relu1|\r\n3|aten.add.Tensor|add|\r\n4|aten.view.default|flatten|\r\n5|aten.view.default|l__self___fc1|\r\n6|aten.t.default|l__self___fc1|\r\n7|aten.addmm.default|l__self___fc1|\r\n8|aten.view.default|l__self___fc1|\r\n9|aten.sub.Tensor|l__self___loss_fn|\r\n10|aten.abs.default|l__self___loss_fn|\r\n11|aten.mean.default|l__self___loss_fn|\r\n11|aten.ones_like.default||l__self___loss_fn\r\n11|aten.expand.default||l__self___loss_fn\r\n11|aten.div.Scalar||l__self___loss_fn\r\n10|aten.sgn.default||l__self___loss_fn\r\n10|aten.mul.Tensor||l__self___loss_fn\r\n8|aten.view.default||l__self___fc1\r\n7|aten.t.default||l__self___fc1\r\n7|aten.mm.default||l__self___fc1\r\n7|aten.t.default||l__self___fc1\r\n7|aten.mm.default||l__self___fc1\r\n7|aten.t.default||l__self___fc1\r\n7|aten.sum.dim_IntList||l__self___fc1\r\n7|aten.view.default||l__self___fc1\r\n6|aten.t.default||l__self___fc1\r\n5|aten.view.default||l__self___fc1\r\n4|aten.view.default||\r\n2|aten.detach.default||l__self___relu1\r\n2|aten.detach.default||l__self___relu1\r\n2|aten.threshold_backward.default||l__self___relu1\r\n1|aten.native_batch_norm_backward.default||l__self___bn1\r\n0|aten.convolution_backward.default||l__self___conv1\r\n11|aten.add.Tensor||l__self___loss_fn\r\n\"\"\"\r\n            ),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_basic(self):\r\n        def model(x, y):\r\n            return (x + y) * y\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(x, y):\r\n            for _ in range(N_ITERS):\r\n                loss = model(x, y).sum()\r\n                loss.backward()\r\n\r\n        x = torch.randn(3, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn(3, device=\"cuda\")\r\n        fn(x, y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_mutate_constant(self):\r\n        def model(x, y):\r\n            c = torch.tensor(1)\r\n            c.add_(2)\r\n            return x * y * 0 + c\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(x, y):\r\n            for i in range(N_ITERS):\r\n                with self.subTest(i):\r\n                    loss = model(x, y).sum()\r\n                    self.assertTrue(same(loss, torch.tensor(3.0, device=\"cuda\")))\r\n                    loss.backward()\r\n\r\n        x = torch.randn(1, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn(1, device=\"cuda\")\r\n        fn(x, y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_factory(self):\r\n        def model(y):\r\n            x = torch.zeros(3, device=\"cuda:0\")\r\n            x.add_(3)\r\n            return x * y\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(y):\r\n            for i in range(N_ITERS):\r\n                with self.subTest(i):\r\n                    loss = model(y).sum()\r\n                    loss.backward()\r\n\r\n        y = torch.randn(3, device=\"cuda:0\", requires_grad=True)\r\n        fn(y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_dtoh(self):\r\n        def model(x, y):\r\n            a = x + y\r\n            b = a.cpu() * 3\r\n            return b\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(x, y):\r\n            for _ in range(N_ITERS):\r\n                loss = model(x, y).sum()\r\n                loss.backward()\r\n\r\n        x = torch.randn(3, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn(3, device=\"cuda\")\r\n        fn(x, y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_htod(self):\r\n        def model(x, y):\r\n            a = x + y\r\n            return a * 3\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(x, y):\r\n            for _ in range(N_ITERS):\r\n                loss = model(x, y).sum()\r\n                loss.backward()\r\n\r\n        x = torch.randn(3, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn((), device=\"cpu\")\r\n        fn(x, y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_mutate_input(self):\r\n        def model(x, y):\r\n            y.add_(3)\r\n            return x * y\r\n\r\n        @torch.compile(backend=\"cudagraphs\")\r\n        def fn(x, y):\r\n            for i in range(N_ITERS):\r\n                with self.subTest(i):\r\n                    y_orig = y.clone()\r\n                    loss = model(x, y).sum()\r\n                    self.assertTrue(same(y, y_orig + 3))\r\n                    loss.backward()\r\n\r\n        x = torch.randn(3, device=\"cuda\", requires_grad=True)\r\n        y = torch.randn(3, device=\"cuda\")\r\n        fn(x, y)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_grad_scaling_autocast(self, device, dtype, optim_info, foreach, fused):\r\n        try_pickle = False\r\n\r\n        def run(device, data, model, optimizer, scaler, loss_fn, skip_iter, try_scaling_api):\r\n            for i, (input, target) in enumerate(data):\r\n                optimizer.zero_grad()\r\n                with torch.autocast(device_type=device, dtype=torch.half, enabled=try_scaling_api):\r\n                    output = model(input)\r\n                    loss = loss_fn(output, target)\r\n                if try_scaling_api:\r\n                    scaler.scale(loss).backward()\r\n                    if i == skip_iter and scaler.is_enabled():\r\n                        with torch.no_grad():\r\n                            model[1].weight.grad.fill_(float('inf'))\r\n                    scaler.step(optimizer)\r\n                    scaler.update()\r\n                    if try_pickle:\r\n                        scaler = pickle.loads(pickle.dumps(scaler))\r\n                else:\r\n                    loss.backward()\r\n                    if (not scaler.is_enabled()) or (i != skip_iter):\r\n                        optimizer.step()\r\n            return scaler\r\n\r\n        optimizer_ctor = optim_info.optim_cls\r\n\r\n        # Compares no scaling + no autocasting against scaling + autocasting.\r\n        # NOTE(mkozuki): With current way of testing, `torch.optim.Adam` is failing in spite of `foreach` and `fused`.\r\n        #   Giving some flexibility to this test might help.\r\n        context = contextlib.nullcontext\r\n        if optimizer_ctor in (torch.optim.Adam, torch.optim.AdamW):\r\n            from functools import partial\r\n            context = partial(self.assertRaises, AssertionError)\r\n        with context():\r\n            # sets atol=1e-3 because we're comparing pure fp32 arithmetic vs a mixture of fp16 and fp32\r\n            self._run_scaling_case(\r\n                device, run, unskipped=3, skipped=1, atol=1e-3,\r\n                optimizer_ctor=optimizer_ctor, optimizer_kwargs={\"foreach\": foreach, \"fused\": fused},\r\n            )\r\n            # this will be picked up by try_pickle within run():\r\n            try_pickle = True\r\n            self._run_scaling_case(\r\n                device, run, unskipped=3, skipped=1, atol=1e-3,\r\n                optimizer_ctor=optimizer_ctor, optimizer_kwargs={\"foreach\": foreach, \"fused\": fused},\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_capture_symbolic_tracing_within_fake_mode(self):\r\n        from torch._dynamo.output_graph import config\r\n        from torch._subclasses import fake_tensor\r\n        from torch.fx.experimental.symbolic_shapes import ShapeEnv\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2, 2)\r\n                self.linear2 = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                out = self.linear(x)\r\n                out = self.linear2(out)\r\n                return out\r\n\r\n        # User-instantiated FakeTensorMode\r\n        fake_mode = fake_tensor.FakeTensorMode(\r\n            allow_non_fake_inputs=False,\r\n            allow_fallback_kernels=True,\r\n            shape_env=ShapeEnv(\r\n                allow_scalar_outputs=config.capture_scalar_outputs,\r\n                allow_dynamic_output_shape_ops=config.capture_dynamic_output_shape_ops,\r\n            ),\r\n        )\r\n        # Fakefy input+model before exporting it\r\n        with fake_mode:\r\n            x = torch.rand(5, 2, 2)\r\n            model = Model()\r\n\r\n            # Export the model with fake inputs and parameters\r\n            for aten_graph in [True, False]:\r\n                graph_module, _ = torch._dynamo.export(model, aten_graph=aten_graph)(x)\r\n                self.assertTrue(\r\n                    isinstance(graph_module, torch.fx.GraphModule),\r\n                    msg=\"test_capture_symbolic_tracing_within_fake_mode_aten_graph_\"\r\n                    + str(aten_graph),\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_symbolic_tracing_within_fake_mode_with_constraints_with_parameters(self):\r\n        from torch._subclasses import fake_tensor\r\n\r\n        fake_mode = fake_tensor.FakeTensorMode()\r\n\r\n        # TODO: Seems to choke if you don't make a fresh model and\r\n        # just try to export Linear directly...\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                out = self.linear(x)\r\n                return out\r\n\r\n        with fake_mode:\r\n            model = Model()\r\n            inputs = (torch.randn(10, 2, 2),)\r\n            dynamic_shapes = ({0: torch.export.Dim(\"dim\")},)\r\n            for aten_graph in [True, False]:\r\n                torch._dynamo.export(\r\n                    model,\r\n                    dynamic_shapes=dynamic_shapes,\r\n                    aten_graph=aten_graph,\r\n                )(*inputs).graph_module",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_hooks(self):\r\n        class ToyModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.net = torch.nn.Linear(10, 10)\r\n\r\n            def forward(self, x):\r\n                return self.net(x)\r\n\r\n        model = ToyModel()\r\n        forward_handles = {}\r\n        activations = {}\r\n\r\n        def save_activations(mod, inp, out):\r\n            activations[name] = inp\r\n\r\n        for name, module in model.named_children():\r\n            forward_handles[name] = module.register_forward_hook(save_activations)\r\n\r\n        @torch.compile(backend=\"eager\")\r\n        def fn(x):\r\n            return wrap(lambda x: model(x), x)\r\n\r\n        for i in range(2):\r\n            # second iteration is key, hooks would have fired during aot trace\r\n            # on first iter\r\n            activations.clear()\r\n            x = torch.randn((10, 10))\r\n            pred = fn(x)\r\n            loss = pred.sum()\r\n            loss.backward()\r\n\r\n        self.assertTrue(activations.keys() == forward_handles.keys())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_if_cond_nn_mod2(self):\r\n        class MockModule(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.layer = torch.nn.Sequential()\r\n\r\n            def forward(self, x):\r\n                if self.layer:\r\n                    return x + 1\r\n                else:\r\n                    return x - 1\r\n\r\n        model = MockModule()\r\n        x = torch.rand(4)\r\n        ref = model(x)\r\n        opt_model = torch.compile(backend=\"eager\")(model)\r\n        res = opt_model(x)\r\n        self.assertTrue(same(ref, res))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                super().__init__()\r\n                example_inputs = (torch.randn(5, 5),)\r\n                self.model = torch.nn.Linear(5, 5)\r\n                self.quantized_model = prepare_qat_fx(\r\n                    self.model, qconfig_dict, example_inputs=example_inputs\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                super().__init__()\r\n                example_inputs = (torch.randn(5, 5),)\r\n                self.model = torch.nn.Linear(5, 5)\r\n                self.quantized_model = prepare_qat_fx(\r\n                    self.model, qconfig_dict, example_inputs=example_inputs\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                super().__init__()\r\n                self.mod = torch.nn.Sequential(\r\n                    torch.nn.Linear(3, 3), torch.nn.Linear(3, 3)\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                super().__init__()\r\n                self.layer = torch.nn.Sequential()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, layers):\r\n                torch._dynamo.graph_break()\r\n                super().__init__(*layers)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.linears = torch.nn.Sequential(\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                    )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.linears = torch.nn.Sequential(\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                        torch.nn.Linear(2, 2),\r\n                    )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_none(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                x = x + 1\r\n                return CausalLMOutputWithPast(loss=None, logits=x)[0]\r\n\r\n        model = Model()\r\n        opt_model = torch.compile(model, backend=\"eager\", fullgraph=True)\r\n        x = torch.randn(1, 1, 1, 1)\r\n\r\n        self.assertTrue(same(model(x), opt_model(x)))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_reconstruction(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                x = x + 1\r\n                return CausalLMOutputWithPast(loss=x, logits=None)\r\n\r\n        model = Model()\r\n        x = torch.randn(1, 1, 1, 1)\r\n        eo = torch._dynamo.export(Model(), aten_graph=True)(x)\r\n        self.assertTrue(same(model(x), eo.graph_module(x)))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _forward_hook_test_helper(self, model):\r\n        forward_handles = {}\r\n        compiled_activations = {}\r\n        eager_activations = {}\r\n        activations = None\r\n\r\n        def save_activations(name, mod, inp, out):\r\n            activations[name] = inp\r\n\r\n        for name, module in model.named_modules():\r\n            forward_handles[name] = module.register_forward_hook(\r\n                partial(save_activations, name)\r\n            )\r\n\r\n        compiled_model = torch.compile(model, backend=\"aot_eager\")\r\n\r\n        activations = compiled_activations\r\n        for i in range(2):\r\n            # second iteration is key, hooks would have fired during aot trace\r\n            # on first iter\r\n            compiled_activations.clear()\r\n            x = torch.randn((20, 10))\r\n            pred = compiled_model(x)\r\n            loss = pred.sum()\r\n            loss.backward()\r\n\r\n        activations = eager_activations\r\n        for i in range(2):\r\n            # second iteration is key, hooks would have fired during aot trace\r\n            # on first iter\r\n            eager_activations.clear()\r\n            x = torch.randn((20, 10))\r\n            pred = model(x)\r\n            loss = pred.sum()\r\n            loss.backward()\r\n\r\n        print(f\"Recorded Layers: {compiled_activations.keys()}\\n\\n\")\r\n        print(f\"Expected Layers: {eager_activations.keys()}\")\r\n\r\n        self.assertTrue(compiled_activations.keys() == eager_activations.keys())\r\n        self.assertTrue(activations.keys() == forward_handles.keys())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_hooks_allowed_modules_compiles(self):\r\n        class ToyModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.net = torch.nn.Sequential(\r\n                    *[torch.nn.Linear(10, 10000), torch.nn.ReLU()]\r\n                    + [torch.nn.Linear(10000, 5), torch.nn.ReLU()]\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.net(x)\r\n\r\n        model = ToyModel()\r\n        activations = []\r\n\r\n        def save_activations(mod, inp, out):\r\n            activations.append(inp)\r\n\r\n        for name, module in model.named_modules():\r\n            module.register_forward_hook(save_activations)\r\n\r\n        cnt = torch._dynamo.testing.CompileCounter()\r\n        model = torch.compile(model, backend=cnt, fullgraph=True)\r\n        for i in range(2):\r\n            # second iteration is key, hooks would have fired during aot trace\r\n            # on first iter\r\n            activations.clear()\r\n            x = torch.randn((20, 10))\r\n            pred = model(x)\r\n            loss = pred.sum()\r\n            loss.backward()\r\n        self.assertEqual(len(activations), 6)\r\n        self.assertEqual(cnt.frame_count, 1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_backward_hooks(self):\r\n        # this test shouldn't care whether hook guards are enabled or not\r\n\r\n        class CustomLinear(torch.nn.Module):\r\n            # not an 'allowed module', so should not graph-break\r\n            def __init__(self, a, b):\r\n                super().__init__()\r\n                self.weight = torch.nn.Parameter(torch.randn(a, b))\r\n\r\n            def forward(self, x):\r\n                return torch.mm(x, self.weight)\r\n\r\n        class ToyModel(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.net = torch.nn.Sequential(\r\n                    *[CustomLinear(10, 10)]\r\n                    + [CustomLinear(10, 10000)]\r\n                    + [CustomLinear(10000, 5)]\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.net(x)\r\n\r\n        model = ToyModel()\r\n        backward_hook_handles = {}\r\n        pre_backward_hook_handles = {}\r\n\r\n        grad_sizes = {}\r\n\r\n        def backward_hook(name, mod, grad_inp, grad_out):\r\n            grad_sizes[name] = (\r\n                (gi.shape for gi in grad_inp),\r\n                (go.shape for go in grad_out),\r\n            )\r\n            return None\r\n\r\n        pre_grad_sizes = {}\r\n\r\n        def backward_pre_hook(name, mod, grad_out):\r\n            pre_grad_sizes[name] = (go.shape for go in grad_out)\r\n            return None\r\n\r\n        for name, module in model.named_modules():\r\n            backward_hook_handles[name] = module.register_full_backward_hook(\r\n                partial(backward_hook, name)\r\n            )\r\n\r\n            pre_backward_hook_handles[name] = module.register_full_backward_pre_hook(\r\n                partial(backward_pre_hook, name)\r\n            )\r\n\r\n        model = torch.compile(model, backend=\"aot_eager\")\r\n\r\n        for i in range(2):\r\n            # second iteration is key, hooks would have fired during aot trace\r\n            # on first iter\r\n            x = torch.randn((20, 10))\r\n            pred = model(x)\r\n            loss = pred.sum()\r\n            loss.backward()\r\n\r\n        self.assertTrue(grad_sizes.keys() == backward_hook_handles.keys())\r\n        self.assertTrue(pre_grad_sizes.keys() == pre_backward_hook_handles.keys())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_checkpoint(self):\r\n        # https://github.com/pytorch/pytorch/issues/144035\r\n        DIM = 256\r\n        SEQ_LEN = 32\r\n\r\n        @torch.compile(backend=\"eager\", fullgraph=True)\r\n        def mlp_forward(x, w1, w2, b1, b2):\r\n            y = F.linear(x, w1, b1)\r\n            y = F.relu(y)\r\n            y = F.linear(y, w2, b2)\r\n            return y\r\n\r\n        class MLP(nn.Module):\r\n            def __init__(\r\n                self,\r\n                in_features: int,\r\n                hidden_features: int,\r\n                out_features: int,\r\n            ):\r\n                super().__init__()\r\n                self.w_in = nn.Parameter(torch.randn(hidden_features, in_features))\r\n                self.w_out = nn.Parameter(torch.randn(out_features, hidden_features))\r\n                self.b_in = nn.Parameter(torch.randn(hidden_features))\r\n                self.b_out = nn.Parameter(torch.randn(out_features))\r\n\r\n            def forward(self, x):\r\n                result = torch.utils.checkpoint.checkpoint(\r\n                    mlp_forward,\r\n                    x,\r\n                    self.w_in,\r\n                    self.w_out,\r\n                    self.b_in,\r\n                    self.b_out,\r\n                    use_reentrant=False,\r\n                )\r\n                assert isinstance(result, torch.Tensor)\r\n                return result\r\n\r\n        x = torch.randn(100, SEQ_LEN, DIM)\r\n        y = torch.zeros(100)\r\n        dataset = torch.utils.data.TensorDataset(x, y)\r\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=10)\r\n        model = MLP(DIM, 4 * DIM, DIM)\r\n\r\n        try:\r\n            # required for DDP wrapper initialization\r\n            prior_master_addr = os.environ.get(\"MASTER_ADDR\", None)\r\n            prior_master_port = os.environ.get(\"MASTER_PORT\", None)\r\n            os.environ[\"MASTER_ADDR\"] = \"localhost\"\r\n            os.environ[\"MASTER_PORT\"] = \"12355\"\r\n            dist.init_process_group(backend=\"nccl\", world_size=1, rank=0)\r\n            model = model.to(\"cuda\")\r\n            model = nn.parallel.DistributedDataParallel(model)\r\n\r\n            for batch in dataloader:\r\n                x, y = batch\r\n                x = x.to(\"cuda\")\r\n                output = model(x)\r\n                loss = output.sum()\r\n                loss.backward()\r\n        finally:\r\n            dist.destroy_process_group()\r\n            if prior_master_addr:\r\n                os.environ[\"MASTER_ADDR\"] = prior_master_addr\r\n            else:\r\n                del os.environ[\"MASTER_ADDR\"]\r\n\r\n            if prior_master_port:\r\n                os.environ[\"MASTER_PORT\"] = prior_master_port\r\n            else:\r\n                del os.environ[\"MASTER_PORT\"]",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_compiled_autograd_attribution(self):\r\n        # multiple dynamo recompiles should still be attributed to the parent compiled autograd id\r\n        def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    print(\"graph break\")\r\n                    return gO * torch.cos(x)\r\n\r\n            grads = []\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                grads.append(x.grad)\r\n\r\n            return grads\r\n\r\n        fn_opt = torch.compile(fn)\r\n        fn_opt()\r\n        self.assertParses()\r\n        expected = [\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"frame_id\": 0, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"frame_id\": 1, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 2, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 3, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 4, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 5, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 6, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 7, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 0, \"frame_id\": 8, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"frame_id\": 1, \"frame_compile_id\": 1, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 9, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 5, \"frame_compile_id\": 1, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 6, \"frame_compile_id\": 1, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 10, \"frame_compile_id\": 0, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 9, \"frame_compile_id\": 1, \"attempt\": 0}',\r\n            '{\"dynamo_start\": {\"stack\": \"STACK\"}, \"compiled_autograd_id\": 1, \"frame_id\": 10, \"frame_compile_id\": 1, \"attempt\": 0}',\r\n        ]\r\n        logs = self.buffer.getvalue()\r\n        self.assertTrue(all(event in logs for event in expected))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_compiled_autograd_chromium(self):\r\n        with torch._dynamo.compiled_autograd._enable(torch.compile):\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                loss = x.sum()\r\n                loss.backward()\r\n\r\n        self.assertParses()\r\n        expected = [\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 0, \"attempt\": 0, \"has_payload\": \"HASH\"}',\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 0, \"frame_id\": 0, \"frame_compile_id\": 0, \"attempt\": 0, '\r\n            '\"has_payload\": \"HASH\"}',\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 1, \"frame_id\": 1, \"frame_compile_id\": 1, \"attempt\": 0, '\r\n            '\"has_payload\": \"HASH\"}',\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 1, \"attempt\": 0, \"has_payload\": \"HASH\"}',\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 1, \"frame_id\": 1, \"frame_compile_id\": 0, \"attempt\": 0, '\r\n            '\"has_payload\": \"HASH\"}',\r\n            '{\"chromium_event\": {}, \"compiled_autograd_id\": 1, \"frame_id\": 1, \"frame_compile_id\": 1, \"attempt\": 0, '\r\n            '\"has_payload\": \"HASH\"}',\r\n        ]\r\n        logs = self.buffer.getvalue()\r\n        self.assertTrue(all(event in logs for event in expected))",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    print(\"graph break\")\r\n                    return gO * torch.cos(x)\r\n\r\n            grads = []\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                grads.append(x.grad)\r\n\r\n            return grads",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_export_with_fake_tensor_inputs(self):\r\n        fake_mode = torch._subclasses.fake_tensor.FakeTensorMode()\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                out = self.linear(x)\r\n                return out\r\n\r\n        # Put the inputs on a device\r\n        with fake_mode, torch.device(\"meta\"):\r\n            x = torch.rand(5, 2, 2)\r\n            model = Model()\r\n\r\n            exported_program = torch.export.export(model, (x,))\r\n            export_res = exported_program.module()(x)\r\n            exp_res = model(x)\r\n            all_meta_val = [\r\n                node.meta[\"val\"]\r\n                for node in exported_program.graph_module.graph.nodes\r\n                if \"val\" in node.meta\r\n            ]\r\n            self.assertTrue(export_res.size() == exp_res.size())\r\n            self.assertTrue(all(val.device == x.device for val in all_meta_val))\r\n            self.assertTrue(\r\n                all(val.fake_mode is all_meta_val[0].fake_mode for val in all_meta_val)\r\n            )\r\n            decomposed_ep = exported_program.run_decompositions()\r\n            export_res = decomposed_ep.module()(x)\r\n            self.assertTrue(export_res.size() == exp_res.size())",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_export_with_fake_tensor_inputs_on_cuda_devices(self):\r\n        fake_mode = torch._subclasses.fake_tensor.FakeTensorMode()\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                out = self.linear(x)\r\n                return out\r\n\r\n        # Put the inputs on a device\r\n        with fake_mode, torch.device(\"meta\"):\r\n            x = torch.rand(5, 2, 2)\r\n            model = Model()\r\n\r\n        # Manualy set the fake_device of fake tensors.\r\n        x.fake_device = torch.device(\"cuda:0\")\r\n        for n, p in model.named_parameters():\r\n            p.fake_device = torch.device(\"cuda:0\")\r\n\r\n        # Need to set all the requires_grad of tensors to False, because fake_tensor with CUDA device\r\n        # doesn't quite work well with aot_autograd right now due to some logic fails\r\n        # the check in call getDeviceGuardImpl in InputMetadata.\r\n        x.requires_grad = False\r\n        for n, p in model.named_parameters():\r\n            p.requires_grad = False\r\n\r\n        def check_device_and_fake_mode():\r\n            exported_program = torch.export.export(model, (x,))\r\n            export_res = exported_program.module()(x)\r\n            exp_res = model(x)\r\n            all_meta_val = [\r\n                node.meta[\"val\"]\r\n                for node in exported_program.graph_module.graph.nodes\r\n                if \"val\" in node.meta\r\n            ]\r\n            self.assertTrue(export_res.size() == exp_res.size())\r\n            self.assertTrue(all(val.device == x.device for val in all_meta_val))\r\n            self.assertTrue(\r\n                all(val.fake_mode is all_meta_val[0].fake_mode for val in all_meta_val)\r\n            )\r\n\r\n        check_device_and_fake_mode()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predispatch_cond(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.pred = torch.nn.Buffer(torch.tensor(False))\r\n                self.t = torch.nn.Buffer(torch.tensor(10))\r\n\r\n            def forward(self, x, y):\r\n                def true_fn(x, y):\r\n                    with torch.enable_grad():\r\n                        return x - 1 + self.t + y\r\n\r\n                return torch.cond(\r\n                    self.pred,\r\n                    true_fn,\r\n                    lambda x, y: x + 1 - self.t + y,\r\n                    [x, y],\r\n                )\r\n\r\n        model = Model()\r\n        with torch.no_grad():\r\n            exported_program = torch.export.export_for_training(\r\n                model,\r\n                (torch.tensor(10), torch.tensor(12)),\r\n                {},\r\n                dynamic_shapes=None,\r\n                strict=False,\r\n            )\r\n\r\n        schema = get_hop_schema(exported_program)\r\n        self.assertExpectedInline(\r\n            str(schema),\r\n            \"\"\"cond(Tensor pred, GraphModule true_fn, GraphModule false_fn, Tensor[3] operands) -> Tensor[1]\"\"\",  # noqa: B950\r\n        )\r\n\r\n        self.assertExpectedInline(\r\n            str(exported_program.graph_module.code.strip()),\r\n            \"\"\"\\\r\ndef forward(self, b_pred, b_t, x, y):\r\n    true_graph_0 = self.true_graph_0\r\n    false_graph_0 = self.false_graph_0\r\n    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\r\n    getitem = cond[0];  cond = None\r\n    return (getitem,)\"\"\",\r\n        )  # noqa: B950\r\n\r\n        self.assertExpectedInline(\r\n            str(exported_program.graph_module.true_graph_0.code.strip()),\r\n            \"\"\"\\\r\ndef forward(self, b_t, x, y):\r\n    submod_3 = self.submod_1\r\n    add_1 = torch.ops.higher_order.wrap_with_set_grad_enabled(True, submod_3, x, b_t, y);  submod_3 = x = b_t = y = None\r\n    getitem = add_1[0];  add_1 = None\r\n    return (getitem,)\"\"\",\r\n        )\r\n\r\n        self.assertExpectedInline(\r\n            str(exported_program.graph_module.true_graph_0.submod_1.code.strip()),\r\n            \"\"\"\\\r\ndef forward(self, x, b_t, y):\r\n    sub = torch.ops.aten.sub.Tensor(x, 1);  x = None\r\n    add = torch.ops.aten.add.Tensor(sub, b_t);  sub = b_t = None\r\n    add_1 = torch.ops.aten.add.Tensor(add, y);  add = y = None\r\n    return (add_1,)\"\"\",\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predispatch_grad_wrappers(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                with torch.enable_grad():\r\n                    x = x - y\r\n                with torch.no_grad():\r\n                    x = x + y\r\n                return x\r\n\r\n        # no grad\r\n        model = Model()\r\n        with torch.no_grad():\r\n            ep_nograd = torch.export.export_for_training(\r\n                model,\r\n                (torch.tensor(10), torch.tensor(12)),\r\n                {},\r\n                dynamic_shapes=None,\r\n                strict=False,\r\n            )\r\n        # check that only sub op is wrapped with grad_enabled\r\n        getattr_nodes = [\r\n            node for node in ep_nograd.graph.nodes if node.op == \"get_attr\"\r\n        ]\r\n        self.assertEqual(len(getattr_nodes), 1)\r\n        grad_subgraph = getattr(ep_nograd.graph_module, getattr_nodes[0].target)\r\n        op_node = [\r\n            node for node in grad_subgraph.graph.nodes if node.op == \"call_function\"\r\n        ][0]\r\n        self.assertEqual(op_node.target._name, \"aten::sub.Tensor\")\r\n\r\n        # enable grad\r\n        model = Model()\r\n        ep_grad = torch.export.export_for_training(\r\n            model,\r\n            (torch.tensor(10), torch.tensor(12)),\r\n            {},\r\n            dynamic_shapes=None,\r\n            strict=False,\r\n        )\r\n        # check that only add op is wrapped with grad_enabled\r\n        getattr_nodes = [node for node in ep_grad.graph.nodes if node.op == \"get_attr\"]\r\n        self.assertEqual(len(getattr_nodes), 1)\r\n        grad_subgraph = getattr(ep_grad.graph_module, getattr_nodes[0].target)\r\n        op_node = [\r\n            node for node in grad_subgraph.graph.nodes if node.op == \"call_function\"\r\n        ][0]\r\n        self.assertEqual(op_node.target._name, \"aten::add.Tensor\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_export_with_set_grad_enabled(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(4, 4)\r\n\r\n            def forward(self, x):\r\n                with torch.no_grad():\r\n                    return self.linear(x)\r\n\r\n        model = Model()\r\n        ep = export(model, (torch.randn(4, 4),), {})\r\n        # _export_for_traininig is using pre_dispatch=False\r\n        # Therefore the set_grad calls are not replaced with a hop.\r\n        if not is_training_ir_test(self._testMethodName):\r\n            self.assertIn(\r\n                \"torch.ops.higher_order.wrap_with_set_grad_enabled\",\r\n                ep.graph_module.code,\r\n            )\r\n        gm = torch.export.export_for_training(model, (torch.randn(4, 4),)).module()\r\n        self.assertIn(\r\n            \"set_grad_enabled\",\r\n            gm.code,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_export_with_autocast(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                with torch.autocast(\r\n                    device_type=\"cuda\", dtype=torch.int16, enabled=True\r\n                ):\r\n                    y = x.sin().sum()\r\n                with torch.autocast(\r\n                    device_type=\"cpu\", dtype=torch.float16, enabled=True\r\n                ):\r\n                    z = y.sin().sum()\r\n                return z\r\n\r\n        model = Model()\r\n        ep = export(model, (torch.randn(4, 4),), {})\r\n        # autocast nodes do not exist after run_decomposition()\r\n        if not is_training_ir_test(self._testMethodName):\r\n            self.assertIn(\r\n                \"torch.ops.higher_order.wrap_with_autocast\",\r\n                ep.graph_module.code,\r\n            )\r\n        # _export_for_traininig is using pre_dispatch=False\r\n        # Therefore the autocast calls are not replaced with a hop.\r\n        gm = torch.export.export_for_training(model, (torch.randn(4, 4),)).module()\r\n        self.assertIn(\r\n            \"autocast\",\r\n            gm.code,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_reshape_view_helper(self):\r\n        # see: https://github.com/pytorch/pytorch/issues/126607\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x):\r\n                x = x.view(x.size(1), -1)\r\n                # torch/_refs/__init__/_reshape_view_helper() will generate guards on reshape kernel(?)\r\n                # Ne(s0, 20), so that reshape isn't no-op\r\n                # Ne(Mod(s0, 20), 0), so that reshape needs to first flatten [s0, 20, 16] -> [s0*20, 16]\r\n                # then split_dim -> [20, s0, 16]\r\n                # check that these show up in graph\r\n                return torch.nn.functional.softmax(\r\n                    x, dim=0\r\n                )  # don't think softmax actually creates any issues, just part of original test\r\n\r\n        model = Model()\r\n        x = torch.rand(1024, 20, 16)\r\n        dynamic_shapes = {\"x\": {0: Dim(\"batch\")}}\r\n        ep = torch.export._trace._export(\r\n            model,\r\n            (x,),\r\n            dynamic_shapes=dynamic_shapes,\r\n            allow_complex_guards_as_runtime_asserts=True,\r\n        )\r\n        with self.assertRaisesRegex(\r\n            RuntimeError,\r\n            r\"Runtime assertion failed for expression Ne\\(s0, 20\\)\",\r\n        ):\r\n            ep.module()(torch.randn(20, 20, 16))\r\n        with self.assertRaisesRegex(\r\n            RuntimeError,\r\n            r\"Runtime assertion failed for expression Ne\\(Mod\\(s0, 20\\), 0\\)\",\r\n        ):\r\n            ep.module()(torch.randn(400, 20, 16))\r\n        ep.module()(torch.randn(42, 20, 16))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_graph_provenance(self):\r\n        def check_node_source(node_source_dict, name, pass_name, action):\r\n            self.assertEqual(node_source_dict[\"name\"], name)\r\n            self.assertEqual(node_source_dict[\"pass_name\"], pass_name)\r\n            self.assertEqual(node_source_dict[\"action\"], action)\r\n\r\n        def get_first_node_source_and_check(node_source_dict):\r\n            \"\"\"\r\n            Get the first node source from the from_node list.\r\n            \"\"\"\r\n            self.assertEqual(len(node_source_dict[\"from_node\"]), 1)\r\n            return node_source_dict[\"from_node\"][0]\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.fc1 = torch.nn.Linear(10, 16)\r\n                self.relu = torch.nn.ReLU()\r\n                self.fc2 = torch.nn.Linear(16, 1)\r\n                self.sigmoid = torch.nn.Sigmoid()\r\n\r\n            def forward(self, x):\r\n                x = self.fc1(x)\r\n                x = self.relu(x)\r\n                x = self.fc2(x)\r\n                x = self.sigmoid(x)\r\n                return (x,)\r\n\r\n        model = Model()\r\n        example_inputs = (torch.randn(8, 10),)\r\n        ep = torch.export.export(model, example_inputs, strict=True)\r\n        gm = ep.module()\r\n        provenance = get_graph_provenance_json(gm.graph)\r\n        provenance = json.loads(provenance)\r\n        self.assertEqual(\r\n            set(provenance.keys()), {\"relu\", \"linear\", \"sigmoid\", \"linear_1\"}\r\n        )\r\n\r\n        # Check node \"linear\" is created from node \"x\" in PropagateUnbackedSymInts\r\n        key_provenance = provenance[\"linear\"]\r\n        self.assertEqual(len(key_provenance), 1)\r\n        key_provenance = key_provenance[0]\r\n        check_node_source(\r\n            key_provenance,\r\n            \"x\",\r\n            \"Interpreter_PropagateUnbackedSymInts\",\r\n            CREATE_STR,\r\n        )\r\n\r\n        # Check node \"x\" is then created from another node \"x\" in FlattenInputOutputSignature\r\n        key_provenance = get_first_node_source_and_check(key_provenance)\r\n        check_node_source(\r\n            key_provenance,\r\n            \"x\",\r\n            \"Interpreter_FlattenInputOutputSignature\",\r\n            CREATE_STR,\r\n        )\r\n\r\n        gm, graph_signature = aot_export_module(\r\n            gm,\r\n            example_inputs,\r\n            trace_joint=False,\r\n        )\r\n\r\n        provenance = get_graph_provenance_json(gm.graph)\r\n        provenance = json.loads(provenance)\r\n\r\n        self.assertEqual(\r\n            set(provenance.keys()), {\"t\", \"addmm\", \"relu\", \"t_1\", \"addmm_1\", \"sigmoid\"}\r\n        )\r\n        for key in [\"t\", \"addmm\"]:\r\n            # The node provenance hierarchy should be:\r\n            # t -> linear -> x -> x\r\n            #\r\n            # x -> y means x is created from y\r\n\r\n            key_provenance = provenance[key]\r\n            self.assertEqual(len(key_provenance), 1)\r\n            key_provenance = key_provenance[0]\r\n\r\n            # Check node \"t\" and \"addmm\" is created from node \"linear\" in PropagateUnbackedSymInts\r\n            check_node_source(\r\n                key_provenance,\r\n                \"linear\",\r\n                \"Interpreter_PropagateUnbackedSymInts\",\r\n                CREATE_STR,\r\n            )\r\n\r\n            # Check node \"linear\" is then created from node \"x\" in PropagateUnbackedSymInts\r\n            key_provenance = get_first_node_source_and_check(key_provenance)\r\n            check_node_source(\r\n                key_provenance,\r\n                \"x\",\r\n                \"Interpreter_PropagateUnbackedSymInts\",\r\n                CREATE_STR,\r\n            )\r\n\r\n            # Check node \"x\" is then created from another node \"x\" in FlattenInputOutputSignature\r\n            key_provenance = get_first_node_source_and_check(key_provenance)\r\n            check_node_source(\r\n                key_provenance,\r\n                \"x\",\r\n                \"Interpreter_FlattenInputOutputSignature\",\r\n                CREATE_STR,\r\n            )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_getitem_tensor(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def forward(self, x: TensorType([4, 4])):\r\n                getitem = x[\r\n                    (None, None, slice(None, None, None), slice(None, None, None))\r\n                ]\r\n                return getitem\r\n\r\n        B = BasicBlock()\r\n        b = B.forward(torch.rand(4, 4))\r\n\r\n        symbolic_traced: torch.fx.GraphModule = symbolic_trace(B)\r\n        transformed = transform_all_constraints(symbolic_traced, counter=0)\r\n\r\n        s = z3.Solver()\r\n        s.add(transformed)\r\n        self.assertEqual(s.check(), z3.sat)\r\n        get_item_res = z3.Const(2, tensor_type)\r\n        assert s.model()[get_item_res].arg(0).arg(1) == b.shape[0]\r\n        assert s.model()[get_item_res].arg(1).arg(1) == b.shape[1]\r\n        assert s.model()[get_item_res].arg(2).arg(1) == b.shape[2]\r\n        assert s.model()[get_item_res].arg(3).arg(1) == b.shape[3]\r\n\r\n        # change the annotation on the input to make sure it propagates\r\n        # to the output\r\n        for n in symbolic_traced.graph.nodes:\r\n            if n.op == \"placeholder\":\r\n                n.type = TensorType([Dyn, 4])\r\n\r\n        transformed = transform_all_constraints(symbolic_traced, counter=0)\r\n        s = z3.Solver()\r\n        s.add(transformed)\r\n        self.assertEqual(s.check(), z3.sat)\r\n        # dyn check\r\n        assert s.model()[get_item_res].arg(2).arg(0) == 0",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_getitem_tensor2(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def forward(self, x: TensorType([4, 4])):\r\n                getitem = x[(None, None)]\r\n                return getitem\r\n\r\n        B = BasicBlock()\r\n        b = B.forward(torch.rand(4, 4))\r\n\r\n        symbolic_traced: torch.fx.GraphModule = symbolic_trace(B)\r\n        transformed = transform_all_constraints(symbolic_traced, counter=0)\r\n        s = z3.Solver()\r\n        s.add(transformed)\r\n        self.assertEqual(s.check(), z3.sat)\r\n        get_item_res = z3.Const(2, tensor_type)\r\n        assert s.model()[get_item_res].arg(0).arg(1) == b.shape[0]\r\n        assert s.model()[get_item_res].arg(1).arg(1) == b.shape[1]\r\n        assert s.model()[get_item_res].arg(2).arg(1) == b.shape[2]\r\n        assert s.model()[get_item_res].arg(3).arg(1) == b.shape[3]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_getitem_tensor_3(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def forward(self, x: TensorType([4, 4])):\r\n                getitem = x[\r\n                    (None, slice(None, None, None), None, slice(None, None, None))\r\n                ]\r\n                return getitem\r\n\r\n        B = BasicBlock()\r\n        b = B.forward(torch.rand(4, 4))\r\n        symbolic_traced: torch.fx.GraphModule = symbolic_trace(B)\r\n        transformed = transform_all_constraints(symbolic_traced, counter=0)\r\n        s = z3.Solver()\r\n        s.add(transformed)\r\n        self.assertEqual(s.check(), z3.sat)\r\n        get_item_res = z3.Const(2, tensor_type)\r\n        assert s.model()[get_item_res].arg(0).arg(1) == b.shape[0]\r\n        assert s.model()[get_item_res].arg(1).arg(1) == b.shape[1]\r\n        assert s.model()[get_item_res].arg(2).arg(1) == b.shape[2]\r\n        assert s.model()[get_item_res].arg(3).arg(1) == b.shape[3]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_conv_reshape_add_0_2(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n                in_planes,\r\n                out_planes,\r\n                kernel_size,\r\n                stride,\r\n                padding,\r\n                groups,\r\n                dilation,\r\n            ):\r\n                super().__init__()\r\n                self.conv1 = torch.nn.Conv2d(\r\n                    in_channels=in_planes,\r\n                    out_channels=out_planes,\r\n                    kernel_size=kernel_size,\r\n                    stride=stride,\r\n                    padding=padding,\r\n                    groups=groups,\r\n                    bias=False,\r\n                    dilation=dilation,\r\n                )\r\n\r\n            def forward(self, x: Dyn, y: TensorType([4, 1])):\r\n                return torch.add(self.conv1(torch.reshape(x, (1, 2, 10, 20))), y)\r\n\r\n        B = BasicBlock(2, 2, 2, 3, 2, 2, 2)\r\n\r\n        #        4,1\r\n        # 1, 2, 4, 8\r\n        res = B.forward(torch.rand(20, 20), torch.rand(1, 2, 4, 8)).size()\r\n        ast_rewriter = RewritingTracer()\r\n        graph = ast_rewriter.trace(B)\r\n        traced = GraphModule(ast_rewriter.root, graph, \"gm\")\r\n        new_transformed_c = transform_all_constraints(traced)\r\n        solver = z3.Solver()\r\n        solver.add(new_transformed_c)\r\n        self.assertEqual(solver.check(), z3.sat)\r\n\r\n        conv_result = z3.Const(4, tensor_type)\r\n        add_result = z3.Const(9, tensor_type)\r\n        input_2 = z3.Const(2, tensor_type)\r\n\r\n        s1, s2, s3, s4 = z3.Ints(\"x1 x2 x3 x4\")\r\n        s11, s22, s33, s44 = z3.Ints(\"x11 x22 x33 x44\")\r\n        d1, d2, d3, d4 = (\r\n            D(s11, s1),\r\n            D(s22, s2),\r\n            D(s33, s3),\r\n            D(s44, s4),\r\n        )\r\n\r\n        solver.add(conv_result == tensor_type.tensor4(d1, d2, d3, d4))\r\n        solver.check()\r\n        assert solver.model()[s1].as_long() == res[0]\r\n        assert solver.model()[s2].as_long() == res[1]\r\n        assert solver.model()[s3].as_long() == res[2]\r\n        assert solver.model()[s4].as_long() == res[3]\r\n\r\n        solver.add(input_2 == tensor_type.tensor2(D(1, 4), D(1, 1)))\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        solver.add(add_result == tensor_type.tensor4(d1, d2, d3, d4))\r\n        self.assertEqual(solver.check(), z3.sat)\r\n\r\n        # first dimension could be anything because we have broadcasting\r\n        assert solver.model()[s1] == res[0]\r\n        assert solver.model()[s2] == res[1]\r\n        assert solver.model()[s3] == res[2]\r\n        assert solver.model()[s4] == res[3]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_conv_reshape0(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n                in_planes,\r\n                out_planes,\r\n                kernel_size,\r\n                stride,\r\n                padding,\r\n                groups,\r\n                dilation,\r\n            ):\r\n                super().__init__()\r\n                self.conv1 = torch.nn.Conv2d(\r\n                    in_channels=in_planes,\r\n                    out_channels=out_planes,\r\n                    kernel_size=kernel_size,\r\n                    stride=stride,\r\n                    padding=padding,\r\n                    groups=groups,\r\n                    bias=False,\r\n                    dilation=dilation,\r\n                )\r\n\r\n            def forward(self, x: Dyn):\r\n                return self.conv1(torch.reshape(x, (1, 2, 10, 20)))\r\n\r\n        B = BasicBlock(2, 2, 2, 3, 2, 2, 2)\r\n        res = B.forward(torch.rand(20, 20)).size()\r\n        ast_rewriter = RewritingTracer()\r\n        graph = ast_rewriter.trace(B)\r\n        traced = GraphModule(ast_rewriter.root, graph, \"gm\")\r\n        new_transformed_c = transform_all_constraints(traced)\r\n\r\n        solver = z3.Solver()\r\n        solver.add(new_transformed_c)\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        conv_result = z3.Const(3, tensor_type)\r\n\r\n        s1, s2, s3, s4 = z3.Ints(\"x1 x2 x3 x4\")\r\n        s11, s22, s33, s44 = z3.Ints(\"x11 x22 x33 x44\")\r\n        d1, d2, d3, d4 = (\r\n            D(s11, s1),\r\n            D(s22, s2),\r\n            D(s33, s3),\r\n            D(s44, s4),\r\n        )\r\n\r\n        solver.add(conv_result == tensor_type.tensor4(d1, d2, d3, d4))\r\n        solver.check()\r\n        # print(solver.model())\r\n        # print(type(solver.model()[s1]))\r\n        assert solver.model()[s1].as_long() == res[0]\r\n        assert solver.model()[s2].as_long() == res[1]\r\n        assert solver.model()[s3].as_long() == res[2]\r\n        assert solver.model()[s4].as_long() == res[3]\r\n\r\n        s1, s2, s3, s4 = z3.Ints(\"y1 y2 y3 y4\")\r\n        s11, s22, s33, s44 = z3.Ints(\"y11 y22 y33 y44\")\r\n        d1, d2, d3, d4 = (\r\n            D(s11, s1),\r\n            D(s22, s2),\r\n            D(s33, s3),\r\n            D(s44, s4),\r\n        )\r\n\r\n        input = z3.Const(1, tensor_type)\r\n        solver.add(input == tensor_type.tensor4(d1, d2, d3, d4))",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_conv_reshape1(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n                in_planes,\r\n                out_planes,\r\n                kernel_size,\r\n                stride,\r\n                padding,\r\n                groups,\r\n                dilation,\r\n            ):\r\n                super().__init__()\r\n                self.conv1 = torch.nn.Conv2d(\r\n                    in_channels=in_planes,\r\n                    out_channels=out_planes,\r\n                    kernel_size=kernel_size,\r\n                    stride=stride,\r\n                    padding=padding,\r\n                    groups=groups,\r\n                    bias=False,\r\n                    dilation=dilation,\r\n                )\r\n\r\n            def forward(self, x: TensorType([20, 20])):\r\n                return self.conv1(torch.reshape(x, (1, -1, 10, 20)))\r\n\r\n        B = BasicBlock(2, 2, 2, 3, 2, 2, 2)\r\n        res = B.forward(torch.rand(20, 20)).size()\r\n        ast_rewriter = RewritingTracer()\r\n        graph = ast_rewriter.trace(B)\r\n        traced = GraphModule(ast_rewriter.root, graph, \"gm\")\r\n        new_transformed_c = transform_all_constraints(traced)\r\n\r\n        solver = z3.Solver()\r\n        solver.add(new_transformed_c)\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        conv_result = z3.Const(3, tensor_type)\r\n\r\n        s1, s2, s3, s4 = z3.Ints(\"x1 x2 x3 x4\")\r\n        s11, s22, s33, s44 = z3.Ints(\"x11 x22 x33 x44\")\r\n        d1, d2, d3, d4 = (\r\n            D(s11, s1),\r\n            D(s22, s2),\r\n            D(s33, s3),\r\n            D(s44, s4),\r\n        )\r\n\r\n        solver.add(conv_result == tensor_type.tensor4(d1, d2, d3, d4))\r\n        solver.check()\r\n        # print(solver.model())\r\n        assert solver.model()[s1].as_long() == res[0]\r\n        assert solver.model()[s2].as_long() == res[1]\r\n        assert solver.model()[s3].as_long() == res[2]\r\n        assert solver.model()[s4].as_long() == res[3]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_conv_static(self):\r\n        s1, s2, s3, s4 = z3.Ints(\"s1 s2 s3 s4\")\r\n        e1, e2, e3, e4 = z3.Ints(\"e1 e2 e3 e4\")\r\n        s11, s22, s33, s44 = z3.Ints(\"s11 s22 s33 s44\")\r\n        e11, e22, e33, e44 = z3.Ints(\"e11 e22 e33 e44\")\r\n        d1, d2, d3, d4 = (\r\n            D(s11, s1),\r\n            D(s22, s2),\r\n            D(s33, s3),\r\n            D(s44, s4),\r\n        )\r\n        b1, b2, b3, b4 = D(e11, e1), D(e22, e2), D(e33, e3), D(e44, e4)\r\n\r\n        class BasicBlock(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n                in_planes,\r\n                out_planes,\r\n                kernel_size,\r\n                stride,\r\n                padding,\r\n                groups,\r\n                dilation,\r\n            ):\r\n                super().__init__()\r\n                self.conv1 = torch.nn.Conv2d(\r\n                    in_channels=in_planes,\r\n                    out_channels=out_planes,\r\n                    kernel_size=kernel_size,\r\n                    stride=stride,\r\n                    padding=padding,\r\n                    dilation=dilation,\r\n                )\r\n\r\n            def forward(self, x: TensorType((1, 2, 10, 20))):\r\n                return self.conv1(x)\r\n\r\n        ast_rewriter = RewritingTracer()\r\n\r\n        B = BasicBlock(2, 2, 2, 3, 2, 2, 2)\r\n        res = B.forward(torch.rand(1, 2, 10, 20)).size()\r\n\r\n        graph = ast_rewriter.trace(B)\r\n        traced = GraphModule(ast_rewriter.root, graph, \"gm\")\r\n        new_transformed_c = transform_all_constraints(traced)\r\n        solver = z3.Solver()\r\n        solver.add(new_transformed_c)\r\n        self.assertEqual(solver.check(), z3.sat)\r\n\r\n        x = z3.Const(1, tensor_type)\r\n        y = z3.Const(2, tensor_type)\r\n\r\n        solver.add(x == tensor_type.tensor4(d1, d2, d3, d4))\r\n        solver.add(y == tensor_type.tensor4(b1, b2, b3, b4))\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        # print(solver.model())\r\n        assert solver.model()[e3].as_long() == res[2]\r\n        assert solver.model()[e4].as_long() == res[3]\r\n\r\n        B2 = BasicBlock(2, 4, 5, 2, 9, 2, 2)\r\n        res2 = B2.forward(torch.rand(1, 2, 10, 20)).size()\r\n\r\n        graph2 = ast_rewriter.trace(B2)\r\n        traced2 = GraphModule(ast_rewriter.root, graph2, \"gm\")\r\n        new_transformed_c = transform_all_constraints(traced2)\r\n        solver = z3.Solver()\r\n        solver.add(new_transformed_c)\r\n\r\n        solver.add(x == tensor_type.tensor4(d1, d2, d3, d4))\r\n        solver.add(y == tensor_type.tensor4(b1, b2, b3, b4))\r\n\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        assert solver.model()[e3].as_long() == res2[2]\r\n        assert solver.model()[e4].as_long() == res2[3]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_conv2D_maxpool2d_flatten(self):\r\n        class BasicBlock(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n                self.conv1 = torch.nn.Conv2d(3, 6, 5)\r\n                self.pool = torch.nn.MaxPool2d(2, 2)\r\n                self.conv2 = torch.nn.Conv2d(6, 16, 5)\r\n                self.fc1 = torch.nn.Linear(5, 120)\r\n                self.pool2 = torch.nn.AdaptiveAvgPool2d((6, 7))\r\n\r\n            def forward(self, x: TensorType((4, 3, 32, 32))):\r\n                out = self.conv1(x)\r\n                out = self.pool(out)\r\n                out = self.conv2(out)\r\n                out = self.pool(out)\r\n                out = self.fc1(out)\r\n                out = self.pool2(out)\r\n                out = torch.flatten(out, 1)\r\n                return out\r\n\r\n        B = BasicBlock()\r\n        res = B.forward(torch.rand(4, 3, 32, 32)).shape\r\n        ast_rewriter = RewritingTracer()\r\n        graph = ast_rewriter.trace(B)\r\n        traced = GraphModule(ast_rewriter.root, graph, \"gm\")\r\n        constraints = transform_all_constraints(traced, counter=0)\r\n        solver = z3.Solver()\r\n        solver.add(constraints)\r\n        solver.check()\r\n        input = z3.Const(1, tensor_type)\r\n        solver.add(input == tensor_type.tensor4(D(1, 4), D(1, 3), D(1, 32), D(1, 32)))\r\n        solver.check()\r\n        output = z3.Const(48, tensor_type)\r\n        assert solver.model()[output].arg(0).arg(1) == res[0]\r\n        assert solver.model()[output].arg(1).arg(1) == res[1]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_alexnet1(self):\r\n        alexnet = models.alexnet()\r\n        symbolic_traced: torch.fx.GraphModule = symbolic_trace(alexnet)\r\n\r\n        for n in symbolic_traced.graph.nodes:\r\n            n.type = Dyn\r\n\r\n        # print(symbolic_traced)\r\n\r\n        res = alexnet.forward(torch.rand(10, 3, 227, 227)).size()\r\n        constraints = transform_all_constraints(symbolic_traced, counter=0)\r\n        solver = z3.Solver()\r\n        solver.add(constraints)\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        input = z3.Const(1, tensor_type)\r\n        conv = z3.Const(2, tensor_type)\r\n        solver.add(\r\n            input == tensor_type.tensor4(D(1, 10), D(1, 3), D(1, 227), D(1, 227))\r\n        )\r\n        self.assertEqual(solver.check(), z3.sat)\r\n        assert solver.model()[conv] == tensor_type.tensor4(\r\n            D(1, 10), D(1, 64), D(1, 56), D(1, 56)\r\n        )\r\n\r\n        relu = z3.Const(7, tensor_type)\r\n        assert solver.model()[relu] == tensor_type.tensor4(\r\n            D(1, 10), D(1, 64), D(1, 56), D(1, 56)\r\n        )\r\n\r\n        maxpool = z3.Const(8, tensor_type)\r\n        assert solver.model()[maxpool] == tensor_type.tensor4(\r\n            D(1, 10), D(1, 64), D(1, 27), D(1, 27)\r\n        )\r\n\r\n        maxpool2 = z3.Const(42, tensor_type)\r\n        assert solver.model()[maxpool2] == tensor_type.tensor4(\r\n            D(1, 10), D(1, 256), D(1, 6), D(1, 6)\r\n        )\r\n\r\n        flatten = z3.Const(52, tensor_type)\r\n        assert solver.model()[flatten] == tensor_type.tensor2(D(1, 10), D(1, 9216))\r\n\r\n        linear = z3.Const(64, tensor_type)\r\n        assert solver.model()[linear] == tensor_type.tensor2(D(1, 10), D(1, 4096))\r\n\r\n        linear2 = z3.Const(109, tensor_type)\r\n        assert solver.model()[linear2] == tensor_type.tensor2(\r\n            D(1, res[0]), D(1, res[1])\r\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_find_learning_rate_ensembling(self, device, dropout_layer, mechanism):\r\n        # This example mimics what a user might do when trying to find the optimal learning rate. They would\r\n        # want to run a bunch of models with the same behavior (including the same dropout!) and have them\r\n        # each run with different learning rates. Specifically, this is an example of using same randomness with vmap\r\n        points, labels = torch.randn(100, 2, 2, 2, 2, device=device), torch.randint(\r\n            0, 2, (100,), device=device\r\n        )\r\n\r\n        class MLPClassifier(nn.Module):\r\n            def __init__(self, hidden_dim=32, n_classes=2):\r\n                super().__init__()\r\n                self.hidden_dim = hidden_dim\r\n                self.n_classes = n_classes\r\n\r\n                self.dropout = dropout_layer()\r\n                self.fc1 = nn.Linear(16, self.hidden_dim)\r\n                self.fc2 = nn.Linear(self.hidden_dim, self.n_classes)\r\n\r\n            def forward(self, x):\r\n                x = self.dropout(x)\r\n                x = torch.flatten(x, start_dim=1)\r\n                x = self.fc1(x)\r\n                x = F.relu(x)\r\n                x = self.fc2(x)\r\n                x = F.log_softmax(x, -1)\r\n                return x\r\n\r\n        loss_fn = nn.NLLLoss()\r\n\r\n        func_model, weights = _get_weights_and_functional_call(\r\n            MLPClassifier().to(device), mechanism\r\n        )\r\n\r\n        def train_step_fn(weights, batch, targets, lr):\r\n            def compute_loss(weights, batch, targets):\r\n                output = func_model(weights, batch)\r\n                loss = loss_fn(output, targets)\r\n                return loss\r\n\r\n            grad_weights, loss = grad_and_value(compute_loss)(weights, batch, targets)\r\n            new_weights = self._update_params(weights, grad_weights, lr, mechanism)\r\n            if mechanism != \"make_functional\":\r\n                new_weights = list(new_weights.values())\r\n            # NB: return looks weird because torch.vmap must return Tensors\r\n            return (loss, *new_weights)\r\n\r\n        def unpack(train_result):\r\n            return train_result[0], train_result[1:]\r\n\r\n        def init_fn(num_models):\r\n            og_model = MLPClassifier().to(device)\r\n            models = tuple(\r\n                copy.deepcopy(og_model) for _ in range(num_models)\r\n            )  # have same initialization\r\n            if mechanism == \"make_functional\":\r\n                return combine_state_for_ensemble(models)[1]\r\n            else:\r\n                return stack_module_state(models)[0]\r\n\r\n        batched_weights = init_fn(num_models=2)\r\n        parallel_train_step_fn = vmap(\r\n            train_step_fn, in_dims=(0, None, None, 0), randomness=\"same\"\r\n        )\r\n\r\n        lrs = torch.tensor([0.2, 0.4], device=device)\r\n        result_loss, result_weights = unpack(\r\n            parallel_train_step_fn(batched_weights, points, labels, lrs)\r\n        )\r\n\r\n        self.assertEqual(result_loss[0], result_loss[1])\r\n        self.assertNotEqual(\r\n            tuple(weight[0] for weight in result_weights),\r\n            tuple(weight[1] for weight in result_weights),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_simple(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(10, 10)\r\n\r\n            def forward(self, x, y):\r\n                return x + self.linear(y)\r\n\r\n        example_inputs = (\r\n            torch.randn(10, 10, device=self.device),\r\n            torch.randn(10, 10, device=self.device),\r\n        )\r\n        model = Model()\r\n        self.check_model(model, example_inputs)\r\n        if self.use_minimal_arrayref_interface:\r\n            self.code_check_count(\r\n                model, example_inputs, \"AOTInductorModelRunMinimalArrayrefInterface(\", 1\r\n            )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_foreach_multiple_dynamic(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x, y):\r\n                x_unsqueeze = torch.unsqueeze(x, dim=0)\r\n                y_unsqueeze = torch.unsqueeze(y, dim=0)\r\n                cat = torch.cat([x_unsqueeze, y_unsqueeze], dim=0)\r\n                return cat\r\n\r\n        model = Model()\r\n        x = torch.randn(128, 2048, device=self.device)\r\n        y = torch.randn(128, 2048, device=self.device)\r\n        dim0_x = Dim(\"dim0_x\", min=1, max=2048)\r\n        dynamic_shapes = {\"x\": {0: dim0_x}, \"y\": {0: dim0_x}}\r\n        list_example_inputs = [(x, y)]\r\n        list_example_inputs.append(\r\n            (\r\n                torch.randn(64, 2048, device=self.device),\r\n                torch.randn(64, 2048, device=self.device),\r\n            ),\r\n        )\r\n        list_example_inputs.append(\r\n            (\r\n                torch.randn(211, 2048, device=self.device),\r\n                torch.randn(211, 2048, device=self.device),\r\n            ),\r\n        )\r\n        self.check_model_with_multiple_inputs(\r\n            model,\r\n            list_example_inputs,\r\n            dynamic_shapes=dynamic_shapes,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_buffer_mutation_and_force_mmap_weights(self):\r\n        class Model(nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.linear1 = torch.nn.Linear(16, 15)\r\n                self.linear2 = torch.nn.Linear(15, 14)\r\n\r\n            def forward(self, x):\r\n                x = self.linear1(x)\r\n                out = self.linear2(x)\r\n                return out\r\n\r\n        example_inputs = (torch.randn(32, 16),)\r\n        model = Model().eval()\r\n        with config.patch(\r\n            {\"freezing\": True, \"aot_inductor.force_mmap_weights\": True}\r\n        ), torch.no_grad():\r\n            exported_model = export_for_training(model, example_inputs).module()\r\n            quantizer = X86InductorQuantizer()\r\n            quantizer.set_global(\r\n                xiq.get_default_x86_inductor_quantization_config(reduce_range=True)\r\n            )\r\n            prepared_model = prepare_pt2e(exported_model, quantizer)\r\n            prepared_model(*example_inputs)\r\n            converted_model = convert_pt2e(prepared_model)\r\n            torch.ao.quantization.move_exported_model_to_eval(converted_model)\r\n\r\n            self.check_model(converted_model, example_inputs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_on_gpu_device1(self):\r\n        if self.device != GPU_TYPE:\r\n            raise unittest.SkipTest(\"requires GPU\")\r\n\r\n        device_interface = get_interface_for_device(GPU_TYPE)\r\n        try:\r\n            device_interface.get_device_properties(1)\r\n        except AssertionError:\r\n            raise unittest.SkipTest(\"GPU device 1 is not available\") from None\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.fc1 = torch.nn.Linear(10, 16)\r\n                self.relu = torch.nn.ReLU()\r\n                self.fc2 = torch.nn.Linear(16, 1)\r\n                self.sigmoid = torch.nn.Sigmoid()\r\n\r\n            def forward(self, x):\r\n                x = self.fc1(x)\r\n                x = self.relu(x)\r\n                x = self.fc2(x)\r\n                x = self.sigmoid(x)\r\n                return x\r\n\r\n        device = f\"{GPU_TYPE}:1\"\r\n        model = Model().to(device)\r\n        example_inputs = (torch.randn(8, 10, device=device),)\r\n        expected = model(*example_inputs)\r\n\r\n        so_path = AOTIRunnerUtil.compile(model, example_inputs)\r\n        optimized = AOTIRunnerUtil.load(device, so_path)\r\n        actual = optimized(*example_inputs)\r\n        torch.testing.assert_close(actual, expected)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_reuse_kernel(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x, y):\r\n                a = torch.sin(x)\r\n                b = torch.mm(a, y)\r\n                c = torch.sin(b)\r\n                d = torch.mm(b, c)\r\n                return d\r\n\r\n        example_inputs = (\r\n            torch.randn(87, 87, device=self.device),\r\n            torch.randn(87, 87, device=self.device),\r\n        )\r\n        model = Model()\r\n        self.check_model(\r\n            model, example_inputs, atol=1e-4, rtol=1e-4\r\n        )  # 1e-4 is the tol value used in pytorch/torch/_dynamo/utils.py\r\n\r\n        if self.device == GPU_TYPE:\r\n            self.code_check_count(\r\n                model, example_inputs, \"triton_poi_fused_sin_0 = loadKernel(\", 1\r\n            )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_missing_cubin(self):\r\n        from torchvision.models.resnet import Bottleneck, ResNet\r\n\r\n        class Model(ResNet):\r\n            def __init__(self) -> None:\r\n                super().__init__(\r\n                    block=Bottleneck,\r\n                    layers=[3, 4, 6, 3],\r\n                    replace_stride_with_dilation=[False, False, True],\r\n                    norm_layer=None,\r\n                )\r\n\r\n            def forward(self, x):\r\n                x = self.conv1(x)\r\n                x = self.bn1(x)\r\n                x = self.relu(x)\r\n                f1 = x\r\n                x = self.maxpool(x)\r\n                x = self.layer1(x)\r\n                f2 = x\r\n                x = self.layer2(x)\r\n                f3 = x\r\n                x = self.layer3(x)\r\n                x = self.layer4(x)\r\n                f4 = x\r\n                return [f1, f2, f3, f4]\r\n\r\n        # Call eval() here so that batch_norm won't update the running stats\r\n        # Use float64 to avoid numeric difference failure\r\n        model = Model().to(device=self.device, dtype=torch.float64).eval()\r\n        example_inputs = (\r\n            torch.randn(4, 3, 64, 64, device=self.device, dtype=torch.float64),\r\n        )\r\n        self.check_model(model, example_inputs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_runtime_checks_dtype_failed(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x):\r\n                y = x.type(torch.float)\r\n                return y\r\n\r\n        x = torch.randn(1, 4, dtype=torch.float16, device=self.device)\r\n        model = Model()\r\n        with torch.no_grad(), config.patch(\r\n            {\r\n                \"aot_inductor.debug_compile\": True,\r\n            }\r\n        ):\r\n            so_path: str = AOTIRunnerUtil.compile(\r\n                model,\r\n                (x,),\r\n            )\r\n        aot_inductor_module = AOTIRunnerUtil.load(self.device, so_path)\r\n        x_casted = x.float()\r\n        with self.assertRaisesRegex(Exception, \"\"):\r\n            aot_inductor_module(x_casted)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_non_contiguous_output_alias(self):\r\n        # Test return x, x.contiguous() where x is non-contiguous.\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                squared = x * x\r\n                transposed = squared.t()  # non-contiguous\r\n                contig = transposed.contiguous()\r\n                return transposed, contig\r\n\r\n        x = torch.randn(3, 4, dtype=torch.float16, device=self.device)\r\n        model = Model()\r\n        with torch.no_grad():\r\n            result = AOTIRunnerUtil.run(\r\n                self.device,\r\n                model,\r\n                (x,),\r\n            )\r\n        actual = model(x)\r\n        self.assertTrue(same(result, actual))\r\n\r\n        # contiguous() should create a new tensor\r\n        self.assertTrue(result[0].data_ptr() != result[1].data_ptr())",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multiple_output_alias(self):\r\n        # Test when mutliple outputs alias the same tensor\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                squared = x * x\r\n                contig = squared.contiguous()  # alias\r\n                reshaped = squared.reshape(squared.shape)  # alias\r\n                cubed = squared * x\r\n                return squared, contig, reshaped, cubed\r\n\r\n        x = torch.randn(3, 4, dtype=torch.float32, device=self.device)\r\n        model = Model()\r\n\r\n        with torch.no_grad():\r\n            result = AOTIRunnerUtil.run(\r\n                self.device,\r\n                model,\r\n                (x,),\r\n            )\r\n        actual = model(x)\r\n        self.assertTrue(same(result, actual))\r\n\r\n        # squared, contig and reshaped alias the same tensor.\r\n        self.assertTrue(result[0].data_ptr() == result[1].data_ptr())\r\n        self.assertTrue(result[0].data_ptr() == result[2].data_ptr())\r\n        # cubed shouldn't be an alias.\r\n        self.assertTrue(result[0].data_ptr() != result[3].data_ptr())",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_runtime_checks_shape_failed(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x):\r\n                return x\r\n\r\n        x = torch.randn(4, 4, 4, dtype=torch.float16, device=self.device)\r\n        y0 = torch.randn(8, 4, 4, dtype=torch.float16, device=self.device)\r\n        y1 = torch.randn(4, 8, 4, dtype=torch.float16, device=self.device)\r\n        y2 = rand_strided(\r\n            (4, 4, 4), (16, 1, 4), dtype=torch.float16, device=self.device\r\n        )\r\n        # batch size is outside of the range\r\n        y3 = torch.randn(2048, 3, 4, dtype=torch.float16, device=self.device)\r\n        y4 = torch.randn(2048, 4, 4, dtype=torch.float16, device=self.device)\r\n        dim0 = Dim(\"s0\", min=4, max=1024)\r\n        dynamic_shapes = {\r\n            \"x\": {0: dim0},\r\n        }\r\n        model = Model()\r\n        with torch.no_grad(), config.patch(\r\n            {\r\n                \"aot_inductor.debug_compile\": True,\r\n            }\r\n        ):\r\n            so_path: str = AOTIRunnerUtil.compile(\r\n                model, (x,), dynamic_shapes=dynamic_shapes\r\n            )\r\n        aot_inductor_module = AOTIRunnerUtil.load(self.device, so_path)\r\n        # dynamic dim works fine\r\n        _ = aot_inductor_module(y0)\r\n        with self.assertRaisesRegex(Exception, \"\"):\r\n            aot_inductor_module(y1)\r\n        with self.assertRaisesRegex(Exception, \"\"):\r\n            aot_inductor_module(y2)\r\n        with self.assertRaisesRegex(Exception, \"\"):\r\n            aot_inductor_module(y3)\r\n        with self.assertRaisesRegex(Exception, \"\"):\r\n            aot_inductor_module(y4)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nested_tensor_from_jagged(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.mlp = nn.Sequential(\r\n                    nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32), nn.Sigmoid()\r\n                )\r\n\r\n            def forward(self, values, offsets):\r\n                nt = torch.nested.nested_tensor_from_jagged(values, offsets)\r\n                res = self.mlp(nt)\r\n                return res.values()\r\n\r\n        model = Model().to(device=self.device)\r\n\r\n        example_inputs_1 = (\r\n            torch.randn((15, 128), device=self.device),\r\n            torch.tensor([0, 3, 4, 10, 15], device=self.device),\r\n        )\r\n\r\n        # same \"NT batch size\", different actual amount of data\r\n        example_inputs_2 = (\r\n            torch.randn((31, 128), device=self.device),\r\n            torch.tensor([0, 1, 20, 25, 31], device=self.device),\r\n        )\r\n\r\n        # same actual amount of data, different \"NT batch size\"\r\n        example_inputs_3 = (\r\n            torch.randn((15, 128), device=self.device),\r\n            torch.tensor([0, 3, 10, 15], device=self.device),\r\n        )\r\n\r\n        # different \"NT batch size\"\r\n        example_inputs_4 = (\r\n            torch.randn((37, 128), device=self.device),\r\n            torch.tensor([0, 5, 16, 25, 29, 37], device=self.device),\r\n        )\r\n\r\n        dim0_values = Dim(\"dim0_values\", min=1, max=128)\r\n        dim0_offsets = Dim(\"dim0_offsets\", min=1, max=9)\r\n        dynamic_shapes = {\"values\": {0: dim0_values}, \"offsets\": {0: dim0_offsets}}\r\n        example_inputs_list = [\r\n            example_inputs_1,\r\n            example_inputs_2,\r\n            example_inputs_3,\r\n            example_inputs_4,\r\n        ]\r\n\r\n        self.check_model_with_multiple_inputs(\r\n            model, example_inputs_list, dynamic_shapes=dynamic_shapes\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_misc_1(self, max_autotune):\r\n        if self.device == \"cpu\" and IS_MACOS and max_autotune:\r\n            raise unittest.SkipTest(\"max_autotune not supported on macos\")\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.mlp = nn.Sequential(\r\n                    nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32), nn.Sigmoid()\r\n                )\r\n                self.emb = nn.EmbeddingBag(num_embeddings=128, embedding_dim=32)\r\n                self.over_arch = nn.Sequential(\r\n                    nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 32), nn.Sigmoid()\r\n                )\r\n\r\n            def forward(self, x, y):\r\n                mlp_output = self.mlp(x)\r\n                emb_output = self.emb(y)\r\n                return self.over_arch(torch.concat([mlp_output, emb_output], dim=1))\r\n\r\n        example_inputs = (\r\n            torch.randn(16, 128, device=self.device),\r\n            torch.randint(0, 128, (16, 10), device=self.device),\r\n        )\r\n        self.check_model(\r\n            Model(), example_inputs, options=dict(max_autotune=max_autotune)\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_aoti_debug_printing_model_inputs_codegen(self):\r\n        if self.device != \"cuda\":\r\n            raise unittest.SkipTest(\"requires CUDA\")\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n\r\n            def forward(self, a, b, c):\r\n                x = a * 3.14\r\n                y = torch.addmm(c, x, b)\r\n                z = torch.nn.functional.gelu(y)\r\n                return z\r\n\r\n        example_inputs = (\r\n            torch.randn(10, 20, device=\"cuda\"),\r\n            torch.randn(20, 30, device=\"cuda\"),\r\n            torch.randn(10, 30, device=\"cuda\"),\r\n        )\r\n        model = Model()\r\n        kernel_calls = [\r\n            (\"aoti_model_inputs\", 3),\r\n        ]\r\n\r\n        with config.patch({\"aot_inductor.debug_intermediate_value_printer\": \"2\"}):\r\n            result, code = run_and_get_cpp_code(\r\n                AOTIRunnerUtil.compile, model, example_inputs\r\n            )\r\n            self.assertEqual(\"aoti_torch_print_tensor_handle\" in code, True)\r\n            # check the codegen for debug printing around aoti model inputs is expected\r\n            for kernel_call, count in kernel_calls:\r\n                FileCheck().check_count(\r\n                    f\"{kernel_call}\",\r\n                    count,\r\n                ).run(code)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_issue_140766(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.mlp = torch.nn.Sequential(\r\n                    torch.nn.Linear(128, 512),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(512, 128),\r\n                )\r\n                self.norm = torch.nn.LayerNorm(128)\r\n                self.attn = torch.nn.functional.scaled_dot_product_attention\r\n\r\n            def forward(self, x):\r\n                # [2, 128, 4096]\r\n                x = x.transpose(1, 2)\r\n                # [2, 4096, 128]\r\n                for _ in range(2):\r\n                    x = self.forward_block(x)\r\n                return x\r\n\r\n            def forward_block(self, x):\r\n                # x: B, H*W, C\r\n                B = x.shape[0]\r\n                H, W, C = 64, 64, 128\r\n                shortcut = x\r\n                x = self.norm(x)\r\n                x = x.reshape(B, H, W, C)\r\n                # B, H, W, C\r\n                x = self.attn(x, x, x)\r\n                x = x.reshape(B, H // 8, W // 8, 8, 8, -1)\r\n                x = x.transpose(2, 3).reshape(B, H * W, -1)\r\n\r\n                x = shortcut + x\r\n                x = x + self.mlp(self.norm(x))\r\n                return x\r\n\r\n        bs = torch.export.Dim(\"bs\", max=12)\r\n        example_inputs = (torch.randn(2, 128, 4096, device=self.device),)\r\n        self.check_model(Model(), example_inputs, dynamic_shapes={\"x\": {0: bs}})",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_misaligned_input_1(self):\r\n        if self.device != \"cuda\":\r\n            raise unittest.SkipTest(\"CUDA test only\")\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return x.sin() + x.cos()\r\n\r\n        N = 64 * 64 * 64 + 64\r\n        arg = torch.randn(N, device=self.device)\r\n        example_inputs = (arg,)\r\n        model = Model()\r\n        expected = model(*example_inputs)\r\n        so_path = AOTIRunnerUtil.compile(model, example_inputs)\r\n        optimized = AOTIRunnerUtil.load(self.device, so_path)\r\n        # If the model is compiled with aligned inputs, the generated\r\n        # code will check inputs alignment at runtime\r\n        self.code_check_count(\r\n            model, example_inputs, \"aoti_torch_clone_preserve_strides\", 1\r\n        )\r\n\r\n        misaligned_arg = torch.zeros(N + 1, device=self.device)\r\n        misaligned_arg = misaligned_arg[1:]\r\n        misaligned_arg.copy_(arg)\r\n        actual = optimized(misaligned_arg)\r\n        torch.testing.assert_close(actual, expected)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_misaligned_input_2(self):\r\n        if self.device != \"cuda\":\r\n            raise unittest.SkipTest(\"CUDA test only\")\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return x.sin() + x.cos()\r\n\r\n        N = 64 * 64 * 64 + 64\r\n        arg = torch.randn(N, device=self.device)\r\n        misaligned_arg = torch.zeros(N + 1, device=self.device)\r\n        misaligned_arg = misaligned_arg[1:]\r\n        misaligned_arg.copy_(arg)\r\n        example_inputs = (misaligned_arg,)\r\n\r\n        model = Model()\r\n        self.check_model(model, example_inputs)\r\n        # If the model is already compiled with a misaligned input, the\r\n        # generated code should NOT contain an alignment check for that input.\r\n        self.code_check_count(\r\n            model, example_inputs, \"aoti_torch_clone_preserve_strides\", 0\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_boxed_run_inputs_clearing(self):\r\n        # Borrowed from test_torchinductor\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return torch.ops.aoti_custom_ops.custom_add(x, y)\r\n\r\n        inps = [\r\n            torch.rand(5, 5, device=self.device),\r\n            torch.rand(5, 5, device=self.device),\r\n        ]\r\n        model = Model().to(device=self.device)\r\n        # NOTE: There are additional references to inps if we use\r\n        # strict=True here, which will cause inps not deallocated\r\n        # in time later in this test.\r\n        ep = torch.export.export(model, tuple(inps), strict=False)\r\n        package = torch._inductor.aoti_compile_and_package(ep)\r\n        fn_compiled = torch._inductor.aoti_load_package(package)\r\n\r\n        test_self = self\r\n        sentinel_seen = False\r\n\r\n        class TestRefMode(TorchDispatchMode):\r\n            def __torch_dispatch__(self, func, types, args=(), kwargs=None):\r\n                kwargs = kwargs if kwargs else {}\r\n                nonlocal inps\r\n                nonlocal test_self\r\n                nonlocal sentinel_seen\r\n                if func is torch.ops.aoti_custom_ops.custom_add.default:\r\n                    # inputs should be deallocated by this point\r\n                    sentinel_seen = True\r\n                    test_self.assertEqual(len(inps), 0)\r\n\r\n                return func(*args, **kwargs)\r\n\r\n        with TestRefMode():\r\n            fn_compiled.loader.boxed_run(inps)\r\n\r\n        self.assertEqual(len(inps), 0)\r\n        self.assertTrue(sentinel_seen)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_compile_wrapper_with_O0(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(10, 10)\r\n\r\n            def forward(self, x, y):\r\n                return x + self.linear(y)\r\n\r\n        example_inputs = (\r\n            torch.randn(10, 10, device=self.device),\r\n            torch.randn(10, 10, device=self.device),\r\n        )\r\n        model = Model()\r\n        with config.patch(\"aot_inductor.compile_wrapper_with_O0\", True):\r\n            self.check_model(model, example_inputs)\r\n            self.code_check_count(model, example_inputs, \"__attribute__((\", 2)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_remove_intermediate_files(self):\r\n        # For CUDA, generated cpp files contain absolute path to the generated cubin files.\r\n        # With the package artifact, that cubin path should be overriden at the run time,\r\n        # so removing those intermeidate files in this test to verify that.\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return x + y\r\n\r\n        example_inputs = (\r\n            torch.randn(10, 10, device=self.device),\r\n            torch.randn(10, 10, device=self.device),\r\n        )\r\n        model = Model()\r\n        with torch.no_grad():\r\n            torch.manual_seed(0)\r\n            model = model.to(self.device)\r\n            ref_model = copy.deepcopy(model)\r\n            ref_inputs = copy.deepcopy(example_inputs)\r\n            expected = ref_model(*ref_inputs)\r\n\r\n            torch.manual_seed(0)\r\n            with tempfile.NamedTemporaryFile(suffix=\".pt2\") as f:\r\n                ep = torch.export.export(model, example_inputs, strict=True)\r\n                with fresh_inductor_cache():\r\n                    # cubin files are removed when exiting this context\r\n                    package_path = torch._inductor.aoti_compile_and_package(\r\n                        ep,\r\n                        package_path=f.name,\r\n                    )  # type: ignore[arg-type]\r\n                loaded = torch._inductor.aoti_load_package(package_path)\r\n                actual = loaded(*example_inputs)\r\n\r\n            self.assertEqual(actual, expected)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_compile_after_package(self):\r\n        if not self.package_cpp_only:\r\n            raise unittest.SkipTest(\"Only meant to test cpp package\")\r\n        if shutil.which(\"cmake\") is None:\r\n            raise unittest.SkipTest(\"cmake is not available\")\r\n        if shutil.which(\"make\") is None:\r\n            raise unittest.SkipTest(\"make is not available\")\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(10, 10)\r\n\r\n            def forward(self, x, y):\r\n                return x + self.linear(y)\r\n\r\n        with torch.no_grad():\r\n            example_inputs = (\r\n                torch.randn(10, 10, device=self.device),\r\n                torch.randn(10, 10, device=self.device),\r\n            )\r\n            model = Model().to(device=self.device)\r\n            expected = model(*example_inputs)\r\n\r\n            options = {\r\n                \"aot_inductor.package_cpp_only\": self.package_cpp_only,\r\n            }\r\n            ep = torch.export.export(model, example_inputs, strict=True)\r\n            package_path = torch._inductor.aoti_compile_and_package(\r\n                ep, inductor_configs=options\r\n            )\r\n            with tempfile.TemporaryDirectory() as tmp_dir, zipfile.ZipFile(\r\n                package_path, \"r\"\r\n            ) as zip_ref:\r\n                zip_ref.extractall(tmp_dir)\r\n                tmp_path = Path(tmp_dir) / \"data\" / \"aotinductor\" / \"model\"\r\n                self.assertTrue(tmp_path.exists())\r\n                build_path = tmp_path / \"build\"\r\n                self.assertTrue(not build_path.exists())\r\n\r\n                # Create a build directory to run cmake\r\n                build_path.mkdir()\r\n                custom_env = os.environ.copy()\r\n                custom_env[\"CMAKE_PREFIX_PATH\"] = str(Path(torch.__file__).parent)\r\n                subprocess.run(\r\n                    [\"cmake\", \"..\"],\r\n                    cwd=build_path,\r\n                    env=custom_env,\r\n                )\r\n                subprocess.run([\"make\"], cwd=build_path)\r\n\r\n                # Check if the .so file was build successfully\r\n                so_path = build_path / \"libaoti_model.so\"\r\n                self.assertTrue(so_path.exists())\r\n                optimized = torch._export.aot_load(str(so_path), self.device)\r\n                actual = optimized(*example_inputs)\r\n                self.assertTrue(torch.allclose(actual, expected))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_deepcopy_compiled_model(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return x + y\r\n\r\n        example_inputs = (\r\n            torch.randn(10, 10, device=self.device),\r\n            torch.randn(10, 10, device=self.device),\r\n        )\r\n\r\n        model = Model()\r\n\r\n        compiled = compile(model, example_inputs)\r\n\r\n        copmiled_copy = copy.deepcopy(compiled)\r\n\r\n        expected = model(*example_inputs)\r\n        output = compiled(*example_inputs)\r\n        output_copy = copmiled_copy(*example_inputs)\r\n        self.assertEqual(expected, output)\r\n        self.assertEqual(expected, output_copy)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_dynamic_smem_above_default_limit(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return x @ y\r\n\r\n        model = Model().to(self.device)\r\n        # on A100, the generated Triton kernel for this MM\r\n        # requires 55296 bytes of dynamic SMEM which is above\r\n        # the A100's default dynamic SMEM limit of 49152 bytes.\r\n        example_inputs = (\r\n            torch.randn(10285, 96, device=self.device),\r\n            torch.randn(96, 1, device=self.device),\r\n        )\r\n        self.check_model(\r\n            model,\r\n            example_inputs,\r\n            options={\r\n                \"max_autotune\": True,\r\n                \"max_autotune_gemm_backends\": \"TRITON\",\r\n            },\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_output_path_2(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(10, 10)\r\n\r\n            def forward(self, x, y):\r\n                return x + self.linear(y)\r\n\r\n        model = Model().to(device=self.device)\r\n        example_inputs = (\r\n            torch.randn(10, 10, device=self.device),\r\n            torch.randn(10, 10, device=self.device),\r\n        )\r\n        expected_path = os.path.join(tempfile.mkdtemp(dir=cache_dir()), \"model.so\")\r\n        actual_path = AOTIRunnerUtil.compile(\r\n            model, example_inputs, options={\"aot_inductor.output_path\": expected_path}\r\n        )\r\n        self.assertTrue(actual_path == expected_path)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_large_weight(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2048, 262144)\r\n\r\n            def forward(self, x, y):\r\n                return x + self.linear(y)\r\n\r\n        example_inputs = (\r\n            torch.randn(1, 262144, device=self.device),\r\n            torch.randn(1, 2048, device=self.device),\r\n        )\r\n\r\n        # We only test compilation since we often get OOM running in CI.\r\n        model = Model()\r\n        model = model.to(self.device)\r\n        AOTIRunnerUtil.compile(model, example_inputs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_cache_hit(self):\r\n        def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([2, 4])\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_torch_compile(self):\r\n        def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n            opt_model = torch.compile(model, fullgraph=True)\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                model.zero_grad()\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_torch_compile_api_inductor(self):\r\n        def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res\r\n\r\n        expected = fn()\r\n        with config.patch(compiled_autograd=True):\r\n            compiled_fn = torch.compile(fn)\r\n        actual = compiled_fn()\r\n        self.assertEqual(expected, actual)\r\n        self.assertEqual(counters[\"compiled_autograd\"][\"captures\"], 1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_torch_compile_api_aot_eager(self):\r\n        def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res\r\n\r\n        expected = fn()\r\n        with config.patch(compiled_autograd=True):\r\n            compiled_fn = torch.compile(fn, backend=\"aot_eager\")\r\n        actual = compiled_fn()\r\n        self.assertEqual(expected, actual)\r\n        self.assertEqual(counters[\"compiled_autograd\"][\"captures\"], 1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_torch_compile_api_eager(self):\r\n        def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res\r\n\r\n        expected = fn()\r\n        with config.patch(compiled_autograd=True):\r\n            compiled_fn = torch.compile(fn, backend=\"eager\")\r\n        actual = compiled_fn()\r\n        self.assertEqual(expected, actual)\r\n        self.assertEqual(counters[\"compiled_autograd\"][\"captures\"], 1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_implicit_add(self):\r\n        def fn():\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                # y is used multiple times, gradients get added\r\n                return torch.sigmoid(x * y + torch.sin(y) + torch.cos(y))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield result\r\n                yield y.grad\r\n                y.grad = None\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_no_output_nodes_all_leaves(self):\r\n        def fn():\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(x * z + torch.sin(y) + torch.cos(y))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n                result = model(x).sum()\r\n                out = result.backward()\r\n                assert out is None\r\n                assert y.grad is not None\r\n                assert z.grad is not None\r\n                yield y.grad\r\n                yield z.grad\r\n                y.grad = None\r\n                z.grad = None\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_no_output_nodes_some_leaves(self):\r\n        def fn():\r\n            class UnreachableBwd(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    raise RuntimeError\r\n\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n            a = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(x * y * z * UnreachableBwd.apply(a))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n                result = model(x).sum()\r\n                out = result.backward(inputs=[y, z])\r\n                assert out is None\r\n                assert y.grad is not None\r\n                assert z.grad is not None\r\n                assert a.grad is None\r\n                yield y.grad\r\n                yield z.grad\r\n                y.grad = None\r\n                z.grad = None\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_dynamic_shapes(self):\r\n        def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for b in range(10, 100, 10):\r\n                x = torch.randn([b, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n                model.zero_grad()\r\n\r\n        # TODO(jansel): we should be able to get this count to 1\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_dynamic_shapes_eager_node(self):\r\n        # Here, we have no way of marking the symbolic sizes using in SumBackward as dynamic\r\n        def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for b, s in zip([10, 20, 30], [2, 4, 8]):\r\n                x = torch.randn([b, 4])\r\n                result = opt_model(x)\r\n                view = result.view(s, -1)\r\n                # sum will save dynamic sizes\r\n                loss = view.sum()\r\n                loss.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n                model.zero_grad()\r\n\r\n        self.check_output_and_recompiles(fn, count=3)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_accumulate_without_zero(self):\r\n        def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for _ in range(10):\r\n                x = torch.randn([10, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad.clone()\r\n                yield model[0].bias.grad.clone()\r\n                yield model[2].weight.grad.clone()\r\n                yield model[2].bias.grad.clone()\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_inplace_grad_update(self):\r\n        def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for _ in range(10):\r\n                w_grad = torch.rand_like(model[0].weight)\r\n                b_grad = torch.rand_like(model[0].bias)\r\n                model[0].weight.grad = w_grad\r\n                model[0].bias.grad = b_grad\r\n\r\n                x = torch.randn([10, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                assert model[0].weight.grad is w_grad\r\n                assert model[0].bias.grad is b_grad\r\n                yield w_grad.clone()\r\n                yield b_grad.clone()\r\n\r\n        self.check_output_and_recompiles(fn, count=1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_issue106555(self):\r\n        DEVICE = torch.device(GPU_TYPE, 0)\r\n        NUM_FEATURES = 256\r\n\r\n        def bias_sigmoid_mul(x1, x2, bias):\r\n            x2 = torch.sigmoid(x2 + bias)\r\n            y = x1 * x2\r\n            return y\r\n\r\n        bias_sigmoid_mul_jit = torch.compile(bias_sigmoid_mul)\r\n\r\n        class ModuleWithJit(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear_1 = nn.Linear(NUM_FEATURES, NUM_FEATURES, bias=True)\r\n                self.linear_2 = nn.Linear(NUM_FEATURES, NUM_FEATURES, bias=False)\r\n                self.linear_2_bias = nn.Parameter(torch.zeros(NUM_FEATURES))\r\n\r\n            def forward(self, input_tensor):\r\n                x1 = self.linear_1(input_tensor)\r\n                x2 = self.linear_2(input_tensor)\r\n                output = bias_sigmoid_mul_jit(x1, x2, self.linear_2_bias)\r\n                return output\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.module_with_jit_1 = ModuleWithJit()\r\n                self.module_with_jit_2 = ModuleWithJit()\r\n\r\n            def forward(self, x, gradient_checkpointing: bool):\r\n                if gradient_checkpointing:\r\n                    y = torch.utils.checkpoint.checkpoint(\r\n                        self._forward, x, use_reentrant=True\r\n                    )\r\n                else:\r\n                    y = self._forward(x)\r\n                return y\r\n\r\n            def _forward(self, x):\r\n                x = x + self.module_with_jit_1(x)\r\n                x = x + self.module_with_jit_2(x.transpose(-2, -3)).transpose(-2, -3)\r\n                return x\r\n\r\n        device_interface = get_interface_for_device(GPU_TYPE)\r\n        device_interface.set_device(device=DEVICE)\r\n        torch.manual_seed(1234567890)\r\n        model = Model()\r\n        model.train()\r\n        model.to(device=DEVICE)\r\n        model_parameters = list(model.parameters())\r\n\r\n        torch.manual_seed(1234567890)\r\n        input_tensor = torch.randn(1, 128, 256, NUM_FEATURES).to(device=DEVICE)\r\n        input_tensor.requires_grad = True\r\n        target_tensor = torch.randn(1, 128, 256, NUM_FEATURES).to(\r\n            dtype=input_tensor.dtype, device=DEVICE\r\n        )\r\n\r\n        for iteration in range(10):\r\n            for param in model_parameters:\r\n                param.grad = None\r\n            output_tensor = model(\r\n                x=input_tensor.clone(),\r\n                gradient_checkpointing=True,\r\n            )\r\n            loss = torch.mean(torch.abs(target_tensor - output_tensor))\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_fn_saved_tensors(self):\r\n        def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_saved_multiple_tensors(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y):\r\n                    ctx.save_for_backward(x, y)\r\n                    return torch.sin(x), torch.sin(y)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_x, gO_y):\r\n                    (x, y) = ctx.saved_tensors\r\n                    return gO_x * torch.cos(x), gO_y * torch.cos(y)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2 = MyFn.apply(x, y)\r\n                loss = (out1 * out2).sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_saved_multiple_tensors_dedup(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x, x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x1, x2) = ctx.saved_tensors\r\n                    return gO * torch.cos(x1) * torch.cos(x2)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_saved_shape_tensor(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * x.shape[0]\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_saved_attr(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.shape = x.shape\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    x_shape = ctx.shape[0]\r\n                    return gO * x_shape\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(\r\n            fn, count=2, compiler_fn=make_compiler_fn(fullgraph=False)\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_multiple_grads(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y):\r\n                    return x + y, y\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_1, gO_2):\r\n                    return gO_1, gO_2\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2 = MyFn.apply(x, y)\r\n                loss = (out1 + out2).sum()\r\n                loss.backward()\r\n                yield x.grad\r\n                yield y.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_non_variable_input(self):\r\n        def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y, z):\r\n                    return x * 2, y * 3, z * 4\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_1, gO_2, gO_3):\r\n                    return gO_1, gO_2, gO_3\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = 1\r\n                z = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2, out3 = MyFn.apply(x, y, z)\r\n                loss = (out1 + out2 + out3).sum()\r\n                loss.backward()\r\n                yield x\r\n                yield y\r\n                yield z\r\n\r\n        self.check_output_and_recompiles(fn, count=2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_with_same_graph(self):\r\n        def fn():\r\n            class MyFn1(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    return gO\r\n\r\n            # same as MyFn1, but different autograd function id\r\n            # should not be using same graph as MyFn1\r\n            class MyFn2(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    return gO\r\n\r\n            for myfn in [MyFn1, MyFn2, MyFn1, MyFn2]:\r\n                x = torch.arange(0.0, 10, requires_grad=True)\r\n                out = myfn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(\r\n            fn, count=2\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_dynamically_defined_class(self):\r\n        def fn():\r\n            def create_class(multiplier: int):\r\n                class DynamicFn(torch.autograd.Function):\r\n                    @staticmethod\r\n                    def forward(ctx, x):\r\n                        return x * multiplier\r\n\r\n                    @staticmethod\r\n                    def backward(ctx, gO):\r\n                        return gO * multiplier\r\n\r\n                return DynamicFn\r\n\r\n            for multiplier in [10, 20, 30]:\r\n                x = torch.arange(0.0, 10, requires_grad=True)\r\n                out = create_class(multiplier).apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, count=3)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_bw_graph_break(self):\r\n        def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    print(\"graph break\")\r\n                    return gO * torch.cos(x)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(\r\n            fn, count=[2, 6], compiler_fn=make_compiler_fn(fullgraph=False)\r\n        )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_compiled_fw_graph_break(self):\r\n        def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    print(\"graph break\")\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            opt_model = torch.compile(MySin.apply)\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = opt_model(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(\r\n            fn, count=2, compiler_fn=make_compiler_fn(fullgraph=False)\r\n        )\r\n        self.assertEqual(counters[\"stats\"][\"unique_graphs\"], 5)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_custom_fn_compiled_fw_bw_graph_break(self):\r\n        def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    print(\"graph break\")\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            opt_model = torch.compile(MySin.apply)\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = opt_model(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(\r\n            fn, count=[2, 6], compiler_fn=make_compiler_fn(fullgraph=False)\r\n        )\r\n        self.assertEqual(counters[\"stats\"][\"unique_graphs\"], 9)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n\r\n  static torch::Tensor forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x) {\r\n    return x;\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    return grad_output;\r\n  }\r\n};\r\n\r\ntorch::Tensor custom_op_backed_by_autograd_fn(torch::Tensor x) {\r\n  return CustomOpAutogradFunction::apply(x);\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node.custom_op_backed_by_autograd_fn(\r\n                    x\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        # compiles for 10 (static) and 100 (dynamic)\r\n        self.check_output_and_recompiles(fn, 2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node_saved(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n\r\n  static torch::Tensor forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x,\r\n      const torch::Tensor& y,\r\n      const torch::Tensor& fixed) {\r\n    ctx->save_for_backward({x, y});\r\n    ctx->saved_data[\"fixed_tensor\"] = fixed;\r\n    ctx->saved_data[\"bool\"] = true;\r\n    ctx->saved_data[\"int\"] = 1;\r\n    c10::List<std::string> list({\"string\"});\r\n    ctx->saved_data[\"list\"] = std::move(list);\r\n    c10::Dict<std::string, double> dict;\r\n    dict.insert(\"string\", 1.0);\r\n    ctx->saved_data[\"dict\"] = std::move(dict);\r\n    return x;\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    const auto& saved_variables = ctx->get_saved_variables();\r\n    assert(saved_variables.size() == 2);\r\n    torch::Tensor x = saved_variables[0];\r\n    torch::Tensor y = saved_variables[1];\r\n    torch::Tensor fixed = ctx->saved_data[\"fixed_tensor\"].toTensor();\r\n    assert(ctx->saved_data[\"bool\"].isBool());\r\n    c10::SymInt i = ctx->saved_data[\"int\"].toSymInt();\r\n    c10::List<c10::IValue> list = ctx->saved_data[\"list\"].toList();\r\n    assert(list.size() == 1);\r\n    assert(list.get(0).toStringRef() == \"string\");\r\n    c10::Dict<c10::IValue, c10::IValue> dict = ctx->saved_data[\"dict\"].toGenericDict();\r\n    assert(dict.size() == 1);\r\n    assert(dict.at(\"string\") == 1.0);\r\n\r\n    torch::autograd::variable_list grad_inputs(3);\r\n    grad_inputs[0] = x + y + torch::sum(fixed) + i;\r\n    return grad_inputs;\r\n  }\r\n};\r\n\r\ntorch::Tensor custom_op_backed_by_autograd_fn(const torch::Tensor& x, const torch::Tensor& y, const torch::Tensor& fixed) {\r\n  return CustomOpAutogradFunction::apply(x, y, fixed);\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node_saved, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node_saved\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            fixed = torch.ones(2, 2)\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                y = torch.randn(i, i)\r\n                out = torch.ops.test_autograd_cpp_node_saved.custom_op_backed_by_autograd_fn(\r\n                    x, y, fixed\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, 2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node_saved_dynamic(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n\r\n  static torch::Tensor forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x) {\r\n    ctx->save_for_backward({x});\r\n    ctx->saved_data[\"dynamic\"] = x.view(-1);\r\n    return x;\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    const auto& saved_variables = ctx->get_saved_variables();\r\n    assert(saved_variables.size() == 1);\r\n    torch::Tensor x = saved_variables[0];\r\n    torch::Tensor z = ctx->saved_data[\"dynamic\"].toTensor();\r\n\r\n    torch::autograd::variable_list grad_inputs(1);\r\n    grad_inputs[0] = x + torch::sum(z);\r\n    return grad_inputs;\r\n  }\r\n};\r\n\r\ntorch::Tensor custom_op_backed_by_autograd_fn(const torch::Tensor& x) {\r\n  return CustomOpAutogradFunction::apply(x);\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node_saved_dynamic, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node_saved_dynamic\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_dynamic.custom_op_backed_by_autograd_fn(\r\n                    x\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        # compiles for 10 (static) and 100 (dynamic)\r\n        self.check_output_and_recompiles(fn, 2)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node_saved_int(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n\r\n  static torch::Tensor forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x,\r\n      int64_t y) {\r\n    ctx->save_for_backward({x});\r\n    ctx->saved_data[\"int\"] = y;\r\n    ctx->saved_data[\"symint\"] = c10::SymInt(y);\r\n    return x;\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    const auto& saved_variables = ctx->get_saved_variables();\r\n    assert(saved_variables.size() == 1);\r\n    torch::Tensor x = saved_variables[0];\r\n    c10::SymInt y = ctx->saved_data[\"int\"].toSymInt();\r\n    c10::SymInt ys = ctx->saved_data[\"symint\"].toSymInt();\r\n\r\n    torch::autograd::variable_list grad_inputs(2);\r\n    grad_inputs[0] = x + y + ys;\r\n    return grad_inputs;\r\n  }\r\n};\r\n\r\ntorch::Tensor custom_op_backed_by_autograd_fn(const torch::Tensor& x, int64_t y) {\r\n  return CustomOpAutogradFunction::apply(x, y);\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node_saved_int, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node_saved_int\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            for y in [1, 2, 3, 1]:\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_int.custom_op_backed_by_autograd_fn(\r\n                    x, y\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, 1)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node_saved_float(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n\r\n  static torch::Tensor forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x,\r\n      double z) {\r\n    ctx->save_for_backward({x});\r\n    ctx->saved_data[\"float\"] = z;\r\n    ctx->saved_data[\"symfloat\"] = c10::SymFloat(z);\r\n    return x;\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    const auto& saved_variables = ctx->get_saved_variables();\r\n    assert(saved_variables.size() == 1);\r\n    torch::Tensor x = saved_variables[0];\r\n    c10::SymFloat z = ctx->saved_data[\"float\"].toSymFloat();\r\n    c10::SymFloat zs = ctx->saved_data[\"symfloat\"].toSymFloat();\r\n\r\n    torch::autograd::variable_list grad_inputs(2);\r\n    grad_inputs[0] = x + z + zs;\r\n    return grad_inputs;\r\n  }\r\n};\r\n\r\ntorch::Tensor custom_op_backed_by_autograd_fn(const torch::Tensor& x, double z) {\r\n  return CustomOpAutogradFunction::apply(x, z);\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node_saved_float, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node_saved_float\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            for z in [1.1, 2.2, 3.3, 1.1]:\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_float.custom_op_backed_by_autograd_fn(\r\n                    x, z\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        # compiled autograd and dynamo both support symfloat, but not backend\r\n        self.check_output_and_recompiles(fn, [1, 4])\r\n        # 1 restart analysis due to specialize_float=False\r\n        self.assertEqual(counters[\"stats\"][\"unique_graphs\"], 3)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_autograd_cpp_node_data_dependent(self, load_inline):\r\n        cpp_source = \"\"\"\r\nstruct CustomOpAutogradFunction : public torch::autograd::Function<CustomOpAutogradFunction> {\r\n  static constexpr bool is_traceable = true;\r\n  static int iteration;\r\n\r\n  static torch::autograd::variable_list forward(\r\n      torch::autograd::AutogradContext* ctx,\r\n      const torch::Tensor& x,\r\n      const torch::Tensor& y) {\r\n    ctx->save_for_backward({x, y});\r\n    ctx->saved_data[\"bool\"] = true;\r\n    ctx->saved_data[\"int\"] = 1;\r\n\r\n    switch (iteration) {\r\n        case 0: {\r\n            break;\r\n        }\r\n        case 1: {\r\n            // recompile\r\n            ctx->saved_data[\"forces_recompile\"] = iteration;\r\n            break;\r\n        }\r\n        case 2: {\r\n            // recompile\r\n            ctx->set_materialize_grads(false);\r\n            break;\r\n        }\r\n        case 3: {\r\n            // reuse\r\n            break;\r\n        }\r\n        default: {\r\n            throw std::runtime_error(\"unexpected iteration\");\r\n        }\r\n    }\r\n    iteration++;\r\n    return {x, y};\r\n  }\r\n\r\n  static torch::autograd::variable_list backward(\r\n      torch::autograd::AutogradContext *ctx,\r\n      torch::autograd::variable_list grad_output) {\r\n    const auto& saved_variables = ctx->get_saved_variables();\r\n    assert(saved_variables.size() == 2);\r\n    torch::Tensor x = saved_variables[0];\r\n    torch::Tensor y = saved_variables[1];\r\n    c10::SymInt i = ctx->saved_data[\"int\"].toSymInt();\r\n\r\n    torch::autograd::variable_list grad_inputs(2);\r\n    grad_inputs[0] = x + y + i;\r\n    return grad_inputs;\r\n  }\r\n};\r\n\r\nint CustomOpAutogradFunction::iteration = 0;\r\n\r\ntorch::autograd::variable_list custom_op_backed_by_autograd_fn(const torch::Tensor& x, const torch::Tensor& y) {\r\n  return CustomOpAutogradFunction::apply(x, y);\r\n}\r\n\r\nvoid reset() {\r\n    CustomOpAutogradFunction::iteration = 0;\r\n}\r\n\r\nTORCH_LIBRARY(test_autograd_cpp_node_data_dependent, m) {\r\n    m.def(\"custom_op_backed_by_autograd_fn\", custom_op_backed_by_autograd_fn);\r\n    m.def(\"reset\", reset);\r\n}\r\n        \"\"\"\r\n\r\n        module = load_inline(\r\n            name=\"test_autograd_cpp_node_data_dependent\",\r\n            cpp_sources=cpp_source,\r\n            functions=\"custom_op_backed_by_autograd_fn\",\r\n            verbose=True,\r\n        )\r\n\r\n        def fn():\r\n            torch.ops.test_autograd_cpp_node_data_dependent.reset()\r\n            for i in [10, 10, 10, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                y = torch.randn(i, i)\r\n                (\r\n                    out1,\r\n                    out2,\r\n                ) = torch.ops.test_autograd_cpp_node_data_dependent.custom_op_backed_by_autograd_fn(\r\n                    x, y\r\n                )\r\n                loss = (out1 + out2).sum()\r\n                loss.backward()\r\n                yield x.grad\r\n\r\n        self.check_output_and_recompiles(fn, 3)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            x = torch.randn([2, 4])\r\n            result = model(x).sum()\r\n            result.backward()\r\n            yield model[0].weight.grad\r\n            yield model[0].bias.grad\r\n            yield model[2].weight.grad\r\n            yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([2, 4])\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([2, 4])\r\n\r\n                model[0].weight.register_hook(hook1)\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.grad_fn.register_prehook(hook2)\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.grad_fn.register_hook(hook3)\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n            opt_model = torch.compile(model, fullgraph=True)\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                model.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            torch.manual_seed(123)\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.Sigmoid(),\r\n            )\r\n\r\n            res = []\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                res.append(model[0].weight.grad)\r\n                res.append(model[0].bias.grad)\r\n                model.zero_grad()\r\n            return res",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn(inputs):\r\n            args_0, args_1, args_2 = inputs\r\n            out = torch.mm(args_0, args_1)\r\n            out = torch.mm(out, args_2)\r\n            loss = out.sum()\r\n            with compiled_autograd._enable(bwd_compiler_fn):\r\n                loss.backward()\r\n            yield args_0.grad\r\n            yield args_1.grad\r\n            yield args_2.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                # y is used multiple times, gradients get added\r\n                return torch.sigmoid(x * y + torch.sin(y) + torch.cos(y))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield result\r\n                yield y.grad\r\n                y.grad = None",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(x * z + torch.sin(y) + torch.cos(y))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                gy, gz = torch.autograd.grad(result, inputs=[y, z])\r\n                assert y.grad is None\r\n                assert z.grad is None\r\n                yield gy\r\n                yield gz",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class UnreachableBwd(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    raise RuntimeError\r\n\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(UnreachableBwd.apply(y) * z)\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                gz = torch.autograd.grad(result, inputs=[z])\r\n                assert y.grad is None\r\n                assert z.grad is None\r\n                yield gz",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(x * z + torch.sin(y) + torch.cos(y))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n                result = model(x).sum()\r\n                out = result.backward()\r\n                assert out is None\r\n                assert y.grad is not None\r\n                assert z.grad is not None\r\n                yield y.grad\r\n                yield z.grad\r\n                y.grad = None\r\n                z.grad = None",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class UnreachableBwd(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    raise RuntimeError\r\n\r\n            y = torch.randn(1, 4, requires_grad=True)\r\n            z = torch.randn(1, 4, requires_grad=True)\r\n            a = torch.randn(1, 4, requires_grad=True)\r\n\r\n            def model(x):\r\n                return torch.sigmoid(x * y * z * UnreachableBwd.apply(a))\r\n\r\n            for _ in range(3):\r\n                x = torch.randn([1, 4])\r\n                result = model(x).sum()\r\n                out = result.backward(inputs=[y, z])\r\n                assert out is None\r\n                assert y.grad is not None\r\n                assert z.grad is not None\r\n                assert a.grad is None\r\n                yield y.grad\r\n                yield z.grad\r\n                y.grad = None\r\n                z.grad = None",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            def fwd(x, y, z):\r\n                out = x * y  # MulBackward0\r\n                out2 = out * z  # MulBackward0\r\n                return out2.sum()  # SumBackward0\r\n\r\n            x = torch.randn(5, requires_grad=True)\r\n            y = torch.randn(5, requires_grad=True)\r\n            z = torch.randn(5, requires_grad=True)\r\n            loss = fwd(x, y, z)\r\n            torch.compile(lambda: torch.autograd.backward(loss, inputs=[x]))()\r\n            yield x.grad\r\n            x.grad = None\r\n\r\n            loss = fwd(x, y, z)\r\n            torch.compile(lambda: torch.autograd.backward(loss, inputs=[y]))()\r\n            yield y.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for b in range(10, 100, 10):\r\n                x = torch.randn([b, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n                model.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for b, s in zip([10, 20, 30], [2, 4, 8]):\r\n                x = torch.randn([b, 4])\r\n                result = opt_model(x)\r\n                view = result.view(s, -1)\r\n                # sum will save dynamic sizes\r\n                loss = view.sum()\r\n                loss.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n                model.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn(call_backward):\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n\r\n            for b, s in zip([10, 20, 30], [2, 4, 8]):\r\n                x = torch.randn([b, 4])\r\n                result = model(x)\r\n                view = result.view(s, -1)\r\n                # sum will save dynamic sizes\r\n                loss = view.sum()\r\n                call_backward(loss)\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad\r\n                model.zero_grad()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for _ in range(10):\r\n                x = torch.randn([10, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad.clone()\r\n                yield model[0].bias.grad.clone()\r\n                yield model[2].weight.grad.clone()\r\n                yield model[2].bias.grad.clone()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            opt_model = torch.compile(model, dynamic=True)\r\n\r\n            for _ in range(10):\r\n                w_grad = torch.rand_like(model[0].weight)\r\n                b_grad = torch.rand_like(model[0].bias)\r\n                model[0].weight.grad = w_grad\r\n                model[0].bias.grad = b_grad\r\n\r\n                x = torch.randn([10, 4])\r\n                result = opt_model(x).sum()\r\n                result.backward()\r\n                assert model[0].weight.grad is w_grad\r\n                assert model[0].bias.grad is b_grad\r\n                yield w_grad.clone()\r\n                yield b_grad.clone()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y):\r\n                    ctx.save_for_backward(x, y)\r\n                    return torch.sin(x), torch.sin(y)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_x, gO_y):\r\n                    (x, y) = ctx.saved_tensors\r\n                    return gO_x * torch.cos(x), gO_y * torch.cos(y)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2 = MyFn.apply(x, y)\r\n                loss = (out1 * out2).sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x, x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x1, x2) = ctx.saved_tensors\r\n                    return gO * torch.cos(x1) * torch.cos(x2)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * x.shape[0]\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.shape = x.shape\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    x_shape = ctx.shape[0]\r\n                    return gO * x_shape\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MyFn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y):\r\n                    return x + y, y\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_1, gO_2):\r\n                    return gO_1, gO_2\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2 = MyFn.apply(x, y)\r\n                loss = (out1 + out2).sum()\r\n                loss.backward()\r\n                yield x.grad\r\n                yield y.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x, y, z):\r\n                    return x * 2, y * 3, z * 4\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO_1, gO_2, gO_3):\r\n                    return gO_1, gO_2, gO_3\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                y = 1\r\n                z = torch.arange(0.0, i, requires_grad=True)\r\n                out1, out2, out3 = MyFn.apply(x, y, z)\r\n                loss = (out1 + out2 + out3).sum()\r\n                loss.backward()\r\n                yield x\r\n                yield y\r\n                yield z",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            def _fn(x):\r\n                return x\r\n\r\n            x = torch.arange(\r\n                1, 10, requires_grad=True, dtype=torch.float16, device=GPU_TYPE\r\n            )\r\n            out = _fn(x)\r\n            loss = out.sum()\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    return gO\r\n\r\n            x = torch.arange(\r\n                1, 10, requires_grad=True, dtype=torch.float16, device=GPU_TYPE\r\n            )\r\n            x_view = x.view(3, 3)\r\n            out = MyFn.apply(x_view)\r\n            loss = out.sum()\r\n            loss.backward()\r\n            yield x.dtype\r\n            yield x.device\r\n            yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MyFn1(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    return gO\r\n\r\n            # same as MyFn1, but different autograd function id\r\n            # should not be using same graph as MyFn1\r\n            class MyFn2(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    return x\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    return gO\r\n\r\n            for myfn in [MyFn1, MyFn2, MyFn1, MyFn2]:\r\n                x = torch.arange(0.0, 10, requires_grad=True)\r\n                out = myfn.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            def create_class(multiplier: int):\r\n                class DynamicFn(torch.autograd.Function):\r\n                    @staticmethod\r\n                    def forward(ctx, x):\r\n                        return x * multiplier\r\n\r\n                    @staticmethod\r\n                    def backward(ctx, gO):\r\n                        return gO * multiplier\r\n\r\n                return DynamicFn\r\n\r\n            for multiplier in [10, 20, 30]:\r\n                x = torch.arange(0.0, 10, requires_grad=True)\r\n                out = create_class(multiplier).apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    print(\"graph break\")\r\n                    return gO * torch.cos(x)\r\n\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = MySin.apply(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    print(\"graph break\")\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            opt_model = torch.compile(MySin.apply)\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = opt_model(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class MySin(torch.autograd.Function):\r\n                @staticmethod\r\n                def forward(ctx, x):\r\n                    print(\"graph break\")\r\n                    ctx.save_for_backward(x)\r\n                    return torch.sin(x)\r\n\r\n                @staticmethod\r\n                def backward(ctx, gO):\r\n                    print(\"graph break\")\r\n                    (x,) = ctx.saved_tensors\r\n                    return gO * torch.cos(x)\r\n\r\n            opt_model = torch.compile(MySin.apply)\r\n            for i in [10, 100, 10, 15, 20, 25]:\r\n                x = torch.arange(0.0, i, requires_grad=True)\r\n                out = opt_model(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(2, 1, bias=False),\r\n                torch.nn.Linear(1, 2, bias=False),\r\n            )\r\n            x = torch.randn(2, 2)\r\n\r\n            out = model(x)\r\n            loss = out.sum()\r\n            torch.manual_seed(0)\r\n            loss.backward()\r\n\r\n            yield model[0].weight.grad\r\n            yield model[1].weight.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            x = torch.ones(10, 10, requires_grad=True)\r\n            out = torch.ops.test_non_traceable_autograd_cpp_node.custom_op_backed_by_autograd_fn(\r\n                x\r\n            )\r\n            loss = out.sum()\r\n            loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node.custom_op_backed_by_autograd_fn(\r\n                    x\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            fixed = torch.ones(2, 2)\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                y = torch.randn(i, i)\r\n                out = torch.ops.test_autograd_cpp_node_saved.custom_op_backed_by_autograd_fn(\r\n                    x, y, fixed\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for i in [10, 100, 10, 20, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_dynamic.custom_op_backed_by_autograd_fn(\r\n                    x\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for y in [1, 2, 3, 1]:\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_int.custom_op_backed_by_autograd_fn(\r\n                    x, y\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            for z in [1.1, 2.2, 3.3, 1.1]:\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = torch.ops.test_autograd_cpp_node_saved_float.custom_op_backed_by_autograd_fn(\r\n                    x, z\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            torch.ops.test_autograd_cpp_node_data_dependent.reset()\r\n            for i in [10, 10, 10, 10]:\r\n                x = torch.ones(i, i, requires_grad=True)\r\n                y = torch.randn(i, i)\r\n                (\r\n                    out1,\r\n                    out2,\r\n                ) = torch.ops.test_autograd_cpp_node_data_dependent.custom_op_backed_by_autograd_fn(\r\n                    x, y\r\n                )\r\n                loss = (out1 + out2).sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            x = torch.randn([2, 4])\r\n            result = model(x).sum()\r\n            result.backward()\r\n            yield model[0].weight.grad\r\n            yield model[0].bias.grad\r\n            yield model[2].weight.grad\r\n            yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            x = torch.randn([2, 4])\r\n\r\n            @torch.compile\r\n            def forward(model, x):\r\n                return model(x)\r\n\r\n            result = forward(model, x).sum()\r\n            result.backward()\r\n            yield model[0].weight.grad\r\n            yield model[0].bias.grad\r\n            yield model[2].weight.grad\r\n            yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            for i in [10, 11, 12]:\r\n                model.zero_grad()\r\n                x = torch.randn([i, 4])\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n                yield model[2].weight.grad\r\n                yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            model = torch.nn.Sequential(\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(4, 4),\r\n                torch.nn.ReLU(),\r\n            )\r\n            x = torch.randn([2, 4])\r\n            result = model(x).sum()\r\n            result.backward()\r\n            yield model[0].weight.grad\r\n            yield model[0].bias.grad\r\n            yield model[2].weight.grad\r\n            yield model[2].bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            order = []\r\n\r\n            class MyModule(nn.Module):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.linear = torch.nn.Linear(10, 10, bias=False)\r\n\r\n                def forward(self, x):\r\n                    return self.linear(x)\r\n\r\n            x = torch.randn(10, 10)\r\n            module = MyModule()\r\n\r\n            def make_pre_hook(id):\r\n                return lambda _: order.append(f\"pre_hook_{id}\")\r\n\r\n            def make_post_hook(id):\r\n                return lambda _1, _2: order.append(f\"post_hook_{id}\")\r\n\r\n            count = 0\r\n\r\n            def register_hooks_on_all_nodes(nodes):\r\n                nonlocal count\r\n                for node, _ in nodes:\r\n                    if node is None:\r\n                        continue\r\n                    count += 1\r\n                    id = f\"{node.name()}_{count}\"\r\n                    node.register_prehook(make_pre_hook(id))\r\n                    node.register_hook(make_post_hook(id))\r\n                    register_hooks_on_all_nodes(node.next_functions)\r\n\r\n            loss = module(x).sum()\r\n            register_hooks_on_all_nodes(((loss.grad_fn, None),))\r\n\r\n            def make_tensor_pre_hook(id):\r\n                return lambda _: order.append(f\"tensor_pre_hook_{id}\")\r\n\r\n            def make_post_acc_grad_hook(id):\r\n                return lambda _: order.append(f\"post_acc_grad_hook_{id}\")\r\n\r\n            module.linear.weight.register_hook(make_tensor_pre_hook(\"weight\"))\r\n\r\n            module.linear.weight.register_post_accumulate_grad_hook(\r\n                make_post_acc_grad_hook(\"weight\")\r\n            )\r\n\r\n            loss.backward()\r\n            yield tuple(order)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n            class mlp(nn.Module):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.layer1 = nn.Linear(10, 10)\r\n                    self.layer2 = nn.Linear(10, 10)\r\n                    self.layer3 = nn.Linear(10, 10)\r\n                    self.layer4 = nn.Linear(10, 10)\r\n\r\n                def forward(self, x):\r\n                    x = self.layer1(x)\r\n                    x = self.layer2(x)\r\n                    x = self.layer3(x)\r\n                    x = self.layer4(x)\r\n                    return x\r\n\r\n            recompute_list = [torch.ops.aten.addmm.default]\r\n\r\n            def recompute_policy(ctx, op, *args, **kwargs):\r\n                if op in recompute_list:\r\n                    return CheckpointPolicy.MUST_RECOMPUTE\r\n                else:\r\n                    return CheckpointPolicy.PREFER_SAVE\r\n\r\n            def context_fn():\r\n                return create_selective_checkpoint_contexts(recompute_policy)\r\n\r\n            model = mlp()\r\n            input = torch.randn(1, 10)\r\n\r\n            out = checkpoint(model, input, use_reentrant=False, context_fn=context_fn)\r\n            out.sum().backward()\r\n            yield model.layer1.weight.grad\r\n            yield model.layer1.bias.grad\r\n            yield model.layer2.weight.grad\r\n            yield model.layer2.bias.grad\r\n            yield model.layer3.weight.grad\r\n            yield model.layer3.bias.grad\r\n            yield model.layer4.weight.grad\r\n            yield model.layer4.bias.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn():\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = (\r\n                    torch.ops.test_autograd_cpp_node_id.custom_op_backed_by_autograd_fn(\r\n                        x\r\n                    )\r\n                )\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fn(op):\r\n                x = torch.ones(10, 10, requires_grad=True)\r\n                out = op(x)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                yield x.grad",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_correctness(self, device, dtype, optim_info, use_closure):\r\n        optim_cls = optim_info.optim_cls\r\n        all_optim_inputs = _get_optim_inputs_including_global_cliquey_kwargs(\r\n            device, dtype, optim_info, skip=(\"differentiable\",)\r\n        )\r\n\r\n        if optim_info.step_requires_closure and not use_closure:\r\n            return\r\n\r\n        for optim_input in all_optim_inputs:\r\n            kwargs = optim_input.kwargs\r\n\r\n            use_scheduler = isinstance(kwargs.get(\"lr\", None), torch.Tensor)\r\n            scheduler_classes = (\r\n                list(LR_SCHEDULER_TO_KWARGS.keys()) if use_scheduler else [None]\r\n            )\r\n\r\n            for scheduler_cls in scheduler_classes:\r\n                torch._dynamo.reset()\r\n                torch._inductor.metrics.reset()\r\n                input = torch.ones([10, 10], device=device)\r\n                model_eager = torch.nn.Sequential(\r\n                    *[torch.nn.Linear(10, 10, device=device) for _ in range(2)]\r\n                )\r\n                model_eager(input).sum().backward()\r\n                model_compiled = deepcopy(model_eager)\r\n                model_compiled(input).sum().backward()\r\n\r\n                if optim_cls is SparseAdam:\r\n                    for param in model_eager.parameters():\r\n                        param.grad = param.grad.to_sparse()\r\n                    for param in model_compiled.parameters():\r\n                        param.grad = param.grad.to_sparse()\r\n\r\n                opt_compiled = optim_cls(\r\n                    model_compiled.parameters(), **deepcopy(kwargs)\r\n                )\r\n                opt_eager = optim_cls(model_eager.parameters(), **deepcopy(kwargs))\r\n                if scheduler_cls:\r\n                    scheduler_compiled = create_scheduler(scheduler_cls, opt_compiled)\r\n                    scheduler_eager = create_scheduler(scheduler_cls, opt_eager)\r\n                    # some schedulers only change after at least an epoch has passed\r\n                    scheduler_compiled.last_epoch = 1\r\n                    scheduler_eager.last_epoch = 1\r\n\r\n                num_steps = 2\r\n                if use_closure:\r\n\r\n                    @torch.compile()\r\n                    def fn():\r\n                        def closure():\r\n                            loss = model_compiled(input).sum()\r\n                            loss.backward()\r\n                            if optim_info.only_supports_sparse_grads:\r\n                                for param in model_compiled.parameters():\r\n                                    param.grad = param.grad.to_sparse()\r\n                            return loss\r\n\r\n                        opt_compiled.step(closure)\r\n                        if scheduler_cls:\r\n                            call_scheduler(scheduler_compiled)\r\n\r\n                    def closure_eager():\r\n                        loss = model_eager(input).sum()\r\n                        loss.backward()\r\n                        if optim_info.only_supports_sparse_grads:\r\n                            for param in model_eager.parameters():\r\n                                param.grad = param.grad.to_sparse()\r\n\r\n                        return loss\r\n\r\n                    for _ in range(num_steps):\r\n                        opt_eager.step(closure_eager)\r\n                        if scheduler_cls:\r\n                            call_scheduler(scheduler_eager)\r\n                else:\r\n\r\n                    @torch.compile()\r\n                    def fn():\r\n                        opt_compiled.step()\r\n                        if scheduler_cls:\r\n                            call_scheduler(scheduler_compiled)\r\n\r\n                    for _ in range(num_steps):\r\n                        opt_eager.step()\r\n                        if scheduler_cls:\r\n                            call_scheduler(scheduler_eager)\r\n\r\n                for _ in range(num_steps):\r\n                    fn()\r\n\r\n                check_optim(\r\n                    self,\r\n                    optim_cls,\r\n                    model_eager.parameters(),\r\n                    model_compiled.parameters(),\r\n                    opt_eager.state,\r\n                    opt_compiled.state,\r\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_tensor_grad_hook1(self):\r\n        def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([2, 4])\r\n\r\n                model[0].weight.register_hook(hook1)\r\n\r\n                result = model(x).sum()\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_tensor_grad_hook2(self):\r\n        def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.grad_fn.register_prehook(hook2)\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_tensor_grad_hook3(self):\r\n        def fn():\r\n            for _ in range(3):\r\n                model = torch.nn.Sequential(\r\n                    torch.nn.Linear(4, 4),\r\n                    torch.nn.ReLU(),\r\n                )\r\n                x = torch.randn([1, 4])\r\n\r\n                result = model(x).sum()\r\n                result.grad_fn.register_hook(hook3)\r\n                result.backward()\r\n                yield model[0].weight.grad\r\n                yield model[0].bias.grad\r\n\r\n        self.check_output_and_recompiles(fn)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_bmm_multiple_dynamic(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, a, b):\r\n                return torch.bmm(a, b)\r\n\r\n        M = 8\r\n        N = 6\r\n        K = 16\r\n        model = Model()\r\n        batch = 1024\r\n        a = torch.randn(batch, M, K, device=self.device)\r\n        b = torch.randn(batch, K, N, device=self.device)\r\n        dim0_a = Dim(\"dim0_a\", min=1, max=2048)\r\n        dynamic_shapes = {\"a\": {0: dim0_a}, \"b\": {0: dim0_a}}\r\n        list_example_inputs = [(a, b)]\r\n        batch = 2048\r\n        list_example_inputs.append(\r\n            (\r\n                torch.randn(batch, M, K, device=self.device),\r\n                torch.randn(batch, K, N, device=self.device),\r\n            ),\r\n        )\r\n        batch = 128\r\n        list_example_inputs.append(\r\n            (\r\n                torch.randn(batch, M, K, device=self.device),\r\n                torch.randn(batch, K, N, device=self.device),\r\n            ),\r\n        )\r\n        self.check_model_with_multiple_inputs(\r\n            model,\r\n            list_example_inputs,\r\n            options={\r\n                \"max_autotune\": True,\r\n                \"max_autotune_gemm_backends\": \"TRITON\",\r\n            },\r\n            dynamic_shapes=dynamic_shapes,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_skip_cpp_codegen(self):\r\n        with config.patch({\"disable_cpp_codegen\": True}):\r\n            inps = torch.ones([20]), torch.rand([20])\r\n\r\n            def f(x, y):\r\n                return x + y + torch.tensor(1)\r\n\r\n            f_opt = torch.compile()(f)\r\n\r\n            _, code = run_and_get_cpp_code(f_opt, inps[0], inps[1])\r\n            FileCheck().check_not(\"void kernel\").run(code)\r\n\r\n            self.assertEqual(\r\n                f(*inps),\r\n                f_opt(*inps),\r\n            )\r\n\r\n            # constant needs to be propagated on fallback\r\n            def f(x):\r\n                return x[torch.tensor(1) :] * 2\r\n\r\n            f_opt = torch.compile()(f)\r\n            _, code = run_and_get_cpp_code(f_opt, inps[0])\r\n            FileCheck().check_not(\"void kernel\").run(code)\r\n            self.assertEqual(f_opt(inps[0]), f(inps[0]))\r\n\r\n            class Model(torch.nn.Module):\r\n                def __init__(\r\n                    self,\r\n                ):\r\n                    super().__init__()\r\n\r\n                def forward(self, v1: torch.Tensor):\r\n                    vx = v1.min(dim=1).values\r\n                    v2 = torch.randn_like(vx)\r\n                    return v2\r\n\r\n            model = Model()\r\n            x = torch.rand(10, 3, 0)\r\n            model_f = torch.compile()(model)\r\n\r\n            self.assertEqual(model(x), model_f(x))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transpose_with_norm(self):\r\n        \"\"\"a sub-module from TIMM gmlp_s16_224\"\"\"\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(\r\n                    in_features=256, out_features=1536, bias=True\r\n                )\r\n                self.act = torch.nn.GELU()\r\n                self.norm = torch.nn.LayerNorm(768)\r\n                self.proj = torch.nn.Linear(196, 196)\r\n                self.fc = torch.nn.Linear(in_features=768, out_features=256, bias=True)\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = self.act(x)\r\n                u, v = x.chunk(2, dim=-1)\r\n                v = self.norm(v)\r\n                v = self.proj(v.transpose(-1, -2))\r\n                y = u * v.transpose(-1, -2)\r\n                return self.fc(y)\r\n\r\n        x = torch.randn(128, 196, 256)\r\n        for simdlen in simd_lengths_to_test():\r\n            with config.patch({\"cpp.simdlen\": simdlen}):\r\n                for eval_mode in [True, False]:\r\n                    torch._dynamo.reset()\r\n                    metrics.reset()\r\n                    m = Model().eval() if eval_mode else Model()\r\n                    self.common(m, (x,))\r\n                    check_metrics_vec_kernel_count(8)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_highp_to_lowp_cse_var_cache_with_store(self):\r\n        # Fix issue: https://github.com/pytorch/pytorch/issues/128263\r\n        input = torch.randn(5, 128, dtype=torch.float32)\r\n        input2 = torch.randint(0, 10, (5, 128), dtype=torch.int8)\r\n        input3 = torch.randn(128, 128, dtype=torch.float32)\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x, x2, x3):\r\n                x2 = x2.to(torch.int32)\r\n                temp = test_operators.realize(x2.to(torch.float16))\r\n                temp2 = temp.to(torch.float32)\r\n                temp2 = temp2 * x\r\n                return torch.mm(temp, x3.to(torch.float16)), temp2\r\n\r\n        metrics.reset()\r\n        m = Model()\r\n        self.common(\r\n            m,\r\n            (input, input2, input3),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_add_layernorm(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.dense = torch.nn.Linear(768, 768)\r\n                self.layernorm = torch.nn.LayerNorm(768, eps=1e-12)\r\n\r\n            def forward(self, context_layer, hidden_states):\r\n                attention_output = self.dense(context_layer)\r\n                hidden_states = attention_output + hidden_states\r\n                layer_output = self.layernorm(hidden_states)\r\n                return layer_output\r\n\r\n        model = Model()\r\n        example_batch = (torch.rand(1, 197, 768), torch.rand(1, 197, 768))\r\n        from torch.testing._internal.common_quantization import (\r\n            _generate_qdq_quantized_model,\r\n        )\r\n\r\n        with torch.no_grad():\r\n            converted_model = _generate_qdq_quantized_model(model, example_batch)\r\n            torch.ao.quantization.move_exported_model_to_eval(converted_model)\r\n            metrics.reset()\r\n            torch.compile(converted_model)(*example_batch)\r\n            check_metrics_vec_kernel_count(3)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_regression(self, device, mechanism):\r\n        def make_spirals(n_samples, noise_std=0.0, rotations=1.0):\r\n            ts = torch.linspace(0, 1, n_samples)\r\n            rs = ts**0.5\r\n            thetas = rs * rotations * 2 * math.pi\r\n            signs = torch.randint(0, 2, (n_samples,)) * 2 - 1\r\n            labels = (signs > 0).to(torch.long)\r\n\r\n            xs = rs * signs * torch.cos(thetas) + torch.randn(n_samples) * noise_std\r\n            ys = rs * signs * torch.sin(thetas) + torch.randn(n_samples) * noise_std\r\n            points = torch.stack([xs, ys], dim=1)\r\n            return points.to(device), labels.to(device)\r\n\r\n        points, labels = make_spirals(100, noise_std=0.05)\r\n\r\n        class MLPClassifier(nn.Module):\r\n            def __init__(self, hidden_dim=32, n_classes=2):\r\n                super().__init__()\r\n                self.hidden_dim = hidden_dim\r\n                self.n_classes = n_classes\r\n\r\n                self.fc1 = nn.Linear(2, self.hidden_dim)\r\n                self.fc2 = nn.Linear(self.hidden_dim, self.n_classes)\r\n\r\n            def forward(self, x):\r\n                x = self.fc1(x)\r\n                x = F.relu(x)\r\n                x = self.fc2(x)\r\n                x = F.log_softmax(x, -1)\r\n                return x\r\n\r\n        loss_fn = nn.NLLLoss()\r\n\r\n        func_model, weights = _get_weights_and_functional_call(\r\n            MLPClassifier().to(device), mechanism\r\n        )\r\n\r\n        def train_step_fn(use_transform, weights, batch, targets, lr=0.2):\r\n            def compute_loss(weights, batch, targets):\r\n                output = func_model(weights, batch)\r\n                loss = loss_fn(output, targets)\r\n                return loss\r\n\r\n            if use_transform:\r\n                grad_weights, loss = grad_and_value(compute_loss)(\r\n                    weights, batch, targets\r\n                )\r\n            else:\r\n                loss = compute_loss(weights, batch, targets)\r\n                flat_weights, spec = tree_flatten(weights)\r\n                flat_grad_weights = torch.autograd.grad(loss, flat_weights)\r\n                grad_weights = tree_unflatten(flat_grad_weights, spec)\r\n\r\n            new_weights = self._update_params(weights, grad_weights, lr, mechanism)\r\n            return (loss, new_weights)\r\n\r\n        def unpack(train_result):\r\n            return train_result[0], train_result[1]\r\n\r\n        def init_fn(num_models):\r\n            models = tuple(MLPClassifier().to(device) for _ in range(num_models))\r\n            if mechanism == \"make_functional\":\r\n                return combine_state_for_ensemble(models)[1]\r\n            else:\r\n                return stack_module_state(models)[0]\r\n\r\n        def slice_weights(batched_weights, index):\r\n            return tree_map(\r\n                lambda weight: weight[index].detach().requires_grad_(), batched_weights\r\n            )\r\n\r\n        batched_weights = init_fn(num_models=2)\r\n        parallel_train_step_fn = vmap(\r\n            partial(train_step_fn, True), in_dims=(0, None, None)\r\n        )\r\n\r\n        result_loss, result_weights = unpack(\r\n            parallel_train_step_fn(batched_weights, points, labels)\r\n        )\r\n\r\n        loss0, weights0 = unpack(\r\n            train_step_fn(False, slice_weights(batched_weights, 0), points, labels)\r\n        )\r\n        loss1, weights1 = unpack(\r\n            train_step_fn(False, slice_weights(batched_weights, 1), points, labels)\r\n        )\r\n        expected_loss = torch.stack([loss0, loss1])\r\n\r\n        weights0, spec0 = tree_flatten(weights0)\r\n        weights1, spec1 = tree_flatten(weights1)\r\n        assert spec0 == spec1\r\n        expected_weights = tuple(\r\n            torch.stack([w0, w1]) for w0, w1 in zip(weights0, weights1)\r\n        )\r\n        expected_weights = tree_unflatten(expected_weights, spec0)\r\n\r\n        self.assertEqual(result_loss, expected_loss)\r\n        self.assertEqual(result_weights, expected_weights)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_flash_attention_dynamic(self):\r\n        class Model(nn.Module):\r\n            def __init__(self, *args, **kwargs) -> None:\r\n                super().__init__(*args, **kwargs)\r\n\r\n                self.q = nn.Linear(1024, 1024)\r\n                self.k = nn.Linear(1024, 1024)\r\n                self.v = nn.Linear(1024, 1024)\r\n\r\n            def forward(self, x):\r\n                batch_size, seq_len, _ = x.size()\r\n\r\n                queries = self.q(x).view(batch_size, seq_len, 8, 128).transpose(2, 1)\r\n                keys = self.k(x).view(batch_size, seq_len, 8, 128).transpose(2, 1)\r\n                values = self.v(x).view(batch_size, seq_len, 8, 128).transpose(2, 1)\r\n\r\n                attn = F.scaled_dot_product_attention(\r\n                    queries,\r\n                    keys,\r\n                    values,\r\n                )\r\n\r\n                return attn\r\n\r\n        cnts = torch._dynamo.testing.CompileCounterWithBackend(\"inductor\")\r\n\r\n        model = Model().cuda().half()\r\n        model = torch.compile(model, backend=cnts, dynamic=True)\r\n\r\n        with torch.backends.cuda.sdp_kernel(\r\n            enable_flash=True,\r\n            enable_math=False,\r\n            enable_mem_efficient=False,\r\n            enable_cudnn=False,\r\n        ):\r\n            input1 = torch.rand(5, 512, 1024, device=\"cuda\", dtype=torch.float16)\r\n            input2 = torch.rand(5, 513, 1024, device=\"cuda\", dtype=torch.float16)\r\n            input3 = torch.rand(5, 514, 1024, device=\"cuda\", dtype=torch.float16)\r\n\r\n            out1 = model(input1)\r\n            out2 = model(input2)\r\n            out3 = model(input3)\r\n\r\n        self.assertEqual(cnts.frame_count, 1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_issue100806(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear1 = torch.nn.Linear(10, 20)\r\n                self.linear2 = torch.nn.Linear(20, 30)\r\n                self.relu = torch.nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.linear1(x)\r\n                x = self.linear2(x)\r\n                x = torch.cat((x, x), dim=1)\r\n                x = x.view(-1, 2, 30)\r\n                x = x[:, 1, :]\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        device = \"cuda\"\r\n        batch_size = 2\r\n        x = torch.randn(batch_size, 10).to(device)\r\n        func = Model().to(device)\r\n\r\n        with torch.no_grad():\r\n            func.train(False)\r\n            jit_func = torch.compile(func)\r\n\r\n            res1 = func(x)\r\n            res2 = jit_func(x)\r\n            self.assertEqual(res1, res2)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _test_pattern_fails_with_unsupported_mask(self):\r\n        if not self.use_static_shapes:\r\n            self.skipTest(\"Causes shape specialization. TODO: investigate\")\r\n\r\n        # https://github.com/pytorch/pytorch/issues/100315\r\n        class Model(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n            ):\r\n                super().__init__()\r\n\r\n            def forward(self, query, key, value, attn_mask) -> torch.Tensor:\r\n                attn_weight = torch.softmax(\r\n                    query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\r\n                    + attn_mask,\r\n                    dim=-1,\r\n                )\r\n                return attn_weight @ value\r\n\r\n        tensor_shape = (2, 4, 4, 4)\r\n\r\n        upsupported_masks = [\r\n            torch.randn((2, 4, 4, 4), device=self.device).to(dtype=torch.int),\r\n            2.0,\r\n        ]\r\n        for atte_mask in upsupported_masks:\r\n            args = [\r\n                torch.randn(tensor_shape, device=self.device),\r\n                torch.randn(tensor_shape, device=self.device),\r\n                torch.randn(tensor_shape, device=self.device),\r\n                atte_mask,\r\n            ]\r\n            model = Model().eval()\r\n            # The training path has an accuracy gap compared with eager mode.\r\n            self._check_common(\r\n                model, args1=args, contains=False, atol=1e-4, has_fuse_pattern=False\r\n            )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_linear_and_cel(self):\r\n        # Use nan for torch.empty\r\n        torch.use_deterministic_algorithms(True)\r\n        torch.utils.deterministic.fill_uninitialized_empty = True\r\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\r\n\r\n        B, T, C, V = 32, 1024, 768, 50257\r\n\r\n        linear = nn.Linear(C, V).bfloat16().to(device=GPU_TYPE)\r\n        ce = torch.nn.CrossEntropyLoss()\r\n\r\n        def f(x, y):\r\n            x.grad = None\r\n            linear.weight.grad = None\r\n            linear.bias.grad = None\r\n\r\n            loss = ce(linear(x), y)\r\n            loss.backward()\r\n            return loss\r\n\r\n        x = torch.randn(B * T, C, requires_grad=True).cuda().bfloat16()\r\n        x.retain_grad()\r\n        y = torch.randint(0, V, (B * T,)).cuda()\r\n\r\n        opt_f = torch.compile(f)\r\n\r\n        expect = (f(x, y), x.grad, linear.weight.grad, linear.bias.grad)\r\n        actual = (opt_f(x, y), x.grad, linear.weight.grad, linear.bias.grad)\r\n        assert same(expect, actual, tol=1e-2), f\"ref:\\n{expect}\\nact:\\n{actual}\"\r\n\r\n        # We may disable inplace_padding via env-var to test perf.\r\n        self.assertEqual(num_inplace_padding(), int(inductor_config.inplace_padding))\r\n\r\n        if DO_PERF_TEST:\r\n            from triton.testing import do_bench\r\n\r\n            ms = do_bench(lambda: opt_f(x, y))\r\n            print(f\"{inductor_config.inplace_padding=} {ms=:.3f}\")",
        "labels": [
            "Deterministic Algorithm Option Not Used"
        ]
    },
    {
        "code": "def test_mutation_reinplaced(self):\r\n            import torch.nn as nn\r\n\r\n            class Model(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n\r\n                def forward(self, input, other, out):\r\n                    input = torch.logical_xor(input=input, other=other, out=out)\r\n                    return input\r\n\r\n            x = torch.rand([1, 2, 1, 4, 9, 7], dtype=torch.float32).cuda()\r\n            y = torch.rand([1, 2, 1, 4, 9, 7], dtype=torch.float32).cuda()\r\n            z = torch.rand([1, 2, 1, 4, 9, 7], dtype=torch.float16).cuda()\r\n\r\n            model = Model().cuda()\r\n            eag = model(x, y, z)\r\n            with capture_stderr() as captured_output:\r\n                opt = torch.compile(model.forward, mode=\"reduce-overhead\")(x, y, z)\r\n\r\n            FileCheck().check(\r\n                \"skipping cudagraphs due to mutated inputs (1 instances). Found from\"\r\n            ).check(\"torch.logical_xor\").run(captured_output[0])\r\n            self.assertEqual(counters[\"inductor\"][\"cudagraph_skips\"], 1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nvidia_deeprecommender(self):\r\n        \"\"\"\r\n        Compared the perf with and without comprehensive padding.\r\n        \"\"\"\r\n        layer_sizes = [197951, 512, 512, 1024, 512, 512, 197951]\r\n        x = torch.randn(4, layer_sizes[0])\r\n\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                mod_list = []\r\n                for i in range(len(layer_sizes) - 1):\r\n                    mod_list.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\r\n                    mod_list.append(nn.SELU())\r\n\r\n                    if i == 2:\r\n                        mod_list.append(nn.Dropout(0.8))\r\n                self.seq = nn.Sequential(*mod_list)\r\n\r\n            def forward(self, x):\r\n                return self.seq(x)\r\n\r\n        m = Model()\r\n        perf_inputs = torch.randn(256, layer_sizes[0])\r\n        self.run_acc_and_perf_test(m, x, perf_inputs)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_triton_kernel_to_post_grad_tracing(self):\r\n        a = torch.randn(10, 20, device=\"cuda\")\r\n        b = torch.randn(20, 30, device=\"cuda\")\r\n        c = torch.randn(10, 30, device=\"cuda\")\r\n        example_inputs = (a, b, c)\r\n\r\n        model = Model()\r\n        ep = torch.export._trace._export(model, example_inputs)\r\n        gm = ep.module()\r\n\r\n        for backend in [\"aot_inductor\", \"inductor\"]:\r\n            try:\r\n                with config.patch(\r\n                    {\r\n                        \"trace.debug_dir\": tempfile.mkdtemp(),\r\n                        \"force_disable_caches\": True,\r\n                    }\r\n                ):\r\n                    with self.assertLogs(\r\n                        logging.getLogger(\"torch._inductor.debug\"),\r\n                        level=logging.WARNING,\r\n                    ) as cm:\r\n                        if backend == \"aot_inductor\":\r\n                            so_path = torch._inductor.aot_compile(gm, example_inputs)\r\n                            optimized = AOTIRunnerUtil.load(\"cuda\", so_path)\r\n                            optimized(*example_inputs)\r\n                        else:\r\n                            compiled = torch.compile(gm, backend=backend)\r\n                            compiled(*example_inputs)\r\n                    self.assertEqual(len(cm.output), 1)\r\n                    m = re.match(r\"WARNING.* debug trace: (.*)\", cm.output[0])\r\n                    self.assertTrue(m)\r\n                    filepath = Path(m.group(1))\r\n                    self._check_provenance_tracing_artifact(filepath)\r\n            finally:\r\n                shutil.rmtree(filepath)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_randn_like_empty(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(\r\n                self,\r\n            ):\r\n                super().__init__()\r\n\r\n            def forward(self, v1: torch.Tensor):\r\n                vx = v1.min(dim=1).values\r\n                v2 = torch.randn_like(vx)\r\n                return v2\r\n\r\n        model = Model()\r\n        x = torch.rand(10, 3, 0)\r\n\r\n        self.common(model, (x,))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_inductor_sequence_nr(self):\r\n            class Model(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.conv1 = torch.nn.Conv2d(\r\n                        in_channels=16,\r\n                        out_channels=16,\r\n                        kernel_size=(1, 1),\r\n                        stride=1,\r\n                        padding=\"same\",\r\n                        bias=True,\r\n                    )\r\n                    self.bn1 = torch.nn.BatchNorm2d(num_features=16)\r\n                    self.relu1 = torch.nn.ReLU()\r\n                    self.loss_fn = torch.nn.L1Loss()\r\n\r\n                def forward(self, x, target):\r\n                    y = x\r\n                    x = self.conv1(x)\r\n                    x = self.bn1(x)\r\n                    x = self.relu1(x)\r\n                    x = x + y\r\n                    x = torch.flatten(x)\r\n                    output = self.loss_fn(x, target)\r\n                    return (output,)\r\n\r\n            def get_triton_codegen(optimized_module, args):\r\n                def run_with_backward():\r\n                    result = optimized_module(*args)\r\n                    result[0].backward()\r\n                    return result\r\n\r\n                res, (fwd_code, bwd_code) = run_and_get_code(run_with_backward)\r\n                return fwd_code, bwd_code\r\n\r\n            x = torch.rand(100, 16, 32, 32, requires_grad=True, device=GPU_TYPE)\r\n            target = torch.rand(1, device=GPU_TYPE)\r\n            args = [x, target]\r\n            model = Model().to(device=GPU_TYPE)\r\n            opt_model = torch.compile(model)\r\n            fwd_code, bwd_code = get_triton_codegen(opt_model, args)\r\n\r\n            bwd_seq_nr_set = set()\r\n            fwd_seq_nr_set = set()\r\n            for idx, code in enumerate([fwd_code, bwd_code]):\r\n                seq_nr_set = bwd_seq_nr_set if idx > 0 else fwd_seq_nr_set\r\n                prefix = \"BWD\" if idx > 0 else \"FWD\"\r\n                for line in code.split(\"\\n\"):\r\n                    if \"seq_nr\" in line:\r\n                        res = re.search(r\"seq_nr:(\\d+)\", line)\r\n                        if res:\r\n                            seq_nr_set.add(int(res.group(1)))\r\n            self.assertTrue(bwd_seq_nr_set.issubset(fwd_seq_nr_set))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_rnn_compile_safe(self):\r\n            device = torch.device(GPU_TYPE)\r\n            model = RNNTest.Model().to(device)\r\n            model = torch.compile(model, backend=\"inductor\")\r\n            x = torch.rand(1024, 20, 16).to(device)\r\n            model(x)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_streams_and_events(self):\r\n        # Test default_stream API by passing device ID as an argument and\r\n        # and check if the stream device index matches with the device ID\r\n        @torch.jit.script\r\n        def test_default_streams_with_device_index_args():\r\n            s0 = torch.cuda.default_stream(0)\r\n            s1 = torch.cuda.default_stream(1)\r\n            return s0.device_index(), s1.device_index()\r\n\r\n        d0, d1 = test_default_streams_with_device_index_args()\r\n\r\n        self.assertEqual(d0, 0)\r\n        self.assertEqual(d1, 1)\r\n\r\n        # This test checks for the default stream ID is set to 0 on the device\r\n        @torch.jit.script\r\n        def test_default_streams():\r\n            s0 = torch.cuda.default_stream(torch.device(\"cuda:0\"))\r\n            s1 = torch.cuda.default_stream(torch.device(\"cuda:1\"))\r\n\r\n            d = torch.device(\"cuda:1\")\r\n\r\n            # Check the current stream id and default id are same\r\n            # on the current device. The current device id by default is 0\r\n            s2 = torch.cuda.current_stream(torch.device(\"cuda:0\"))\r\n            check_s2 = s2.id() == s0.id()\r\n            check_d0 = torch.cuda.current_device() == s2.device_index()\r\n\r\n            # Set the current device to d1 and check if the stream\r\n            # has been set to the default stream on d1\r\n            with torch.cuda.device(d):\r\n                s3 = torch.cuda.current_stream(d)\r\n                check_s3 = s3.id() == s1.id()\r\n                check_d1 = torch.cuda.current_device() == s3.device_index()\r\n\r\n            # Check if the current device was reset to 0\r\n            is_device_d0 = torch.cuda.current_device() == s2.device_index()\r\n\r\n            return (\r\n                s0.device_index(),\r\n                s1.device_index(),\r\n                check_s2,\r\n                check_s3,\r\n                check_d0,\r\n                check_d1,\r\n                is_device_d0,\r\n            )\r\n\r\n        (\r\n            d0,\r\n            d1,\r\n            check_s2,\r\n            check_s3,\r\n            check_d0,\r\n            check_d1,\r\n            is_device_d0,\r\n        ) = test_default_streams()\r\n\r\n        self.assertEqual(d0, 0)\r\n        self.assertEqual(d1, 1)\r\n        self.assertTrue(check_s2)\r\n        self.assertTrue(check_s3)\r\n        self.assertTrue(check_d0)\r\n        self.assertTrue(check_d1)\r\n        self.assertTrue(is_device_d0)\r\n\r\n        # This test checks if the Stream Context manager is a no op\r\n        # when the stream is none for `with torch.cuda.stream`\r\n        @torch.jit.script\r\n        def test_set_none_stream():\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            current_stream = torch.cuda.current_stream(device)\r\n            default_stream = torch.cuda.default_stream(device)\r\n\r\n            # When stream is none, check if this operation is a no-op\r\n            with torch.cuda.stream(None):\r\n                cur_device_index = torch.cuda.current_device()\r\n                is_device_index_same = cur_device_index == device_index\r\n                is_current_stream_same = (\r\n                    torch.cuda.current_stream(device).id() == current_stream.id()\r\n                )\r\n                is_default_stream_same = (\r\n                    torch.cuda.default_stream(device).id() == default_stream.id()\r\n                )\r\n\r\n            # Check if the device index, current stream and default streams have not changed\r\n            are_streams_same = (\r\n                is_device_index_same\r\n                and is_current_stream_same\r\n                and is_default_stream_same\r\n            )\r\n            return are_streams_same\r\n\r\n        self.assertTrue(test_set_none_stream())\r\n\r\n        # This test checks if the Device Context manager is a no op\r\n        # when the device is none for `with torch.cuda.device`\r\n        @torch.jit.script\r\n        def test_set_device_none():\r\n            device_index = torch.cuda.current_device()\r\n            # When device is none, check if this operation is a no-op\r\n            with torch.cuda.device(None):\r\n                # Check if the current device is the same\r\n                is_device_same = torch.cuda.current_device() == device_index\r\n            return is_device_same\r\n\r\n        self.assertTrue(test_set_device_none())\r\n\r\n        # Check if a CUDA JIT stream is created\r\n        # on the current_device\r\n        @torch.jit.script\r\n        def test_simple_stream():\r\n            device_index = torch.cuda.current_device()\r\n            s = torch.cuda.Stream()\r\n            return device_index == s.device_index()\r\n\r\n        self.assertTrue(test_simple_stream(), \"Could not create Stream!\")\r\n\r\n        # Class used to store results for the test: test_get_stream.\r\n        class Result(NamedTuple):\r\n            t1: torch.Tensor\r\n            t2: torch.Tensor\r\n            is_current_and_default_stream_same: bool\r\n            is_default_and_user_stream_not_same: bool\r\n            is_stream_set: bool\r\n            is_stream_reset: bool\r\n            default_stream_query: bool\r\n            default_stream_id: int\r\n            user_stream_id: int\r\n\r\n        # The test aims at checking different stream proporties.\r\n        @torch.jit.script\r\n        def test_get_stream():\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            current_stream = torch.cuda.current_stream(device)\r\n            default_stream = torch.cuda.default_stream(device)\r\n            user_stream = torch.cuda.Stream()\r\n\r\n            # Check if the current and default streams are the same on the device\r\n            is_current_and_default_stream_same = (\r\n                current_stream.id() == default_stream.id()\r\n            )\r\n            # Check if user stream and default stream are not the same on the device\r\n            is_default_and_user_stream_not_same = (\r\n                default_stream.id() != user_stream.id()\r\n            )\r\n\r\n            with torch.cuda.stream(user_stream):\r\n                is_stream_set = (\r\n                    torch.cuda.current_stream(device).id() == user_stream.id()\r\n                )\r\n\r\n            # Check if the stream was reset to current_stream\r\n            is_stream_reset = (\r\n                torch.cuda.current_stream(device).id() == current_stream.id()\r\n            )\r\n\r\n            tensor1 = torch.rand(10000, 10000, device=\"cuda\")\r\n            tensor2 = torch.mm(tensor1, tensor1).to(\"cuda\")\r\n            default_stream.synchronize()\r\n            default_stream_query = default_stream.query()\r\n\r\n            # Capture all the results in the class Result\r\n            res = Result(\r\n                tensor1,\r\n                tensor2,\r\n                is_current_and_default_stream_same,\r\n                is_default_and_user_stream_not_same,\r\n                is_stream_set,\r\n                is_stream_reset,\r\n                default_stream_query,\r\n                default_stream.id(),\r\n                user_stream.id(),\r\n            )\r\n            return res\r\n\r\n        result = test_get_stream()\r\n\r\n        self.assertEqual(torch.matmul(result.t1, result.t1), result.t2)\r\n        self.assertTrue(result.is_current_and_default_stream_same)\r\n        self.assertTrue(result.is_default_and_user_stream_not_same)\r\n        self.assertTrue(result.is_stream_set)\r\n        self.assertTrue(result.is_stream_reset)\r\n        self.assertTrue(result.default_stream_query)\r\n        self.assertEqual(\r\n            result.default_stream_id, 0\r\n        )  # Check if the default stream ID is always 0\r\n        self.assertNotEqual(\r\n            result.user_stream_id, 0\r\n        )  # Check if the user stream is always non zero\r\n\r\n        # Test the stream context manager. This test checks if the stream is switched\r\n        # to the user stream on using the stream context manager.\r\n        @torch.jit.script\r\n        def test_stream_context():\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            current_stream = torch.cuda.current_stream(device)\r\n            user_stream = torch.cuda.Stream()\r\n            A = torch.rand(1000, 1000, device=\"cuda\")\r\n\r\n            with torch.cuda.stream(user_stream):\r\n                check = torch.cuda.current_stream(device).id() == user_stream.id()\r\n                B = torch.mm(A, A).to(\"cuda\")\r\n            # Wait for B to be computed\r\n            user_stream.synchronize()\r\n            # Check if the stream has been reset on the current device\r\n            is_stream_reset = (\r\n                torch.cuda.current_stream(device).id() == current_stream.id()\r\n            )\r\n\r\n            return A, B, check, is_stream_reset\r\n\r\n        A, B, is_stream_set, is_stream_reset = test_stream_context()\r\n        self.assertEqual(torch.matmul(A, A), B)\r\n        self.assertTrue(\r\n            is_stream_set, \"Error: Current stream was not set to user stream!\"\r\n        )\r\n        self.assertTrue(\r\n            is_stream_reset, \"Error: The stream was not restored to previous stream!\"\r\n        )\r\n\r\n        # Test multiple nested streams. Check if the operations are computed as expected on the streams\r\n        # This test has been adapted from the eager mode tests available at test/test_cuda.py\r\n        @torch.jit.script\r\n        def test_multiple_stream():\r\n            prev_device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(prev_device_index))\r\n            prev_current_stream = torch.cuda.current_stream(device)\r\n            d1 = torch.device(\"cuda:0\")\r\n            d2 = torch.device(\"cuda:1\")\r\n            s1 = torch.cuda.Stream(d1, 0)\r\n            s2 = torch.cuda.Stream(d2, 0)\r\n\r\n            A = torch.rand(1000, 1000, device=\"cuda\")\r\n            B = torch.rand(1000, 1000, device=\"cuda\")\r\n            with torch.cuda.stream(s1):\r\n                C = torch.mm(A, A).to(\"cuda\")\r\n                # Check if the stream and device have been set to s1\r\n                is_stream_s1 = torch.cuda.current_stream(d1).id() == s1.id()\r\n                is_device_s1 = torch.cuda.current_device() == s1.device_index()\r\n                with torch.cuda.stream(s2):\r\n                    # Check if the stream and device have been set to s2\r\n                    is_stream_s2 = torch.cuda.current_stream(d2).id() == s2.id()\r\n                    is_device_s2 = torch.cuda.current_device() == s2.device_index()\r\n                    D = torch.mm(B, B).to(\"cuda\")\r\n                # Check if the stream and device have been set to s1\r\n                is_stream_s1_after = torch.cuda.current_stream(d1).id() == s1.id()\r\n                is_device_s1_after = torch.cuda.current_device() == s1.device_index()\r\n                # Wait for D to be computed\r\n                s2.synchronize()\r\n            # Wait for C to be computed on S1\r\n            s1.synchronize()\r\n\r\n            # Check if the stream and device has been restored to previous stream and device\r\n            is_device_current = torch.cuda.current_device() == prev_device_index\r\n            is_stream_current = (\r\n                torch.cuda.current_stream(device).id() == prev_current_stream.id()\r\n            )\r\n\r\n            check_stream = (\r\n                is_stream_s1\r\n                and is_stream_s2\r\n                and is_stream_s1_after\r\n                and is_stream_current\r\n            )\r\n            check_device = (\r\n                is_device_s1\r\n                and is_device_s2\r\n                and is_device_s1_after\r\n                and is_device_current\r\n            )\r\n            return A, B, C, D, check_stream, check_device\r\n\r\n        A, B, C, D, check_stream, check_device = test_multiple_stream()\r\n\r\n        self.assertEqual(torch.matmul(A, A), C)\r\n        self.assertEqual(torch.matmul(B, B), D)\r\n        self.assertTrue(check_stream)\r\n        self.assertTrue(check_device)\r\n\r\n        # Test multiple streams waiting on each other for the operations to be completed.\r\n        @torch.jit.script\r\n        def test_data_dependency_between_streams():\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            prev_current_stream = torch.cuda.current_stream(device)\r\n            d = torch.device(\"cuda:0\")\r\n            s1 = torch.cuda.Stream(d, 0)\r\n            s2 = torch.cuda.Stream(d, 0)\r\n            event = torch.cuda.Event(False, False, False)\r\n\r\n            A = torch.rand(1000, 1000, device=\"cuda\")\r\n            with torch.cuda.stream(s1):\r\n                is_stream_s1 = torch.cuda.current_stream(device).id() == s1.id()\r\n                B = torch.mm(A, A).to(\"cuda\")\r\n            s1.record_event(event)\r\n            # Check if the current_stream is reset\r\n            is_current_stream_1 = (\r\n                torch.cuda.current_stream(device).id() == prev_current_stream.id()\r\n            )\r\n            # Wait for ops on s1 to be computed\r\n            s2.wait_event(event)\r\n            with torch.cuda.stream(s2):\r\n                is_stream_s2 = torch.cuda.current_stream(device).id() == s2.id()\r\n                C = torch.mm(B, B).to(\"cuda\")\r\n            # Wait for C to be computed\r\n            s2.synchronize()\r\n            # Check if the current_stream is reset\r\n            is_current_stream_2 = (\r\n                torch.cuda.current_stream(device).id() == prev_current_stream.id()\r\n            )\r\n\r\n            check_stream = (\r\n                is_current_stream_1\r\n                and is_current_stream_2\r\n                and is_stream_s1\r\n                and is_stream_s2\r\n            )\r\n            return A, B, C, check_stream\r\n\r\n        A, B, C, check_stream = test_data_dependency_between_streams()\r\n        self.assertEqual(torch.matmul(A, A), B)\r\n        self.assertEqual(torch.matmul(B, B), C)\r\n        self.assertTrue(check_stream)\r\n\r\n        # Test a simple CUDA event. Test if the CUDA event was created successfully\r\n        @torch.jit.script\r\n        def test_simple_event():\r\n            e = torch.cuda.Event(True, False, False)\r\n            return e is not None\r\n\r\n        self.assertTrue(test_simple_event(), \"Could not create CUDA Event!\")\r\n\r\n        # Record the CUDA event for operation torch.mm on the current stream\r\n        # and then test if the elapsed time is greater than 0. This test is also\r\n        # an adaption from eager mdoe CUDA tests available at test/test_cuda.py\r\n        @torch.jit.script\r\n        def test_event():\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            stream = torch.cuda.current_stream(device)\r\n            event = torch.cuda.Event(True, False, False)\r\n            is_true_event_query = event.query()\r\n            start_event = torch.cuda.Event(True, False, False)\r\n            stream.record_event(start_event)\r\n            tensor1 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n            tensor2 = torch.mm(tensor1, tensor1).to(\"cuda\")\r\n            stream.record_event(event)\r\n            event.synchronize()\r\n            is_again_true_event_query = event.query()\r\n\r\n            if not (is_true_event_query and is_again_true_event_query):\r\n                return -1.0\r\n            return start_event.elapsed_time(event)\r\n\r\n        self.assertGreater(test_event(), 0)\r\n\r\n        # Check for stream synchronization , when a large tensor multiplication is\r\n        # computed on the stream. The stream.query should be true once the synchroniztion is done\r\n        @torch.jit.script\r\n        def test_stream_synchronize() -> float:\r\n            device_index = torch.cuda.current_device()\r\n            s = torch.cuda.Stream()\r\n            e_tik = torch.cuda.Event(True, False, False)\r\n            e_tok = torch.cuda.Event(True, False, False)\r\n\r\n            e_tik.record(s)\r\n            tensor1 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n            with torch.cuda.stream(s):\r\n                tensor2 = torch.mm(tensor1, tensor1).to(\"cuda\")\r\n            s.synchronize()\r\n            e_tok.record(s)\r\n            e_tok.synchronize()\r\n\r\n            if not s.query():\r\n                return -1.0\r\n\r\n            # not necessary to check e_tik and e_tok, as elapsed_time would throw\r\n            # exception if otherwise.\r\n            return e_tik.elapsed_time(e_tok)\r\n\r\n        self.assertGreater(test_stream_synchronize(), 0)\r\n\r\n        # Test event synchronization for the event that records a stream doing\r\n        # a large tensor multiplication. Check if the elapsed time is greater than 0\r\n        # and the stream.query evaluates to true.\r\n        @torch.jit.script\r\n        def test_event_synchronize() -> float:\r\n            s = torch.cuda.Stream()\r\n            e_tik = torch.cuda.Event(True, False, False)\r\n            e_tok = torch.cuda.Event(True, False, False)\r\n\r\n            e_tik.record(s)\r\n            tensor1 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n            with torch.cuda.stream(s):\r\n                tensor = torch.mm(tensor1, tensor1).to(\"cuda\")\r\n            s.record_event(e_tok)\r\n            e_tok.synchronize()\r\n            s.synchronize()\r\n\r\n            if not s.query():\r\n                return -1.0\r\n\r\n            # not necessary to check e_tik and e_tok, as elapsed_time would throw\r\n            # exception if otherwise.\r\n            return e_tik.elapsed_time(e_tok)\r\n\r\n        self.assertGreater(test_event_synchronize(), 0)\r\n\r\n        # Test for event wait. Check if event waits for the all the operations on\r\n        # the stream to be done. Check for synchronizations and query on the streams\r\n        # and events. This test is adapted from eager mode tests for CUDA. Please refer\r\n        # test/test_cuda.py\r\n        @torch.jit.script\r\n        def test_event_wait() -> float:\r\n            device_index = torch.cuda.current_device()\r\n            device = torch.device(\"cuda:\" + str(device_index))\r\n            s0 = torch.cuda.current_stream(device)\r\n            s1 = torch.cuda.Stream()\r\n            e_tik = torch.cuda.Event(True, True, False)\r\n            e_tok = torch.cuda.Event(True, True, False)\r\n\r\n            e_tik.record(s0)\r\n            tensor1 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n            with torch.cuda.stream(s0):\r\n                tensor2 = torch.mm(tensor1, tensor1).cuda()\r\n            e_sync = torch.cuda.Event(True, False, False)\r\n            e_sync.record(torch.cuda.current_stream(device))\r\n            e_sync.wait(s1)\r\n            with torch.cuda.stream(s1):\r\n                tensor3 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n                tensor4 = torch.mm(tensor3, tensor3).cuda()\r\n            s1.synchronize()\r\n            e_tok.record(torch.cuda.current_stream(device))\r\n            e_tok.synchronize()\r\n            s0.synchronize()\r\n\r\n            if not s0.query() or not s1.query() or not e_sync.query():\r\n                return -1.0\r\n\r\n            # not necessary to check e_tik and e_tok, as elapsed_time would throw\r\n            # exception if otherwise.\r\n            return e_tik.elapsed_time(e_tok)\r\n\r\n        self.assertGreater(test_event_wait(), 0)\r\n\r\n        # Test for stream wait_event. Checks if the stream waits on the event\r\n        @torch.jit.script\r\n        def test_wait_event():\r\n            d1 = torch.device(\"cuda:1\")\r\n\r\n            with torch.cuda.device(d1):\r\n                s0 = torch.cuda.current_stream(d1)\r\n                tensor1 = torch.rand(1000000000, 1000000000, device=\"cuda\")\r\n                tensor2 = torch.mm(tensor1, tensor1).to(\"cuda\")\r\n                e0 = torch.cuda.Event(False, False, False)\r\n                s0.record_event(e0)\r\n\r\n            s1 = torch.cuda.current_stream(torch.device(\"cuda:0\"))\r\n            s1.wait_event(e0)\r\n            s1.synchronize()\r\n\r\n            return e0.query() and s0.query() and s1.query()\r\n\r\n        self.assertTrue(test_wait_event())\r\n\r\n        # Test if a scripted module with cuda streams can be saved, loaded and executed\r\n        def test_save_load(self):\r\n            class Model(torch.nn.Module):\r\n                def forward(self):\r\n                    s = torch.cuda.Stream()\r\n                    a = torch.rand(3, 4, device=\"cuda\")\r\n                    b = torch.rand(3, 4, device=\"cuda\")\r\n\r\n                    with torch.cuda.stream(s):\r\n                        is_stream_s = torch.cuda.current_stream(s.device).id() == s.id()\r\n                        c = torch.cat((a, b), 0).cuda()\r\n                    s.synchronize()\r\n                    return is_stream_s, a, b, c\r\n\r\n            model = Model()\r\n\r\n            # Script the model and save\r\n            script_model = torch.jit.script(model)\r\n            is_stream_s, a, b, c = script_model()\r\n            # Verify if the output is correct\r\n            self.assertTrue(is_stream_s)\r\n            self.assertEqual(torch.cat((a, b), 0), c)\r\n\r\n            # Save and load scripted model\r\n            load_model = self.getExportImportCopy(script_model)\r\n            is_stream_s, a_load, b_load, c_load = load_model()\r\n            self.assertTrue(is_stream_s)\r\n            self.assertEqual(torch.cat((a_load, b_load), 0), c_load)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_save_load(self):\r\n            class Model(torch.nn.Module):\r\n                def forward(self):\r\n                    s = torch.cuda.Stream()\r\n                    a = torch.rand(3, 4, device=\"cuda\")\r\n                    b = torch.rand(3, 4, device=\"cuda\")\r\n\r\n                    with torch.cuda.stream(s):\r\n                        is_stream_s = torch.cuda.current_stream(s.device).id() == s.id()\r\n                        c = torch.cat((a, b), 0).cuda()\r\n                    s.synchronize()\r\n                    return is_stream_s, a, b, c\r\n\r\n            model = Model()\r\n\r\n            # Script the model and save\r\n            script_model = torch.jit.script(model)\r\n            is_stream_s, a, b, c = script_model()\r\n            # Verify if the output is correct\r\n            self.assertTrue(is_stream_s)\r\n            self.assertEqual(torch.cat((a, b), 0), c)\r\n\r\n            # Save and load scripted model\r\n            load_model = self.getExportImportCopy(script_model)\r\n            is_stream_s, a_load, b_load, c_load = load_model()\r\n            self.assertTrue(is_stream_s)\r\n            self.assertEqual(torch.cat((a_load, b_load), 0), c_load)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_lazy_init_with_view(self):\r\n        def f(device, reset_storage=False):\r\n            torch.manual_seed(2023)\r\n\r\n            if device == \"lazy\":\r\n                metrics.reset()\r\n\r\n            class Model(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.fc1 = torch.nn.Linear(4, 2, bias=False)\r\n\r\n                def forward(self, x):\r\n                    return x @ self.fc1.weight.transpose(0, 1)\r\n\r\n            with torch.device(device):\r\n                model = Model()\r\n\r\n                if device == \"lazy\":\r\n                    if reset_storage:\r\n                        torch._C._unsafe_reset_storage(model.fc1.weight)\r\n\r\n                    torch._lazy.mark_step()\r\n\r\n                    sync_tensors = metrics.counter_value(\"SyncedTensorsWithIR\")\r\n                    if reset_storage:\r\n                        assert sync_tensors == 1\r\n                    else:\r\n                        # There is an extra tensor being unnecessarily synced if\r\n                        # the functional storage is not reset.\r\n                        assert sync_tensors == 2\r\n\r\n                x = torch.ones(4)\r\n                out = model(x)\r\n\r\n                if device == \"lazy\":\r\n                    torch._lazy.mark_step()\r\n\r\n                return out\r\n\r\n        cpu_out = f(\"cpu\")\r\n        lazy_out_1 = f(\"lazy\", reset_storage=False)\r\n        lazy_out_2 = f(\"lazy\", reset_storage=True)\r\n\r\n        self.assertEqual(cpu_out, lazy_out_1.to(\"cpu\"))\r\n        self.assertEqual(cpu_out, lazy_out_2.to(\"cpu\"))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def f(device, reset_storage=False):\r\n            torch.manual_seed(2023)\r\n\r\n            if device == \"lazy\":\r\n                metrics.reset()\r\n\r\n            class Model(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.fc1 = torch.nn.Linear(4, 2, bias=False)\r\n\r\n                def forward(self, x):\r\n                    return x @ self.fc1.weight.transpose(0, 1)\r\n\r\n            with torch.device(device):\r\n                model = Model()\r\n\r\n                if device == \"lazy\":\r\n                    if reset_storage:\r\n                        torch._C._unsafe_reset_storage(model.fc1.weight)\r\n\r\n                    torch._lazy.mark_step()\r\n\r\n                    sync_tensors = metrics.counter_value(\"SyncedTensorsWithIR\")\r\n                    if reset_storage:\r\n                        assert sync_tensors == 1\r\n                    else:\r\n                        # There is an extra tensor being unnecessarily synced if\r\n                        # the functional storage is not reset.\r\n                        assert sync_tensors == 2\r\n\r\n                x = torch.ones(4)\r\n                out = model(x)\r\n\r\n                if device == \"lazy\":\r\n                    torch._lazy.mark_step()\r\n\r\n                return out",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bundled_input_with_dynamic_type(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: dict[int, torch.Tensor],\r\n                y: dict[int, torch.Tensor],\r\n                z: dict[int, torch.Tensor],\r\n            ):\r\n                return x\r\n\r\n        model = Model()\r\n        script_module = torch.jit.script(model)\r\n\r\n        sample_input = {\r\n            script_module.forward: [\r\n                (\r\n                    {0: torch.ones(1)},\r\n                    {1: torch.ones(1)},\r\n                    {2: torch.ones(1)},\r\n                )\r\n            ]\r\n        }\r\n\r\n        bundled_model = torch.utils.bundled_inputs.bundle_inputs(\r\n            script_module, sample_input\r\n        )\r\n\r\n        buf = bundled_model._save_to_buffer_for_lite_interpreter()\r\n        mobile_module = _load_for_lite_interpreter(io.BytesIO(buf))\r\n\r\n        i = mobile_module.run_method(\"get_all_bundled_inputs\")\r\n\r\n        self.assertEqual(\r\n            i[0],\r\n            (\r\n                {0: torch.ones(1)},\r\n                {1: torch.ones(1)},\r\n                {2: torch.ones(1)},\r\n            ),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fake_tensor_mode_simple(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = torch.nn.Linear(2, 2)\r\n\r\n            def forward(self, x):\r\n                out = self.linear(x)\r\n                return out\r\n\r\n        with torch.onnx.enable_fake_mode() as fake_context:\r\n            x = torch.rand(5, 2, 2)\r\n            model = Model()\r\n            export_options = ExportOptions(fake_context=fake_context)\r\n            onnx_program = torch.onnx.dynamo_export(\r\n                model, x, export_options=export_options\r\n            )\r\n\r\n        assert (\r\n            onnx_program is not None\r\n        ), \"ONNXProgram must be created on successful export\"\r\n\r\n        onnx_program.apply_weights(Model().state_dict())\r\n\r\n        assert (\r\n            onnx_program.model_proto is not None\r\n        ), \"A model protobuf must be created on a successful export\"\r\n        onnx.checker.check_model(onnx_program.model_proto, full_check=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_aten_div_no_opmath_type_promotion(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, input):\r\n                return input / 2\r\n\r\n        model = Model()\r\n        input = torch.randn(3, 5, requires_grad=True, dtype=torch.float16)\r\n\r\n        model_proto = torch.onnx.dynamo_export(model, input).model_proto\r\n        model_proto = onnx.inliner.inline_local_functions(model_proto)\r\n        div_node = next(\r\n            node for node in model_proto.graph.node if node.op_type == \"Div\"\r\n        )\r\n        # The input of Div node should be the input of the model,\r\n        # with no Cast node in between.\r\n        self.assertEqual(div_node.input[0], model_proto.graph.input[0].name)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_rnn_trace_override(self):\r\n        from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n\r\n        num_layers = 3\r\n        T, B, C = 11, 5, 7\r\n\r\n        class RNNTraceWrapper(torch.nn.Module):\r\n            def __init__(self, cell_type):\r\n                super().__init__()\r\n                if cell_type == \"RNN\":\r\n                    self.rnn = torch.nn.RNN(\r\n                        input_size=C, hidden_size=C, num_layers=num_layers\r\n                    )\r\n                elif cell_type == \"LSTM\":\r\n                    self.rnn = torch.nn.LSTM(\r\n                        input_size=C, hidden_size=C, num_layers=num_layers\r\n                    )\r\n                elif cell_type == \"GRU\":\r\n                    self.rnn = torch.nn.GRU(\r\n                        input_size=C, hidden_size=C, num_layers=num_layers\r\n                    )\r\n\r\n            def forward(self, x, seq_lens):\r\n                x = pack_padded_sequence(x, seq_lens)\r\n                x, _ = self.rnn(x)\r\n                x, _ = pad_packed_sequence(x)\r\n                return x\r\n\r\n        for cell_type in [\"RNN\", \"LSTM\", \"GRU\"]:\r\n            x = torch.ones(T, B, C, requires_grad=True)\r\n            seq_lens = torch.from_numpy(np.array([11, 3, 2, 2, 1], dtype=np.int32))\r\n\r\n            m = RNNTraceWrapper(cell_type)\r\n            m_traced = torch.jit.trace(\r\n                m,\r\n                (\r\n                    x,\r\n                    seq_lens,\r\n                ),\r\n            )\r\n\r\n            y = m(x, seq_lens)\r\n            loss = torch.sum(y)\r\n            loss.backward()\r\n            grad = x.grad.clone()\r\n            x.grad.zero_()\r\n\r\n            y_traced = m_traced(x, seq_lens)\r\n            loss_traced = torch.sum(y_traced)\r\n            loss_traced.backward()\r\n            grad_traced = x.grad.clone()\r\n\r\n            self.assertEqual(y_traced, y)\r\n            self.assertEqual(grad, grad_traced)\r\n\r\n            f = io.BytesIO()\r\n            torch.onnx.export(m, (x, seq_lens), f)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_onnx_proto_checker(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return 2 * x\r\n\r\n        x = torch.randn(1, 2, 3, requires_grad=True)\r\n        f = io.BytesIO()\r\n        torch.onnx.export(Model(), (x,), f)\r\n        model = onnx.load(f)\r\n        model.ir_version = 0\r\n\r\n        def check_proto():\r\n            torch._C._check_onnx_proto(model.SerializeToString())\r\n\r\n        self.assertRaises(RuntimeError, check_proto)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_scatter_reduce(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x, index, input):\r\n                y_max = input.scatter_reduce(0, index, x, reduce=\"amax\")\r\n                y_sum = input.scatter_reduce(0, index, x, reduce=\"sum\")\r\n                y_min = input.scatter_reduce(0, index, x, reduce=\"amin\")\r\n                y_mul = input.scatter_reduce(0, index, x, reduce=\"prod\")\r\n                return y_max, y_sum, y_min, y_mul\r\n\r\n        model = Model()\r\n        model.eval()\r\n\r\n        src = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\r\n        index = torch.tensor([0, 1, 0, 1, 2, 1])\r\n        input = torch.tensor([1.0, 2.0, 3.0, 8.0])\r\n\r\n        self.run_test(model, (src, index, input))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_scatter_reduce_self_rank_zero(self):\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n\r\n            def forward(self, x, index, input):\r\n                y_max = input.scatter_reduce(0, index, x, reduce=\"amax\")\r\n                y_sum = input.scatter_reduce(0, index, x, reduce=\"sum\")\r\n                y_min = input.scatter_reduce(0, index, x, reduce=\"amin\")\r\n                y_mul = input.scatter_reduce(0, index, x, reduce=\"prod\")\r\n                return y_max, y_sum, y_min, y_mul\r\n\r\n        model = Model()\r\n        model.eval()\r\n\r\n        empty_tensor = torch.tensor([])\r\n        empty_idx = torch.tensor([], dtype=torch.int64)\r\n\r\n        self.run_test(model, (empty_tensor, empty_idx, empty_tensor))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_amax_amin(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return torch.amax(x, dim=0, keepdim=True), torch.amin(\r\n                    x, dim=[0, 1], keepdim=False\r\n                )\r\n\r\n        model = Model()\r\n        x = torch.randn(4, 4)\r\n        self.run_test(model, x)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_aminmax(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return torch.aminmax(x, dim=1, keepdim=True), torch.aminmax(\r\n                    x, keepdim=False\r\n                )\r\n\r\n        model = Model()\r\n        x = torch.randn(3, 4)\r\n        self.run_test(model, x)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def setUp(self):\r\n        super().setUp()\r\n        self.opset_version = _constants.ONNX_DEFAULT_OPSET\r\n\r\n        def incorrect_relu_symbolic_function(g, self):\r\n            return g.op(\"Add\", self, g.op(\"Constant\", value_t=torch.tensor(1.0)))\r\n\r\n        torch.onnx.register_custom_op_symbolic(\r\n            \"aten::relu\",\r\n            incorrect_relu_symbolic_function,\r\n            opset_version=self.opset_version,\r\n        )\r\n\r\n        class Model(torch.nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.layers = torch.nn.Sequential(\r\n                    torch.nn.Linear(3, 4),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(4, 5),\r\n                    torch.nn.ReLU(),\r\n                    torch.nn.Linear(5, 6),\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.layers(x)\r\n\r\n        self.graph_info = verification.find_mismatch(\r\n            Model(),\r\n            (torch.randn(2, 3),),\r\n            opset_version=self.opset_version,\r\n            options=verification.VerificationOptions(backend=self.onnx_backend),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_translation_table_supports_overloading_ops(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return torch.ops.aten.logical_and.default(x, y)\r\n\r\n        def custom_add_bool(self: BOOL, other: BOOL) -> BOOL:\r\n            # Replace add with sub\r\n            return op.Sub(self, other)\r\n\r\n        def custom_add(self: FLOAT, other: FLOAT) -> FLOAT:\r\n            # Replace add with mul\r\n            return op.Mul(self, other)\r\n\r\n        custom_translation_table = {\r\n            torch.ops.aten.logical_and.default: [custom_add, custom_add_bool],\r\n        }\r\n\r\n        onnx_program = torch.onnx.export(\r\n            Model(),\r\n            (torch.tensor(1, dtype=torch.bool), torch.tensor(1, dtype=torch.bool)),\r\n            custom_translation_table=custom_translation_table,\r\n            dynamo=True,\r\n        )\r\n        all_nodes = [n.op_type for n in onnx_program.model.graph]\r\n        # The dispatcher should pick the correct overload based on the input types\r\n        self.assertIn(\"Sub\", all_nodes)\r\n        self.assertNotIn(\"Add\", all_nodes)\r\n        self.assertNotIn(\"Mul\", all_nodes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_translation_table_supports_custom_op_as_target(self):\r\n        # Define the custom op and use it in the model\r\n        @torch.library.custom_op(\"custom::add\", mutates_args=())\r\n        def custom_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\r\n            return a + b\r\n\r\n        @custom_add.register_fake\r\n        def _(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\r\n            return torch.empty_like(a) + torch.empty_like(b)\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return custom_add(x, y)\r\n\r\n        def onnx_add(self: FLOAT, other: FLOAT) -> FLOAT:\r\n            # Replace add with Sub\r\n            return op.Sub(self, other)\r\n\r\n        custom_translation_table = {\r\n            torch.ops.custom.add.default: onnx_add,\r\n        }\r\n\r\n        onnx_program = torch.onnx.export(\r\n            Model(),\r\n            (torch.tensor(1, dtype=torch.bool), torch.tensor(1, dtype=torch.bool)),\r\n            custom_translation_table=custom_translation_table,\r\n            dynamo=True,\r\n        )\r\n        all_nodes = [n.op_type for n in onnx_program.model.graph]\r\n        self.assertIn(\"Sub\", all_nodes)\r\n        self.assertNotIn(\"Add\", all_nodes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_translation_table_overrides_ops(self):\r\n        from onnxscript import opset18 as op\r\n\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y):\r\n                return x + y\r\n\r\n        def custom_add(self, other):\r\n            # Replace add with sub\r\n            return op.Sub(self, other)\r\n\r\n        custom_translation_table = {torch.ops.aten.add.Tensor: custom_add}\r\n\r\n        onnx_program = torch.onnx.export(\r\n            Model(),\r\n            (torch.randn(2, 2), torch.randn(2, 2)),\r\n            custom_translation_table=custom_translation_table,\r\n            dynamo=True,\r\n        )\r\n        all_nodes = [n.op_type for n in onnx_program.model.graph]\r\n        self.assertIn(\"Sub\", all_nodes)\r\n        self.assertNotIn(\"Add\", all_nodes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_jit_isinstance(self, strategy_cls):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, a, b):\r\n                if torch.jit.isinstance(a, torch.Tensor):\r\n                    return a.cos()\r\n                return b.sin()\r\n\r\n        model = Model()\r\n        a = torch.tensor(0.0)\r\n        b = torch.tensor(1.0)\r\n\r\n        result = strategy_cls()(model, (a, b), kwargs=None, dynamic_shapes=None)\r\n        ep = result.exported_program\r\n        assert ep is not None\r\n        torch.testing.assert_close(ep.module()(a, b), model(a, b))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_insert_contiguous_between_transpose_and_view(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, query, key, value):\r\n                res = torch.nn.functional.scaled_dot_product_attention(\r\n                    query, key, value\r\n                )\r\n                rest = res.transpose(0, 1)\r\n                return rest.view(8, 32, 128 * 64)\r\n\r\n        model = Model()\r\n\r\n        query = torch.rand(32, 8, 128, 64, dtype=torch.float16)\r\n        key = torch.rand(32, 8, 128, 64, dtype=torch.float16)\r\n        value = torch.rand(32, 8, 128, 64, dtype=torch.float16)\r\n\r\n        ep = torch.export.export(model, (query, key, value), strict=False)\r\n        self.assertNotIn(\"call_method\", str(ep.graph))\r\n\r\n        onnx_program = torch.onnx.export(\r\n            model, (query, key, value), dynamo=True, fallback=False\r\n        )\r\n        onnx_testing.assert_onnx_program(onnx_program, atol=1e-3, rtol=1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pow_does_not_trigger_type_promotion(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                return x**2.0\r\n\r\n        x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float16)\r\n\r\n        onnx_program = torch.onnx.export(Model(), (x,), dynamo=True)\r\n        onnx_testing.assert_onnx_program(onnx_program)\r\n        self.assertNotIn(\"Cast\", [node.op_type for node in onnx_program.model.graph])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _multistep_backprop_diff_hyperparams_fn(\r\n    params: Tensor,\r\n    grad: Tensor,\r\n    opt_differentiable_state: dict[str, Any],\r\n    opt_class: type[Optimizer],\r\n    kwargs: dict[str, Any],\r\n    *ignored: Any,\r\n) -> tuple[Tensor, ...]:\r\n    assert (\r\n        kwargs[\"differentiable\"] is True\r\n    ), \"Only call this test function when differentiable=True\"\r\n\r\n    params = params.clone()\r\n    params.grad = grad\r\n\r\n    opt_differentiable_state = {\r\n        k: v.clone() if isinstance(v, torch.Tensor) else v\r\n        for k, v in opt_differentiable_state.items()\r\n    }\r\n\r\n    # This copy is necessary so the update on line 78 doesn't overwrite the original kwargs values\r\n    kwargs = kwargs.copy()\r\n\r\n    # Have to pass in beta1 and beta2 separately\r\n    # so they're passed in as Tensors (not a tuple) and recognized by gradcheck\r\n    if \"beta1\" in kwargs or \"beta2\" in kwargs:\r\n        # Prevent just one beta kwarg from being passed in\r\n        assert (\r\n            \"beta1\" in kwargs and \"beta2\" in kwargs\r\n        ), \"Both betas should be defined in kwargs\"\r\n        kwargs.update({\"betas\": (kwargs.pop(\"beta1\"), kwargs.pop(\"beta2\"))})\r\n\r\n    kwargs.update(\r\n        {k: v.clone() if isinstance(v, torch.Tensor) else v for k, v in kwargs.items()}\r\n    )\r\n    differentiable_kwargs = [\r\n        v for v in kwargs.values() if isinstance(v, torch.Tensor) and v.requires_grad\r\n    ] + (list(kwargs[\"betas\"]) if \"betas\" in kwargs else [])\r\n\r\n    criterion = nn.MSELoss()\r\n\r\n    optimizer = opt_class([params], **kwargs)\r\n    optimizer.state[params].update(opt_differentiable_state)\r\n\r\n    # Simple x, y pair\r\n    x = torch.tensor([1.0], dtype=torch.float64)\r\n    y = torch.tensor([2.0], dtype=torch.float64)\r\n\r\n    for _ in range(2):\r\n        loss = criterion(x * torch.sum(params), y)\r\n        loss.backward(\r\n            inputs=(params,),\r\n            create_graph=True,\r\n        )\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n    meta_loss = loss\r\n    meta_loss.backward(inputs=(*differentiable_kwargs,), create_graph=True)\r\n\r\n    # Extra check to make sure the test properly computed a gradient for all kwargs\r\n    for kwarg in differentiable_kwargs:\r\n        assert kwarg.grad is not None\r\n\r\n    return (\r\n        (meta_loss,)\r\n        + tuple(\r\n            v\r\n            for v in optimizer.state[params].values()\r\n            if isinstance(v, torch.Tensor) and v.requires_grad\r\n        )\r\n        + tuple(differentiable_kwargs)\r\n    )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_profiler_grad_not_set_to_none_pattern(self):\r\n        x = torch.ones((100, 100))\r\n        model = nn.Sequential(\r\n            nn.Linear(100, 100),\r\n            nn.ReLU(),\r\n            nn.Linear(100, 10),\r\n        )\r\n        optimizer = torch.optim.Adam(model.parameters())\r\n        cases = (\r\n            (0, lambda: optimizer.zero_grad()),\r\n            (0, lambda: model.zero_grad()),\r\n            (1, lambda: optimizer.zero_grad(set_to_none=False)),\r\n            (1, lambda: model.zero_grad(set_to_none=False)),\r\n        )\r\n        num_matched = []\r\n        for _, fn in cases:\r\n            with profile(with_stack=True) as prof:\r\n                y_hat = model(x)\r\n                loss = torch.nn.functional.cross_entropy(\r\n                    y_hat, torch.randint(0, 10, (100,))\r\n                )\r\n                loss.backward()\r\n                optimizer.step()\r\n                fn()\r\n            pattern = GradNotSetToNonePattern(prof)\r\n            num_matched.append(len(pattern.matched_events()))\r\n        self.assertEqual(num_matched, [i for i, _ in cases])",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_module_hierarchy(self):\r\n        class A(nn.Module):\r\n            def my_new_method(self, x):\r\n                return x * 3\r\n\r\n            def forward_impl_(self, x, y):\r\n                return self.my_new_method(x) + y\r\n\r\n            def forward(self, x, y):\r\n                y = y - 2\r\n                return self.forward_impl_(x, y)\r\n\r\n        class B(nn.Module):\r\n            def forward(self, x):\r\n                return x + 2\r\n\r\n        class C(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.A0 = A()\r\n                self.B0 = B()\r\n\r\n            def call_b(self, x):\r\n                return self.B0.forward(x)\r\n\r\n            def forward(self, x, y):\r\n                return self.A0.forward(x, y) + self.call_b(x)\r\n\r\n        model = C()\r\n        model = torch.jit.script(model)\r\n        input_a = torch.rand(128, 128)\r\n        input_b = torch.rand(128, 128)\r\n        op_to_module_hierarchy = {}\r\n        op_to_module_hierarchy[\"aten::sub\"] = [\"TOP(C)::forward.A0(A)::forward.\"]\r\n        op_to_module_hierarchy[\"aten::mul\"] = [\r\n            \"TOP(C)::forward.A0(A)::forward.SELF(A)::forward_impl_.SELF(A)::my_new_method.\"\r\n        ]\r\n        op_to_module_hierarchy[\"aten::add\"] = [\r\n            \"TOP(C)::forward.A0(A)::forward.SELF(A)::forward_impl_.\",\r\n            \"TOP(C)::forward.SELF(C)::call_b.B0(B)::forward.\",\r\n            \"TOP(C)::forward.\",\r\n        ]\r\n        with TemporaryFileName(mode=\"w+\") as fname:\r\n            with profile(\r\n                activities=[torch.profiler.ProfilerActivity.CPU],\r\n                with_modules=True,\r\n            ) as prof:\r\n                model(input_a, input_b)\r\n            prof.export_chrome_trace(fname)\r\n            with open(fname) as f:\r\n                trace = json.load(f)\r\n                assert \"traceEvents\" in trace\r\n                events = trace[\"traceEvents\"]\r\n                found_memory_events = False\r\n                for evt in events:\r\n                    assert \"name\" in evt\r\n                    if \"args\" in evt:\r\n                        op_name = evt[\"name\"]\r\n                        if \"Module Hierarchy\" in evt[\"args\"]:\r\n                            hierarchy = evt[\"args\"][\"Module Hierarchy\"]\r\n                            if op_name in op_to_module_hierarchy:\r\n                                assert hierarchy in op_to_module_hierarchy[op_name]",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_default_qat_qconfig(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = nn.Linear(5, 5)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        model = Model()\r\n        model.linear.weight = torch.nn.Parameter(torch.randn(5, 5))\r\n        model.qconfig = torch.ao.quantization.get_default_qat_qconfig(\"fbgemm\")\r\n        ref_model = torch.ao.quantization.QuantWrapper(model)\r\n        ref_model = torch.ao.quantization.prepare_qat(ref_model)\r\n        self._test_obs(\r\n            ref_model, input_size=[5, 5], generate=False, check_numerics=False\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fx_qat_convbn_fused_jit_scriptable(self):\r\n        \"\"\"\r\n        Tests jit scriptability works for fused ConvBN.\r\n        \"\"\"\r\n        for qengine in ['fbgemm', 'qnnpack']:\r\n            with override_quantized_engine(qengine):\r\n                # create conv-bn\r\n                class Model(nn.Module):\r\n                    def __init__(self) -> None:\r\n                        super().__init__()\r\n                        self.conv = nn.Conv2d(4, 1, 3, padding=1)\r\n                        self.bn = nn.BatchNorm2d(1)\r\n\r\n                    def forward(self, x):\r\n                        x = self.conv(x)\r\n                        x = self.bn(x)\r\n                        return x\r\n\r\n                model = Model()\r\n                model = torch.fx.symbolic_trace(model)\r\n\r\n                # fuse it\r\n                fused_model = torch.ao.quantization.fuse_modules_qat(\r\n                    model,\r\n                    [['conv', 'bn']],\r\n                )\r\n                # convert to QAT\r\n                qconfig_mapping = torch.ao.quantization.get_default_qat_qconfig_mapping(qengine)\r\n\r\n                quantizable_model = torch.ao.quantization.quantize_fx.prepare_qat_fx(fused_model,\r\n                                                                                     qconfig_mapping,\r\n                                                                                     example_inputs=None)\r\n                assert isinstance(quantizable_model.conv, torch.ao.nn.intrinsic.qat.ConvBn2d)\r\n\r\n                # jit script\r\n                scripted_model = torch.jit.script(quantizable_model)\r\n\r\n                self.assertTrue(\r\n                    isinstance(scripted_model, torch.jit.ScriptModule),\r\n                    \"Expected prepared model with to be scriptable\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_qat_convbn_fused_jit_scriptable(self):\r\n        \"\"\"\r\n        Tests jit scriptability works for fused ConvBN.\r\n        \"\"\"\r\n        for qengine in ['fbgemm', 'qnnpack']:\r\n            with override_quantized_engine(qengine):\r\n                # create conv-bn\r\n                class Model(nn.Module):\r\n                    def __init__(self) -> None:\r\n                        super().__init__()\r\n                        self.conv = nn.Conv2d(4, 1, 3, padding=1)\r\n                        self.bn = nn.BatchNorm2d(1)\r\n\r\n                    def forward(self, x):\r\n                        x = self.conv(x)\r\n                        x = self.bn(x)\r\n                        return x\r\n\r\n                model = Model()\r\n\r\n                # fuse it\r\n                fused_model = torch.ao.quantization.fuse_modules_qat(\r\n                    model,\r\n                    [['conv', 'bn']],\r\n                )\r\n                # convert to QAT\r\n                fused_model.qconfig = torch.ao.quantization.get_default_qconfig(qengine)\r\n                torch.ao.quantization.prepare_qat(fused_model, inplace=True)\r\n                assert isinstance(fused_model.conv, torch.ao.nn.intrinsic.qat.ConvBn2d)\r\n\r\n                # Test jit script fails\r\n                # Prepared eager module fails due to observer hooks not being scriptable\r\n                with self.assertRaises(RuntimeError):\r\n                    torch.jit.script(fused_model)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_qat_convbn_fused_syncbn_replacement(self):\r\n        \"\"\"\r\n        Tests that SyncBatchNorm replacement works for fused ConvBN.\r\n        \"\"\"\r\n        if 'fbgemm' not in torch.backends.quantized.supported_engines:\r\n            return\r\n        with override_quantized_engine('fbgemm'):\r\n            # create conv-bn\r\n            class Model(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.conv = nn.Conv2d(4, 1, 3, padding=1)\r\n                    self.bn = nn.BatchNorm2d(1)\r\n\r\n                def forward(self, x):\r\n                    x = self.conv(x)\r\n                    x = self.bn(x)\r\n                    return x\r\n\r\n            model = Model()\r\n            # fuse it\r\n            fused_model = torch.ao.quantization.fuse_modules_qat(\r\n                model,\r\n                [['conv', 'bn']],\r\n            )\r\n            # convert to QAT\r\n            fused_model.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\r\n            torch.ao.quantization.prepare_qat(fused_model, inplace=True)\r\n            # replace with DDP\r\n            fused_model = nn.SyncBatchNorm.convert_sync_batchnorm(fused_model)\r\n            self.assertTrue(\r\n                isinstance(fused_model.conv.bn, nn.SyncBatchNorm),\r\n                \"Expected BN to be converted to SyncBN\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_device_affinity(self):\r\n        \"\"\"\r\n        Tests that converting a model to QAT respects device affinity\r\n        \"\"\"\r\n        class Model(nn.Module):\r\n\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.conv = nn.Conv2d(1, 1, 1)\r\n                self.bn = nn.BatchNorm2d(1)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.conv(x)\r\n                x = self.bn(x)\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        model = Model()\r\n        model.qconfig = torch.ao.quantization.get_default_qat_qconfig(torch.backends.quantized.engine)\r\n        device = torch.device('cuda:0')\r\n        model.to(device)\r\n        torch.ao.quantization.prepare_qat(model, inplace=True)\r\n        model_devices = {p.device for p in model.parameters()} | \\\r\n            {p.device for p in model.buffers()}\r\n        self.assertEqual(len(model_devices), 1)\r\n        model_device = next(iter(model_devices))\r\n        self.assertEqual(model_device, device)\r\n\r\n        # ensure that running an input on CUDA works without any needed changes\r\n        input = torch.randn(4, 1, 4, 4, device=device)\r\n        model(input)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_embedding_bag_qat_config(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.emb1 = torch.nn.EmbeddingBag(num_embeddings=10, embedding_dim=12,\r\n                                                  include_last_offset=True, scale_grad_by_freq=False, mode='sum')\r\n                self.emb2 = torch.nn.EmbeddingBag(num_embeddings=10, embedding_dim=12,\r\n                                                  include_last_offset=True, scale_grad_by_freq=False, mode='sum')\r\n\r\n            def forward(self, indices):\r\n                return torch.cat((self.emb1(indices), self.emb2(indices)))\r\n\r\n\r\n        qconfigs = [torch.ao.quantization.default_embedding_qat_qconfig,\r\n                    torch.ao.quantization.default_embedding_qat_qconfig_4bit]\r\n        for qconfig in qconfigs:\r\n            model = Model().train()\r\n            indices = torch.randint(0, 10, (5, 12))\r\n\r\n            model.qconfig = qconfig\r\n\r\n            quant_model = prepare_qat(model,\r\n                                      mapping=get_embedding_qat_module_mappings())\r\n\r\n            count_fake_quant = 0\r\n            for name, mod in quant_model.named_modules():\r\n                if name.endswith('weight_fake_quant'):\r\n                    count_fake_quant += 1\r\n                    self.assertEqual(type(mod), FakeQuantize)\r\n            self.assertEqual(count_fake_quant, 2)\r\n\r\n            quant_model(indices)\r\n\r\n            # Ensure that EmbeddingBags have float zero_point values\r\n            self.assertEqual(quant_model.emb1.weight_fake_quant.zero_point.dtype, torch.float32)\r\n            self.assertEqual(quant_model.emb2.weight_fake_quant.zero_point.dtype, torch.float32)\r\n\r\n            inference_gm = convert(quant_model.eval().cpu(),\r\n                                   mapping=get_embedding_static_quant_module_mappings())\r\n\r\n            # Ensure that EmbeddingBags are now quantized with the appropriate bitwidth.\r\n            self.assertEqual(type(inference_gm.emb1), torch.ao.nn.quantized.EmbeddingBag)\r\n            self.assertEqual(type(inference_gm.emb2), torch.ao.nn.quantized.EmbeddingBag)\r\n            self.assertEqual(inference_gm.emb1.dtype, qconfig.weight().dtype)\r\n            self.assertEqual(inference_gm.emb2.dtype, qconfig.weight().dtype)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_default_fused_qat_config(self):\r\n        class Model(nn.Module):\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.linear = nn.Linear(2, 2)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.linear(x)\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        for qengine in [\"fbgemm\", \"qnnpack\"]:\r\n            model = Model()\r\n            model.linear.weight = torch.nn.Parameter(torch.randn(2, 2))\r\n            sample_input = torch.randn(2, 2)\r\n            model.qconfig = torch.ao.quantization.get_default_qat_qconfig(qengine, version=1)\r\n            ref_model = torch.ao.quantization.QuantWrapper(model)\r\n            ref_model = torch.ao.quantization.prepare_qat(ref_model)\r\n            ref_model(sample_input)\r\n            count_fake_quant = 0\r\n            for name, mod in ref_model.named_modules():\r\n                if name.endswith('weight_fake_quant'):\r\n                    count_fake_quant += 1\r\n                    self.assertEqual(type(mod), FusedMovingAvgObsFakeQuantize)\r\n\r\n                if name.count('activation_post_process') == 1 and 'weight_fake_quant' not in name:\r\n                    count_fake_quant += 1\r\n                    self.assertEqual(type(mod), FusedMovingAvgObsFakeQuantize)\r\n\r\n            self.assertEqual(count_fake_quant, 3)\r\n\r\n            if qengine == \"fbgemm\":\r\n                lower_bnd = 0\r\n                upper_bnd = 127\r\n                obs2match = MovingAveragePerChannelMinMaxObserver\r\n\r\n            else:\r\n                lower_bnd = 0\r\n                upper_bnd = 255\r\n                obs2match = MovingAverageMinMaxObserver\r\n\r\n            self.assertEqual(ref_model.quant.activation_post_process.activation_post_process.quant_min, lower_bnd)\r\n            self.assertEqual(ref_model.quant.activation_post_process.activation_post_process.quant_max, upper_bnd)\r\n            self.assertEqual(type(ref_model.module.linear.weight_fake_quant.activation_post_process),\r\n                             obs2match)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_conv_bn_relu(\r\n            self,\r\n            batch_size,\r\n            input_channels_per_group,\r\n            height,\r\n            width,\r\n            output_channels_per_group,\r\n            groups,\r\n            kernel_h,\r\n            kernel_w,\r\n            stride_h,\r\n            stride_w,\r\n            pad_h,\r\n            pad_w,\r\n            dilation,\r\n            padding_mode,\r\n            use_relu,\r\n            eps,\r\n            momentum,\r\n            freeze_bn,\r\n            zero_gamma,\r\n            has_bias,\r\n            use_slow_fusion,\r\n    ):\r\n        input_channels = input_channels_per_group * groups\r\n        output_channels = output_channels_per_group * groups\r\n        dilation_h = dilation_w = dilation\r\n\r\n        conv_op = Conv2d(\r\n            input_channels,\r\n            output_channels,\r\n            (kernel_h, kernel_w),\r\n            (stride_h, stride_w),\r\n            (pad_h, pad_w),\r\n            (dilation_h, dilation_w),\r\n            groups,\r\n            has_bias,\r\n            padding_mode\r\n        ).to(dtype=torch.double)\r\n        bn_op = BatchNorm2d(output_channels, eps, momentum).to(dtype=torch.double)\r\n        relu_op = ReLU()\r\n\r\n        cls = ConvBnReLU2d if use_relu else ConvBn2d\r\n        qat_op = cls(\r\n            input_channels,\r\n            output_channels,\r\n            (kernel_h, kernel_w),\r\n            (stride_h, stride_w),\r\n            (pad_h, pad_w),\r\n            (dilation_h, dilation_w),\r\n            groups,\r\n            has_bias,\r\n            padding_mode,\r\n            eps,\r\n            momentum,\r\n            freeze_bn=True,\r\n            qconfig=default_qat_qconfig\r\n        ).to(dtype=torch.double)\r\n        qat_op._enable_slow_path_for_better_numerical_stability = use_slow_fusion\r\n\r\n        # the approximate fusion will not work if bn.weight has 0\r\n        if zero_gamma and use_slow_fusion:\r\n            torch.nn.init.zeros_(qat_op.bn.weight)\r\n\r\n        qat_op.apply(torch.ao.quantization.disable_fake_quant)\r\n        if freeze_bn:\r\n            qat_op.apply(torch.ao.nn.intrinsic.qat.freeze_bn_stats)\r\n        else:\r\n            qat_op.apply(torch.ao.nn.intrinsic.qat.update_bn_stats)\r\n\r\n        # align inputs and internal parameters\r\n        input = torch.randn(batch_size, input_channels, height, width, dtype=torch.double, requires_grad=True)\r\n        conv_op.weight = torch.nn.Parameter(qat_op.weight.detach())\r\n        if has_bias:\r\n            conv_op.bias = torch.nn.Parameter(qat_op.bias.detach())\r\n        bn_op.running_mean = qat_op.bn.running_mean.clone()\r\n        bn_op.running_var = qat_op.bn.running_var.clone()\r\n        bn_op.weight = torch.nn.Parameter(qat_op.bn.weight.detach())\r\n        bn_op.bias = torch.nn.Parameter(qat_op.bn.bias.detach())\r\n\r\n        def compose(functions):\r\n            # functions are reversed for natural reading order\r\n            return reduce(lambda f, g: lambda x: f(g(x)), functions[::-1], lambda x: x)\r\n\r\n        if not use_relu:\r\n            def relu_op(x):  # noqa: F811\r\n                return x\r\n\r\n        if freeze_bn:\r\n            def ref_op(x):\r\n                x = conv_op(x)\r\n                x = (x - bn_op.running_mean.reshape([1, -1, 1, 1])) * \\\r\n                    (bn_op.weight / torch.sqrt(bn_op.running_var + bn_op.eps)) \\\r\n                    .reshape([1, -1, 1, 1]) + bn_op.bias.reshape([1, -1, 1, 1])\r\n                x = relu_op(x)\r\n                return x\r\n        else:\r\n            ref_op = compose([conv_op, bn_op, relu_op])\r\n\r\n        input_clone = input.detach().clone().requires_grad_()\r\n        for _ in range(2):\r\n            result_ref = ref_op(input)\r\n            result_actual = qat_op(input_clone)\r\n            self.assertEqual(result_ref, result_actual)\r\n\r\n            # backward\r\n            dout = torch.randn(result_ref.size(), dtype=torch.double)\r\n            loss = (result_ref - dout).sum()\r\n            loss.backward()\r\n            input_grad_ref = input.grad.cpu()\r\n            weight_grad_ref = conv_op.weight.grad.cpu()\r\n            gamma_grad_ref = bn_op.weight.grad.cpu()\r\n            beta_grad_ref = bn_op.bias.grad.cpu()\r\n            running_mean_ref = bn_op.running_mean\r\n            running_var_ref = bn_op.running_var\r\n            num_batches_tracked_ref = bn_op.num_batches_tracked\r\n            loss = (result_actual - dout).sum()\r\n            loss.backward()\r\n            input_grad_actual = input_clone.grad.cpu()\r\n            weight_grad_actual = qat_op.weight.grad.cpu()\r\n            gamma_grad_actual = qat_op.bn.weight.grad.cpu()\r\n            beta_grad_actual = qat_op.bn.bias.grad.cpu()\r\n            running_mean_actual = qat_op.bn.running_mean\r\n            running_var_actual = qat_op.bn.running_var\r\n            num_batches_tracked_actual = qat_op.bn.num_batches_tracked\r\n            precision = 1e-10\r\n            self.assertEqual(input_grad_ref, input_grad_actual, atol=precision, rtol=0)\r\n            self.assertEqual(weight_grad_ref, weight_grad_actual, atol=precision, rtol=0)\r\n            self.assertEqual(gamma_grad_ref, gamma_grad_actual, atol=precision, rtol=0)\r\n            self.assertEqual(beta_grad_ref, beta_grad_actual, atol=precision, rtol=0)\r\n            self.assertEqual(num_batches_tracked_ref, num_batches_tracked_actual, atol=precision, rtol=0)\r\n            self.assertEqual(running_mean_ref, running_mean_actual, atol=precision, rtol=0)\r\n            self.assertEqual(running_var_ref, running_var_actual, atol=precision, rtol=0)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_mixed_optional_default_tensor_script(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x,\r\n                y: Optional[Tensor] = torch.ones(2, 3),\r\n                z: Optional[Tensor] = torch.zeros(2, 3),\r\n            ):\r\n                if y is not None:\r\n                    return x + y\r\n                if z is not None:\r\n                    return x + z\r\n                return x\r\n\r\n        x = torch.randn(2, 3)\r\n        y = torch.randn(2, 3)\r\n        z = torch.randn(2, 3)\r\n        model = torch.jit.script(Model())\r\n\r\n        self.run_test(model, (x, y, z), input_names=(\"x\", \"y\", \"z\"))\r\n        self.run_test(model, (x,), {\"y\": y, \"z\": z}, input_names=(\"x\", \"y\", \"z\"))\r\n        self.run_test(model, (x,), {\"y\": y}, input_names=(\"x\", \"y\"))\r\n\r\n        for example_inputs, example_kwargs in (\r\n            ((x, y, None), {}),\r\n            ((x, None, z), {}),\r\n            ((x,), {\"y\": y, \"z\": None}),\r\n            ((x,), {\"y\": None, \"z\": z}),\r\n        ):\r\n            with self.assertRaisesRegex(\r\n                ValueError, \"args contained 1 None's after flattening.\"\r\n            ):\r\n                self.run_test(\r\n                    model, example_inputs, example_kwargs, input_names=(\"x\", \"y\", \"z\")\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_all_optional_default_none(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x: Optional[Tensor] = None, y: Optional[Tensor] = None):\r\n                if x is not None:\r\n                    return x\r\n                if y is not None:\r\n                    return y\r\n                else:\r\n                    return torch.tensor(-1.0)\r\n\r\n        x = torch.randn(2, 3)\r\n        model = Model()\r\n        self.run_test(model, (x, None))\r\n        self.run_test(\r\n            model,\r\n            (),\r\n            {\"x\": x, \"y\": None},\r\n            # y disappears in tracing.\r\n            input_names=(\"x\",),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_all_optional_default_tensor(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: Optional[Tensor] = torch.ones(2, 3),\r\n                y: Optional[Tensor] = torch.zeros(2, 3),\r\n            ):\r\n                if x is not None:\r\n                    return x\r\n                elif y is not None:\r\n                    return y\r\n                else:\r\n                    return torch.tensor(-1.0)\r\n\r\n        x = torch.randn(2, 3)\r\n        y = torch.randn(2, 3)\r\n        model = Model()\r\n        self.run_test(model, (x, None))\r\n        self.run_test(model, (None, y))\r\n        # tracing means y is never used so it's removed from the exported model inputs,\r\n        # and we fail when trying to run ORT.\r\n        with self.assertRaisesRegex(ValueError, \"got too many positional inputs\"):\r\n            self.run_test(model, (x, y))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_mixed_optional_default_tensor(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x,\r\n                y: Optional[Tensor] = torch.ones(2, 3),\r\n                z: Optional[Tensor] = torch.zeros(2, 3),\r\n            ):\r\n                if y is not None:\r\n                    return x + y\r\n                if z is not None:\r\n                    return x + z\r\n                return x\r\n\r\n        x = torch.randn(2, 3)\r\n        y = torch.randn(2, 3)\r\n        z = torch.randn(2, 3)\r\n        model = Model()\r\n\r\n        self.run_test(model, (x, y, None))\r\n        self.run_test(model, (x, None, z))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_mixed_optional_default_none(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x,\r\n                y: Optional[Tensor] = None,\r\n                z: Optional[Tensor] = None,\r\n            ):\r\n                if y is not None:\r\n                    return x + y\r\n                if z is not None:\r\n                    return x + z\r\n                return x\r\n\r\n        x = torch.randn(2, 3)\r\n        y = torch.randn(2, 3)\r\n        z = torch.randn(2, 3)\r\n        model = Model()\r\n        # Without kwargs dict.\r\n        self.run_test(model, (x, y, None))\r\n        self.run_test(model, (x, None, z))\r\n        # With kwargs dict.\r\n        self.run_test(model, (x,), {\"y\": y, \"z\": None})\r\n        self.run_test(model, (x,), {\"y\": None, \"z\": z})\r\n        self.run_test(model, (x,), {\"z\": z})\r\n        self.run_test(model, (x,), {\"y\": y})",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_all_optional_default_tensor_script(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: Optional[Tensor] = torch.ones(2, 3),\r\n                y: Optional[Tensor] = torch.zeros(2, 3),\r\n            ):\r\n                if x is not None:\r\n                    return x\r\n                elif y is not None:\r\n                    return y\r\n                else:\r\n                    return torch.tensor(-1.0)\r\n\r\n        x = torch.randn(2, 3)\r\n        y = torch.randn(2, 3)\r\n        model = torch.jit.script(Model())\r\n\r\n        # Optional supports None inputs\r\n        self.run_test(model, (x,))\r\n        # NOTE: default value is not supported on ONNX, so torch and ONNX has\r\n        # different behavior\r\n        with self.assertRaisesRegex(AssertionError, \"Tensor-likes are not close!\"):\r\n            self.run_test(model, (), {\"y\": y}, input_names=[\"y\"])\r\n\r\n        self.run_test(model, (x, y))\r\n        self.run_test(model, (), {\"x\": x, \"y\": y}, input_names=(\"x\", \"y\"))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_mixed_optional(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x, y: Optional[Tensor]):\r\n                if y is not None:\r\n                    return x + y\r\n                return x\r\n\r\n        x = torch.randn(2, 3)\r\n        model = Model()\r\n        self.run_test(model, (x, None))\r\n        self.run_test(model, (x, x))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_prepare_obs_or_fq_callback(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(self, x):\r\n                x = torch.nn.functional.max_pool2d(x, 2, 2)\r\n                x = torch.nn.functional.pixel_shuffle(x, 2)\r\n                return x.permute(0, 2, 3, 1)\r\n\r\n        class BackendAQuantizer(Quantizer):\r\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\r\n                act_qspec = QuantizationSpec(\r\n                    dtype=torch.uint8,\r\n                    quant_min=0,\r\n                    quant_max=255,\r\n                    qscheme=torch.per_tensor_affine,\r\n                    is_dynamic=False,\r\n                    observer_or_fake_quant_ctr=observer.default_observer,\r\n                )\r\n                for node in model.graph.nodes:\r\n                    if node.op == \"call_function\" and node.target in (\r\n                        torch.ops.aten.max_pool2d.default,\r\n                        torch.ops.aten.permute.default,\r\n                        torch.ops.aten.pixel_shuffle.default,\r\n                    ):\r\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\r\n                            input_qspec_map={\r\n                                node.args[0]: act_qspec,\r\n                            },\r\n                            output_qspec=SharedQuantizationSpec((node.args[0], node)),\r\n                            _annotated=True,\r\n                        )\r\n\r\n            def validate(self, model: torch.fx.GraphModule) -> None:\r\n                pass\r\n\r\n            def prepare_obs_or_fq_callback(\r\n                self,\r\n                model: torch.fx.GraphModule,\r\n                edge_or_node_to_obs_or_fq: dict[EdgeOrNode, ObserverOrFakeQuantize],\r\n            ) -> None:\r\n                # hard code output quant by updating entire sharing group\r\n                output_node = next(n for n in model.graph.nodes if n.op == \"output\")\r\n                output_value = output_node.args[0][0]\r\n                old_observer = edge_or_node_to_obs_or_fq[output_value]\r\n                sharing_group = [\r\n                    k for k, v in edge_or_node_to_obs_or_fq.items() if v is old_observer\r\n                ]\r\n                new_observer = observer.FixedQParamsObserver(\r\n                    scale=0.125,\r\n                    zero_point=42,\r\n                    dtype=torch.uint8,\r\n                    quant_min=0,\r\n                    quant_max=255,\r\n                    qscheme=torch.per_tensor_affine,\r\n                )\r\n                for x in sharing_group:\r\n                    edge_or_node_to_obs_or_fq[x] = new_observer\r\n\r\n        example_inputs = (torch.rand(1, 32, 16, 16),)\r\n        gm = export_for_training(Model().eval(), example_inputs).module()\r\n        gm = prepare_pt2e(gm, BackendAQuantizer())\r\n        gm = convert_pt2e(gm)\r\n        for n in gm.graph.nodes:\r\n            if n.op == \"call_function\" and n.target in (\r\n                torch.ops.quantized_decomposed.quantize_per_tensor.default,\r\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default,\r\n            ):\r\n                # Entire graph share the same qspec which was overriden by FixedQParamsObserver\r\n                self.assertEqual(n.args[1], 0.125)\r\n                self.assertEqual(n.args[2], 42)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def sparsify_model(path_to_model, sparsified_model_dump_path):\r\n    \"\"\"Sparsifies the embedding layers of the dlrm model for different sparsity levels, norms and block shapes\r\n    using the DataNormSparsifier.\r\n    The function tracks the step time of the sparsifier and the size of the compressed checkpoint and collates\r\n    it into a csv.\r\n\r\n    Note::\r\n        This function dumps a csv sparse_model_metadata.csv in the current directory.\r\n\r\n    Args:\r\n        path_to_model (str)\r\n            path to the trained criteo model ckpt file\r\n        sparsity_levels (List of float)\r\n            list of sparsity levels to be sparsified on\r\n        norms (List of str)\r\n            list of norms to be sparsified on\r\n        sparse_block_shapes (List of tuples)\r\n            List of sparse block shapes to be sparsified on\r\n    \"\"\"\r\n    sparsity_levels = [sl / 10 for sl in range(0, 10)]\r\n    sparsity_levels += [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\r\n\r\n    norms = [\"L1\", \"L2\"]\r\n    sparse_block_shapes = [(1, 1), (1, 4)]\r\n\r\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n\r\n    print(\"Running for sparsity levels - \", sparsity_levels)\r\n    print(\"Running for sparse block shapes - \", sparse_block_shapes)\r\n    print(\"Running for norms - \", norms)\r\n\r\n    orig_model = get_dlrm_model()\r\n    saved_state = torch.load(path_to_model, map_location=device)\r\n    orig_model.load_state_dict(saved_state[\"state_dict\"])\r\n\r\n    orig_model = orig_model.to(device)\r\n    step_time_dict = {}\r\n\r\n    stat_dict: dict[str, list] = {\r\n        \"norm\": [],\r\n        \"sparse_block_shape\": [],\r\n        \"sparsity_level\": [],\r\n        \"step_time_sec\": [],\r\n        \"zip_file_size\": [],\r\n        \"path\": [],\r\n    }\r\n    for norm in norms:\r\n        for sbs in sparse_block_shapes:\r\n            if norm == \"L2\" and sbs == (1, 1):\r\n                continue\r\n            for sl in sparsity_levels:\r\n                model = copy.deepcopy(orig_model)\r\n                sparsifier = create_attach_sparsifier(\r\n                    model, sparse_block_shape=sbs, norm=norm, sparsity_level=sl\r\n                )\r\n\r\n                t1 = time.time()\r\n                sparsifier.step()\r\n                t2 = time.time()\r\n\r\n                step_time = t2 - t1\r\n                norm_sl = f\"{norm}_{sbs}_{sl}\"\r\n                print(f\"Step Time for {norm_sl}=: {step_time} s\")\r\n\r\n                step_time_dict[norm_sl] = step_time\r\n\r\n                sparsifier.squash_mask()\r\n\r\n                saved_state[\"state_dict\"] = model.state_dict()\r\n                file_name = f\"criteo_model_norm={norm}_sl={sl}.ckpt\"\r\n                state_path, file_size = save_model_states(\r\n                    saved_state, sparsified_model_dump_path, file_name, sbs, norm=norm\r\n                )\r\n\r\n                stat_dict[\"norm\"].append(norm)\r\n                stat_dict[\"sparse_block_shape\"].append(sbs)\r\n                stat_dict[\"sparsity_level\"].append(sl)\r\n                stat_dict[\"step_time_sec\"].append(step_time)\r\n                stat_dict[\"zip_file_size\"].append(file_size)\r\n                stat_dict[\"path\"].append(state_path)\r\n\r\n    df = pd.DataFrame(stat_dict)\r\n    filename = \"sparse_model_metadata.csv\"\r\n    df.to_csv(filename, index=False)\r\n\r\n    print(f\"Saved sparsified metadata file in {filename}\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def evaluate_metrics(test_dataloader, sparse_model_metadata):\r\n    \"\"\"Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\r\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\r\n    evaluation metrics in a csv file [model_performance.csv]\r\n    \"\"\"\r\n    metadata = pd.read_csv(sparse_model_metadata)\r\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n\r\n    metrics_dict: dict[str, list] = {\r\n        \"norm\": [],\r\n        \"sparse_block_shape\": [],\r\n        \"sparsity_level\": [],\r\n        \"precision\": [],\r\n        \"recall\": [],\r\n        \"f1\": [],\r\n        \"roc_auc\": [],\r\n        \"accuracy\": [],\r\n        \"log_loss\": [],\r\n    }\r\n\r\n    for _, row in metadata.iterrows():\r\n        norm, sbs, sl = row[\"norm\"], row[\"sparse_block_shape\"], row[\"sparsity_level\"]\r\n        model_path = row[\"path\"]\r\n        model = fetch_model(model_path, device)\r\n\r\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\r\n        key = f\"{norm}_{sbs}_{sl}\"\r\n        print(key, \"=\", model_metrics)\r\n\r\n        metrics_dict[\"norm\"].append(norm)\r\n        metrics_dict[\"sparse_block_shape\"].append(sbs)\r\n        metrics_dict[\"sparsity_level\"].append(sl)\r\n\r\n        for key, value in model_metrics.items():\r\n            if key in metrics_dict:\r\n                metrics_dict[key].append(value)\r\n\r\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\r\n    print(sparse_model_metrics)\r\n\r\n    filename = \"sparse_model_metrics.csv\"\r\n    sparse_model_metrics.to_csv(filename, index=False)\r\n    print(f\"Model metrics file saved to {filename}\")",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Unnecessary Iteration"
        ]
    },
    {
        "code": "def test_tuple_of_optional_default_tensor_script(self):\r\n        class Model(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x,\r\n                y: tuple[Optional[Tensor], Optional[Tensor]] = (\r\n                    torch.zeros(2, 3),\r\n                    torch.zeros(2, 3),\r\n                ),\r\n            ):\r\n                y0, y1 = y\r\n                if y0 is not None:\r\n                    return x + y0\r\n                if y1 is not None:\r\n                    return x + y1\r\n                return x\r\n\r\n        x = torch.randn(2, 3)\r\n        y0 = torch.randn(2, 3)\r\n        y1 = torch.randn(2, 3)\r\n        model = torch.jit.script(Model())\r\n        with self.assertRaisesRegex(\r\n            ValueError, \"args contained 1 None's after flattening.\"\r\n        ):\r\n            self.run_test(model, (x, (None, y1)))\r\n        self.run_test(model, (x, (y0, y1)))\r\n        # export succeeds, but running ORT through run_test would fail because the exported model\r\n        # has the inputs flattened into 3 inputs.\r\n        torch.onnx.export(\r\n            model, (x, {\"y\": (y0, y1)}), io.BytesIO(), opset_version=self.opset_version\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train(log_interval, model, device, train_loader, optimizer, epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data, target = data.to(device), target.to(device)\r\n        optimizer.zero_grad(set_to_none=True)\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        torch._lazy.mark_step()\r\n\r\n        if batch_idx % log_interval == 0:\r\n            print(\r\n                f\"Train Epoch: {epoch} \"\r\n                f\"[{batch_idx * len(data)}/{len(train_loader.dataset)} ({100.0 * batch_idx / len(train_loader):.0f}%)]\"\r\n                f\"\\tLoss: {loss.item():.6f}\"\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def run(rank, world_size):\r\n    # Set up world pg\r\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\r\n    os.environ[\"MASTER_PORT\"] = \"12355\"\r\n\r\n    dist.init_process_group(\"cpu:gloo,cuda:nccl\", rank=rank, world_size=world_size)\r\n    torch.cuda.set_device(rank)\r\n\r\n    model, optim = _init_model(rank, world_size)\r\n    state_dict = {\"model\": model, \"optim\": optim}\r\n    loss_calc = torch.nn.BCELoss()\r\n\r\n    f = None\r\n    for epoch in range(NUM_EPOCHS):\r\n        try:\r\n            torch.manual_seed(epoch)\r\n            x, y = _input()\r\n\r\n            loss = loss_calc(model(x), y)\r\n\r\n            _print(f\"{epoch=} {loss=}\")\r\n\r\n            loss.backward()\r\n            optim.step()\r\n            optim.zero_grad()\r\n\r\n            if epoch % SAVE_PERIOD == 0:\r\n                if f is not None:\r\n                    f.result()\r\n                f = dcp.state_dict_saver.async_save(\r\n                    state_dict, checkpoint_id=CHECKPOINT_DIR\r\n                )\r\n\r\n            if FAULT_PERIOD > 0 and epoch % FAULT_PERIOD == 0:\r\n                raise InjectedException(\"Fault injection!\")\r\n\r\n        except InjectedException as e:\r\n            dist.barrier()\r\n\r\n            _print(\"Trainer encountered exception:\")\r\n            traceback.print_tb(e.__traceback__)\r\n\r\n            _print(\"Reloading model from last checkpoint!\")\r\n            if f is not None:\r\n                f.result()\r\n            dcp.load(state_dict)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _init_model(rank, world_size):\r\n    device_mesh = init_device_mesh(DEVICE, (world_size,))\r\n\r\n    # Create a dummy model and wrap it in FSDP\r\n    model = Model().cuda()\r\n    device_mesh = init_device_mesh(DEVICE, (world_size,))\r\n    model = FSDP(model, device_mesh=device_mesh, use_orig_params=True)\r\n\r\n    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\r\n\r\n    _patch_model_state_dict(model)\r\n    _patch_optimizer_state_dict(model, optimizers=optim)\r\n\r\n    return model, optim",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _init_model(device, world_size):\r\n    device_mesh = init_device_mesh(device, (world_size,))\r\n    model = Model().cuda()\r\n    model = FSDP(\r\n        model,\r\n        device_mesh=device_mesh,\r\n        use_orig_params=True,\r\n    )\r\n    optim = torch.optim.Adam(model.parameters(), lr=0.1)\r\n    _make_stateful(model, optim)\r\n\r\n    return model, optim",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _train(model, optim, train_steps=1):\r\n    torch.manual_seed(0)\r\n    loss = None\r\n    for _ in range(train_steps):\r\n        loss = model(model.get_input()).sum()\r\n        loss.backward()\r\n        optim.step()\r\n        optim.zero_grad()\r\n\r\n    return loss",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_qat_prepare_device_affinity(self):\r\n        \"\"\"\r\n        Tests that FX QAT prepare pass respects device affinity\r\n        \"\"\"\r\n        class Model(nn.Module):\r\n\r\n            def __init__(self) -> None:\r\n                super().__init__()\r\n                self.conv = nn.Conv2d(1, 1, 1)\r\n                self.bn = nn.BatchNorm2d(1)\r\n                self.relu = nn.ReLU()\r\n\r\n            def forward(self, x):\r\n                x = self.conv(x)\r\n                x = self.bn(x)\r\n                x = self.relu(x)\r\n                return x\r\n\r\n        model = Model()\r\n        qengine = torch.backends.quantized.engine\r\n        qconfig_dict = {'': torch.ao.quantization.get_default_qat_qconfig(qengine)}\r\n        device = torch.device('cuda:0')\r\n        model.to(device)\r\n\r\n        example_inputs = (torch.randn(4, 1, 4, 4, device=device),)\r\n        # QAT prepare\r\n        model = prepare_qat_fx(model, qconfig_dict, example_inputs=example_inputs)\r\n\r\n        # ensure that running an input on CUDA works without any needed changes\r\n        model(*example_inputs)\r\n\r\n        # ensure all buffers and parameters are on the device we expect\r\n        model_devices = {p.device for p in model.parameters()} | \\\r\n            {p.device for p in model.buffers()}\r\n        self.assertEqual(len(model_devices), 1)\r\n        model_device = next(iter(model_devices))\r\n        self.assertEqual(model_device, device)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train_batch(\r\n        self,\r\n        mini_batch: FeatureSet,\r\n        trainer_has_less_inputs: bool,\r\n        simulate_uneven_inputs: bool,\r\n    ):\r\n        grads_dict = None\r\n\r\n        if not simulate_uneven_inputs:\r\n            input_batches = [mini_batch]\r\n        else:\r\n            # Split into microbatches, and trim to simulate uneven inputs.\r\n            dense_features = mini_batch.dense_features\r\n            sparse_features = mini_batch.sparse_features\r\n            values = mini_batch.values\r\n\r\n            dense_microbatch = torch.split(dense_features, 2)\r\n            sparse_microbatch = torch.split(sparse_features, 2)\r\n            values_microbatch = torch.split(values, 2)\r\n            batches = []\r\n            for d, s, v in zip(dense_microbatch, sparse_microbatch, values_microbatch):\r\n                feature_set = FeatureSet(dense_features=d, sparse_features=s, values=v)\r\n                batches.append(feature_set)\r\n\r\n            if trainer_has_less_inputs:\r\n                input_batches = batches[: len(batches) // 2]\r\n                gLogger.info(\r\n                    \"Trainer reduced input patches from %s \"\r\n                    \"to %s to simulate uneven inputs.\",\r\n                    len(batches), len(input_batches)\r\n                )\r\n            else:\r\n                input_batches = batches\r\n\r\n        with self.hybrid_module.join() if simulate_uneven_inputs else contextlib.nullcontext():\r\n            for b in input_batches:\r\n                with dist_autograd.context() as context_id:\r\n                    output = self.hybrid_module.forward(b)\r\n                    loss = (output * mini_batch.values).sum()\r\n                    dist_autograd.backward(context_id, [loss])\r\n                    grads_dict = dist_autograd.get_gradients(context_id)\r\n                    gLogger.info(\r\n                        \"Loss is %s for mini batch: %s. \"\r\n                        \"Grads dict has %s entries: %s\", loss, mini_batch, len(grads_dict), grads_dict\r\n                    )\r\n        return (\r\n            tuple(grads_dict[param] for param in self.ddp_params),\r\n            tuple(grads_dict[param] for param in self.non_ddp_params),\r\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_ddp_dist_autograd_local_vs_remote(self):\r\n        # Each trainer uses a different random seed. Otherwise, they are going\r\n        # to have exactly the same initial model parameters, input, and\r\n        # therefore grads. That means the grads will be the same before and\r\n        # after DDP's all-reduce.\r\n        torch.manual_seed(self.rank)\r\n        dist.init_process_group(\r\n            backend=\"gloo\",\r\n            init_method=INIT_METHOD_TEMPLATE.format(file_name=self.file_name),\r\n            world_size=self.world_size,\r\n            rank=self.rank,\r\n        )\r\n\r\n        # Use two different remote device input string, w/ and w/o the default\r\n        # device string \"cpu\", respectively.\r\n        for remote_device in [\"worker0/cpu\", \"worker0\"]:\r\n            remote_layer1 = RemoteModule(\r\n                remote_device=remote_device, module_cls=nn.Linear, args=(10, 5, False)\r\n            )\r\n            layer1 = nn.Linear(10, 5, False)\r\n            # Start with the same parameters for remote and local\r\n            layer1.weight = remote_layer1.module_rref.to_here().weight\r\n\r\n            # Run local case.\r\n            layer2 = nn.Linear(5, 1)\r\n            inputs = torch.rand((10, 10))\r\n            ddp_model = DistributedDataParallel(layer2)\r\n            loss = ddp_model(layer1(inputs)).sum()\r\n            loss.backward()\r\n\r\n            # Run remote case.\r\n            with dist_autograd.context() as context_id:\r\n                loss = ddp_model(remote_layer1(inputs)).sum()\r\n                dist_autograd.backward(context_id, [loss])\r\n                grads_dict = dist_autograd.get_gradients(context_id)\r\n                dist.barrier()\r\n                self.assertEqual(layer2.weight.grad, grads_dict[layer2.weight])\r\n                self.assertEqual(\r\n                    layer1.weight.grad,\r\n                    rpc.rpc_sync(\r\n                        \"worker0\",\r\n                        CommonDdpComparisonTest.get_remote_grads,\r\n                        args=(remote_layer1.module_rref, context_id),\r\n                    ),\r\n                )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _compare_script_and_mobile(self,\r\n                                   model: torch.nn.Module,\r\n                                   input: torch.Tensor):\r\n        # Compares the numerical outputs for script and lite modules\r\n        qengine = \"qnnpack\"\r\n        with override_quantized_engine(qengine):\r\n            script_module = torch.jit.script(model)\r\n            script_module_result = script_module(input)\r\n\r\n            max_retry = 5\r\n            for retry in range(1, max_retry + 1):\r\n                # retries `max_retry` times; breaks iff succeeds else throws exception\r\n                try:\r\n                    buffer = io.BytesIO(script_module._save_to_buffer_for_lite_interpreter())\r\n                    buffer.seek(0)\r\n                    mobile_module = _load_for_lite_interpreter(buffer)\r\n\r\n                    mobile_module_result = mobile_module(input)\r\n\r\n                    torch.testing.assert_close(script_module_result, mobile_module_result)\r\n                    mobile_module_forward_result = mobile_module.forward(input)\r\n                    torch.testing.assert_close(script_module_result, mobile_module_forward_result)\r\n\r\n                    mobile_module_run_method_result = mobile_module.run_method(\"forward\", input)\r\n                    torch.testing.assert_close(script_module_result, mobile_module_run_method_result)\r\n                except AssertionError as e:\r\n                    if retry == max_retry:\r\n                        raise e\r\n                    else:\r\n                        continue\r\n                break",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_ddp_native_mixed_precision(\r\n            self, gradient_as_bucket_view, set_grad_to_none\r\n        ):\r\n            rank = self.rank\r\n            torch.manual_seed(rank)\r\n            torch.cuda.manual_seed(rank)\r\n            torch.cuda.set_device(rank)\r\n            inp = torch.randn(10, 1)\r\n            mp_config = self._get_fp16_config()\r\n\r\n            class MyModel(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.m = torch.nn.Linear(1, 5)\r\n                    self.register_buffer('buffer', torch.randn(1, 2))\r\n                    self.p = torch.nn.Parameter(\r\n                        torch.randn(10, 5), requires_grad=False\r\n                    )\r\n\r\n                def forward(self_, x):  # noqa: B902\r\n                    params = self_.m.parameters()\r\n                    for p in params:\r\n                        self.assertEqual(mp_config.param_dtype, p.dtype)\r\n\r\n                    self.assertEqual(self_.buffer.dtype, mp_config.buffer_dtype)\r\n\r\n                    self.assertEqual(mp_config.param_dtype, x.dtype)\r\n                    return self_.m(x) + self_.p\r\n\r\n            m = MyModel()\r\n\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                m.to(rank),\r\n                device_ids=[rank],\r\n                mixed_precision=mp_config,\r\n                gradient_as_bucket_view=gradient_as_bucket_view,\r\n            )\r\n            # Buffers are casted in constructor.\r\n            self.assertEqual(net.module.buffer.dtype, mp_config.buffer_dtype)\r\n            # Each param should have an mp_param in the lower precision, and\r\n            # an fp_param in the higher precision.\r\n            for p in net.parameters():\r\n                self.assertEqual(mp_config.param_dtype, p._mp_param.dtype)\r\n                self.assertEqual(torch.float32, p._fp_param.dtype)\r\n\r\n            for _ in range(6):\r\n                loss = net(inp).sum()\r\n                loss.backward()\r\n                # Verify gradient synchronization and params and grads are fp32.\r\n                for n, param in net.named_parameters():\r\n                    self.assertEqual(param.dtype, torch.float32)\r\n                    if param.grad is None:\r\n                        assert n == 'module.p'  # Only param that doesn't require grad\r\n                    else:\r\n                        self.assertEqual(param.grad.dtype, torch.float32)\r\n                        tensor_list = [\r\n                            torch.zeros_like(param.grad)\r\n                            for _ in range(dist.get_world_size(net.process_group))\r\n                        ]\r\n                        dist.all_gather(tensor_list, param.grad)\r\n                        g, rest = tensor_list[0], tensor_list[1:]\r\n                        self.assertEqual(g.dtype, torch.float32)\r\n                        for g_ in rest:\r\n                            self.assertEqual(g_.dtype, torch.float32)\r\n                            self.assertEqual(g, g_)\r\n                net.zero_grad(set_to_none=set_grad_to_none)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_ddp_hook_parity(self, state, hook, num_validated_iters=100):\r\n            rank = self.rank\r\n            m = torch.nn.Linear(1, 5)\r\n            try:\r\n                process_group = state.process_group\r\n            except AttributeError:\r\n                process_group = state\r\n\r\n            net_with_hook = torch.nn.parallel.DistributedDataParallel(\r\n                copy.deepcopy(m).to(rank),\r\n                device_ids=[rank],\r\n                process_group=process_group,\r\n            )\r\n            net_with_hook.register_comm_hook(state=state, hook=hook)\r\n            net_without_hook = torch.nn.parallel.DistributedDataParallel(\r\n                copy.deepcopy(m).to(rank),\r\n                device_ids=[rank],\r\n                process_group=process_group,\r\n            )\r\n            for i in range(100):\r\n                # Clear gradients manually.\r\n                for g in [\r\n                    net_without_hook.module.weight.grad,\r\n                    net_with_hook.module.weight.grad,\r\n                ]:\r\n                    if g is not None:\r\n                        g.requires_grad_(False)\r\n                        g.zero_()\r\n                # Forward + BW\r\n                batch = torch.tensor([rank]).float().cuda(rank)\r\n                loss = net_without_hook(batch).sum()\r\n                loss.backward()\r\n                # For each worker, the gradient on the weight should be worker_rank.\r\n                grad = net_without_hook.module.weight.grad\r\n                avg = grad.clone()\r\n                expected_grad = (\r\n                    sum(i for i in range(dist.get_world_size())) / dist.get_world_size()\r\n                )\r\n                loss_hook = net_with_hook(batch).sum()\r\n                loss_hook.backward()\r\n                grad_hook = net_with_hook.module.weight.grad\r\n                avg_hook = grad_hook.clone()\r\n\r\n                if i < num_validated_iters:\r\n                    # Verify hook grad with expected.\r\n                    self.assertEqual(\r\n                        avg_hook[0, 0].item(),\r\n                        expected_grad,\r\n                        msg=f\"Expected hook grad of {expected_grad} but got {avg_hook[0, 0]}\",\r\n                    )\r\n                    # Verify hook grad with vanilla allreduce\r\n                    self.assertEqual(\r\n                        avg_hook[0, 0],\r\n                        avg[0, 0],\r\n                        msg=f\"Expected hook grad to be close to allreduce {avg[0, 0]}, but got {avg_hook[0, 0]}\",\r\n                    )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_grad_div_uneven_inputs(self):\r\n            # Test gradient division during training with join() API. If\r\n            # divide_by_initial_world_size=False, we scale by the effective world\r\n            # size when allreducing grads.\r\n            dim = 5\r\n            batch = 1\r\n            grad_scale = 50\r\n            rank = self.rank\r\n            model = nn.Linear(dim, dim, bias=False)\r\n            inp = torch.ones(batch, dim, device=self.rank) * grad_scale\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                model.cuda(rank), device_ids=[self.rank], bucket_cap_mb=1\r\n            )\r\n            n_iters = 3\r\n            if self.rank > 0:\r\n                n_iters += 2\r\n\r\n            with net.join(divide_by_initial_world_size=False):\r\n                for _ in range(n_iters):\r\n                    loss = net(inp).sum()\r\n                    loss.backward()\r\n                    # The grad is always expected_grad, since we divide by the number\r\n                    # of currently active processes and inactive processes contribute\r\n                    # zero gradient. If we kept dividing by static initial world\r\n                    # size as processes leave, the grad would be smaller.\r\n                    expected_grad = torch.ones(dim, dim, device=self.rank) * grad_scale\r\n                    param = next(iter(net.parameters()))\r\n                    self.assertEqual(expected_grad, param.grad)\r\n                    # Avoid accumulating grads so that it's the same every iteration\r\n                    net.zero_grad()\r\n                    torch.cuda.synchronize(device=self.rank)\r\n\r\n            # If divide_by_initial_world_size=True (default), we always scale grads\r\n            # by the initial world_size.\r\n            with net.join(divide_by_initial_world_size=True):\r\n                for i in range(n_iters):\r\n                    loss = net(inp).sum()\r\n                    loss.backward()\r\n                    effective_ws = dist.get_world_size()\r\n                    if i >= 3:\r\n                        effective_ws -= 1\r\n                    expected_grad = (\r\n                        torch.ones(dim, dim, device=self.rank)\r\n                        * grad_scale\r\n                        * effective_ws\r\n                    ) / dist.get_world_size()\r\n                    param = next(iter(net.parameters()))\r\n                    self.assertEqual(expected_grad, param.grad)\r\n                    # Avoid accumulating grad so that it's the same every iteration.\r\n                    net.zero_grad()\r\n                    torch.cuda.synchronize(device=self.rank)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_ddp_profiling(self, profiler_ctx, profiler_ctx2=None):\r\n            \"\"\"Runs DDP based model training and captures profiles.\r\n            This test will do two profiler runs.\r\n            1. An inital basic run to check if profiler events are correctly captured.\r\n            2. A second profiling pass after running some iterations of DDP, to check robustness of thread local state.\r\n\r\n            args\r\n                profiler_ctx : Profiler context manager for pass 1\r\n                profiler_ctx2 : Profiler context manager for pass 2.\r\n                    This can be left out as None, in which case a deepcopy\r\n                    of profiler_ctx is used.\r\n            Returns:\r\n                prof: Instantiated profiler object that can be used for post analysis.\r\n            \"\"\"\r\n            batch = 3\r\n            dim = 10\r\n            num_iters = 6\r\n            torch.cuda.set_device(self.rank)\r\n            model = nn.Linear(dim, dim, bias=False)\r\n            inp = torch.rand(batch, dim, device=self.rank)\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                model.cuda(self.rank),\r\n                device_ids=[self.rank],\r\n            )\r\n            if profiler_ctx2 is None:\r\n                profiler_ctx2 = copy.deepcopy(profiler_ctx)\r\n\r\n            with profiler_ctx as prof:\r\n                for _ in range(num_iters):\r\n                    loss = net(inp).sum()\r\n                    loss.backward()\r\n\r\n            all_reduce_event_name = f\"{dist.get_backend()}:all_reduce\"\r\n            events = get_profiling_event(all_reduce_event_name, prof, dedup_gpu_user_annotation=True)\r\n            event_count = sum(e.count for e in events)\r\n            self.assertEqual(event_count, num_iters)\r\n            for event in events:\r\n                self.assertTrue(event.is_async)\r\n                self.assertEqual(event.name, all_reduce_event_name)\r\n\r\n            broadcast_event_name = f\"{dist.get_backend()}:broadcast\"\r\n            broadcast_events = get_profiling_event(broadcast_event_name, prof, dedup_gpu_user_annotation=True)\r\n            event_count = sum(e.count for e in broadcast_events)\r\n            # Broadcast is called during rebuild_buckets\r\n            self.assertGreaterEqual(event_count, 1)\r\n            for event in broadcast_events:\r\n                self.assertEqual(event.name, broadcast_event_name)\r\n\r\n            # Run DDP with profiling for a few iterations, then enable profiling\r\n            # for a single pass, and ensure it is recorded. This tests that the\r\n            # thread local state is correctly updated.\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                model.cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=True,\r\n            )\r\n            for _ in range(3):\r\n                loss = net(inp).sum()\r\n                loss.backward()\r\n            # Now enable the profiler.\r\n            with profiler_ctx2 as prof:\r\n                loss = net(inp).sum()\r\n                loss.backward()\r\n\r\n            events = get_profiling_event(all_reduce_event_name, prof, dedup_gpu_user_annotation=True)\r\n            self.assertGreaterEqual(len(events), 1)\r\n            self.assertGreaterEqual(events[0].count, 1)\r\n            self.assertEqual(events[0].name, all_reduce_event_name)\r\n            for event in events:\r\n                self.assertTrue(event.is_async)\r\n            # Ensure searching unused parameters was profiled\r\n            events = get_profiling_event(\"search_unused_parameters\", prof)\r\n            self.assertEqual(len(events), 1)\r\n\r\n            return prof",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _run_uneven_inputs_test(\r\n            self,\r\n            test_case,\r\n            iteration_mapping,\r\n            find_unused_params,\r\n        ):\r\n            model = test_case.model\r\n            inp = test_case.inp\r\n            rank = self.rank\r\n            sync_interval = test_case.sync_interval\r\n            torch.cuda.set_device(rank)\r\n            # Ensure all outstanding GPU work is completed so this test runs independently.\r\n            dist.barrier()\r\n            # Bucket_cap_mb is intentionally low to test allreduce scheduling when\r\n            # there are many buckets.\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                model.cuda(rank),\r\n                device_ids=[rank],\r\n                bucket_cap_mb=1,\r\n                find_unused_parameters=find_unused_params,\r\n            )\r\n            # Register hook if specified\r\n            if test_case.hook is not None:\r\n                net.register_comm_hook(test_case.state, test_case.hook)\r\n                print(f\"registered hook {test_case.hook}\")\r\n\r\n            # Determine num iters for this rank via the passed in mapping.\r\n            num_iters = iteration_mapping[rank]\r\n            # If we throw when earliest rank terminates, we should ensure\r\n            # that we iterate for that minimum number of times.\r\n            num_iters_tensor = torch.tensor(\r\n                [num_iters], device=torch.cuda.current_device()\r\n            )\r\n            dist.all_reduce(num_iters_tensor, op=dist.ReduceOp.MIN)\r\n            min_num_iters = num_iters_tensor.item()\r\n            total_iters = 0\r\n            if test_case.throw_on_early_termination:\r\n                if min_num_iters == num_iters:\r\n                    # Early termination rank(s)\r\n                    exception_ctx = self.assertRaisesRegex(\r\n                        RuntimeError, f\"Rank {self.rank} exhausted all inputs\"\r\n                    )\r\n                else:\r\n                    # Non early termination rank\r\n                    exception_ctx = self.assertRaisesRegex(\r\n                        RuntimeError,\r\n                        \"Detected at least one rank that exhausted inputs.\",\r\n                    )\r\n            else:\r\n                exception_ctx = nullcontext()\r\n            with exception_ctx:\r\n                with net.join(\r\n                    throw_on_early_termination=test_case.throw_on_early_termination\r\n                ):\r\n                    for i in range(num_iters):\r\n                        # Use model.no_sync() to disable grad synchronization every\r\n                        # sync_interval.\r\n                        if i % sync_interval != 0:\r\n                            context = net.no_sync()\r\n                        else:\r\n                            context = nullcontext()\r\n                        with context:\r\n                            if isinstance(inp, tuple):\r\n                                loss = net(*inp).sum()\r\n                            else:\r\n                                loss = net(inp).sum()\r\n                            loss.backward()\r\n                            self._model_step(net)\r\n                            # Ensure completion of GPU kernels (including allreduce). If the\r\n                            # join API is not properly implemented, then this should hang\r\n                            # since the allreduce will hang.\r\n                            torch.cuda.synchronize(device=rank)\r\n                        total_iters += 1\r\n            if test_case.throw_on_early_termination:\r\n                # Ensure we iterated min_num_iters times.\r\n                self.assertEqual(total_iters, min_num_iters)\r\n            else:\r\n                # Ensure we iterated at least min_num_iters times.\r\n                self.assertGreaterEqual(total_iters, min_num_iters)\r\n\r\n            # Ensure completion of all GPU kernels.\r\n            torch.cuda.synchronize(device=rank)\r\n            # When throwing on early rank termination, we do not\r\n            # broadcast model state from an authoritative rank. All models\r\n            # should already be in sync.\r\n            if not test_case.throw_on_early_termination:\r\n                self.assertTrue(net._authoritative_rank)\r\n                # All ranks should have agreed on the same authoritative_rank!\r\n                final_rank_tensor = torch.tensor(\r\n                    [net._authoritative_rank], device=self.rank\r\n                )\r\n                tensor_list = [\r\n                    torch.zeros_like(final_rank_tensor)\r\n                    for _ in range(dist.get_world_size())\r\n                ]\r\n                dist.all_gather(tensor_list, final_rank_tensor)\r\n                max_rank = dist.get_world_size() - 1\r\n                self.assertSetEqual(\r\n                    {max_rank}, {tensor.item() for tensor in tensor_list}\r\n                )\r\n                # Ensure that all models are the same across ranks after all have joined.\r\n                self.validate_net_equivalence(net)\r\n                # Ensure that running with DDP uneven inputs was logged.\r\n                ddp_logging_data = net._get_ddp_logging_data()\r\n                self.assertTrue(ddp_logging_data.get(\"join_uneven_inputs\"))\r\n                dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_uneven_inputs_stop_iteration_sync_bn(self):\r\n            # Tests that uneven inputs join handler correctly throws StopIteration\r\n            # for models with SyncBN or general collective comm when\r\n            # throw_on_early_termination=True.\r\n            class ModelWithComm(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.lin = nn.Linear(2, 40, bias=False)\r\n\r\n                def forward(self, x):\r\n                    x = self.lin(x)\r\n                    dist.all_reduce(x)\r\n                    return x\r\n\r\n            torch.cuda.set_device(self.rank)\r\n            model_bn = BN_NET\r\n            model_bn = nn.SyncBatchNorm.convert_sync_batchnorm(\r\n                copy.deepcopy(model_bn)\r\n            ).cuda(self.rank)\r\n            comm_model = ModelWithComm().cuda(self.rank)\r\n            model_input = torch.randn(10, 2).cuda(torch.cuda.current_device())\r\n\r\n            for model in [model_bn, comm_model]:\r\n                model = torch.nn.parallel.DistributedDataParallel(\r\n                    model,\r\n                    device_ids=[self.rank],\r\n                )\r\n                min_num_iters = 5\r\n                if self.rank != 0:\r\n                    # Early termination rank(s)\r\n                    num_iters = min_num_iters\r\n                    exception_ctx = self.assertRaisesRegex(\r\n                        RuntimeError, f\"Rank {self.rank} exhausted all inputs\"\r\n                    )\r\n                else:\r\n                    # Non early termination rank\r\n                    num_iters = min_num_iters * 2\r\n                    exception_ctx = self.assertRaisesRegex(\r\n                        RuntimeError,\r\n                        \"Detected at least one rank that exhausted inputs.\",\r\n                    )\r\n                n = 0\r\n                with exception_ctx:\r\n                    with model.join(throw_on_early_termination=True):\r\n                        for _ in range(num_iters):\r\n                            loss = model(model_input).sum()\r\n                            loss.backward()\r\n                            self._model_step(model)\r\n                            n += 1\r\n\r\n                self.assertEqual(n, min_num_iters)\r\n                # Verify model equivalence\r\n                self.validate_net_equivalence(model)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_uneven_input_join_disable(self):\r\n            # tests that if net.join() with enable=False is specified, DDP works as\r\n            # expected with even inputs.\r\n            torch.manual_seed(self.rank)\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                torch.nn.Linear(1, 1).cuda(self.rank), device_ids=[self.rank]\r\n            )\r\n            inp = torch.ones(1) * self.rank\r\n            n_iters = 5\r\n            world_size = dist.get_world_size()\r\n            with net.join(enable=False):\r\n                for _ in range(n_iters):\r\n                    # Clear grads\r\n                    grad = net.module.weight.grad\r\n                    if grad is not None:\r\n                        grad.requires_grad_(False)\r\n                        grad.zero_()\r\n                    out = net(inp)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n                    # Validate gradients to ensure that we divide by the correct\r\n                    # world_size when join mode is disabled.\r\n                    expected_grad = sum(i for i in range(world_size)) / world_size\r\n                    self.assertEqual(net.module.weight.grad.item(), expected_grad)\r\n\r\n            join_config = net._join_config\r\n            self.assertFalse(join_config.enable)\r\n            self.validate_net_equivalence(net)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_shared_grad_acc_unused_params(self):\r\n            # When find_unused_parameters=True, ensure we mark unused parameters\r\n            # even if they share gradient accumulators.\r\n            class ToyModel(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    # net1, bias, and net1.bias are all unused params.\r\n                    self.net1 = nn.Linear(10, 5, bias=False)\r\n                    self.bias = nn.Parameter(torch.zeros(5))\r\n                    # net1.bias and self.bias are names for the same underlying\r\n                    # parameter, so they share the same grad acc. This caused\r\n                    # the bug reported in https://github.com/pytorch/pytorch/issues/41324.\r\n                    self.net1.bias = self.bias\r\n                    self.net2 = nn.Linear(10, 5)\r\n\r\n                def forward(self, x):\r\n                    return self.net2(x).sum()\r\n\r\n            torch.cuda.set_device(self.rank)\r\n            model = ToyModel().to(torch.cuda.current_device())\r\n            for static in [True, False]:\r\n                ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n                    copy.deepcopy(model),\r\n                    device_ids=[self.rank],\r\n                    find_unused_parameters=True,\r\n                    static_graph=static,\r\n                )\r\n                inp = torch.randn(20, 10, device=self.rank)\r\n                for _ in range(6):\r\n                    loss = ddp_model(inp)\r\n                    # To test https://github.com/pytorch/pytorch/issues/61982\r\n                    loss /= 10\r\n                    loss.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_grads_same_across_ranks_with_no_sync(self):\r\n            _group, _group_id, rank = self._init_global_test()\r\n            world_size = dist.get_world_size()\r\n            if world_size < 2:\r\n                self.skipTest(\"This test requires at least two ranks.\")\r\n\r\n            class SimpleConditionalModel(nn.Module):\r\n                # if rank is 0, uses nn1 on the first pass and nn2 on the second pass.\r\n                # else, uses nn3 on the first pass and nn4 on the second pass.\r\n\r\n                def __init__(self, rank):\r\n                    super().__init__()\r\n\r\n                    self.rank = rank\r\n                    self.nn1 = nn.Linear(1, 1)\r\n                    self.nn2 = nn.Linear(1, 1)\r\n                    self.nn3 = nn.Linear(1, 1)\r\n                    self.nn4 = nn.Linear(1, 1)\r\n                    self.state = 0\r\n\r\n                def forward(self, input):\r\n                    if self.state == 0:\r\n                        self.state = 1\r\n                        if self.rank == 0:\r\n                            return self.nn1(input)\r\n                        else:\r\n                            return self.nn3(input)\r\n                    else:\r\n                        self.state = 0\r\n                        if self.rank == 0:\r\n                            return self.nn2(input)\r\n                        else:\r\n                            return self.nn4(input)\r\n\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                SimpleConditionalModel(rank), find_unused_parameters=True\r\n            )\r\n            mse_loss = nn.MSELoss()\r\n            grad_accumulation = 2\r\n\r\n            for microbatch_idx in range(grad_accumulation):\r\n                if microbatch_idx < grad_accumulation - 1:\r\n                    context = model.no_sync\r\n                else:\r\n                    context = nullcontext\r\n\r\n                with context():\r\n                    input = torch.rand((1, ))\r\n                    output = model.forward(input)\r\n                    target = torch.rand((1, ))\r\n\r\n                    loss = mse_loss(output, target)\r\n                    loss.backward()\r\n\r\n            self.assertTrue(\r\n                not any(p.grad is None for p in model.parameters()),\r\n                \"Gradients can't be None for any model parameter.\"\r\n            )\r\n            grads = torch.cat([p.grad.view(-1) for p in model.parameters()])\r\n\r\n            # Gather all gradients to rank 0.\r\n            if rank == 0:\r\n                gathered_grads = [torch.zeros_like(grads) for _ in range(world_size)]\r\n            else:\r\n                gathered_grads = []\r\n\r\n            dist.gather(grads, gather_list=gathered_grads, dst=0)\r\n            if rank == 0:\r\n                for g in gathered_grads[1:]:\r\n                    self.assertTrue(\r\n                        torch.allclose(gathered_grads[0], g),\r\n                        \"Gradients are not the same for all ranks.\"\r\n                    )",
        "labels": [
            "PyTorch Call Method Misused",
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_control_flow_same_across_ranks(self):\r\n            # Control flow that is the same across ranks.\r\n            batch = 20\r\n            dim = 10\r\n\r\n            world_size = dist.get_world_size()\r\n            torch.cuda.set_device(self.rank)\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                ControlFlowToyModel().cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=True,\r\n            )\r\n            random_input = torch.randn(batch, dim, device=self.rank)\r\n            ones_input = torch.ones(batch, dim, device=self.rank)\r\n            for i in range(6):\r\n                if i % 2 == 0:\r\n                    out = model(random_input)\r\n                else:\r\n                    out = model(ones_input)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                # On even iterations, 2nd param goes unused, on odd iterations,\r\n                # it is used.\r\n                local_used_map = model.reducer._get_local_used_map()\r\n                if i % 2 == 0:\r\n                    expected = torch.tensor(\r\n                        [world_size, 0], device=self.rank, dtype=torch.int32\r\n                    )\r\n                else:\r\n                    expected = torch.tensor(\r\n                        [world_size, world_size], device=self.rank, dtype=torch.int32\r\n                    )\r\n\r\n                # Validate parameter usage.\r\n                variable_usage_tensor = local_used_map\r\n                self.assertEqual(variable_usage_tensor, expected)\r\n\r\n            # Validate appropriate error message when DDP is used with\r\n            # find_unused_parameters=False.\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                ControlFlowToyModel().cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=False,\r\n            )\r\n            for i in range(2):\r\n                if i == 0:\r\n                    loss = model(random_input).sum()\r\n                    loss.backward()\r\n                else:\r\n                    try:\r\n                        loss = model(random_input).sum()\r\n                        loss.backward()\r\n                    except RuntimeError as e:\r\n                        msg = str(e)\r\n                        verify_ddp_error_logged(model, msg)\r\n                        # 2nd linear layer is unused\r\n                        unused_param_index = 1\r\n                        expected_strs = [\r\n                            ddp_prev_reduction_unfinished_str,\r\n                            ddp_recommend_find_unused_params_str,\r\n                            ddp_outputs_not_used_in_loss_str,\r\n                            f\"Parameter indices which did not receive grad for rank {self.rank}: {unused_param_index}\",\r\n                        ]\r\n                        # In debug mode, should show parameters that weren't reduced.\r\n                        # Without debug mode, should show suggestion to use debug mode.\r\n                        if dist.get_debug_level() == dist.DebugLevel.OFF:\r\n                            expected_strs.append(ddp_suggest_debug_mode_str)\r\n                        else:\r\n                            unreduced_params = \", \".join([\"lin2.weight\"])\r\n                            expected_strs.append(\r\n                                f\"did not receive grad for rank {self.rank}: {unreduced_params}\"\r\n                            )\r\n                        for s in expected_strs:\r\n                            self.assertTrue(s in msg, f\"Expected {s} to be in {msg}\")\r\n                        self.assertFalse(ddp_find_unused_params_enabled_str in msg)\r\n                    else:\r\n                        self.assertFalse(True, \"DDP error not raised\")\r\n\r\n            dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_invalid_static_graph(self):\r\n            torch.cuda.set_device(self.rank)\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                ControlFlowToyModel().cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                static_graph=True,\r\n            )\r\n            random_input = torch.randn(20, 10, device=self.rank)\r\n            ones_input = torch.ones(20, 10, device=self.rank)\r\n            # unused parameter in the first iteration got used\r\n            # in second iteration.\r\n            expected_err = \"Your training graph has changed in this iteration\"\r\n            with self.assertRaisesRegex(RuntimeError, expected_err):\r\n                for i in range(2):\r\n                    if i % 2 == 0:\r\n                        out = model(random_input)\r\n                    else:\r\n                        out = model(ones_input)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n\r\n            verify_ddp_error_logged(model, expected_err)\r\n\r\n            # used parameter in the first iteration got unused\r\n            # in second iteration.\r\n            with self.assertRaisesRegex(\r\n                RuntimeError,\r\n                \"Expected to have finished reduction in the prior iteration \"\r\n                \"before starting a new one. This error indicates that your \"\r\n                \"training graph has changed in this iteration, \"\r\n                \"e.g., one parameter is used in first iteration, \"\r\n                \"but then got unused in the second iteration. \"\r\n                \"this is not compatible with static_graph set to True.\\n\"\r\n                \"Parameter indices which did not receive grad for\",\r\n            ):\r\n                for i in range(2):\r\n                    if i % 2 != 0:\r\n                        out = model(random_input)\r\n                    else:\r\n                        out = model(ones_input)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n\r\n            verify_ddp_error_logged(model, \"Expected to have finished reduction\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_control_flow_different_across_ranks(self):\r\n            # Control flow that is different across ranks.\r\n            batch = 20\r\n            dim = 10\r\n\r\n            class ToyModel(nn.Module):\r\n                def __init__(self, rank):\r\n                    super().__init__()\r\n                    self.lin1 = nn.Linear(10, 10, bias=False)\r\n                    self.lin2 = nn.Linear(10, 10, bias=False)\r\n                    self.rank = rank\r\n\r\n                def forward(self, x):\r\n                    # Control-flow that is rank and input dependent for the\r\n                    # model.\r\n                    use_second_layer = (\r\n                        torch.equal(x, torch.ones(batch, dim, device=x.device))\r\n                        and self.rank == 1\r\n                    )\r\n\r\n                    if use_second_layer:\r\n                        return self.lin2(F.relu(self.lin1(x)))\r\n                    else:\r\n                        return F.relu(self.lin1(x))\r\n\r\n            world_size = dist.get_world_size()\r\n            torch.cuda.set_device(self.rank)\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                ToyModel(self.rank).cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=True,\r\n            )\r\n            random_input = torch.randn(batch, dim, device=self.rank)\r\n            ones_input = torch.ones(batch, dim, device=self.rank)\r\n            for i in range(6):\r\n                if i % 2 == 0:\r\n                    out = model(random_input)\r\n                else:\r\n                    out = model(ones_input)\r\n                loss = out.sum()\r\n                loss.backward()\r\n                # On even iterations, 2nd param goes unused, on odd iterations,\r\n                # it is used only on rank 1.\r\n                local_used_map = model.reducer._get_local_used_map()\r\n\r\n                if i % 2 == 0:\r\n                    expected = torch.tensor(\r\n                        [world_size, 0], device=self.rank, dtype=torch.int32\r\n                    )\r\n                else:\r\n                    expected = torch.tensor(\r\n                        [world_size, 1], device=self.rank, dtype=torch.int32\r\n                    )\r\n\r\n                variable_usage_tensor = local_used_map\r\n                # Validate parameter usage. On odd iterations, 2nd param is only\r\n                # used on rank 1.\r\n                self.assertEqual(variable_usage_tensor, expected)\r\n\r\n            # Validate appropriate error message when DDP is used with\r\n            # find_unused_parameters=False.\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                ToyModel(self.rank).cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=False,\r\n            )\r\n            for i in range(2):\r\n                if i == 0:\r\n                    loss = model(random_input).sum()\r\n                    loss.backward()\r\n                else:\r\n                    try:\r\n                        loss = model(random_input).sum()\r\n                        loss.backward()\r\n                    except RuntimeError as e:\r\n                        msg = str(e)\r\n                        verify_ddp_error_logged(model, msg)\r\n                        unused_param_index = 1\r\n                        expected_strs = [\r\n                            ddp_prev_reduction_unfinished_str,\r\n                            ddp_recommend_find_unused_params_str,\r\n                            ddp_outputs_not_used_in_loss_str,\r\n                            f\"Parameter indices which did not receive grad for rank {self.rank}: {unused_param_index}\",\r\n                        ]\r\n                        # In debug mode, should show parameters that weren't reduced.\r\n                        # Without debug mode, should show suggestion to use debug mode.\r\n                        if dist.get_debug_level() == dist.DebugLevel.OFF:\r\n                            expected_strs.append(ddp_suggest_debug_mode_str)\r\n                        else:\r\n                            unreduced_params = \", \".join([\"lin2.weight\"])\r\n                            expected_strs.append(\r\n                                f\"did not receive grad for rank {self.rank}: {unreduced_params}\"\r\n                            )\r\n                        for s in expected_strs:\r\n                            self.assertTrue(s in msg, f\"Expected {s} to be in {msg}\")\r\n                        self.assertFalse(ddp_find_unused_params_enabled_str in msg)\r\n                    else:\r\n                        self.assertFalse(True, \"DDP error not raised\")\r\n\r\n            dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_output_unused_in_loss(self, module_cls, gradient_as_bucket_view):\r\n            model = module_cls()\r\n            local_net = copy.deepcopy(model)\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                copy.deepcopy(model).cuda(self.rank),\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=True,\r\n            )\r\n\r\n            # Tests that certain parameters not getting gradient since the\r\n            # output is unused in loss computation is supported. Specifically,\r\n            # checks that the grads remain unchanged and are the same as local\r\n            # training.\r\n            inp = torch.randn(10, 10)\r\n\r\n            # Ensure that if a param is not used in loss computation, its\r\n            # gradient is untouched, i.e. if it is None before it is None after,\r\n            # not zero.\r\n            if module_cls == DictOutputModule:\r\n                a, b = local_net(inp)[\"predictions\"]\r\n                a_dist, b_dist = net(inp)[\"predictions\"]\r\n            else:\r\n                a, b = local_net(inp)\r\n                a_dist, b_dist = net(inp)\r\n\r\n            loss_dist = b_dist.sum()\r\n            loss_dist.backward()\r\n\r\n            # Ensure that gradient corresponding to parameter \"a\" was not\r\n            # touched, i.e. it is None and matches the local grad.\r\n            if module_cls == DictOutputModule:\r\n                self.assertTrue(net.module.module.a.weight.grad is None)\r\n                self.assertEqual(\r\n                    net.module.module.a.weight.grad, local_net.module.a.weight.grad\r\n                )\r\n            else:\r\n                self.assertTrue(net.module.a.weight.grad is None)\r\n                self.assertEqual(net.module.a.weight.grad, local_net.a.weight.grad)\r\n\r\n            saved_a_local_grad = None\r\n            saved_a_dist_grad = None\r\n            net.zero_grad()\r\n            local_net.zero_grad()\r\n            for i in range(6):\r\n                if module_cls == DictOutputModule:\r\n                    a, b = local_net(inp)[\"predictions\"]\r\n                    a_dist, b_dist = net(inp)[\"predictions\"]\r\n                else:\r\n                    a, b = local_net(inp)\r\n                    a_dist, b_dist = net(inp)\r\n                if i < 2:\r\n                    # Use both params in loss computation. Later, \"a\" will go\r\n                    # unused and we check to ensure DDP supports this and\r\n                    # gradients remain the same as local training.\r\n                    t = a @ b\r\n                    t_dist = a_dist @ b_dist\r\n                    loss = t.sum()\r\n                    loss_dist = t_dist.sum()\r\n                else:\r\n                    # Model output \"a\" unused in loss.\r\n                    loss = b.sum()\r\n                    loss_dist = b_dist.sum()\r\n                loss.backward()\r\n                loss_dist.backward()\r\n                if i == 1:\r\n                    # Save grads to compare with them in next iterations.\r\n                    if module_cls == DictOutputModule:\r\n                        saved_a_local_grad = local_net.module.a.weight.grad\r\n                        saved_a_dist_grad = net.module.module.a.weight.grad\r\n                    else:\r\n                        saved_a_local_grad = local_net.a.weight.grad\r\n                        saved_a_dist_grad = net.module.a.weight.grad\r\n                    self.assertEqual(saved_a_local_grad, saved_a_dist_grad)\r\n                elif i >= 2:\r\n                    # parameter \"a\" of both models should be the same and not change\r\n                    if module_cls == DictOutputModule:\r\n                        self.assertEqual(\r\n                            net.module.module.a.weight.grad, saved_a_dist_grad\r\n                        )\r\n                        self.assertEqual(\r\n                            local_net.module.a.weight.grad, saved_a_local_grad\r\n                        )\r\n                    else:\r\n                        self.assertEqual(net.module.a.weight.grad, saved_a_dist_grad)\r\n                        self.assertEqual(local_net.a.weight.grad, saved_a_local_grad)\r\n\r\n                # Verify grads are the same\r\n                for (local_param, dist_param) in zip(\r\n                    local_net.parameters(), net.parameters()\r\n                ):\r\n                    local_grad = local_param.grad\r\n                    dist_grad = dist_param.grad\r\n                    self.assertEqual(local_grad, dist_grad)\r\n\r\n            dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_different_graph_across_ranks(\r\n            self, find_unused_parameters=False, static_graph=False\r\n        ):\r\n            class ToyModel(nn.Module):\r\n                def __init__(self, rank):\r\n                    super().__init__()\r\n                    self.lin1 = nn.Linear(10, 10, bias=False)\r\n                    self.lin2 = nn.Linear(10, 10, bias=False)\r\n                    self.rank = rank\r\n\r\n                def forward(self, x):\r\n                    if self.rank == 0:\r\n                        return self.lin2(F.relu(self.lin1(x)))\r\n                    else:\r\n                        return F.relu(self.lin1(x))\r\n\r\n            torch.manual_seed(31415)\r\n            torch.cuda.set_device(self.rank)\r\n            model = ToyModel(self.rank).cuda(self.rank)\r\n            ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=find_unused_parameters,\r\n                gradient_as_bucket_view=True,\r\n                static_graph=static_graph,\r\n            )\r\n            random_input = torch.randn(20, 10, device=self.rank)\r\n            for _ in range(10):\r\n                out = ddp_model(random_input)\r\n                loss = out.sum()\r\n                loss.backward()\r\n            return ddp_model",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_ddp_multiple_nested_unused_params_error(self, ignore_sparse):\r\n            debug_mode_off = dist.get_debug_level() == dist.DebugLevel.OFF\r\n\r\n            class SubModule(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.embedding_net = EmbeddingNetDifferentParams(0)\r\n                    self.lin = TwoLinLayerNet()\r\n                    self.bn = BatchNormNet()\r\n                    self.lin_layer = nn.Linear(4, 10, bias=False)\r\n\r\n                def forward(self, x):\r\n                    x = self.bn(x)\r\n                    x = self.lin_layer(x)\r\n                    x = self.lin.a(x)  # self.lin.b param unused\r\n                    # EmbeddingNetDifferentParams entirely unused: self.embedding_net.embedding and\r\n                    # self.embedding_net.lin unused.\r\n                    return x\r\n\r\n            class MyModel(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.sub_module = SubModule()\r\n\r\n                def forward(self, x):\r\n                    return self.sub_module(x)\r\n\r\n            model = MyModel()\r\n            sparse_embedding_fqns = []\r\n            if ignore_sparse:\r\n                for module_name, module in model.named_modules():\r\n                    if module == model.sub_module.embedding_net.embedding:\r\n                        for parameter_name, _param in module.named_parameters(recurse=False):\r\n                            fqn = f\"{module_name}.{parameter_name}\"\r\n                            sparse_embedding_fqns.append(fqn)\r\n\r\n                torch.nn.parallel.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(\r\n                    model, sparse_embedding_fqns\r\n                )\r\n                unused_modules = [\r\n                    model.sub_module.embedding_net.lin,\r\n                    model.sub_module.lin.b,\r\n                ]\r\n            else:\r\n                unused_modules = list(model.sub_module.embedding_net.modules()) + [\r\n                    model.sub_module.lin.b,\r\n                ]\r\n\r\n            expected_unused_param_fqns = []\r\n            used_param_fqns = []  # Validate that these don't mistakenly show up.\r\n            fqn_to_param_index = {}\r\n            index = 0\r\n            for module_name, module in model.named_modules():\r\n                for parameter_name, _param in module.named_parameters(recurse=False):\r\n                    fqn = f\"{module_name}.{parameter_name}\"\r\n                    fqn_to_param_index[fqn] = index\r\n                    if fqn not in sparse_embedding_fqns:\r\n                        index += 1\r\n                    if module in unused_modules:\r\n                        expected_unused_param_fqns.append(fqn)\r\n                    else:\r\n                        if (\r\n                            not ignore_sparse\r\n                            or module != model.sub_module.embedding_net.embedding\r\n                        ):\r\n                            used_param_fqns.append(fqn)\r\n\r\n            net = torch.nn.parallel.DistributedDataParallel(\r\n                model.cuda(self.rank),\r\n                device_ids=[self.rank],\r\n            )\r\n            batch, dim = 10, 2\r\n            inp = torch.ones(batch, dim)\r\n            for i in range(2):\r\n                if i == 0:\r\n                    out = net(inp)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n                else:\r\n                    try:\r\n                        out = net(inp)\r\n                        loss = out.sum()\r\n                        loss.backward()\r\n                    except RuntimeError as e:\r\n                        e = str(e)\r\n\r\n                        unused_param_substr = e[e.find(\"did not receive grad\") :]\r\n                        # Validate that each unused param fully qualified name\r\n                        # shows up in error logs. We do this instead of\r\n                        # constructing a joined string since order of parameters\r\n                        # can be different in Reducer. In addition, validate\r\n                        # param indices show up as well.\r\n                        for unused_param_fqn in expected_unused_param_fqns:\r\n                            self.assertTrue(\r\n                                unused_param_fqn in unused_param_substr\r\n                                or debug_mode_off\r\n                            )\r\n                            self.assertTrue(\r\n                                str(fqn_to_param_index[unused_param_fqn])\r\n                                in unused_param_substr,\r\n                                f\"Did not find index {fqn_to_param_index[unused_param_fqn]} for {unused_param_fqn}\",\r\n                            )\r\n\r\n                        # Validate that used param fqns don't show up in error\r\n                        # logs.\r\n                        for used_param_fqn in used_param_fqns:\r\n                            self.assertFalse(used_param_fqn in unused_param_substr)\r\n                        # Validate that ignored param fqns don't show up as unused\r\n                        # (since DDP does not track them)\r\n                        for sparse_param_fqn in sparse_embedding_fqns:\r\n                            self.assertFalse(sparse_param_fqn in unused_param_substr)\r\n                    else:\r\n                        self.assertTrue(False, \"Expected error was not raised!\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_sync_bn_training_vs_eval(self):\r\n            rank = self.rank\r\n            torch.cuda.set_device(rank)\r\n            # Need to set track_running_stats=False, when track_running_stats=True,\r\n            # bn_training is False and sync could not occur in eval model.\r\n            model = nn.SyncBatchNorm(2, momentum=0.99, track_running_stats=False).cuda(\r\n                rank\r\n            )\r\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\r\n            # Test sync occurs in training mode.\r\n            with torch.autograd.profiler.profile() as prof:\r\n                for _ in range(6):\r\n                    inp = torch.randn(10, 2, 4, 4).cuda(rank)\r\n                    out = model(inp)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n\r\n            # SyncBN allgathers stats across all ranks, so verify call to\r\n            # all_gather in profiler.\r\n            if BACKEND == \"nccl\":\r\n                all_gather_calls = get_profiling_event(\"_all_gather_base\", prof)\r\n            else:\r\n                all_gather_calls = get_profiling_event(\"all_gather\", prof)\r\n            self.assertNotEqual([], all_gather_calls)\r\n\r\n            # Only do inference on one rank. If SyncBN did collective stats sync,\r\n            # this would hang/error.\r\n            model_inference = model.module\r\n            if self.rank == 0:\r\n                model_inference.eval()\r\n                with torch.autograd.profiler.profile() as prof:\r\n                    for _ in range(6):\r\n                        inp = torch.randn(10, 2, 4, 4).cuda(rank)\r\n                        out = model_inference(inp)\r\n                        loss = out.sum()\r\n                        loss.backward()\r\n\r\n                # Ensure sync does not occur in eval() mode.\r\n                if BACKEND == \"nccl\":\r\n                    all_gather_calls = get_profiling_event(\"_all_gather_base\", prof)\r\n                else:\r\n                    all_gather_calls = get_profiling_event(\"all_gather\", prof)\r\n                self.assertEqual([], all_gather_calls)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_static_graph_nested_types(self):\r\n            # Tests for static graph training when outputs are not just tensors\r\n            # but can be (nested) tuple, list, dict, etc.\r\n            rank = self.rank\r\n            torch.cuda.set_device(rank)\r\n\r\n            class NestedOutputModule(torch.nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.lin = nn.Linear(100, 1, bias=False)\r\n\r\n                def forward(self, inp, output_type):\r\n                    if output_type == \"tuple\":\r\n                        return (\r\n                            self.lin(inp),\r\n                            (\r\n                                self.lin(inp),\r\n                                self.lin(inp),\r\n                            ),\r\n                        )\r\n                    elif output_type == \"list\":\r\n                        return [\r\n                            self.lin(inp),\r\n                            [\r\n                                self.lin(inp),\r\n                                self.lin(inp),\r\n                            ],\r\n                        ]\r\n                    elif output_type == \"dict\":\r\n                        return {\r\n                            \"a\": self.lin(inp),\r\n                            \"b\": {\r\n                                \"c\": self.lin(inp),\r\n                            },\r\n                        }\r\n\r\n            def get_loss(model_output):\r\n                loss = 0.0\r\n                if isinstance(model_output, torch.Tensor):\r\n                    return model_output.sum()\r\n                elif isinstance(model_output, dict):\r\n                    for value in model_output.values():\r\n                        loss += get_loss(value)\r\n                elif isinstance(model_output, (tuple, list)):\r\n                    for x in model_output:\r\n                        loss += get_loss(x)\r\n                else:\r\n                    raise ValueError(f\"Unknown model output type {type(model_output)}\")\r\n                return loss\r\n\r\n            model = NestedOutputModule().cuda(rank)\r\n            model_static_graph = copy.deepcopy(model)\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[rank],\r\n            )\r\n            model_static_graph = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[rank],\r\n                static_graph=True,\r\n            )\r\n            inp = torch.randn(10, 100)\r\n            type_mapping = {\r\n                \"list\": list,\r\n                \"tuple\": tuple,\r\n                \"dict\": dict,\r\n            }\r\n            for output_type in type_mapping.keys():\r\n                for _ in range(6):\r\n                    out = model(inp, output_type=output_type)\r\n                    loss = get_loss(out)\r\n                    loss.backward()\r\n                    self._model_step(model)\r\n                    out_static = model_static_graph(inp, output_type=output_type)\r\n                    self.assertTrue(isinstance(out_static, type_mapping[output_type]))\r\n                    loss_static = get_loss(out_static)\r\n                    loss_static.backward()\r\n                    self._model_step(model_static_graph)\r\n                    for (p, p_static) in zip(\r\n                        model.parameters(), model_static_graph.parameters()\r\n                    ):\r\n                        self.assertEqual(p, p_static)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_returns_tensor_with_no_grad(self):\r\n            # Tests case where module returns tensor that does not require grad.\r\n            torch.cuda.set_device(self.rank)\r\n\r\n            class MyModel(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.fc1 = nn.Linear(10, 10, bias=False)\r\n                    self.fc2 = nn.Linear(10, 10, bias=False)\r\n\r\n                def forward(self, x):\r\n                    x = self.fc2(F.relu(self.fc1(x)))\r\n                    y = x.clone()\r\n                    x = x.detach()\r\n                    assert not x.requires_grad\r\n                    return (x, y)\r\n\r\n            model = MyModel().to(self.rank)\r\n            inp = torch.randn(1, 10, device=self.rank)\r\n            for (find_unused, static_graph) in itertools.product(\r\n                [True, False], [True, False]\r\n            ):\r\n                ddp = DistributedDataParallel(\r\n                    model,\r\n                    device_ids=[self.rank],\r\n                    output_device=self.rank,\r\n                    find_unused_parameters=find_unused,\r\n                    static_graph=static_graph,\r\n                )\r\n                for _ in range(6):\r\n                    out = ddp(inp)\r\n                    self.assertFalse(out[0].requires_grad)\r\n                    o = (out[0] + out[1]).sum()\r\n                    o.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_detect_ddp_is_actually_static(self):\r\n            class ToyModel(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.net1 = nn.Linear(10, 10, bias=False)\r\n                    self.net2 = nn.Linear(10, 10)\r\n\r\n                def forward(self, x, find_unused, dynamic):\r\n                    if find_unused:\r\n                        if dynamic:\r\n                            return self.net2(self.net1(x))\r\n                        else:\r\n                            return self.net2(x)\r\n                    else:\r\n                        return self.net2(self.net1(x))\r\n\r\n            # Set of unused parameters don't change across iterations\r\n            torch.cuda.set_device(self.rank)\r\n            model = ToyModel().cuda()\r\n            for find_unused in [True, False]:\r\n                ddp = torch.nn.parallel.DistributedDataParallel(\r\n                    model,\r\n                    device_ids=[self.rank],\r\n                    find_unused_parameters=find_unused,\r\n                )\r\n                inp = torch.randn(1, 10, device=\"cuda\")\r\n                for _ in range(6):\r\n                    out = ddp(inp, find_unused=find_unused, dynamic=False)\r\n                    loss = out.sum()\r\n                    loss.backward()\r\n                    self.assertTrue(ddp.reducer._ddp_graph_static())\r\n\r\n            # Set of unused parameters dynamically change\r\n            ddp = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[self.rank],\r\n                find_unused_parameters=True,\r\n            )\r\n            inp = torch.randn(1, 10, device=\"cuda\")\r\n            for i in range(6):\r\n                out = ddp(inp, find_unused=True, dynamic=i % 2 == 0)\r\n                loss = out.sum()\r\n                loss.backward()\r\n            self.assertFalse(ddp.reducer._ddp_graph_static())",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_ddp_buffer_hook_allreduce(self, return_futures):\r\n            rank = self.rank\r\n            torch.cuda.set_device(rank)\r\n            torch.manual_seed(rank)\r\n            torch.cuda.manual_seed(rank)\r\n\r\n            def buffer_comm_hook(ddp, named_buffers):\r\n                buffers = [buffer for (_, buffer) in named_buffers.items()]\r\n                futs = [\r\n                    dist.all_reduce(\r\n                        buffer, group=ddp.process_group, async_op=True\r\n                    ).get_future()\r\n                    for buffer in buffers\r\n                ]\r\n                if return_futures:\r\n                    return futs\r\n                else:\r\n                    torch.futures.collect_all(futs).wait()\r\n\r\n            hook_pre_fwd = (\r\n                torch.nn.parallel.distributed._BufferCommHookLocation.PRE_FORWARD\r\n            )\r\n            hook_post_fwd = (\r\n                torch.nn.parallel.distributed._BufferCommHookLocation.POST_FORWARD\r\n            )\r\n            for hook_run_location in [\r\n                hook_pre_fwd,\r\n                hook_post_fwd,\r\n            ]:\r\n                model = NetWithBuffers().cuda(rank)\r\n                model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n                    model,\r\n                    device_ids=[self.rank],\r\n                )\r\n                model_ddp._register_buffer_comm_hook(\r\n                    model_ddp, buffer_comm_hook, hook_run_location\r\n                )\r\n                model_ddp_no_hook = torch.nn.parallel.DistributedDataParallel(\r\n                    copy.deepcopy(model),\r\n                    device_ids=[self.rank],\r\n                    broadcast_buffers=False,\r\n                )\r\n                inp = torch.randn(2, 10, device=rank)\r\n                for _ in range(2):\r\n                    loss_hook = model_ddp(inp).sum()\r\n                    # Since buffer reduction is done pre-forward, simulate it for\r\n                    # no hook case here.\r\n                    # Simulate allreduce appropriately depending on hook location.\r\n                    if hook_run_location == hook_pre_fwd:\r\n                        model_no_hook_buffers = list(model_ddp_no_hook.module.buffers())\r\n                        for tensor in model_no_hook_buffers:\r\n                            dist.all_reduce(tensor)\r\n\r\n                    loss_no_hook = model_ddp_no_hook(inp).sum()\r\n                    if hook_run_location == hook_post_fwd:\r\n                        model_no_hook_buffers = list(model_ddp_no_hook.module.buffers())\r\n                        for tensor in model_no_hook_buffers:\r\n                            dist.all_reduce(tensor)\r\n                    torch.cuda.synchronize()\r\n\r\n                    # if return_futures, they are only awaited on by DDP\r\n                    # at the end of the backwards pass for maximum overlap.\r\n                    if not return_futures:\r\n                        self._verify_buffers_equal(model_ddp, model_ddp_no_hook)\r\n                    loss_hook.backward()\r\n                    loss_no_hook.backward()\r\n                    # Note that when custom hooks return futures, this\r\n                    # comparison is not expected to work when hook run location\r\n                    # is pre-forward pass. This is because the hook does async\r\n                    # communication and forward pass modifies the buffer without\r\n                    # appropriate synchronization. Therefore, if returning\r\n                    # futures from custom buffer hooks, it is advised to set\r\n                    # hook run location to post forward.\r\n                    if return_futures and hook_run_location == hook_post_fwd:\r\n                        self._verify_buffers_equal(model_ddp, model_ddp_no_hook)\r\n                dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_broadcast_buffer_via_hook(self):\r\n            # test that _distributed_broadcast_coalesced via registered hook is\r\n            # equivalent to DDP's default broadcast coalesced.\r\n            rank = self.rank\r\n            torch.cuda.set_device(rank)\r\n            torch.manual_seed(rank)\r\n            torch.cuda.manual_seed(rank)\r\n\r\n            def buffer_comm_hook(ddp, named_buffers):\r\n                # named_buffers is a Dict[str, Tensor] representing a mapping\r\n                # from buffer name to buffer.\r\n                buffers = [buffer for (_, buffer) in named_buffers.items()]\r\n                ddp._default_broadcast_coalesced(buffers)\r\n\r\n            model = NetWithBuffers().cuda(rank)\r\n            model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[self.rank],\r\n            )\r\n            model_ddp._register_buffer_comm_hook(model_ddp, buffer_comm_hook)\r\n            model_ddp_no_hook = torch.nn.parallel.DistributedDataParallel(\r\n                copy.deepcopy(model),\r\n                device_ids=[self.rank],\r\n            )\r\n            inp = torch.randn(2, 10, device=rank)\r\n            for _ in range(2):\r\n                loss_hook = model_ddp(inp).sum()\r\n                loss_no_hook = model_ddp_no_hook(inp).sum()\r\n                self._verify_buffers_equal(model_ddp, model_ddp_no_hook)\r\n                loss_hook.backward()\r\n                loss_no_hook.backward()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_broadcast_buffer(self):\r\n            rank = self.rank\r\n            torch.cuda.set_device(rank)\r\n            torch.manual_seed(rank)\r\n            torch.cuda.manual_seed(rank)\r\n\r\n            class NetWithBuffers(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.a = nn.Linear(10, 10, bias=False)\r\n                    self.b = nn.Linear(10, 1, bias=False)\r\n                    self.register_buffer(\"buffer\", torch.randn(1, 2))\r\n\r\n                def forward(self, x):\r\n                    return self.b(self.a(x))\r\n\r\n            model = NetWithBuffers().cuda(rank)\r\n            model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[self.rank],\r\n            )\r\n            inp = torch.randn(2, 10, device=rank)\r\n            for _ in range(2):\r\n                if rank == 0:\r\n                    model_ddp.module.buffer = model_ddp.module.buffer + 1\r\n                loss = model_ddp(inp).sum()\r\n                loss.backward()\r\n                # Ensure all buffers are synchronized.\r\n                bufs = [\r\n                    torch.empty_like(model_ddp.module.buffer)\r\n                    for _ in range(dist.get_world_size())\r\n                ]\r\n                dist.all_gather(bufs, model_ddp.module.buffer)\r\n                rank_0_buf = bufs[0]\r\n                for buf in bufs[1:]:\r\n                    self.assertEqual(rank_0_buf, buf)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_static_graph_multi_forward(self):\r\n            class Net(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.lin = nn.Linear(10, 10)\r\n                    self.relu = nn.ReLU()\r\n\r\n                def forward(self, x):\r\n                    return self.relu(self.lin(x))\r\n\r\n            torch.cuda.set_device(self.rank)\r\n            torch.manual_seed(42 << 1337 % (self.rank + 1))\r\n            model = Net().cuda(self.rank)\r\n            local_model = copy.deepcopy(model)\r\n            model = torch.nn.parallel.DistributedDataParallel(\r\n                model, device_ids=[self.rank], static_graph=True\r\n            )\r\n            inp = torch.ones(2, 10, device=\"cuda\")\r\n            for _ in range(3):\r\n                model.zero_grad()\r\n                local_model.zero_grad()\r\n                a = model(inp)\r\n                b = model(inp)\r\n                loss = a.sum() + b.sum()\r\n                loss.backward()\r\n                # Grads should be equal to a local model that ran through inp\r\n                # `world_size` times and averaged grads\r\n                if self.rank == 0:\r\n                    inp_clone = inp.clone()\r\n                    iters = dist.get_world_size()\r\n                    for _ in range(iters):\r\n                        a = local_model(inp_clone)\r\n                        b = local_model(inp_clone)\r\n                        loss = a.sum() + b.sum()\r\n                        loss.backward()\r\n\r\n                    for p in local_model.parameters():\r\n                        p.grad.data = p.grad / iters\r\n\r\n                    for p_ddp, p_local in zip(\r\n                        model.parameters(),\r\n                        local_model.parameters()\r\n                    ):\r\n                        self.assertTrue(\r\n                            torch.allclose(\r\n                                p_ddp.grad, p_local.grad\r\n                            ),\r\n                            f\"{p_ddp.grad} vs {p_local.grad}\"\r\n                        )\r\n\r\n            dist.barrier()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_compile_static_graph(self):\r\n            \"Tests that DDP works with torch compile when static_graph=True\"\r\n            model = torch.nn.Linear(10, 10).cuda(self.rank)\r\n            model_clone = copy.deepcopy(model)\r\n            ddp = torch.nn.parallel.DistributedDataParallel(\r\n                model,\r\n                device_ids=[self.rank],\r\n            )\r\n            ddp_static = torch.nn.parallel.DistributedDataParallel(\r\n                model_clone,\r\n                device_ids=[self.rank],\r\n                static_graph=True\r\n            )\r\n            ddp = torch.compile(ddp)\r\n            ddp_static = torch.compile(ddp_static)\r\n            input = torch.rand(10, 10).cuda(self.rank)\r\n            # verify output and gradient parity\r\n            for _ in range(6):\r\n                out_ddp = ddp(input).sum()\r\n                out_ddp_static = ddp_static(input).sum()\r\n                self.assertEqual(out_ddp, out_ddp_static)\r\n                out_ddp.backward()\r\n                out_ddp_static.backward()\r\n                for p1, p2 in zip(ddp.parameters(), ddp_static.parameters()):\r\n                    self.assertEqual(p1.grad, p2.grad)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_dist_autograd_sync_streams(self):\r\n\r\n        options = self.rpc_backend_options\r\n        dst = worker_name((self.rank + 1) % self.world_size)\r\n\r\n        # The reverse of this device mapping should be used for the backward pass.\r\n        options.set_device_map(dst, {self.rank: (self.rank + 1) % self.world_size})\r\n\r\n        rpc.init_rpc(\r\n            name=worker_name(self.rank),\r\n            backend=self.rpc_backend,\r\n            rank=self.rank,\r\n            world_size=self.world_size,\r\n            rpc_backend_options=options,\r\n        )\r\n\r\n        remote_compute = rpc.remote(dst, TensorPipeCudaDistAutogradTest.MyRemoteCompute)\r\n        local_compute = TensorPipeCudaDistAutogradTest.MyLocalCompute(remote_compute)\r\n        for _ in range(10):\r\n            input = torch.rand([1000, 10000], device=self.rank, requires_grad=True)\r\n            # Run local autograd\r\n            result = input * 2.0\r\n            r = random.random()\r\n            loss = result.sum() * r\r\n            loss.backward()\r\n\r\n            # Run distributed autograd\r\n            with dist_autograd.context() as context_id:\r\n                result = local_compute(input)\r\n                loss = result.sum() * r\r\n                dist_autograd.backward(context_id, [loss])\r\n\r\n                # Compare grads.\r\n                grads = dist_autograd.get_gradients(context_id)\r\n                self.assertEqual(input.grad, grads[input])\r\n\r\n        rpc.shutdown()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def _test_dist_optim_base(self, optim_cls, *args, **kwargs):\r\n        # local version\r\n        module1 = MyModule()\r\n        module2 = MyModule()\r\n        params = [module1.get_w(), module2.get_w()]\r\n        local_optim = optim_cls(params, *args, **kwargs)\r\n\r\n        old_w1 = module1.w.detach().clone()\r\n        old_w2 = module2.w.detach().clone()\r\n\r\n        g_cpu = torch.Generator()\r\n        g_cpu.manual_seed(0)\r\n        t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n        t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n        output1 = module1.forward(t2)\r\n        output2 = module2.forward(output1)\r\n        loss = torch.add(output2, t1).sum()\r\n\r\n        loss.backward()\r\n        local_optim.step()\r\n\r\n        # distributed version\r\n        owner1 = f\"worker{(self.rank + 1) % self.world_size:d}\"\r\n        owner2 = f\"worker{(self.rank + 2) % self.world_size:d}\"\r\n\r\n        remote_module1 = rpc.remote(owner1, MyModule)\r\n        remote_module2 = rpc.remote(owner2, MyModule)\r\n        remote_param1 = remote_method(MyModule.get_w, remote_module1)\r\n        remote_param2 = remote_method(MyModule.get_w, remote_module2)\r\n\r\n        # sanity check: local and remote initial weights should match\r\n        self.assertEqual(old_w1, remote_param1.to_here())\r\n        self.assertEqual(old_w2, remote_param2.to_here())\r\n\r\n        dist_optim = DistributedOptimizer(\r\n            optim_cls, [remote_param1, remote_param2], *args, **kwargs\r\n        )\r\n\r\n        with dist_autograd.context() as context_id:\r\n            g_cpu.manual_seed(0)\r\n            t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n            t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n            output1 = rpc_async_method(MyModule.forward, remote_module1, t2)\r\n            output2 = rpc_async_method(MyModule.forward, remote_module2, output1.wait())\r\n            loss = torch.add(output2.wait(), t1)\r\n\r\n            dist_autograd.backward(context_id, [loss.sum()])\r\n            dist_optim.step(context_id)\r\n\r\n            new_w1 = rpc_async_method(MyModule.get_w, remote_module1).wait()\r\n            new_w2 = rpc_async_method(MyModule.get_w, remote_module2).wait()\r\n\r\n            # ensure optimizer changed weights\r\n            self.assertNotEqual(old_w1, new_w1)\r\n            self.assertNotEqual(old_w2, new_w2)\r\n            # ensure local equals remote\r\n            self.assertEqual(new_w1, module1.get_w())\r\n            self.assertEqual(new_w2, module2.get_w())",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _test_dist_optim_none_grads(self, optim_cls, *args, **kwargs):\r\n        # local version\r\n        module1 = MyModule()\r\n        module2 = MyModule(requires_grad=False)\r\n        params = [module1.get_w(), module2.get_w()]\r\n        local_optim = optim_cls(params, *args, **kwargs)\r\n\r\n        old_w1 = module1.w.detach().clone()\r\n        old_w2 = module2.w.detach().clone()\r\n\r\n        g_cpu = torch.Generator()\r\n        g_cpu.manual_seed(0)\r\n        t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n        t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n        output1 = module1.forward(t2)\r\n        output2 = module2.forward(output1)\r\n        loss = torch.add(output2, t1).sum()\r\n\r\n        loss.backward()\r\n        local_optim.step()\r\n\r\n        # distributed version\r\n        owner1 = f\"worker{(self.rank + 1) % self.world_size:d}\"\r\n        owner2 = f\"worker{(self.rank + 2) % self.world_size:d}\"\r\n\r\n        remote_module1 = rpc.remote(owner1, MyModule)\r\n        remote_module2 = rpc.remote(owner2, MyModule, args=(False,))\r\n        remote_param1 = remote_module1.remote().get_w()\r\n        remote_param2 = remote_module2.remote().get_w()\r\n\r\n        # sanity check: local and remote initial weights should match\r\n        self.assertEqual(old_w1, remote_param1.to_here())\r\n        self.assertEqual(old_w2, remote_param2.to_here())\r\n\r\n        dist_optim = DistributedOptimizer(\r\n            optim_cls, [remote_param1, remote_param2], *args, **kwargs\r\n        )\r\n\r\n        with dist_autograd.context() as context_id:\r\n            g_cpu.manual_seed(0)\r\n            t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n            t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\r\n            output1 = remote_module1.rpc_async().forward(t2)\r\n            output2 = remote_module2.rpc_async().forward(output1.wait())\r\n            loss = torch.add(output2.wait(), t1)\r\n\r\n            dist_autograd.backward(context_id, [loss.sum()])\r\n            dist_optim.step(context_id)\r\n\r\n            new_w1 = remote_module1.rpc_async().get_w().wait()\r\n            new_w2 = remote_module2.rpc_async().get_w().wait()\r\n\r\n            # ensure optimizer changed weights for w1\r\n            self.assertNotEqual(old_w1, new_w1)\r\n\r\n            # ensure optimizer not changed weights for w2\r\n            self.assertEqual(old_w2, new_w2)\r\n            # ensure local equals remote\r\n            self.assertEqual(new_w1, module1.get_w())\r\n            self.assertEqual(new_w2, module2.get_w())",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_DistributedDataParallel_non_default_stream(self):\r\n            stream = torch.cuda.Stream(self.rank)\r\n            rank = self.rank\r\n            with torch.cuda.stream(stream):\r\n                net = torch.nn.parallel.DistributedDataParallel(\r\n                    torch.nn.Linear(1, 1, bias=False).cuda(rank), device_ids=[rank]\r\n                )\r\n                for i in range(1000):\r\n                    # Clear gradients manually\r\n                    grad = net.module.weight.grad\r\n                    if grad is not None:\r\n                        grad.requires_grad_(False)\r\n                        grad.zero_()\r\n                    # Forward + BW\r\n                    batch = torch.tensor([rank]).float().cuda(rank)\r\n                    loss = net(batch).sum()\r\n                    loss.backward()\r\n                    # For each worker, the gradient on the weight should be worker_rank.\r\n                    grad = net.module.weight.grad\r\n                    avg = grad.clone()\r\n                    # All-reducing the gradient averages should give us the gradient\r\n                    # average. If not, then one of the workers has not correctly\r\n                    # written back the averaged gradient before this all-reduce call.\r\n                    dist.all_reduce(avg)\r\n                    world_size = int(os.environ[\"WORLD_SIZE\"])\r\n                    avg.div_(world_size)\r\n                    expected_grad = sum(i for i in range(world_size)) / world_size\r\n                    self.assertEqual(\r\n                        avg[0, 0],\r\n                        expected_grad,\r\n                        msg=f\"Expected gradient of {expected_grad} but got {avg} on rank {self.rank}\",\r\n                    )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_ddp_create_graph(self):\r\n            class Model(nn.Module):\r\n                def __init__(self) -> None:\r\n                    super().__init__()\r\n                    self.p = nn.Parameter(torch.tensor(1.0))\r\n\r\n                def forward(self):\r\n                    return self.p.pow(2)\r\n\r\n            model = Model()\r\n            ddp_model = torch.nn.parallel.DistributedDataParallel(model)\r\n            for _ in range(6):\r\n                # Verify DDP doesn't throw when ran with create_graph=True.\r\n                # Although we do warn about potential issues, please see\r\n                # https://github.com/pytorch/pytorch/issues/63929 for details.\r\n                ddp_model().backward(create_graph=True)\r\n                # grad tensors should require grad.\r\n                self.assertTrue(\r\n                    all(param.requires_grad for param in ddp_model.parameters())\r\n                )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ddp_comm_hook_logging(self):\r\n            hooks = [\r\n                default.allreduce_hook,\r\n                default.fp16_compress_hook,\r\n                powerSGD.powerSGD_hook,\r\n                powerSGD.batched_powerSGD_hook,\r\n                quantization_hooks.quantization_pertensor_hook,\r\n                quantization_hooks.quantization_perchannel_hook,\r\n            ]\r\n\r\n            cpp_builtin_hooks = [\r\n                dist.BuiltinCommHookType.ALLREDUCE,\r\n                dist.BuiltinCommHookType.FP16_COMPRESS,\r\n            ]\r\n\r\n            for hook in hooks:\r\n                ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n                    torch.nn.Linear(1, 1, bias=False).cuda(self.rank),\r\n                    device_ids=[self.rank],\r\n                )\r\n                ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n                # Hook not registered yet, so should be empty\r\n                self.assertEqual(ddp_logging_data.get(\"comm_hook\"), None)\r\n                ddp_model.register_comm_hook(None, hook)\r\n                ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n                self.assertEqual(ddp_logging_data.get(\"comm_hook\"), hook.__qualname__)\r\n\r\n            for hook in cpp_builtin_hooks:\r\n                ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n                    torch.nn.Linear(1, 1, bias=False).cuda(self.rank),\r\n                    device_ids=[self.rank],\r\n                )\r\n                ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n                # Hook not registered yet, so should be empty\r\n                self.assertEqual(ddp_logging_data.get(\"comm_hook\"), None)\r\n                ddp_model._register_builtin_comm_hook(hook)\r\n                ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n                self.assertEqual(ddp_logging_data.get(\"comm_hook\"), str(hook))\r\n\r\n            # No hook registered\r\n            ddp_model = torch.nn.parallel.DistributedDataParallel(\r\n                torch.nn.Linear(1, 1, bias=False).cuda(self.rank),\r\n                device_ids=[self.rank],\r\n            )\r\n            ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n            # Hook not registered yet, so should be empty\r\n            self.assertEqual(ddp_logging_data.get(\"comm_hook\"), None)\r\n            # After second forward pass, hook should still be empty string\r\n            for _ in range(2):\r\n                inp = torch.ones(1, 1, device=self.rank)\r\n                loss = ddp_model(inp).sum()\r\n                loss.backward()\r\n\r\n            ddp_logging_data = ddp_model._get_ddp_logging_data()\r\n            # Note: DETAIL debug mode logs DDP logging data to stdout and\r\n            # thus accesses std::map, which fills in a default value for the\r\n            # type if it didn't exist.\r\n            self.assertEqual(ddp_logging_data.get(\"comm_hook\", \"\"), \"\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_and_evaluate_models(\r\n        self,\r\n        datasets,\r\n        max_depths,\r\n        min_samples_leafs,\r\n        criterion_list,\r\n        feature_columns,\r\n        ranking=False,\r\n    ):\r\n        \"\"\"\r\n        Does a grid search over max_depths, min_samples_leafs, and criterion_list and returns the best model.\r\n        \"\"\"\r\n\r\n        results = []\r\n        best_model = None\r\n        best_model_safe_proba = 0\r\n        best_model_num_correct = 0\r\n        best_model_unsafe_leaves = []\r\n        columns = [\"set\", \"crit\", \"max_depth\", \"min_samples_leaf\"]\r\n        metrics_columns = []\r\n        for max_depth, min_samples_leaf, criterion in itertools.product(\r\n            max_depths, min_samples_leafs, criterion_list\r\n        ):\r\n            print(\r\n                f\"max_depth={max_depth} min_samples_leaf={min_samples_leaf} criterion={criterion}\"\r\n            )\r\n            model = DecisionTreeClassifier(\r\n                max_depth=max_depth,\r\n                min_samples_leaf=min_samples_leaf,\r\n                criterion=criterion,\r\n                random_state=42,\r\n            )\r\n            df_train = datasets[\"train\"]\r\n            df_val = datasets[\"val\"]\r\n            if ranking:\r\n                model.fit(\r\n                    df_train[feature_columns],\r\n                    df_train[\"winner\"],\r\n                    sample_weight=df_train[\"relative_performance\"],\r\n                )\r\n            else:\r\n                model.fit(df_train[feature_columns], df_train[\"winner\"])\r\n\r\n            model = DecisionTree(model, feature_columns)\r\n\r\n            if ranking:\r\n                model.prune(df_train, \"winner\", k=self.ranking_num_choices())\r\n\r\n            unsafe_leaves = self.get_unsafe_leaves(model, df_train, feature_columns)\r\n            predictions, proba, leaf_ids = self.predict(model, df_val, feature_columns)\r\n\r\n            wrong_pct = self.get_allowed_wrong_prediction_pct()\r\n            evaluator = DecisionEvaluator(\r\n                self,\r\n                model,\r\n                predictions,\r\n                df_val,\r\n                proba,\r\n                wrong_pct=wrong_pct,\r\n                unsafe_leaves=unsafe_leaves,\r\n                leaf_ids=leaf_ids,\r\n                k=self.ranking_num_choices(),\r\n                ranking=ranking,\r\n            )\r\n            safe_proba = evaluator.get_safe_proba()\r\n            print(f\"safe_proba={safe_proba}\")\r\n\r\n            def eval(name, df):\r\n                if ranking:\r\n                    # when ranking is enabled, we duplicate each input for each choice that\r\n                    # is almost as good as the best choice\r\n                    # we do not want to evaluate the same input multiple times, so we remove duplicates here\r\n                    df = df[df[\"winner\"] == df[\"actual_winner\"]]\r\n                predictions, proba, leaf_ids = self.predict(model, df, feature_columns)\r\n                evaluator = DecisionEvaluator(\r\n                    self,\r\n                    model,\r\n                    predictions,\r\n                    df,\r\n                    proba,\r\n                    wrong_pct=wrong_pct,\r\n                    threshold=safe_proba,\r\n                    unsafe_leaves=unsafe_leaves,\r\n                    leaf_ids=leaf_ids,\r\n                    k=self.ranking_num_choices(),\r\n                    ranking=ranking,\r\n                )\r\n                return evaluator.get_results()\r\n\r\n            for dataset_name, dataset in datasets.items():\r\n                eval_result: EvalResults = eval(dataset_name, dataset)\r\n                eval_result_metrics = eval_result.to_map()\r\n                if dataset_name == \"val\":\r\n                    num_correct = eval_result.accuracy.num_correct\r\n                    num_wrong = eval_result.accuracy.num_wrong\r\n                    num_total = eval_result.accuracy.total\r\n                    if num_wrong <= num_total * wrong_pct:\r\n                        if num_correct > best_model_num_correct:\r\n                            print(\r\n                                f\"new best model with {num_correct} correct and {num_wrong} wrong\"\r\n                            )\r\n                            best_model = model\r\n                            best_model_num_correct = num_correct\r\n                            best_model_safe_proba = safe_proba\r\n                            best_model_unsafe_leaves = unsafe_leaves\r\n\r\n                result = (dataset_name, criterion, max_depth, min_samples_leaf)\r\n                result += tuple(eval_result_metrics.values())\r\n                results.append(result)\r\n                if len(metrics_columns) == 0:\r\n                    metrics_columns = list(eval_result_metrics.keys())\r\n                    columns += metrics_columns\r\n\r\n        return (\r\n            pd.DataFrame(results, columns=columns),\r\n            best_model,\r\n            best_model_safe_proba,\r\n            best_model_unsafe_leaves,\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def main(cfg):\r\n\r\n    # ============ Retrieve config ============ #\r\n    #############################################\r\n\r\n    # make path absolute\r\n    cfg.model.name_or_path = resolve_name_or_path(cfg.model.name_or_path)\r\n\r\n    # Get some constants: number of iters, grad clip...\r\n    batch_size = cfg.data.batch_size\r\n    num_rollouts_per_epoch = cfg.train.ppo.num_rollouts_per_epoch\r\n    collection_iters = num_rollouts_per_epoch // batch_size\r\n\r\n    grad_clip = cfg.train.grad_clip\r\n    max_epochs = cfg.train.max_epochs\r\n\r\n    ppo_batch_size = cfg.train.ppo.ppo_batch_size\r\n    ppo_num_epochs = cfg.train.ppo.ppo_num_epochs\r\n\r\n    device = cfg.sys.device\r\n\r\n    # ============ Instantiate utils ============ #\r\n    ###############################################\r\n    ctx = setup(cfg.sys)\r\n\r\n    logger = get_logger(\r\n        logger_type=cfg.io.logger,\r\n        logger_name=\"./log\",\r\n        experiment_name=\"torchrlhf-gpt2\",\r\n        wandb_kwargs={\r\n            \"config\": dict(cfg),\r\n            \"project\": cfg.io.project_name,\r\n            \"group\": cfg.io.group_name,\r\n        },\r\n    )\r\n\r\n    # =============== Dataloaders =============== #\r\n    ###############################################\r\n    # We use prompts to get generated data from the generative model\r\n\r\n    train_prompt_loader, val_prompt_loader = get_prompt_loaders(cfg.data, cfg.sys)\r\n\r\n    # ================= Models ================= #\r\n    ##############################################\r\n    # Actor (gen model) - critic (value predictor)\r\n    actor, critic, critic_head, model = init_actor_critic(cfg.model, cfg.sys)\r\n    # Freeze initial model to use as ref\r\n    ref_model = make_ref_model(model, sys_cfg=cfg.sys)\r\n    # Freeze layers of the model -- can be customized\r\n    freeze_layers(model)\r\n\r\n    reward_model = make_reward_model(reward_model_cfg=cfg.reward_model, sys_cfg=cfg.sys)\r\n\r\n    # ================= Loss and optimizer ================= #\r\n    ##########################################################\r\n    loss_fn, advantage = make_loss(actor, critic, critic_head)\r\n\r\n    optimizer, lr_scheduler = make_optimizer(cfg.train, loss_fn)\r\n\r\n    # ================= Replay buffer ================= #\r\n    #####################################################\r\n    rb = make_replay_buffer(cfg.train.ppo, cfg.data)\r\n\r\n    # ================= Data collector ================= #\r\n    ######################################################\r\n    #\r\n    # Because we interact with HuggingFace's transformers models,\r\n    # using a Gym-like API (querying steps etc) introduces some\r\n    # extra code that we can spare.\r\n    #\r\n    kl_scheduler = AdaptiveKLController(init_kl_coef=0.1, target=6, horizon=10000)\r\n    rollout_from_model = RolloutFromModel(\r\n        model,\r\n        ref_model,\r\n        reward_model,\r\n        kl_scheduler=kl_scheduler,\r\n        num_steps=collection_iters,\r\n    )\r\n\r\n    # ================= Evaluation utils ================= #\r\n    ########################################################\r\n    evaluator = make_evaluator(\r\n        ppo_cfg=cfg.train.ppo,\r\n        io_cfg=cfg.io,\r\n        model_cfg=cfg.model,\r\n        train_cfg=cfg.train,\r\n        val_prompt_loader=val_prompt_loader,\r\n        model=model,\r\n        ref_model=ref_model,\r\n        reward_model=reward_model,\r\n        ctx=ctx,\r\n        logger=logger,\r\n    )\r\n\r\n    # ================= Training loop ================= #\r\n    #####################################################\r\n\r\n    stats_logger = TrainLogger(\r\n        collection_iters, log_interval=cfg.io.log_interval, logger=logger\r\n    )\r\n    pbar = tqdm(total=max_epochs * collection_iters)\r\n    for _ in range(max_epochs):\r\n        # ----------------- 1. Collect data, fill replay buffer ----------------- #\r\n        # it's possible we didn't fill the replay buffer in the last iteration if\r\n        # generation stopped early, so we empty first before repopulating\r\n        rb.empty()\r\n        for _ in range(collection_iters):\r\n            batch = next(train_prompt_loader)\r\n            td = rollout_from_model.rollout_from_data(batch)\r\n            with torch.no_grad(), ctx:\r\n                # TODO: moving this to within epoch\r\n                advantage(td)\r\n            rb.extend(flatten_td(td))\r\n            stats_logger(td)\r\n        stats_logger.aggregate()\r\n        stats_logger.log()\r\n\r\n        rollout_from_model.step_scheduler()\r\n\r\n        # ----------------- 2. Feed model ----------------- #\r\n        for batch in rb:\r\n            rb_ppo = make_sub_replay_buffer(batch, batch_size=ppo_batch_size)\r\n            for _ in range(ppo_num_epochs):  # PPO epochs\r\n                optimizer.zero_grad()\r\n                for minibatch in rb_ppo:  # GO over RB\r\n                    minibatch = minibatch.to(device, non_blocking=True)\r\n                    with ctx:\r\n                        loss_vals = loss_fn(minibatch)\r\n                    loss_val = sum(\r\n                        value\r\n                        for key, value in loss_vals.items()\r\n                        if key.startswith(\"loss\")\r\n                    )\r\n                    loss_val.backward()\r\n                    torch.nn.utils.clip_grad_norm_(loss_fn.parameters(), grad_clip)\r\n                optimizer.step()\r\n                if lr_scheduler is not None:\r\n                    lr_scheduler.step()\r\n            pbar.update(1)\r\n\r\n            # ----------------- 3. Possibly evaluate ----------------- #\r\n            evaluator.maybe_evaluate()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(cfg: \"DictConfig\"):  # noqa: F821\r\n    # Device\r\n    cfg.train.device = \"cpu\" if not torch.cuda.device_count() else \"cuda:0\"\r\n    cfg.env.device = cfg.train.device\r\n\r\n    # Seeding\r\n    torch.manual_seed(cfg.seed)\r\n\r\n    # Sampling\r\n    cfg.env.vmas_envs = cfg.collector.frames_per_batch // cfg.env.max_steps\r\n    cfg.collector.total_frames = cfg.collector.frames_per_batch * cfg.collector.n_iters\r\n    cfg.buffer.memory_size = cfg.collector.frames_per_batch\r\n\r\n    # Create env and env_test\r\n    env = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.env.vmas_envs,\r\n        continuous_actions=False,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n    env = TransformedEnv(\r\n        env,\r\n        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\r\n    )\r\n\r\n    env_test = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.eval.evaluation_episodes,\r\n        continuous_actions=False,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n\r\n    # Policy\r\n    net = MultiAgentMLP(\r\n        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n        n_agent_outputs=env.full_action_spec[\"agents\", \"action\"].space.n,\r\n        n_agents=env.n_agents,\r\n        centralised=False,\r\n        share_params=cfg.model.shared_parameters,\r\n        device=cfg.train.device,\r\n        depth=2,\r\n        num_cells=256,\r\n        activation_class=nn.Tanh,\r\n    )\r\n    module = TensorDictModule(\r\n        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\r\n    )\r\n    value_module = QValueModule(\r\n        action_value_key=(\"agents\", \"action_value\"),\r\n        out_keys=[\r\n            env.action_key,\r\n            (\"agents\", \"action_value\"),\r\n            (\"agents\", \"chosen_action_value\"),\r\n        ],\r\n        spec=env.full_action_spec_unbatched,\r\n        action_space=None,\r\n    )\r\n    qnet = SafeSequential(module, value_module)\r\n\r\n    qnet_explore = TensorDictSequential(\r\n        qnet,\r\n        EGreedyModule(\r\n            eps_init=0.3,\r\n            eps_end=0,\r\n            annealing_num_steps=int(cfg.collector.total_frames * (1 / 2)),\r\n            action_key=env.action_key,\r\n            spec=env.full_action_spec_unbatched,\r\n        ),\r\n    )\r\n\r\n    collector = SyncDataCollector(\r\n        env,\r\n        qnet_explore,\r\n        device=cfg.env.device,\r\n        storing_device=cfg.train.device,\r\n        frames_per_batch=cfg.collector.frames_per_batch,\r\n        total_frames=cfg.collector.total_frames,\r\n        postproc=DoneTransform(reward_key=env.reward_key, done_keys=env.done_keys),\r\n    )\r\n\r\n    replay_buffer = TensorDictReplayBuffer(\r\n        storage=LazyTensorStorage(cfg.buffer.memory_size, device=cfg.train.device),\r\n        sampler=SamplerWithoutReplacement(),\r\n        batch_size=cfg.train.minibatch_size,\r\n    )\r\n\r\n    loss_module = DQNLoss(qnet, delay_value=True)\r\n    loss_module.set_keys(\r\n        action_value=(\"agents\", \"action_value\"),\r\n        action=env.action_key,\r\n        value=(\"agents\", \"chosen_action_value\"),\r\n        reward=env.reward_key,\r\n        done=(\"agents\", \"done\"),\r\n        terminated=(\"agents\", \"terminated\"),\r\n    )\r\n    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=cfg.loss.gamma)\r\n    target_net_updater = SoftUpdate(loss_module, eps=1 - cfg.loss.tau)\r\n\r\n    optim = torch.optim.Adam(loss_module.parameters(), cfg.train.lr)\r\n\r\n    # Logging\r\n    if cfg.logger.backend:\r\n        model_name = (\"Het\" if not cfg.model.shared_parameters else \"\") + \"IQL\"\r\n        logger = init_logging(cfg, model_name)\r\n\r\n    total_time = 0\r\n    total_frames = 0\r\n    sampling_start = time.time()\r\n    for i, tensordict_data in enumerate(collector):\r\n        torchrl_logger.info(f\"\\nIteration {i}\")\r\n\r\n        sampling_time = time.time() - sampling_start\r\n\r\n        current_frames = tensordict_data.numel()\r\n        total_frames += current_frames\r\n        data_view = tensordict_data.reshape(-1)\r\n        replay_buffer.extend(data_view)\r\n\r\n        training_tds = []\r\n        training_start = time.time()\r\n        for _ in range(cfg.train.num_epochs):\r\n            for _ in range(cfg.collector.frames_per_batch // cfg.train.minibatch_size):\r\n                subdata = replay_buffer.sample()\r\n                loss_vals = loss_module(subdata)\r\n                training_tds.append(loss_vals.detach())\r\n\r\n                loss_value = loss_vals[\"loss\"]\r\n\r\n                loss_value.backward()\r\n\r\n                total_norm = torch.nn.utils.clip_grad_norm_(\r\n                    loss_module.parameters(), cfg.train.max_grad_norm\r\n                )\r\n                training_tds[-1].set(\"grad_norm\", total_norm.mean())\r\n\r\n                optim.step()\r\n                optim.zero_grad()\r\n                target_net_updater.step()\r\n\r\n        qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\r\n        collector.update_policy_weights_()\r\n\r\n        training_time = time.time() - training_start\r\n\r\n        iteration_time = sampling_time + training_time\r\n        total_time += iteration_time\r\n        training_tds = torch.stack(training_tds)\r\n\r\n        # More logs\r\n        if cfg.logger.backend:\r\n            log_training(\r\n                logger,\r\n                training_tds,\r\n                tensordict_data,\r\n                sampling_time,\r\n                training_time,\r\n                total_time,\r\n                i,\r\n                current_frames,\r\n                total_frames,\r\n                step=i,\r\n            )\r\n\r\n        if (\r\n            cfg.eval.evaluation_episodes > 0\r\n            and i % cfg.eval.evaluation_interval == 0\r\n            and cfg.logger.backend\r\n        ):\r\n            evaluation_start = time.time()\r\n            with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC):\r\n                env_test.frames = []\r\n                rollouts = env_test.rollout(\r\n                    max_steps=cfg.env.max_steps,\r\n                    policy=qnet,\r\n                    callback=rendering_callback,\r\n                    auto_cast_to_device=True,\r\n                    break_when_any_done=False,\r\n                    # We are running vectorized evaluation we do not want it to stop when just one env is done\r\n                )\r\n\r\n                evaluation_time = time.time() - evaluation_start\r\n\r\n                log_evaluation(logger, rollouts, env_test, evaluation_time, step=i)\r\n\r\n        if cfg.logger.backend == \"wandb\":\r\n            logger.experiment.log({}, commit=True)\r\n        sampling_start = time.time()\r\n\r\n    collector.shutdown()\r\n    if not env.is_closed:\r\n        env.close()\r\n    if not env_test.is_closed:\r\n        env_test.close()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(cfg: \"DictConfig\"):  # noqa: F821\r\n    # Device\r\n    cfg.train.device = \"cpu\" if not torch.cuda.device_count() else \"cuda:0\"\r\n    cfg.env.device = cfg.train.device\r\n\r\n    # Seeding\r\n    torch.manual_seed(cfg.seed)\r\n\r\n    # Sampling\r\n    cfg.env.vmas_envs = cfg.collector.frames_per_batch // cfg.env.max_steps\r\n    cfg.collector.total_frames = cfg.collector.frames_per_batch * cfg.collector.n_iters\r\n    cfg.buffer.memory_size = cfg.collector.frames_per_batch\r\n\r\n    # Create env and env_test\r\n    env = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.env.vmas_envs,\r\n        continuous_actions=True,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n    env = TransformedEnv(\r\n        env,\r\n        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\r\n    )\r\n\r\n    env_test = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.eval.evaluation_episodes,\r\n        continuous_actions=True,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n\r\n    # Policy\r\n    actor_net = MultiAgentMLP(\r\n        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n        n_agent_outputs=env.action_spec.shape[-1],\r\n        n_agents=env.n_agents,\r\n        centralised=False,\r\n        share_params=cfg.model.shared_parameters,\r\n        device=cfg.train.device,\r\n        depth=2,\r\n        num_cells=256,\r\n        activation_class=nn.Tanh,\r\n    )\r\n    policy_module = TensorDictModule(\r\n        actor_net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"param\")]\r\n    )\r\n    policy = ProbabilisticActor(\r\n        module=policy_module,\r\n        spec=env.full_action_spec_unbatched,\r\n        in_keys=[(\"agents\", \"param\")],\r\n        out_keys=[env.action_key],\r\n        distribution_class=TanhDelta,\r\n        distribution_kwargs={\r\n            \"low\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.low,\r\n            \"high\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.high,\r\n        },\r\n        return_log_prob=False,\r\n    )\r\n\r\n    policy_explore = TensorDictSequential(\r\n        policy,\r\n        AdditiveGaussianModule(\r\n            spec=env.full_action_spec_unbatched,\r\n            annealing_num_steps=int(cfg.collector.total_frames * (1 / 2)),\r\n            action_key=env.action_key,\r\n            device=cfg.train.device,\r\n        ),\r\n    )\r\n\r\n    # Critic\r\n    module = MultiAgentMLP(\r\n        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1]\r\n        + env.action_spec.shape[-1],  # Q critic takes action and value\r\n        n_agent_outputs=1,\r\n        n_agents=env.n_agents,\r\n        centralised=cfg.model.centralised_critic,\r\n        share_params=cfg.model.shared_parameters,\r\n        device=cfg.train.device,\r\n        depth=2,\r\n        num_cells=256,\r\n        activation_class=nn.Tanh,\r\n    )\r\n    value_module = ValueOperator(\r\n        module=module,\r\n        in_keys=[(\"agents\", \"observation\"), env.action_key],\r\n        out_keys=[(\"agents\", \"state_action_value\")],\r\n    )\r\n\r\n    collector = SyncDataCollector(\r\n        env,\r\n        policy_explore,\r\n        device=cfg.env.device,\r\n        storing_device=cfg.train.device,\r\n        frames_per_batch=cfg.collector.frames_per_batch,\r\n        total_frames=cfg.collector.total_frames,\r\n        postproc=DoneTransform(reward_key=env.reward_key, done_keys=env.done_keys),\r\n    )\r\n\r\n    replay_buffer = TensorDictReplayBuffer(\r\n        storage=LazyTensorStorage(cfg.buffer.memory_size, device=cfg.train.device),\r\n        sampler=SamplerWithoutReplacement(),\r\n        batch_size=cfg.train.minibatch_size,\r\n    )\r\n\r\n    loss_module = DDPGLoss(\r\n        actor_network=policy, value_network=value_module, delay_value=True\r\n    )\r\n    loss_module.set_keys(\r\n        state_action_value=(\"agents\", \"state_action_value\"),\r\n        reward=env.reward_key,\r\n        done=(\"agents\", \"done\"),\r\n        terminated=(\"agents\", \"terminated\"),\r\n    )\r\n    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=cfg.loss.gamma)\r\n    target_net_updater = SoftUpdate(loss_module, eps=1 - cfg.loss.tau)\r\n\r\n    optim = torch.optim.Adam(loss_module.parameters(), cfg.train.lr)\r\n\r\n    # Logging\r\n    if cfg.logger.backend:\r\n        model_name = (\r\n            (\"Het\" if not cfg.model.shared_parameters else \"\")\r\n            + (\"MA\" if cfg.model.centralised_critic else \"I\")\r\n            + \"DDPG\"\r\n        )\r\n        logger = init_logging(cfg, model_name)\r\n\r\n    total_time = 0\r\n    total_frames = 0\r\n    sampling_start = time.time()\r\n    for i, tensordict_data in enumerate(collector):\r\n        torchrl_logger.info(f\"\\nIteration {i}\")\r\n\r\n        sampling_time = time.time() - sampling_start\r\n\r\n        current_frames = tensordict_data.numel()\r\n        total_frames += current_frames\r\n        data_view = tensordict_data.reshape(-1)\r\n        replay_buffer.extend(data_view)\r\n\r\n        training_tds = []\r\n        training_start = time.time()\r\n        for _ in range(cfg.train.num_epochs):\r\n            for _ in range(cfg.collector.frames_per_batch // cfg.train.minibatch_size):\r\n                subdata = replay_buffer.sample()\r\n                loss_vals = loss_module(subdata)\r\n                training_tds.append(loss_vals.detach())\r\n\r\n                loss_value = loss_vals[\"loss_actor\"] + loss_vals[\"loss_value\"]\r\n\r\n                loss_value.backward()\r\n\r\n                total_norm = torch.nn.utils.clip_grad_norm_(\r\n                    loss_module.parameters(), cfg.train.max_grad_norm\r\n                )\r\n                training_tds[-1].set(\"grad_norm\", total_norm.mean())\r\n\r\n                optim.step()\r\n                optim.zero_grad()\r\n                target_net_updater.step()\r\n\r\n        policy_explore[1].step(frames=current_frames)  # Update exploration annealing\r\n        collector.update_policy_weights_()\r\n\r\n        training_time = time.time() - training_start\r\n\r\n        iteration_time = sampling_time + training_time\r\n        total_time += iteration_time\r\n        training_tds = torch.stack(training_tds)\r\n\r\n        # More logs\r\n        if cfg.logger.backend:\r\n            log_training(\r\n                logger,\r\n                training_tds,\r\n                tensordict_data,\r\n                sampling_time,\r\n                training_time,\r\n                total_time,\r\n                i,\r\n                current_frames,\r\n                total_frames,\r\n                step=i,\r\n            )\r\n\r\n        if (\r\n            cfg.eval.evaluation_episodes > 0\r\n            and i % cfg.eval.evaluation_interval == 0\r\n            and cfg.logger.backend\r\n        ):\r\n            evaluation_start = time.time()\r\n            with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC):\r\n                env_test.frames = []\r\n                rollouts = env_test.rollout(\r\n                    max_steps=cfg.env.max_steps,\r\n                    policy=policy,\r\n                    callback=rendering_callback,\r\n                    auto_cast_to_device=True,\r\n                    break_when_any_done=False,\r\n                    # We are running vectorized evaluation we do not want it to stop when just one env is done\r\n                )\r\n\r\n                evaluation_time = time.time() - evaluation_start\r\n\r\n                log_evaluation(logger, rollouts, env_test, evaluation_time, step=i)\r\n\r\n        if cfg.logger.backend == \"wandb\":\r\n            logger.experiment.log({}, commit=True)\r\n        sampling_start = time.time()\r\n    collector.shutdown()\r\n    if not env.is_closed:\r\n        env.close()\r\n    if not env_test.is_closed:\r\n        env_test.close()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(cfg: \"DictConfig\"):  # noqa: F821\r\n    # Device\r\n    cfg.train.device = \"cpu\" if not torch.cuda.device_count() else \"cuda:0\"\r\n    cfg.env.device = cfg.train.device\r\n\r\n    # Seeding\r\n    torch.manual_seed(cfg.seed)\r\n\r\n    # Sampling\r\n    cfg.env.vmas_envs = cfg.collector.frames_per_batch // cfg.env.max_steps\r\n    cfg.collector.total_frames = cfg.collector.frames_per_batch * cfg.collector.n_iters\r\n    cfg.buffer.memory_size = cfg.collector.frames_per_batch\r\n\r\n    # Create env and env_test\r\n    env = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.env.vmas_envs,\r\n        continuous_actions=False,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n    env = TransformedEnv(\r\n        env,\r\n        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\r\n    )\r\n\r\n    env_test = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.eval.evaluation_episodes,\r\n        continuous_actions=False,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n\r\n    # Policy\r\n    net = MultiAgentMLP(\r\n        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n        n_agent_outputs=env.full_action_spec[\"agents\", \"action\"].space.n,\r\n        n_agents=env.n_agents,\r\n        centralised=False,\r\n        share_params=cfg.model.shared_parameters,\r\n        device=cfg.train.device,\r\n        depth=2,\r\n        num_cells=256,\r\n        activation_class=nn.Tanh,\r\n    )\r\n    module = TensorDictModule(\r\n        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\r\n    )\r\n    value_module = QValueModule(\r\n        action_value_key=(\"agents\", \"action_value\"),\r\n        out_keys=[\r\n            env.action_key,\r\n            (\"agents\", \"action_value\"),\r\n            (\"agents\", \"chosen_action_value\"),\r\n        ],\r\n        spec=env.full_action_spec_unbatched,\r\n        action_space=None,\r\n    )\r\n    qnet = SafeSequential(module, value_module)\r\n\r\n    qnet_explore = TensorDictSequential(\r\n        qnet,\r\n        EGreedyModule(\r\n            eps_init=0.3,\r\n            eps_end=0,\r\n            annealing_num_steps=int(cfg.collector.total_frames * (1 / 2)),\r\n            action_key=env.action_key,\r\n            spec=env.full_action_spec_unbatched,\r\n        ),\r\n    )\r\n\r\n    if cfg.loss.mixer_type == \"qmix\":\r\n        mixer = TensorDictModule(\r\n            module=QMixer(\r\n                state_shape=env.observation_spec_unbatched[\r\n                    \"agents\", \"observation\"\r\n                ].shape,\r\n                mixing_embed_dim=32,\r\n                n_agents=env.n_agents,\r\n                device=cfg.train.device,\r\n            ),\r\n            in_keys=[(\"agents\", \"chosen_action_value\"), (\"agents\", \"observation\")],\r\n            out_keys=[\"chosen_action_value\"],\r\n        )\r\n    elif cfg.loss.mixer_type == \"vdn\":\r\n        mixer = TensorDictModule(\r\n            module=VDNMixer(\r\n                n_agents=env.n_agents,\r\n                device=cfg.train.device,\r\n            ),\r\n            in_keys=[(\"agents\", \"chosen_action_value\")],\r\n            out_keys=[\"chosen_action_value\"],\r\n        )\r\n    else:\r\n        raise ValueError(\"Mixer type not in the example\")\r\n\r\n    collector = SyncDataCollector(\r\n        env,\r\n        qnet_explore,\r\n        device=cfg.env.device,\r\n        storing_device=cfg.train.device,\r\n        frames_per_batch=cfg.collector.frames_per_batch,\r\n        total_frames=cfg.collector.total_frames,\r\n    )\r\n\r\n    replay_buffer = TensorDictReplayBuffer(\r\n        storage=LazyTensorStorage(cfg.buffer.memory_size, device=cfg.train.device),\r\n        sampler=SamplerWithoutReplacement(),\r\n        batch_size=cfg.train.minibatch_size,\r\n    )\r\n\r\n    loss_module = QMixerLoss(qnet, mixer, delay_value=True)\r\n    loss_module.set_keys(\r\n        action_value=(\"agents\", \"action_value\"),\r\n        local_value=(\"agents\", \"chosen_action_value\"),\r\n        global_value=\"chosen_action_value\",\r\n        action=env.action_key,\r\n    )\r\n    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=cfg.loss.gamma)\r\n    target_net_updater = SoftUpdate(loss_module, eps=1 - cfg.loss.tau)\r\n\r\n    optim = torch.optim.Adam(loss_module.parameters(), cfg.train.lr)\r\n\r\n    # Logging\r\n    if cfg.logger.backend:\r\n        model_name = (\r\n            \"Het\" if not cfg.model.shared_parameters else \"\"\r\n        ) + cfg.loss.mixer_type.upper()\r\n        logger = init_logging(cfg, model_name)\r\n\r\n    total_time = 0\r\n    total_frames = 0\r\n    sampling_start = time.time()\r\n    for i, tensordict_data in enumerate(collector):\r\n        torchrl_logger.info(f\"\\nIteration {i}\")\r\n\r\n        sampling_time = time.time() - sampling_start\r\n\r\n        # Remove agent dimension from reward (since it is shared in QMIX/VDN)\r\n        tensordict_data.set(\r\n            (\"next\", \"reward\"), tensordict_data.get((\"next\", env.reward_key)).mean(-2)\r\n        )\r\n        del tensordict_data[\"next\", env.reward_key]\r\n        tensordict_data.set(\r\n            (\"next\", \"episode_reward\"),\r\n            tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean(-2),\r\n        )\r\n        del tensordict_data[\"next\", \"agents\", \"episode_reward\"]\r\n\r\n        current_frames = tensordict_data.numel()\r\n        total_frames += current_frames\r\n        data_view = tensordict_data.reshape(-1)\r\n        replay_buffer.extend(data_view)\r\n\r\n        training_tds = []\r\n        training_start = time.time()\r\n        for _ in range(cfg.train.num_epochs):\r\n            for _ in range(cfg.collector.frames_per_batch // cfg.train.minibatch_size):\r\n                subdata = replay_buffer.sample()\r\n                loss_vals = loss_module(subdata)\r\n                training_tds.append(loss_vals.detach())\r\n\r\n                loss_value = loss_vals[\"loss\"]\r\n\r\n                loss_value.backward()\r\n\r\n                total_norm = torch.nn.utils.clip_grad_norm_(\r\n                    loss_module.parameters(), cfg.train.max_grad_norm\r\n                )\r\n                training_tds[-1].set(\"grad_norm\", total_norm.mean())\r\n\r\n                optim.step()\r\n                optim.zero_grad()\r\n                target_net_updater.step()\r\n\r\n        qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\r\n        collector.update_policy_weights_()\r\n\r\n        training_time = time.time() - training_start\r\n\r\n        iteration_time = sampling_time + training_time\r\n        total_time += iteration_time\r\n        training_tds = torch.stack(training_tds)\r\n\r\n        # More logs\r\n        if cfg.logger.backend:\r\n            log_training(\r\n                logger,\r\n                training_tds,\r\n                tensordict_data,\r\n                sampling_time,\r\n                training_time,\r\n                total_time,\r\n                i,\r\n                current_frames,\r\n                total_frames,\r\n                step=i,\r\n            )\r\n\r\n        if (\r\n            cfg.eval.evaluation_episodes > 0\r\n            and i % cfg.eval.evaluation_interval == 0\r\n            and cfg.logger.backend\r\n        ):\r\n            evaluation_start = time.time()\r\n            with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC):\r\n                env_test.frames = []\r\n                rollouts = env_test.rollout(\r\n                    max_steps=cfg.env.max_steps,\r\n                    policy=qnet,\r\n                    callback=rendering_callback,\r\n                    auto_cast_to_device=True,\r\n                    break_when_any_done=False,\r\n                    # We are running vectorized evaluation we do not want it to stop when just one env is done\r\n                )\r\n\r\n                evaluation_time = time.time() - evaluation_start\r\n\r\n                log_evaluation(logger, rollouts, env_test, evaluation_time, step=i)\r\n\r\n        if cfg.logger.backend == \"wandb\":\r\n            logger.experiment.log({}, commit=True)\r\n        sampling_start = time.time()\r\n    collector.shutdown()\r\n    if not env.is_closed:\r\n        env.close()\r\n    if not env_test.is_closed:\r\n        env_test.close()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(cfg: \"DictConfig\"):  # noqa: F821\r\n    # Device\r\n    cfg.train.device = \"cpu\" if not torch.cuda.device_count() else \"cuda:0\"\r\n    cfg.env.device = cfg.train.device\r\n\r\n    # Seeding\r\n    torch.manual_seed(cfg.seed)\r\n\r\n    # Sampling\r\n    cfg.env.vmas_envs = cfg.collector.frames_per_batch // cfg.env.max_steps\r\n    cfg.collector.total_frames = cfg.collector.frames_per_batch * cfg.collector.n_iters\r\n    cfg.buffer.memory_size = cfg.collector.frames_per_batch\r\n\r\n    # Create env and env_test\r\n    env = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.env.vmas_envs,\r\n        continuous_actions=True,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n    env = TransformedEnv(\r\n        env,\r\n        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\r\n    )\r\n\r\n    env_test = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.eval.evaluation_episodes,\r\n        continuous_actions=True,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n\r\n    # Policy\r\n    actor_net = nn.Sequential(\r\n        MultiAgentMLP(\r\n            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n            n_agent_outputs=2 * env.action_spec.shape[-1],\r\n            n_agents=env.n_agents,\r\n            centralised=False,\r\n            share_params=cfg.model.shared_parameters,\r\n            device=cfg.train.device,\r\n            depth=2,\r\n            num_cells=256,\r\n            activation_class=nn.Tanh,\r\n        ),\r\n        NormalParamExtractor(),\r\n    )\r\n    policy_module = TensorDictModule(\r\n        actor_net,\r\n        in_keys=[(\"agents\", \"observation\")],\r\n        out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\r\n    )\r\n    policy = ProbabilisticActor(\r\n        module=policy_module,\r\n        spec=env.full_action_spec_unbatched,\r\n        in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\r\n        out_keys=[env.action_key],\r\n        distribution_class=TanhNormal,\r\n        distribution_kwargs={\r\n            \"low\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.low,\r\n            \"high\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.high,\r\n        },\r\n        return_log_prob=True,\r\n    )\r\n\r\n    # Critic\r\n    module = MultiAgentMLP(\r\n        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n        n_agent_outputs=1,\r\n        n_agents=env.n_agents,\r\n        centralised=cfg.model.centralised_critic,\r\n        share_params=cfg.model.shared_parameters,\r\n        device=cfg.train.device,\r\n        depth=2,\r\n        num_cells=256,\r\n        activation_class=nn.Tanh,\r\n    )\r\n    value_module = ValueOperator(\r\n        module=module,\r\n        in_keys=[(\"agents\", \"observation\")],\r\n    )\r\n\r\n    collector = SyncDataCollector(\r\n        env,\r\n        policy,\r\n        device=cfg.env.device,\r\n        storing_device=cfg.train.device,\r\n        frames_per_batch=cfg.collector.frames_per_batch,\r\n        total_frames=cfg.collector.total_frames,\r\n        postproc=DoneTransform(reward_key=env.reward_key, done_keys=env.done_keys),\r\n    )\r\n\r\n    replay_buffer = TensorDictReplayBuffer(\r\n        storage=LazyTensorStorage(cfg.buffer.memory_size, device=cfg.train.device),\r\n        sampler=SamplerWithoutReplacement(),\r\n        batch_size=cfg.train.minibatch_size,\r\n    )\r\n\r\n    # Loss\r\n    loss_module = ClipPPOLoss(\r\n        actor_network=policy,\r\n        critic_network=value_module,\r\n        clip_epsilon=cfg.loss.clip_epsilon,\r\n        entropy_coef=cfg.loss.entropy_eps,\r\n        normalize_advantage=False,\r\n    )\r\n    loss_module.set_keys(\r\n        reward=env.reward_key,\r\n        action=env.action_key,\r\n        done=(\"agents\", \"done\"),\r\n        terminated=(\"agents\", \"terminated\"),\r\n    )\r\n    loss_module.make_value_estimator(\r\n        ValueEstimators.GAE, gamma=cfg.loss.gamma, lmbda=cfg.loss.lmbda\r\n    )\r\n    optim = torch.optim.Adam(loss_module.parameters(), cfg.train.lr)\r\n\r\n    # Logging\r\n    if cfg.logger.backend:\r\n        model_name = (\r\n            (\"Het\" if not cfg.model.shared_parameters else \"\")\r\n            + (\"MA\" if cfg.model.centralised_critic else \"I\")\r\n            + \"PPO\"\r\n        )\r\n        logger = init_logging(cfg, model_name)\r\n\r\n    total_time = 0\r\n    total_frames = 0\r\n    sampling_start = time.time()\r\n    for i, tensordict_data in enumerate(collector):\r\n        torchrl_logger.info(f\"\\nIteration {i}\")\r\n\r\n        sampling_time = time.time() - sampling_start\r\n\r\n        with torch.no_grad():\r\n            loss_module.value_estimator(\r\n                tensordict_data,\r\n                params=loss_module.critic_network_params,\r\n                target_params=loss_module.target_critic_network_params,\r\n            )\r\n        current_frames = tensordict_data.numel()\r\n        total_frames += current_frames\r\n        data_view = tensordict_data.reshape(-1)\r\n        replay_buffer.extend(data_view)\r\n\r\n        training_tds = []\r\n        training_start = time.time()\r\n        for _ in range(cfg.train.num_epochs):\r\n            for _ in range(cfg.collector.frames_per_batch // cfg.train.minibatch_size):\r\n                subdata = replay_buffer.sample()\r\n                loss_vals = loss_module(subdata)\r\n                training_tds.append(loss_vals.detach())\r\n\r\n                loss_value = (\r\n                    loss_vals[\"loss_objective\"]\r\n                    + loss_vals[\"loss_critic\"]\r\n                    + loss_vals[\"loss_entropy\"]\r\n                )\r\n\r\n                loss_value.backward()\r\n\r\n                total_norm = torch.nn.utils.clip_grad_norm_(\r\n                    loss_module.parameters(), cfg.train.max_grad_norm\r\n                )\r\n                training_tds[-1].set(\"grad_norm\", total_norm.mean())\r\n\r\n                optim.step()\r\n                optim.zero_grad()\r\n\r\n        collector.update_policy_weights_()\r\n\r\n        training_time = time.time() - training_start\r\n\r\n        iteration_time = sampling_time + training_time\r\n        total_time += iteration_time\r\n        training_tds = torch.stack(training_tds)\r\n\r\n        # More logs\r\n        if cfg.logger.backend:\r\n            log_training(\r\n                logger,\r\n                training_tds,\r\n                tensordict_data,\r\n                sampling_time,\r\n                training_time,\r\n                total_time,\r\n                i,\r\n                current_frames,\r\n                total_frames,\r\n                step=i,\r\n            )\r\n\r\n        if (\r\n            cfg.eval.evaluation_episodes > 0\r\n            and i % cfg.eval.evaluation_interval == 0\r\n            and cfg.logger.backend\r\n        ):\r\n            evaluation_start = time.time()\r\n            with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC):\r\n                env_test.frames = []\r\n                rollouts = env_test.rollout(\r\n                    max_steps=cfg.env.max_steps,\r\n                    policy=policy,\r\n                    callback=rendering_callback,\r\n                    auto_cast_to_device=True,\r\n                    break_when_any_done=False,\r\n                    # We are running vectorized evaluation we do not want it to stop when just one env is done\r\n                )\r\n\r\n                evaluation_time = time.time() - evaluation_start\r\n\r\n                log_evaluation(logger, rollouts, env_test, evaluation_time, step=i)\r\n\r\n        if cfg.logger.backend == \"wandb\":\r\n            logger.experiment.log({}, commit=True)\r\n        sampling_start = time.time()\r\n    collector.shutdown()\r\n    if not env.is_closed:\r\n        env.close()\r\n    if not env_test.is_closed:\r\n        env_test.close()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(cfg: \"DictConfig\"):  # noqa: F821\r\n    # Device\r\n    cfg.train.device = \"cpu\" if not torch.cuda.device_count() else \"cuda:0\"\r\n    cfg.env.device = cfg.train.device\r\n\r\n    # Seeding\r\n    torch.manual_seed(cfg.seed)\r\n\r\n    # Sampling\r\n    cfg.env.vmas_envs = cfg.collector.frames_per_batch // cfg.env.max_steps\r\n    cfg.collector.total_frames = cfg.collector.frames_per_batch * cfg.collector.n_iters\r\n    cfg.buffer.memory_size = cfg.collector.frames_per_batch\r\n\r\n    # Create env and env_test\r\n    env = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.env.vmas_envs,\r\n        continuous_actions=cfg.env.continuous_actions,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        categorical_actions=cfg.env.categorical_actions,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n    env = TransformedEnv(\r\n        env,\r\n        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\r\n    )\r\n\r\n    env_test = VmasEnv(\r\n        scenario=cfg.env.scenario_name,\r\n        num_envs=cfg.eval.evaluation_episodes,\r\n        continuous_actions=cfg.env.continuous_actions,\r\n        max_steps=cfg.env.max_steps,\r\n        device=cfg.env.device,\r\n        seed=cfg.seed,\r\n        # Scenario kwargs\r\n        **cfg.env.scenario,\r\n    )\r\n\r\n    # Policy\r\n    if cfg.env.continuous_actions:\r\n        actor_net = nn.Sequential(\r\n            MultiAgentMLP(\r\n                n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n                n_agent_outputs=2 * env.action_spec.shape[-1],\r\n                n_agents=env.n_agents,\r\n                centralised=False,\r\n                share_params=cfg.model.shared_parameters,\r\n                device=cfg.train.device,\r\n                depth=2,\r\n                num_cells=256,\r\n                activation_class=nn.Tanh,\r\n            ),\r\n            NormalParamExtractor(),\r\n        )\r\n        policy_module = TensorDictModule(\r\n            actor_net,\r\n            in_keys=[(\"agents\", \"observation\")],\r\n            out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\r\n        )\r\n\r\n        policy = ProbabilisticActor(\r\n            module=policy_module,\r\n            spec=env.full_action_spec_unbatched,\r\n            in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\r\n            out_keys=[env.action_key],\r\n            distribution_class=TanhNormal,\r\n            distribution_kwargs={\r\n                \"low\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.low,\r\n                \"high\": env.full_action_spec_unbatched[(\"agents\", \"action\")].space.high,\r\n            },\r\n            return_log_prob=True,\r\n        )\r\n\r\n        # Critic\r\n        module = MultiAgentMLP(\r\n            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1]\r\n            + env.action_spec.shape[-1],  # Q critic takes action and value\r\n            n_agent_outputs=1,\r\n            n_agents=env.n_agents,\r\n            centralised=cfg.model.centralised_critic,\r\n            share_params=cfg.model.shared_parameters,\r\n            device=cfg.train.device,\r\n            depth=2,\r\n            num_cells=256,\r\n            activation_class=nn.Tanh,\r\n        )\r\n        value_module = ValueOperator(\r\n            module=module,\r\n            in_keys=[(\"agents\", \"observation\"), env.action_key],\r\n            out_keys=[(\"agents\", \"state_action_value\")],\r\n        )\r\n    else:\r\n        actor_net = nn.Sequential(\r\n            MultiAgentMLP(\r\n                n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n                n_agent_outputs=env.action_spec.space.n,\r\n                n_agents=env.n_agents,\r\n                centralised=False,\r\n                share_params=cfg.model.shared_parameters,\r\n                device=cfg.train.device,\r\n                depth=2,\r\n                num_cells=256,\r\n                activation_class=nn.Tanh,\r\n            ),\r\n        )\r\n        policy_module = TensorDictModule(\r\n            actor_net,\r\n            in_keys=[(\"agents\", \"observation\")],\r\n            out_keys=[(\"agents\", \"logits\")],\r\n        )\r\n        policy = ProbabilisticActor(\r\n            module=policy_module,\r\n            spec=env.full_action_spec_unbatched,\r\n            in_keys=[(\"agents\", \"logits\")],\r\n            out_keys=[env.action_key],\r\n            distribution_class=OneHotCategorical\r\n            if not cfg.env.categorical_actions\r\n            else Categorical,\r\n            return_log_prob=True,\r\n        )\r\n\r\n        # Critic\r\n        module = MultiAgentMLP(\r\n            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\r\n            n_agent_outputs=env.action_spec.space.n,\r\n            n_agents=env.n_agents,\r\n            centralised=cfg.model.centralised_critic,\r\n            share_params=cfg.model.shared_parameters,\r\n            device=cfg.train.device,\r\n            depth=2,\r\n            num_cells=256,\r\n            activation_class=nn.Tanh,\r\n        )\r\n        value_module = ValueOperator(\r\n            module=module,\r\n            in_keys=[(\"agents\", \"observation\")],\r\n            out_keys=[(\"agents\", \"action_value\")],\r\n        )\r\n\r\n    collector = SyncDataCollector(\r\n        env,\r\n        policy,\r\n        device=cfg.env.device,\r\n        storing_device=cfg.train.device,\r\n        frames_per_batch=cfg.collector.frames_per_batch,\r\n        total_frames=cfg.collector.total_frames,\r\n        postproc=DoneTransform(reward_key=env.reward_key, done_keys=env.done_keys),\r\n    )\r\n\r\n    replay_buffer = TensorDictReplayBuffer(\r\n        storage=LazyTensorStorage(cfg.buffer.memory_size, device=cfg.train.device),\r\n        sampler=SamplerWithoutReplacement(),\r\n        batch_size=cfg.train.minibatch_size,\r\n    )\r\n\r\n    if cfg.env.continuous_actions:\r\n        loss_module = SACLoss(\r\n            actor_network=policy,\r\n            qvalue_network=value_module,\r\n            delay_qvalue=True,\r\n            action_spec=env.full_action_spec_unbatched,\r\n        )\r\n        loss_module.set_keys(\r\n            state_action_value=(\"agents\", \"state_action_value\"),\r\n            action=env.action_key,\r\n            reward=env.reward_key,\r\n            done=(\"agents\", \"done\"),\r\n            terminated=(\"agents\", \"terminated\"),\r\n        )\r\n    else:\r\n        loss_module = DiscreteSACLoss(\r\n            actor_network=policy,\r\n            qvalue_network=value_module,\r\n            delay_qvalue=True,\r\n            num_actions=env.action_spec.space.n,\r\n            action_space=env.full_action_spec_unbatched,\r\n        )\r\n        loss_module.set_keys(\r\n            action_value=(\"agents\", \"action_value\"),\r\n            action=env.action_key,\r\n            reward=env.reward_key,\r\n            done=(\"agents\", \"done\"),\r\n            terminated=(\"agents\", \"terminated\"),\r\n        )\r\n\r\n    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=cfg.loss.gamma)\r\n    target_net_updater = SoftUpdate(loss_module, eps=1 - cfg.loss.tau)\r\n\r\n    optim = torch.optim.Adam(loss_module.parameters(), cfg.train.lr)\r\n\r\n    # Logging\r\n    if cfg.logger.backend:\r\n        model_name = (\r\n            (\"Het\" if not cfg.model.shared_parameters else \"\")\r\n            + (\"MA\" if cfg.model.centralised_critic else \"I\")\r\n            + \"SAC\"\r\n        )\r\n        logger = init_logging(cfg, model_name)\r\n\r\n    total_time = 0\r\n    total_frames = 0\r\n    sampling_start = time.time()\r\n    for i, tensordict_data in enumerate(collector):\r\n        torchrl_logger.info(f\"\\nIteration {i}\")\r\n\r\n        sampling_time = time.time() - sampling_start\r\n\r\n        current_frames = tensordict_data.numel()\r\n        total_frames += current_frames\r\n        data_view = tensordict_data.reshape(-1)\r\n        replay_buffer.extend(data_view)\r\n\r\n        training_tds = []\r\n        training_start = time.time()\r\n        for _ in range(cfg.train.num_epochs):\r\n            for _ in range(cfg.collector.frames_per_batch // cfg.train.minibatch_size):\r\n                subdata = replay_buffer.sample()\r\n                loss_vals = loss_module(subdata)\r\n                training_tds.append(loss_vals.detach())\r\n\r\n                loss_value = (\r\n                    loss_vals[\"loss_actor\"]\r\n                    + loss_vals[\"loss_alpha\"]\r\n                    + loss_vals[\"loss_qvalue\"]\r\n                )\r\n\r\n                loss_value.backward()\r\n\r\n                total_norm = torch.nn.utils.clip_grad_norm_(\r\n                    loss_module.parameters(), cfg.train.max_grad_norm\r\n                )\r\n                training_tds[-1].set(\"grad_norm\", total_norm.mean())\r\n\r\n                optim.step()\r\n                optim.zero_grad()\r\n                target_net_updater.step()\r\n\r\n        collector.update_policy_weights_()\r\n\r\n        training_time = time.time() - training_start\r\n\r\n        iteration_time = sampling_time + training_time\r\n        total_time += iteration_time\r\n        training_tds = torch.stack(training_tds)\r\n\r\n        # More logs\r\n        if cfg.logger.backend:\r\n            log_training(\r\n                logger,\r\n                training_tds,\r\n                tensordict_data,\r\n                sampling_time,\r\n                training_time,\r\n                total_time,\r\n                i,\r\n                current_frames,\r\n                total_frames,\r\n                step=i,\r\n            )\r\n\r\n        if (\r\n            cfg.eval.evaluation_episodes > 0\r\n            and i % cfg.eval.evaluation_interval == 0\r\n            and cfg.logger.backend\r\n        ):\r\n            evaluation_start = time.time()\r\n            with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC):\r\n                env_test.frames = []\r\n                rollouts = env_test.rollout(\r\n                    max_steps=cfg.env.max_steps,\r\n                    policy=policy,\r\n                    callback=rendering_callback,\r\n                    auto_cast_to_device=True,\r\n                    break_when_any_done=False,\r\n                    # We are running vectorized evaluation we do not want it to stop when just one env is done\r\n                )\r\n\r\n                evaluation_time = time.time() - evaluation_start\r\n\r\n                log_evaluation(logger, rollouts, env_test, evaluation_time, step=i)\r\n\r\n        if cfg.logger.backend == \"wandb\":\r\n            logger.experiment.log({}, commit=True)\r\n        sampling_start = time.time()\r\n    collector.shutdown()\r\n    if not env.is_closed:\r\n        env.close()\r\n    if not env_test.is_closed:\r\n        env_test.close()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def test_hold_out():\r\n    net = torch.nn.Linear(3, 4)\r\n    x = torch.randn(1, 3)\r\n    x_rg = torch.randn(1, 3, requires_grad=True)\r\n    y = net(x)\r\n    assert y.requires_grad\r\n    with hold_out_net(net):\r\n        y = net(x)\r\n        assert not y.requires_grad\r\n        y = net(x_rg)\r\n        assert y.requires_grad\r\n\r\n    y = net(x)\r\n    assert y.requires_grad\r\n\r\n    # nested case\r\n    with hold_out_net(net):\r\n        y = net(x)\r\n        assert not y.requires_grad\r\n        with hold_out_net(net):\r\n            y = net(x)\r\n            assert not y.requires_grad\r\n            y = net(x_rg)\r\n            assert y.requires_grad\r\n\r\n    y = net(x)\r\n    assert y.requires_grad\r\n\r\n    # exception\r\n    net = torch.nn.Sequential()\r\n    with hold_out_net(net):\r\n        pass",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def batch_update(self, x, y):\r\n        with torch.no_grad():\r\n            prediction = self.model.forward(x)\r\n            loss = self.loss(prediction, y)\r\n        return loss, prediction",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def training_function(kwargs: dict):\r\n    print(\"training_function called\")\r\n\r\n    # Train has a bug somewhere that causes ACCELERATE_TORCH_DEVICE to not be set\r\n    # properly on multi-gpu nodes\r\n    cuda_visible_device = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\r\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\r\n    device_id = cuda_visible_device[local_rank]\r\n    os.environ[\"ACCELERATE_TORCH_DEVICE\"] = f\"cuda:{device_id}\"\r\n\r\n    config = kwargs[\"config\"]\r\n    args = argparse.Namespace(**kwargs[\"args\"])\r\n    special_tokens = kwargs.get(\"special_tokens\", [])\r\n    model_id = config[\"model_name\"]\r\n\r\n    # We need to download the model weights on this machine if they don't exit.\r\n    # We need to acquire a lock to ensure that only one process downloads the model\r\n    bucket_uri = get_mirror_link(model_id)\r\n    download_path = get_download_path(model_id)\r\n    base_path = Path(download_path).parent\r\n    base_path.mkdir(parents=True, exist_ok=True)\r\n    lock_file = str(base_path / f'{model_id.replace(\"/\",  \"--\")}.lock')\r\n    with FileLock(lock_file):\r\n        download_model(\r\n            model_id=model_id, bucket_uri=bucket_uri, s3_sync_args=[\"--no-sign-request\"]\r\n        )\r\n\r\n    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\r\n    lr = config[\"lr\"]\r\n    num_epochs = int(config[\"num_epochs\"])\r\n    seed = int(config[\"seed\"])\r\n    batch_size = int(config[\"batch_size\"])\r\n    gradient_accumulation_steps = int(config[\"gradient_accumulation_steps\"])\r\n\r\n    # Get deepspeed config to setup the batch size per device\r\n    ds_plugin = config[\"ds_plugin\"]\r\n    ds_plugin.hf_ds_config.config[\"train_micro_batch_size_per_gpu\"] = batch_size\r\n\r\n    # Initialize accelerator\r\n    accelerator = Accelerator(\r\n        deepspeed_plugin=ds_plugin,\r\n        gradient_accumulation_steps=gradient_accumulation_steps,\r\n        mixed_precision=args.mx,\r\n    )\r\n\r\n    set_seed(seed)\r\n\r\n    # train_ds is the local shard for this model\r\n    train_ds = train.get_dataset_shard(\"train\")\r\n    valid_ds = train.get_dataset_shard(\"valid\")\r\n\r\n    train_ds_len = len(list(train_ds.iter_batches(batch_size=1)))\r\n\r\n    _test_tokenizer(args.model_name)\r\n    tokenizer = get_tokenizer(model_name=args.model_name, special_tokens=special_tokens)\r\n    collate_partial = functools.partial(\r\n        collate_fn,\r\n        tokenizer=tokenizer,\r\n        block_size=config[\"block_size\"],\r\n        device=accelerator.device,\r\n    )\r\n\r\n    pretrained_path = get_pretrained_path(model_id)\r\n    print(f\"Loading model from {pretrained_path} ...\")\r\n    s = time.time()\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n        pretrained_path,\r\n        trust_remote_code=True,\r\n        torch_dtype=torch.bfloat16,\r\n        # `use_cache=True` is incompatible with gradient checkpointing.\r\n        use_cache=False,\r\n        use_flash_attention_2=True,\r\n    )\r\n    print(f\"Done loading model in {time.time() - s} seconds.\")\r\n\r\n    model.resize_token_embeddings(len(tokenizer))\r\n\r\n    if config[\"lora\"]:\r\n        # Apply LoRA\r\n        s = time.time()\r\n        lora_config = LoraConfig(**config[\"lora_config\"])\r\n\r\n        expected_num_parameters = get_expected_lora_num_parameters(\r\n            lora_config=lora_config, model=model\r\n        )\r\n\r\n        print(f\"Attempting to apply LoRA config: {lora_config}\")\r\n\r\n        model.enable_input_require_grads()\r\n        model = get_peft_model(model, lora_config)\r\n\r\n        num_parameters = get_number_of_params(model)\r\n\r\n        if num_parameters != expected_num_parameters:\r\n            raise ValueError(\r\n                f\"Expected {expected_num_parameters} parameters, got {num_parameters} \"\r\n                f\"parameters. LoRA-ification failed.\"\r\n            )\r\n\r\n        print(\r\n            f\"LoRA-ification done in {time.time() - s} seconds. Estimated checkpoint \"\r\n            f\"size (fp16): {num_parameters * 2 / 1e6} MB\"\r\n        )\r\n\r\n    print(f\"Number of checkpointed parameters: {get_number_of_params(model)}\")\r\n\r\n    print(\"Model initialized with pretrained weights. Training starting...\")\r\n    if not args.no_grad_ckpt:\r\n        model.gradient_checkpointing_enable()\r\n\r\n    optimizer_cls = (\r\n        torch.optim.AdamW\r\n        if accelerator.state.deepspeed_plugin is None\r\n        or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\r\n        else DummyOptim\r\n    )\r\n\r\n    optimizer = optimizer_cls(\r\n        model.parameters(),\r\n        lr=lr,\r\n        betas=OPTIM_BETAS,\r\n        weight_decay=OPTIM_WEIGHT_DECAY,\r\n        eps=OPTIM_EPS,\r\n    )\r\n\r\n    # Instantiate scheduler\r\n    # Creates Dummy Scheduler if `scheduler` was specified in the config file or\r\n    # else, creates `args.lr_scheduler_type` Scheduler\r\n    # get train and valid dataset lengths\r\n\r\n    num_steps_per_epoch = math.ceil(train_ds_len / args.batch_size_per_device)\r\n    total_training_steps = (\r\n        num_steps_per_epoch * num_epochs // gradient_accumulation_steps\r\n    )\r\n\r\n    if (\r\n        accelerator.state.deepspeed_plugin is None\r\n        or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\r\n    ):\r\n        lr_scheduler = get_linear_schedule_with_warmup(\r\n            optimizer=optimizer,\r\n            num_warmup_steps=NUM_WARMUP_STEPS * args.num_devices,\r\n            num_training_steps=total_training_steps * args.num_devices,\r\n        )\r\n    else:\r\n        lr_scheduler = DummyScheduler(\r\n            optimizer,\r\n            warmup_num_steps=NUM_WARMUP_STEPS * args.num_devices,\r\n            total_num_steps=total_training_steps * args.num_devices,\r\n        )\r\n\r\n    # Prepare everything\r\n    # There is no specific order to remember, we just need to unpack the objects in the\r\n    # same order we gave them to the prepare method.\r\n    s = time.time()\r\n    model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)\r\n    print(f\"Prepare done in {time.time() - s} seconds.\")\r\n\r\n    # Now we train the model\r\n    if accelerator.is_main_process:\r\n        print(\"Starting training ...\")\r\n        print(\"Number of batches on main process\", train_ds_len // batch_size)\r\n\r\n    for epoch in range(num_epochs):\r\n        fwd_time_sum, bwd_time_sum, optim_step_time_sum = 0, 0, 0\r\n        s_epoch = time.time()\r\n        model.train()\r\n        loss_sum = torch.tensor(0.0).to(accelerator.device)\r\n\r\n        train_dataloader = train_ds.iter_torch_batches(\r\n            batch_size=batch_size,\r\n            collate_fn=collate_partial,\r\n        )\r\n\r\n        for step, batch in tqdm.tqdm(\r\n            enumerate(train_dataloader), total=train_ds_len // batch_size + 1\r\n        ):\r\n\r\n            # We could avoid this line since we set the accelerator with\r\n            # `device_placement=True`.\r\n            with accelerator.accumulate(model):\r\n                s_fwd = time.time()\r\n                outputs = model(**batch)\r\n                loss = outputs.loss\r\n                loss_sum += loss.item()\r\n                e_fwd = time.time()\r\n                fwd_time = e_fwd - s_fwd\r\n                fwd_time_sum += fwd_time\r\n                s_bwd = time.time()\r\n                accelerator.backward(loss)\r\n                e_bwd = time.time()\r\n                bwd_time = e_bwd - s_bwd\r\n                bwd_time_sum += bwd_time\r\n\r\n                s_opt_step = time.time()\r\n                optimizer.step()\r\n                lr_scheduler.step()\r\n                optimizer.zero_grad()\r\n                e_opt_step = time.time()\r\n                optim_step_time_sum += e_opt_step - s_opt_step\r\n\r\n            if accelerator.is_main_process:\r\n                accelerator.print(\r\n                    f\"[epoch {epoch} step {step}] \"\r\n                    f\"loss: {loss.item()} step-time: {e_opt_step - s_fwd}\"\r\n                )\r\n\r\n            aggregated_loss = torch.mean(accelerator.gather(loss[None])).item()\r\n\r\n            if config[\"as_test\"]:\r\n                break\r\n\r\n            # as long as this is not the last step report here\r\n            if step != (train_ds_len // batch_size - 1):\r\n                train.report(\r\n                    {\r\n                        \"epoch\": epoch,\r\n                        \"iteration\": step,\r\n                        \"train_loss_batch\": aggregated_loss,\r\n                        \"avg_train_loss_epoch\": None,\r\n                        \"eval_loss\": None,\r\n                        \"perplexity\": None,\r\n                        \"num_iterations\": step + 1,\r\n                        \"train_time_per_epoch\": None,\r\n                        \"eval_time_per_epoch\": None,\r\n                        \"fwd_time\": fwd_time,\r\n                        \"bwd_time\": bwd_time,\r\n                        \"avg_fwd_time_per_epoch\": None,\r\n                        \"avg_bwd_time_per_epoch\": None,\r\n                        \"learning_rate\": lr_scheduler.get_lr()[0],\r\n                    }\r\n                )\r\n\r\n        e_epoch = time.time()\r\n        accelerator.print(\"Train time per epoch: \", e_epoch - s_epoch)\r\n\r\n        eval_s_epoch = time.time()\r\n        print(\"Running evaluation ...\")\r\n        perplex, eloss = evaluate(\r\n            model=model,\r\n            eval_ds=valid_ds,\r\n            accelerator=accelerator,\r\n            bsize=config[\"eval_batch_size\"],\r\n            ds_kwargs={\"collate_fn\": collate_partial},\r\n            as_test=config[\"as_test\"],\r\n        )\r\n        accelerator.print(\"Eval result loss\", eloss)\r\n        accelerator.print(\"Eval perplex\", perplex)\r\n\r\n        eval_e_epoch = time.time()\r\n        accelerator.print(\"Eval time per epoch: \", eval_e_epoch - eval_s_epoch)\r\n        accelerator.print(\"avg fwd time: \", fwd_time_sum / (step + 1))\r\n        accelerator.print(\"avg bwd time: \", bwd_time_sum / (step + 1))\r\n        accelerator.print(\"avg opt step time: \", optim_step_time_sum / (step + 1))\r\n\r\n        metrics = {\r\n            \"epoch\": epoch,\r\n            \"iteration\": step,\r\n            \"train_loss_batch\": aggregated_loss,\r\n            \"avg_train_loss_epoch\": loss_sum.item() / (step + 1),\r\n            \"eval_loss\": eloss,\r\n            \"perplexity\": perplex,\r\n            \"num_iterations\": step + 1,\r\n            \"train_time_per_epoch\": e_epoch - s_epoch,\r\n            \"eval_time_per_epoch\": eval_e_epoch - eval_s_epoch,\r\n            \"fwd_time\": fwd_time,\r\n            \"bwd_time\": bwd_time,\r\n            \"avg_fwd_time_per_epoch\": fwd_time_sum / (step + 1),\r\n            \"avg_bwd_time_per_epoch\": bwd_time_sum / (step + 1),\r\n            \"learning_rate\": lr_scheduler.get_lr()[0],\r\n        }\r\n\r\n        with tempfile.TemporaryDirectory(dir=args.output_dir) as temp_checkpoint_dir:\r\n            accelerator.print(f\"Saving the model locally at {temp_checkpoint_dir}\")\r\n            accelerator.wait_for_everyone()\r\n\r\n            checkpoint_save_start = time.perf_counter()\r\n\r\n            if accelerator.is_main_process:\r\n                print(\"Saving tokenizer and config.\")\r\n                tokenizer.save_pretrained(temp_checkpoint_dir)\r\n\r\n            accelerator.wait_for_everyone()\r\n\r\n            # Checkpointing strategy 1: Distributed checkpointing\r\n            # This checkpointing method makes deepspeed checkpoints on each node\r\n            # and then Ray Train will aggregate them to a central s3 bucket.\r\n            # It should be done on all processes (not just the Rank 0)\r\n            # aggregate_on_rank_0 = False\r\n            # checkpoint_model(\r\n            #     checkpoint_folder=tempdir,\r\n            #     ckpt_id=epoch,\r\n            #     model=model,\r\n            #     epoch=epoch,\r\n            #     last_global_step=step\r\n            # )\r\n\r\n            # Checkpointing strategy 2: Aggregate model on the rank 0 worker then upload\r\n            aggregate_on_rank_0 = True\r\n            unwrapped_model = accelerator.unwrap_model(model)\r\n            unwrapped_model.save_pretrained(\r\n                temp_checkpoint_dir,\r\n                is_main_process=accelerator.is_main_process,\r\n                save_function=accelerator.save,\r\n                safe_serialization=True,\r\n                state_dict=accelerator.get_state_dict(model),\r\n            )\r\n            accelerator.wait_for_everyone()\r\n            print(\"Checkpoint save time: \", time.perf_counter() - checkpoint_save_start)\r\n\r\n            checkpoint_upload_start = time.perf_counter()\r\n\r\n            # Create the checkpoint object to report to Ray Train and upload to storage.\r\n            # If we aggregated the model on rank 0, we only need to report\r\n            # the checkpoint from the rank 0 worker, since all other checkpoint\r\n            # directories are empty (`save_pretrained` was a noop for other workers).\r\n            if aggregate_on_rank_0:\r\n                checkpoint = (\r\n                    Checkpoint.from_directory(temp_checkpoint_dir)\r\n                    if accelerator.is_main_process\r\n                    else None\r\n                )\r\n            else:\r\n                # Distributed checkpointing should upload shards from each worker.\r\n                checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\r\n\r\n            # Note: After `train.report`, in the case of remote storage,\r\n            # the checkpoint directory will be uploaded to the remote storage.\r\n            train.report(metrics, checkpoint=checkpoint)\r\n\r\n            print(\r\n                \"Checkpoint upload time: \",\r\n                time.perf_counter() - checkpoint_upload_start,\r\n            )\r\n            print(\r\n                \"Total checkpointing time: \",\r\n                time.perf_counter() - checkpoint_save_start,\r\n            )\r\n\r\n        if perplex < args.stop_perplexity:\r\n            print(f\"Perplexity reached {perplex} < {args.stop_perplexity}. Stopping.\")\r\n            break\r\n\r\n        if config[\"as_test\"]:\r\n            break",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(config):\r\n    n = 100\r\n    # create a toy dataset\r\n    X = torch.Tensor(np.random.normal(0, 1, size=(n, 4)))\r\n    X_valid = torch.Tensor(np.random.normal(0, 1, size=(n, 4)))\r\n    Y = torch.Tensor(np.random.uniform(0, 1, size=(n, 1)))\r\n    Y_valid = torch.Tensor(np.random.uniform(0, 1, size=(n, 1)))\r\n    # toy neural network : 1-layer\r\n    # wrap the model in DDP\r\n    model = ray.train.torch.prepare_model(nn.Linear(4, 1))\r\n    criterion = nn.MSELoss()\r\n\r\n    mape = torchmetrics.MeanAbsolutePercentageError()\r\n    # for averaging loss\r\n    mean_valid_loss = torchmetrics.MeanMetric()\r\n\r\n    optimizer = Adam(model.parameters(), lr=3e-4)\r\n    for epoch in range(config[\"num_epochs\"]):\r\n        model.train()\r\n        y = model.forward(X)\r\n\r\n        # compute loss\r\n        loss = criterion(y, Y)\r\n\r\n        # back-propagate loss\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # evaluate\r\n        model.eval()\r\n        with torch.no_grad():\r\n            pred = model(X_valid)\r\n            valid_loss = criterion(pred, Y_valid)\r\n            # save loss in aggregator\r\n            mean_valid_loss(valid_loss)\r\n            mape(pred, Y_valid)\r\n\r\n        # collect all metrics\r\n        # use .item() to obtain a value that can be reported\r\n        valid_loss = valid_loss.item()\r\n        mape_collected = mape.compute().item()\r\n        mean_valid_loss_collected = mean_valid_loss.compute().item()\r\n\r\n        train.report(\r\n            {\r\n                \"mape_collected\": mape_collected,\r\n                \"valid_loss\": valid_loss,\r\n                \"mean_valid_loss_collected\": mean_valid_loss_collected,\r\n            }\r\n        )\r\n\r\n        # reset for next epoch\r\n        mape.reset()\r\n        mean_valid_loss.reset()",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def train_epoch(model, optimizer, train_loader, device=None):\r\n    device = device or torch.device(\"cpu\")\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        if batch_idx * len(data) > EPOCH_SIZE:\r\n            return\r\n        data, target = data.to(device), target.to(device)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def from_path(\r\n        cls,\r\n        path: Union[str, os.PathLike],\r\n        storage_filesystem: Optional[pyarrow.fs.FileSystem] = None,\r\n    ) -> \"Result\":\r\n        \"\"\"Restore a Result object from local or remote trial directory.\r\n\r\n        Args:\r\n            path: A path of a trial directory on local or remote storage\r\n                (ex: s3://bucket/path or /tmp/ray_results).\r\n            storage_filesystem: A custom filesystem to use. If not provided,\r\n                this will be auto-resolved by pyarrow. If provided, the path\r\n                is assumed to be prefix-stripped already, and must be a valid path\r\n                on the filesystem.\r\n\r\n        Returns:\r\n            A :py:class:`Result` object of that trial.\r\n        \"\"\"\r\n        # TODO(justinvyu): Fix circular dependency.\r\n        from ray.train import Checkpoint\r\n        from ray.train._internal.storage import (\r\n            _exists_at_fs_path,\r\n            _list_at_fs_path,\r\n            get_fs_and_path,\r\n        )\r\n        from ray.train.constants import CHECKPOINT_DIR_NAME\r\n\r\n        fs, fs_path = get_fs_and_path(path, storage_filesystem)\r\n        if not _exists_at_fs_path(fs, fs_path):\r\n            raise RuntimeError(f\"Trial folder {fs_path} doesn't exist!\")\r\n\r\n        # Restore metrics from result.json\r\n        result_json_file = Path(fs_path, EXPR_RESULT_FILE).as_posix()\r\n        progress_csv_file = Path(fs_path, EXPR_PROGRESS_FILE).as_posix()\r\n        if _exists_at_fs_path(fs, result_json_file):\r\n            lines = cls._read_file_as_str(fs, result_json_file).split(\"\\n\")\r\n            json_list = [json.loads(line) for line in lines if line]\r\n            metrics_df = pd.json_normalize(json_list, sep=\"/\")\r\n            latest_metrics = json_list[-1] if json_list else {}\r\n        # Fallback to restore from progress.csv\r\n        elif _exists_at_fs_path(fs, progress_csv_file):\r\n            metrics_df = pd.read_csv(\r\n                io.StringIO(cls._read_file_as_str(fs, progress_csv_file))\r\n            )\r\n            latest_metrics = (\r\n                metrics_df.iloc[-1].to_dict() if not metrics_df.empty else {}\r\n            )\r\n        else:\r\n            raise RuntimeError(\r\n                f\"Failed to restore the Result object: Neither {EXPR_RESULT_FILE}\"\r\n                f\" nor {EXPR_PROGRESS_FILE} exists in the trial folder!\"\r\n            )\r\n\r\n        # Restore all checkpoints from the checkpoint folders\r\n        checkpoint_dir_names = sorted(\r\n            _list_at_fs_path(\r\n                fs,\r\n                fs_path,\r\n                file_filter=lambda file_info: file_info.type\r\n                == pyarrow.fs.FileType.Directory\r\n                and file_info.base_name.startswith(\"checkpoint_\"),\r\n            )\r\n        )\r\n\r\n        if checkpoint_dir_names:\r\n            checkpoints = [\r\n                Checkpoint(\r\n                    path=Path(fs_path, checkpoint_dir_name).as_posix(), filesystem=fs\r\n                )\r\n                for checkpoint_dir_name in checkpoint_dir_names\r\n            ]\r\n\r\n            metrics = []\r\n            for checkpoint_dir_name in checkpoint_dir_names:\r\n                metrics_corresponding_to_checkpoint = metrics_df[\r\n                    metrics_df[CHECKPOINT_DIR_NAME] == checkpoint_dir_name\r\n                ]\r\n                if metrics_corresponding_to_checkpoint.empty:\r\n                    logger.warning(\r\n                        \"Could not find metrics corresponding to \"\r\n                        f\"{checkpoint_dir_name}. These will default to an empty dict.\"\r\n                    )\r\n                metrics.append(\r\n                    {}\r\n                    if metrics_corresponding_to_checkpoint.empty\r\n                    else metrics_corresponding_to_checkpoint.iloc[-1].to_dict()\r\n                )\r\n\r\n            latest_checkpoint = checkpoints[-1]\r\n            # TODO(justinvyu): These are ordered by checkpoint index, since we don't\r\n            # know the metric to order these with.\r\n            best_checkpoints = list(zip(checkpoints, metrics))\r\n        else:\r\n            best_checkpoints = latest_checkpoint = None\r\n\r\n        # Restore the trial error if it exists\r\n        error = None\r\n        error_file_path = Path(fs_path, EXPR_ERROR_PICKLE_FILE).as_posix()\r\n        if _exists_at_fs_path(fs, error_file_path):\r\n            with fs.open_input_stream(error_file_path) as f:\r\n                error = ray.cloudpickle.load(f)\r\n\r\n        return Result(\r\n            metrics=latest_metrics,\r\n            checkpoint=latest_checkpoint,\r\n            path=fs_path,\r\n            _storage_filesystem=fs,\r\n            metrics_dataframe=metrics_df,\r\n            best_checkpoints=best_checkpoints,\r\n            error=error,\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_experiment_restore(tmp_path, runner_type):\r\n    \"\"\"\r\n    This is an integration stress test for experiment restoration.\r\n\r\n\r\n    Test setup:\r\n\r\n    - For Tuner.restore:\r\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\r\n        - Each iteration takes 0.5 seconds\r\n        - Each trial runs for 8 iterations --> 4 seconds\r\n        - Each round of 2 trials should take 4 seconds\r\n        - Without any interrupts/restoration:\r\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\r\n        - The test will stop the script with a SIGINT at a random time between\r\n        6-10 iterations each restore.\r\n\r\n    - For Trainer.restore:\r\n        - 1 trial with 4 workers\r\n        - Each iteration takes 0.5 seconds\r\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\r\n        - The test will stop the script with a SIGINT at a random time between\r\n        6-10 iterations after each restore.\r\n\r\n    Requirements:\r\n    - Req 1: Reasonable runtime\r\n        - The experiment should finish within 2 * 16 = 32 seconds.\r\n        - 2x is the passing threshold.\r\n        - 16 seconds is the minimum runtime.\r\n    - Req 2: Training progress persisted\r\n        - The experiment should progress monotonically.\r\n        (The training iteration shouldn't go backward at any point)\r\n        - Trials shouldn't start from scratch.\r\n    - Req 3: Searcher state saved/restored correctly\r\n    - Req 4: Callback state saved/restored correctly\r\n    \"\"\"\r\n\r\n    np.random.seed(2023)\r\n\r\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\r\n\r\n    # Args to pass into the script as environment variables\r\n    exp_name = f\"{runner_type}_restore_integration_test\"\r\n    callback_dump_file = tmp_path / f\"{runner_type}-callback_dump_file.json\"\r\n    storage_path = tmp_path / \"ray_results\"\r\n    if storage_path.exists():\r\n        shutil.rmtree(storage_path)\r\n\r\n    csv_file = str(tmp_path / \"dummy_data.csv\")\r\n    dummy_df = pd.DataFrame({\"x\": np.arange(128), \"y\": 2 * np.arange(128)})\r\n    dummy_df.to_csv(csv_file)\r\n\r\n    run_started_marker = tmp_path / \"run_started_marker\"\r\n\r\n    time_per_iter_s = 0.5\r\n    max_concurrent = 2\r\n\r\n    if runner_type == \"tuner\":\r\n        iters_per_trial = 8\r\n        num_trials = 8\r\n    elif runner_type == \"trainer\":\r\n        iters_per_trial = 32\r\n        num_trials = 1\r\n\r\n    total_iters = iters_per_trial * num_trials\r\n\r\n    env = os.environ.copy()\r\n    env.update(\r\n        {\r\n            \"RUNNER_TYPE\": runner_type,\r\n            \"STORAGE_PATH\": str(storage_path),\r\n            \"EXP_NAME\": exp_name,\r\n            \"CALLBACK_DUMP_FILE\": str(callback_dump_file),\r\n            \"RUN_STARTED_MARKER\": str(run_started_marker),\r\n            \"TIME_PER_ITER_S\": str(time_per_iter_s),\r\n            \"ITERATIONS_PER_TRIAL\": str(iters_per_trial),\r\n            \"NUM_TRIALS\": str(num_trials),\r\n            \"MAX_CONCURRENT_TRIALS\": str(max_concurrent),\r\n            \"CSV_DATA_FILE\": csv_file,\r\n        }\r\n    )\r\n\r\n    # Pass criteria\r\n    no_interrupts_runtime = 16.0\r\n    # Todo(krfricke): See if we can improve the actor startup/shutdown time\r\n    # to reduce the passing factor again.\r\n    passing_factor = 2.5\r\n    passing_runtime = no_interrupts_runtime * passing_factor\r\n    _print_message(\r\n        \"Experiment should finish with a total runtime of\\n\"\r\n        f\"<= {passing_runtime} seconds.\"\r\n    )\r\n\r\n    # Variables used in the loop\r\n    return_code = None\r\n    total_runtime = 0\r\n    run_iter = 0\r\n    progress = 0\r\n    progress_history = []\r\n\r\n    poll_interval_s = 0.1\r\n    test_start_time = time.monotonic()\r\n\r\n    while total_runtime < passing_runtime:\r\n        run_started_marker.write_text(\"\", encoding=\"utf-8\")\r\n\r\n        run = subprocess.Popen([sys.executable, script_path], env=env)\r\n        run_iter += 1\r\n\r\n        _print_message(f\"Started run #{run_iter} w/ PID = {run.pid}\")\r\n\r\n        # Start the timer after the first trial has entered its training loop.\r\n        while run.poll() is None and run_started_marker.exists():\r\n            time.sleep(poll_interval_s)\r\n\r\n        # If the run already finished, then exit immediately.\r\n        if run.poll() is not None:\r\n            return_code = run.poll()\r\n            break\r\n\r\n        timeout_s = min(\r\n            np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s),\r\n            passing_runtime - total_runtime,\r\n        )\r\n\r\n        _print_message(\r\n            \"Training has started...\\n\"\r\n            f\"Interrupting after {timeout_s:.2f} seconds\\n\"\r\n            f\"Currently at {total_runtime:.2f}/{passing_runtime} seconds\"\r\n        )\r\n\r\n        # Sleep for a random amount of time, then stop the run.\r\n        start_time = time.monotonic()\r\n        stopping_time = start_time + timeout_s\r\n        while time.monotonic() < stopping_time:\r\n            time.sleep(poll_interval_s)\r\n        total_runtime += time.monotonic() - start_time\r\n\r\n        return_code = run.poll()\r\n        if return_code is None:\r\n            # Send \"SIGINT\" to stop the run\r\n            _print_message(f\"Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}\")\r\n            run.send_signal(signal.SIGUSR1)\r\n\r\n            # Make sure the process is stopped forcefully after a timeout.\r\n            _kill_process_if_needed(run)\r\n        else:\r\n            _print_message(\"Run has already terminated!\")\r\n            break\r\n\r\n        # Check up on the results.\r\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\r\n        iters = [result.metrics.get(\"training_iteration\", 0) for result in results]\r\n        progress = sum(iters) / total_iters\r\n        progress_history.append(progress)\r\n        _print_message(\r\n            f\"Number of trials = {len(results)}\\n\"\r\n            f\"% completion = {progress} ({sum(iters)} iters / {total_iters})\\n\"\r\n            f\"Currently at {total_runtime:.2f}/{passing_runtime} seconds\"\r\n        )\r\n\r\n    _print_message(\r\n        f\"Total number of restorations = {run_iter}\\n\"\r\n        f\"Total runtime = {total_runtime:.2f}\\n\"\r\n        f\"Return code = {return_code}\"\r\n    )\r\n    test_end_time = time.monotonic()\r\n\r\n    # Req 1: runtime and completion\r\n    assert progress == 1.0\r\n    assert total_runtime <= passing_runtime, (\r\n        f\"Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. \"\r\n        f\"This means the experiment did not finish (iterations still running). Are \"\r\n        f\"there any performance regressions or expensive failure recoveries??\"\r\n    )\r\n\r\n    # The script shouldn't have errored. (It should have finished by this point.)\r\n    assert return_code == 0, (\r\n        f\"The script errored with return code: {return_code}.\\n\"\r\n        f\"Check the `{_RUN_SCRIPT_FILENAME}` script for any issues. \"\r\n    )\r\n\r\n    # Req 2: training progress persisted\r\n    # Check that progress increases monotonically (we never go backwards/start from 0)\r\n    assert np.all(np.diff(progress_history) >= 0), (\r\n        \"Expected progress to increase monotonically. Instead, got:\\n\"\r\n        \"{progress_history}\"\r\n    )\r\n\r\n    # Req 3: searcher state\r\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\r\n    # Check that all trials have unique ids assigned by the searcher (if applicable)\r\n    ids = [result.config.get(\"id\", -1) for result in results]\r\n    ids = [id for id in ids if id >= 0]\r\n    if ids:\r\n        assert sorted(ids) == list(range(1, num_trials + 1)), (\r\n            \"Expected the searcher to assign increasing id for each trial, but got:\"\r\n            f\"{ids}\"\r\n        )\r\n\r\n    # Req 4: callback state\r\n    with open(callback_dump_file, \"r\") as f:\r\n        callback_state = json.load(f)\r\n\r\n    trial_iters = callback_state[\"trial_iters\"]\r\n    for iters in trial_iters.values():\r\n        # Check that the callback has data for each trial, for all iters\r\n        # NOTE: There may be some duplicate data, due to the fact that\r\n        # the callback will be updated on every `on_trial_result` hook,\r\n        # but the trial may crash before the corresponding checkpoint gets processed.\r\n        assert sorted(set(iters)) == list(\r\n            range(1, iters_per_trial + 1)\r\n        ), f\"Expected data from all iterations, but got: {iters}\"\r\n\r\n    _print_message(f\"Success! Test took {test_end_time - test_start_time:.2f} seconds.\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def training_loop(self):\r\n        # You can access any Trainer attributes directly in this method.\r\n        # self.datasets[\"train\"] has already been\r\n        # preprocessed by self.preprocessor\r\n        dataset = self.datasets[\"train\"]\r\n\r\n        loss_fn = torch.nn.MSELoss()\r\n\r\n        for epoch_idx in range(10):\r\n            loss = 0\r\n            num_batches = 0\r\n            for batch in dataset.iter_torch_batches(dtypes=torch.float):\r\n                # Compute prediction error\r\n                X, y = torch.unsqueeze(batch[\"x\"], 1), batch[\"y\"]\r\n                pred = self.model(X)\r\n                batch_loss = loss_fn(pred, y)\r\n\r\n                # Backpropagation\r\n                self.optimizer.zero_grad()\r\n                batch_loss.backward()\r\n                self.optimizer.step()\r\n\r\n                loss += batch_loss.item()\r\n                num_batches += 1\r\n            loss /= num_batches\r\n\r\n            # Use Tune functions to report intermediate\r\n            # results.\r\n            train.report({\"loss\": loss, \"epoch\": epoch_idx})",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def write_bigquery(\r\n        self,\r\n        project_id: str,\r\n        dataset: str,\r\n        max_retry_cnt: int = 10,\r\n        overwrite_table: Optional[bool] = True,\r\n        ray_remote_args: Dict[str, Any] = None,\r\n        concurrency: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Write the dataset to a BigQuery dataset table.\r\n\r\n        To control the number of parallel write tasks, use ``.repartition()``\r\n        before calling this method.\r\n\r\n        Examples:\r\n             .. testcode::\r\n                :skipif: True\r\n\r\n                import ray\r\n                import pandas as pd\r\n\r\n                docs = [{\"title\": \"BigQuery Datasource test\"} for key in range(4)]\r\n                ds = ray.data.from_pandas(pd.DataFrame(docs))\r\n                ds.write_bigquery(\r\n                    project_id=\"my_project_id\",\r\n                    dataset=\"my_dataset_table\",\r\n                    overwrite_table=True\r\n                )\r\n\r\n        Args:\r\n            project_id: The name of the associated Google Cloud Project that hosts\r\n                the dataset to read. For more information, see details in\r\n                `Creating and managing projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\r\n            dataset: The name of the dataset in the format of ``dataset_id.table_id``.\r\n                The dataset is created if it doesn't already exist.\r\n            max_retry_cnt: The maximum number of retries that an individual block write\r\n                is retried due to BigQuery rate limiting errors. This isn't\r\n                related to Ray fault tolerance retries. The default number of retries\r\n                is 10.\r\n            overwrite_table: Whether the write will overwrite the table if it already\r\n                exists. The default behavior is to overwrite the table.\r\n                ``overwrite_table=False`` will append to the table if it exists.\r\n            ray_remote_args: Kwargs passed to :func:`ray.remote` in the write tasks.\r\n            concurrency: The maximum number of Ray tasks to run concurrently. Set this\r\n                to control number of tasks to run concurrently. This doesn't change the\r\n                total number of tasks run. By default, concurrency is dynamically\r\n                decided based on the available resources.\r\n        \"\"\"  # noqa: E501\r\n        if ray_remote_args is None:\r\n            ray_remote_args = {}\r\n\r\n        # Each write task will launch individual remote tasks to write each block\r\n        # To avoid duplicate block writes, the write task should not be retried\r\n        if ray_remote_args.get(\"max_retries\", 0) != 0:\r\n            warnings.warn(\r\n                \"The max_retries of a BigQuery Write Task should be set to 0\"\r\n                \" to avoid duplicate writes.\"\r\n            )\r\n        else:\r\n            ray_remote_args[\"max_retries\"] = 0\r\n\r\n        datasink = BigQueryDatasink(\r\n            project_id=project_id,\r\n            dataset=dataset,\r\n            max_retry_cnt=max_retry_cnt,\r\n            overwrite_table=overwrite_table,\r\n        )\r\n        self.write_datasink(\r\n            datasink,\r\n            ray_remote_args=ray_remote_args,\r\n            concurrency=concurrency,\r\n        )",
        "labels": [
            "Empty Column Misinitialization"
        ]
    },
    {
        "code": "def test_parquet_read_partitioned_with_filter(ray_start_regular_shared, tmp_path):\r\n    df = pd.DataFrame(\r\n        {\"one\": [1, 1, 1, 3, 3, 3], \"two\": [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"]}\r\n    )\r\n    table = pa.Table.from_pandas(df)\r\n    pq.write_to_dataset(\r\n        table, root_path=str(tmp_path), partition_cols=[\"one\"], use_legacy_dataset=False\r\n    )\r\n\r\n    # 2 partitions, 1 empty partition, 1 block/read task\r\n\r\n    ds = ray.data.read_parquet(\r\n        str(tmp_path), override_num_blocks=1, filter=(pa.dataset.field(\"two\") == \"a\")\r\n    )\r\n\r\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\r\n    assert sorted(values) == [[\"1\", \"a\"], [\"1\", \"a\"]]\r\n    assert ds.count() == 2\r\n\r\n    # 2 partitions, 1 empty partition, 2 block/read tasks, 1 empty block\r\n\r\n    ds = ray.data.read_parquet(\r\n        str(tmp_path), override_num_blocks=2, filter=(pa.dataset.field(\"two\") == \"a\")\r\n    )\r\n\r\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\r\n    assert sorted(values) == [[\"1\", \"a\"], [\"1\", \"a\"]]\r\n    assert ds.count() == 2",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_element_spec_shape_with_ragged_tensors(\r\n        self, batch_size, include_additional_columns\r\n    ):\r\n        df = pd.DataFrame(\r\n            {\r\n                \"spam\": [np.zeros([32, 32, 3]), np.zeros([64, 64, 3])],\r\n                \"ham\": [0, 0],\r\n                \"weight\": [np.zeros([32, 32, 3]), np.zeros([64, 64, 3])],\r\n            }\r\n        )\r\n        ds = ray.data.from_pandas(df)\r\n\r\n        if include_additional_columns:\r\n            dataset = ds.to_tf(\r\n                feature_columns=\"spam\",\r\n                label_columns=\"ham\",\r\n                additional_columns=\"weight\",\r\n                batch_size=batch_size,\r\n            )\r\n            feature_spec, _, additional_spec = dataset.element_spec\r\n            assert tuple(additional_spec.shape) == (None, None, None, None)\r\n        else:\r\n            dataset = ds.to_tf(\r\n                feature_columns=\"spam\", label_columns=\"ham\", batch_size=batch_size\r\n            )\r\n            feature_spec, _ = dataset.element_spec\r\n\r\n        assert tuple(feature_spec.shape) == (None, None, None, None)\r\n\r\n        if include_additional_columns:\r\n            features, labels, additional_metadata = next(iter(dataset))\r\n            assert tuple(additional_metadata.shape) == (batch_size, None, None, None)\r\n        else:\r\n            features, labels = next(iter(dataset))\r\n        assert tuple(features.shape) == (batch_size, None, None, None)\r\n        assert tuple(labels.shape) == (batch_size,)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_training(self, include_additional_columns):\r\n        def build_model() -> tf.keras.Model:\r\n            return tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\n\r\n        def train_func():\r\n            strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n            with strategy.scope():\r\n                multi_worker_model = build_model()\r\n                multi_worker_model.compile(\r\n                    optimizer=tf.keras.optimizers.SGD(),\r\n                    loss=mae,\r\n                    metrics=[mse],\r\n                )\r\n\r\n            if include_additional_columns:\r\n                dataset = train.get_dataset_shard(\"train\").to_tf(\r\n                    \"X\", \"Y\", additional_columns=\"W\", batch_size=4\r\n                )\r\n            else:\r\n                dataset = train.get_dataset_shard(\"train\").to_tf(\"X\", \"Y\", batch_size=4)\r\n            multi_worker_model.fit(dataset)\r\n\r\n        dataset = ray.data.from_items(8 * [{\"X0\": 0, \"X1\": 0, \"Y\": 0, \"W\": 0}])\r\n        concatenator = Concatenator(columns=[\"X0\", \"X1\"], output_column_name=\"X\")\r\n        dataset = concatenator.transform(dataset)\r\n\r\n        trainer = TensorflowTrainer(\r\n            train_loop_per_worker=train_func,\r\n            scaling_config=ScalingConfig(num_workers=2),\r\n            datasets={\"train\": dataset},\r\n        )\r\n        trainer.fit()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_element_spec_type_with_multiple_columns(self, include_additional_columns):\r\n        ds = ray.data.from_items(\r\n            [{\"spam\": 0, \"ham\": 0, \"eggs\": 0, \"weight1\": 0, \"weight2\": 0}]\r\n        )\r\n\r\n        if include_additional_columns:\r\n            dataset = ds.to_tf(\r\n                feature_columns=[\"spam\", \"ham\"],\r\n                label_columns=\"eggs\",\r\n                additional_columns=[\"weight1\", \"weight2\"],\r\n            )\r\n            (\r\n                feature_output_signature,\r\n                _,\r\n                additional_output_signature,\r\n            ) = dataset.element_spec\r\n        else:\r\n            dataset = ds.to_tf(feature_columns=[\"spam\", \"ham\"], label_columns=\"eggs\")\r\n            feature_output_signature, _ = dataset.element_spec\r\n\r\n        assert isinstance(feature_output_signature, dict)\r\n        assert feature_output_signature.keys() == {\"spam\", \"ham\"}\r\n        assert all(\r\n            isinstance(value, tf.TypeSpec)\r\n            for value in feature_output_signature.values()\r\n        )\r\n\r\n        if include_additional_columns:\r\n            assert isinstance(additional_output_signature, dict)\r\n            assert additional_output_signature.keys() == {\"weight1\", \"weight2\"}\r\n            assert all(\r\n                isinstance(value, tf.TypeSpec)\r\n                for value in additional_output_signature.values()\r\n            )\r\n\r\n        df = pd.DataFrame(\r\n            {\r\n                \"feature1\": [0, 1, 2],\r\n                \"feature2\": [3, 4, 5],\r\n                \"label\": [0, 1, 1],\r\n                \"weight1\": [0, 0.1, 0.2],\r\n                \"weight2\": [0.3, 0.4, 0.5],\r\n            }\r\n        )\r\n        ds = ray.data.from_pandas(df)\r\n\r\n        if include_additional_columns:\r\n            dataset = ds.to_tf(\r\n                feature_columns=[\"feature1\", \"feature2\"],\r\n                label_columns=\"label\",\r\n                additional_columns=[\"weight1\", \"weight2\"],\r\n                batch_size=3,\r\n            )\r\n            (\r\n                feature_output_signature,\r\n                _,\r\n                additional_output_signature,\r\n            ) = dataset.element_spec\r\n            assert isinstance(additional_output_signature, dict)\r\n            assert additional_output_signature.keys() == {\"weight1\", \"weight2\"}\r\n            assert all(\r\n                isinstance(value, tf.TypeSpec)\r\n                for value in additional_output_signature.values()\r\n            )\r\n        else:\r\n            dataset = ds.to_tf(\r\n                feature_columns=[\"feature1\", \"feature2\"],\r\n                label_columns=\"label\",\r\n                batch_size=3,\r\n            )\r\n            feature_output_signature, _ = dataset.element_spec\r\n\r\n        assert isinstance(feature_output_signature, dict)\r\n        assert feature_output_signature.keys() == {\"feature1\", \"feature2\"}\r\n        assert all(\r\n            isinstance(value, tf.TypeSpec)\r\n            for value in feature_output_signature.values()\r\n        )\r\n\r\n        if include_additional_columns:\r\n            features, labels, additional_metadata = next(iter(dataset))\r\n            assert (\r\n                additional_metadata[\"weight1\"].numpy() == df[\"weight1\"].values\r\n            ).all()\r\n            assert (\r\n                additional_metadata[\"weight2\"].numpy() == df[\"weight2\"].values\r\n            ).all()\r\n        else:\r\n            features, labels = next(iter(dataset))\r\n        assert (labels.numpy() == df[\"label\"].values).all()\r\n        assert (features[\"feature1\"].numpy() == df[\"feature1\"].values).all()\r\n        assert (features[\"feature2\"].numpy() == df[\"feature2\"].values).all()",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_basic(self):\r\n        df = pd.DataFrame(\r\n            {\r\n                \"a\": [1, 2, 3, 4],\r\n                \"b\": [5, 6, 7, 8],\r\n            }\r\n        )\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"a\", \"b\"], output_column_name=\"c\")\r\n        new_ds = prep.transform(ds)\r\n        for i, row in enumerate(new_ds.take()):\r\n            assert np.array_equal(row[\"c\"], np.array([i + 1, i + 5]))",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_preserves_order(self):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 3, 4, 5]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"a\", \"b\"], output_column_name=\"c\")\r\n        prep = prep.fit(ds)\r\n\r\n        df = pd.DataFrame({\"a\": [5, 6, 7, 8], \"b\": [6, 7, 8, 9]})\r\n        concatenated_df = prep.transform_batch(df)\r\n        expected_df = pd.DataFrame({\"c\": [[5, 6], [6, 7], [7, 8], [8, 9]]})\r\n        assert_frame_equal(concatenated_df, expected_df)\r\n\r\n        other_df = pd.DataFrame({\"a\": [9, 10, 11, 12], \"b\": [10, 11, 12, 13]})\r\n        concatenated_other_df = prep.transform_batch(other_df)\r\n        expected_df = pd.DataFrame(\r\n            {\r\n                \"c\": [\r\n                    [9, 10],\r\n                    [10, 11],\r\n                    [11, 12],\r\n                    [12, 13],\r\n                ]\r\n            }\r\n        )\r\n        assert_frame_equal(concatenated_other_df, expected_df)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_exclude_column(self):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 3, 4, 5], \"c\": [3, 4, 5, 6]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"a\", \"c\"])\r\n        new_ds = prep.transform(ds)\r\n        for _, row in enumerate(new_ds.take()):\r\n            assert set(row) == {\"concat_out\", \"b\"}",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_raise_if_missing(self):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3, 4]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(\r\n            columns=[\"a\", \"b\"], output_column_name=\"c\", raise_if_missing=True\r\n        )\r\n\r\n        with pytest.raises(UserCodeException):\r\n            with pytest.raises(ValueError, match=\"'b'\"):\r\n                prep.transform(ds).materialize()",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_strings(self):\r\n        df = pd.DataFrame({\"a\": [\"string\", \"string2\", \"string3\"]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"a\"], output_column_name=\"huh\")\r\n        new_ds = prep.transform(ds)\r\n        assert \"huh\" in set(new_ds.schema().names)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_change_column_order(self):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 3, 4, 5]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"b\", \"a\"])\r\n        new_ds = prep.transform(ds)\r\n        expected_df = pd.DataFrame({\"concat_out\": [[2, 1], [3, 2], [4, 3], [5, 4]]})\r\n        print(new_ds.to_pandas())\r\n        assert_frame_equal(new_ds.to_pandas(), expected_df)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_include_columns(self):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 3, 4, 5], \"c\": [3, 4, 5, 6]})\r\n        ds = ray.data.from_pandas(df)\r\n        prep = Concatenator(columns=[\"a\", \"b\"])\r\n        new_ds = prep.transform(ds)\r\n        for _, row in enumerate(new_ds.take()):\r\n            assert set(row) == {\"concat_out\", \"c\"}",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_imputer_constant_categorical():\r\n    data = {\r\n        \"A_cat\": [\"one\", \"two\", None, \"four\"],\r\n    }\r\n    df = pd.DataFrame(data)\r\n    df[\"A_cat\"] = df[\"A_cat\"].astype(\"category\")\r\n    dataset = ray.data.from_pandas(df)\r\n\r\n    imputer = SimpleImputer(columns=[\"A_cat\"], strategy=\"constant\", fill_value=\"three\")\r\n    imputer.fit(dataset)\r\n\r\n    transformed_df = imputer.transform_batch(df)\r\n\r\n    expected = {\r\n        \"A_cat\": [\"one\", \"two\", \"three\", \"four\"],\r\n    }\r\n\r\n    for column in data.keys():\r\n        np.testing.assert_array_equal(transformed_df[column].values, expected[column])\r\n\r\n    df = pd.DataFrame({\"A\": [1, 2, 3, 4]})\r\n    transformed_df = imputer.transform_batch(df)\r\n\r\n    expected = {\r\n        \"A\": [1, 2, 3, 4],\r\n        \"A_cat\": [\"three\", \"three\", \"three\", \"three\"],\r\n    }\r\n\r\n    for column in df:\r\n        np.testing.assert_array_equal(transformed_df[column].values, expected[column])",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multi_hot_encoder():\r\n    \"\"\"Tests basic MultiHotEncoder functionality.\"\"\"\r\n    col_a = [\"red\", \"green\", \"blue\", \"red\"]\r\n    col_b = [\"warm\", \"cold\", \"hot\", \"cold\"]\r\n    col_c = [1, 10, 5, 10]\r\n    col_d = [[\"warm\"], [], [\"hot\", \"warm\", \"cold\"], [\"cold\", \"cold\"]]\r\n    in_df = pd.DataFrame.from_dict({\"A\": col_a, \"B\": col_b, \"C\": col_c, \"D\": col_d})\r\n    ds = ray.data.from_pandas(in_df)\r\n\r\n    encoder = MultiHotEncoder([\"B\", \"C\", \"D\"])\r\n\r\n    # Transform with unfitted preprocessor.\r\n    with pytest.raises(PreprocessorNotFittedException):\r\n        encoder.transform(ds)\r\n\r\n    # Fit data.\r\n    encoder.fit(ds)\r\n\r\n    assert encoder.stats_ == {\r\n        \"unique_values(B)\": {\"cold\": 0, \"hot\": 1, \"warm\": 2},\r\n        \"unique_values(C)\": {1: 0, 5: 1, 10: 2},\r\n        \"unique_values(D)\": {\"cold\": 0, \"hot\": 1, \"warm\": 2},\r\n    }\r\n\r\n    # Transform data.\r\n    transformed = encoder.transform(ds)\r\n    out_df = transformed.to_pandas()\r\n\r\n    processed_col_a = col_a\r\n    processed_col_b = [[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]]\r\n    processed_col_c = [[1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1]]\r\n    processed_col_d = [[0, 0, 1], [0, 0, 0], [1, 1, 1], [2, 0, 0]]\r\n    expected_df = pd.DataFrame.from_dict(\r\n        {\r\n            \"A\": processed_col_a,\r\n            \"B\": processed_col_b,\r\n            \"C\": processed_col_c,\r\n            \"D\": processed_col_d,\r\n        }\r\n    )\r\n\r\n    assert out_df.equals(expected_df)\r\n\r\n    # Transform batch.\r\n    pred_col_a = [\"blue\", \"yellow\", None]\r\n    pred_col_b = [\"cold\", \"warm\", \"other\"]\r\n    pred_col_c = [10, 1, 20]\r\n    pred_col_d = [[\"cold\", \"warm\"], [], [\"other\", \"cold\"]]\r\n    pred_in_df = pd.DataFrame.from_dict(\r\n        {\"A\": pred_col_a, \"B\": pred_col_b, \"C\": pred_col_c, \"D\": pred_col_d}\r\n    )\r\n\r\n    pred_out_df = encoder.transform_batch(pred_in_df)\r\n    print(pred_out_df.to_string())\r\n\r\n    pred_processed_col_a = [\"blue\", \"yellow\", None]\r\n    pred_processed_col_b = [[1, 0, 0], [0, 0, 1], [0, 0, 0]]\r\n    pred_processed_col_c = [[0, 0, 1], [1, 0, 0], [0, 0, 0]]\r\n    pred_processed_col_d = [[1, 0, 1], [0, 0, 0], [1, 0, 0]]\r\n    pred_expected_df = pd.DataFrame.from_dict(\r\n        {\r\n            \"A\": pred_processed_col_a,\r\n            \"B\": pred_processed_col_b,\r\n            \"C\": pred_processed_col_c,\r\n            \"D\": pred_processed_col_d,\r\n        }\r\n    )\r\n\r\n    assert pred_out_df.equals(pred_expected_df)\r\n\r\n    # Test null behavior.\r\n    null_col = [1, None]\r\n    nonnull_col = [1, 1]\r\n    null_df = pd.DataFrame.from_dict({\"A\": null_col})\r\n    null_ds = ray.data.from_pandas(null_df)\r\n    nonnull_df = pd.DataFrame.from_dict({\"A\": nonnull_col})\r\n    nonnull_ds = ray.data.from_pandas(nonnull_df)\r\n    null_encoder = OneHotEncoder([\"A\"])\r\n\r\n    # Verify fit fails for null values.\r\n    with pytest.raises(ValueError):\r\n        null_encoder.fit(null_ds)\r\n    null_encoder.fit(nonnull_ds)\r\n\r\n    # Verify transform fails for null values.\r\n    with pytest.raises((UserCodeException, ValueError)):\r\n        null_encoder.transform(null_ds).materialize()\r\n    null_encoder.transform(nonnull_ds)\r\n\r\n    # Verify transform_batch fails for null values.\r\n    with pytest.raises(ValueError):\r\n        null_encoder.transform_batch(null_df)\r\n    null_encoder.transform_batch(nonnull_df)\r\n\r\n    # Verify that `fit` and `transform` work with ndarrays.\r\n    df = pd.DataFrame({\"column\": [np.array([\"A\"]), np.array([\"A\", \"B\"])]})\r\n    ds = ray.data.from_pandas(df)\r\n    encoder = MultiHotEncoder([\"column\"])\r\n    transformed = encoder.fit_transform(ds)\r\n    encodings = [record[\"column\"] for record in transformed.take_all()]\r\n    assert encodings == [[1, 0], [1, 1]]",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_imputer_all_nan_raise_error():\r\n    data = {\r\n        \"A\": [np.nan, np.nan, np.nan, np.nan],\r\n    }\r\n    df = pd.DataFrame(data)\r\n    dataset = ray.data.from_pandas(df)\r\n\r\n    imputer = SimpleImputer(columns=[\"A\"], strategy=\"mean\")\r\n    imputer.fit(dataset)\r\n\r\n    with pytest.raises(ValueError):\r\n        imputer.transform_batch(df)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_all_formats(create_dummy_preprocessors, dataset_format):\r\n    (\r\n        with_nothing,\r\n        with_pandas,\r\n        with_numpy,\r\n        with_pandas_and_numpy,\r\n        with_pandas_and_numpy_preferred,\r\n    ) = create_dummy_preprocessors\r\n\r\n    if dataset_format == \"simple\":\r\n        ds = ray.data.range(10)\r\n    elif dataset_format == \"pandas\":\r\n        df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"A\", \"B\", \"C\"])\r\n        ds = ray.data.from_pandas(df)\r\n    elif dataset_format == \"arrow\":\r\n        df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"A\", \"B\", \"C\"])\r\n        ds = ray.data.from_arrow(pyarrow.Table.from_pandas(df))\r\n    else:\r\n        raise ValueError(f\"Untested dataset_format configuration: {dataset_format}.\")\r\n\r\n    with pytest.raises(NotImplementedError):\r\n        with_nothing.transform(ds)\r\n\r\n    patcher = patch.object(ray.data.dataset.Dataset, \"map_batches\")\r\n\r\n    with patcher as mock_map_batches:\r\n        with_pandas.transform(ds)\r\n        mock_map_batches.assert_called_once_with(\r\n            with_pandas._transform_pandas, batch_format=BatchFormat.PANDAS\r\n        )\r\n\r\n    with patcher as mock_map_batches:\r\n        with_numpy.transform(ds)\r\n        mock_map_batches.assert_called_once_with(\r\n            with_numpy._transform_numpy, batch_format=BatchFormat.NUMPY\r\n        )\r\n\r\n    # Pandas preferred by default.\r\n    with patcher as mock_map_batches:\r\n        with_pandas_and_numpy.transform(ds)\r\n    mock_map_batches.assert_called_once_with(\r\n        with_pandas_and_numpy._transform_pandas, batch_format=BatchFormat.PANDAS\r\n    )\r\n\r\n    with patcher as mock_map_batches:\r\n        with_pandas_and_numpy_preferred.transform(ds)\r\n    mock_map_batches.assert_called_once_with(\r\n        with_pandas_and_numpy_preferred._transform_numpy, batch_format=BatchFormat.NUMPY\r\n    )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_config():\r\n    \"\"\"Tests that the transform_config of\r\n    the Preprocessor is respected during transform.\"\"\"\r\n\r\n    batch_size = 2\r\n\r\n    class DummyPreprocessor(Preprocessor):\r\n        _is_fittable = False\r\n\r\n        def _transform_numpy(self, data):\r\n            assert len(data[\"value\"]) == batch_size\r\n            return data\r\n\r\n        def _transform_pandas(self, data):\r\n            raise RuntimeError(\r\n                \"Pandas transform should not be called with numpy batch format.\"\r\n            )\r\n\r\n        def _get_transform_config(self):\r\n            return {\"batch_size\": 2}\r\n\r\n        def _determine_transform_to_use(self):\r\n            return \"numpy\"\r\n\r\n    prep = DummyPreprocessor()\r\n    ds = ray.data.from_pandas(pd.DataFrame({\"value\": list(range(4))}))\r\n    prep.transform(ds)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sklearn_benchmarks(ray_start_cluster_2_nodes):\r\n    ESTIMATORS = {\r\n        \"CART\": DecisionTreeClassifier(),\r\n        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=10),\r\n        \"RandomForest\": RandomForestClassifier(),\r\n        \"Nystroem-SVM\": make_pipeline(\r\n            Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=1)\r\n        ),\r\n        \"SampledRBF-SVM\": make_pipeline(\r\n            RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=1)\r\n        ),\r\n        \"LogisticRegression-SAG\": LogisticRegression(solver=\"sag\", tol=1e-1, C=1e4),\r\n        \"LogisticRegression-SAGA\": LogisticRegression(solver=\"saga\", tol=1e-1, C=1e4),\r\n        \"MultilayerPerceptron\": MLPClassifier(\r\n            hidden_layer_sizes=(32, 32),\r\n            max_iter=100,\r\n            alpha=1e-4,\r\n            solver=\"sgd\",\r\n            learning_rate_init=0.2,\r\n            momentum=0.9,\r\n            verbose=1,\r\n            tol=1e-2,\r\n            random_state=1,\r\n        ),\r\n        \"MLP-adam\": MLPClassifier(\r\n            hidden_layer_sizes=(32, 32),\r\n            max_iter=100,\r\n            alpha=1e-4,\r\n            solver=\"adam\",\r\n            learning_rate_init=0.001,\r\n            verbose=1,\r\n            tol=1e-2,\r\n            random_state=1,\r\n        ),\r\n    }\r\n    # Load dataset.\r\n    print(\"Loading dataset...\")\r\n    unnormalized_X_train, y_train = pickle.load(\r\n        open(os.path.join(os.path.dirname(__file__), \"mnist_784_100_samples.pkl\"), \"rb\")\r\n    )\r\n    # Normalize features.\r\n    X_train = unnormalized_X_train / 255\r\n\r\n    register_ray()\r\n    train_time = {}\r\n    random_seed = 0\r\n    # Use two workers per classifier.\r\n    num_jobs = 2\r\n    with joblib.parallel_backend(\"ray\"):\r\n        for name in sorted(ESTIMATORS.keys()):\r\n            print(\"Training %s ... \" % name, end=\"\")\r\n            estimator = ESTIMATORS[name]\r\n            estimator_params = estimator.get_params()\r\n            estimator.set_params(\r\n                **{\r\n                    p: random_seed\r\n                    for p in estimator_params\r\n                    if p.endswith(\"random_state\")\r\n                }\r\n            )\r\n\r\n            if \"n_jobs\" in estimator_params:\r\n                estimator.set_params(n_jobs=num_jobs)\r\n            time_start = time.time()\r\n            estimator.fit(X_train, y_train)\r\n            train_time[name] = time.time() - time_start\r\n            print(\"training\", name, \"took\", train_time[name], \"seconds\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def train_func(config):\r\n    \"\"\"Your training function that will be launched on each worker.\"\"\"\r\n\r\n    # Unpack training configs\r\n    lr = config[\"lr\"]\r\n    seed = config[\"seed\"]\r\n    num_epochs = config[\"num_epochs\"]\r\n    train_batch_size = config[\"train_batch_size\"]\r\n    eval_batch_size = config[\"eval_batch_size\"]\r\n\r\n    set_seed(seed)\r\n\r\n    # Initialize accelerator\r\n    accelerator = Accelerator()\r\n\r\n    # Load datasets and metrics\r\n    metric = evaluate.load(\"glue\", \"mrpc\")\r\n\r\n    # Prepare PyTorch DataLoaders\r\n    # ====================================================\r\n    hf_datasets = load_dataset(\"glue\", \"mrpc\")\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\n    def collate_fn(batch):\r\n        outputs = tokenizer(\r\n            [sample[\"sentence1\"] for sample in batch],\r\n            [sample[\"sentence2\"] for sample in batch],\r\n            truncation=True,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n        )\r\n        outputs[\"labels\"] = torch.LongTensor([sample[\"label\"] for sample in batch])\r\n        outputs = {k: v.to(accelerator.device) for k, v in outputs.items()}\r\n        return outputs\r\n\r\n    # Instantiate dataloaders.\r\n    train_dataloader = DataLoader(\r\n        hf_datasets[\"train\"],\r\n        shuffle=True,\r\n        collate_fn=collate_fn,\r\n        batch_size=train_batch_size,\r\n        drop_last=True,\r\n    )\r\n    eval_dataloader = DataLoader(\r\n        hf_datasets[\"validation\"],\r\n        shuffle=False,\r\n        collate_fn=collate_fn,\r\n        batch_size=eval_batch_size,\r\n        drop_last=True,\r\n    )\r\n    # ====================================================\r\n\r\n    # Instantiate the model, optimizer, lr_scheduler\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n        \"bert-base-cased\", return_dict=True\r\n    )\r\n\r\n    optimizer = AdamW(params=model.parameters(), lr=lr)\r\n\r\n    steps_per_epoch = len(train_dataloader)\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=100,\r\n        num_training_steps=(steps_per_epoch * num_epochs),\r\n    )\r\n\r\n    # Prepare everything with accelerator\r\n    (\r\n        model,\r\n        optimizer,\r\n        train_dataloader,\r\n        eval_dataloader,\r\n        lr_scheduler,\r\n    ) = accelerator.prepare(\r\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\r\n    )\r\n\r\n    for epoch in range(num_epochs):\r\n        # Training\r\n        model.train()\r\n        for batch in train_dataloader:\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        # Evaluation\r\n        model.eval()\r\n        for batch in eval_dataloader:\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            predictions = outputs.logits.argmax(dim=-1)\r\n\r\n            predictions, references = accelerator.gather_for_metrics(\r\n                (predictions, batch[\"labels\"])\r\n            )\r\n            metric.add_batch(\r\n                predictions=predictions,\r\n                references=references,\r\n            )\r\n\r\n        eval_metric = metric.compute()\r\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\r\n\r\n        # Report Checkpoint and metrics to Ray Train\r\n        # ==========================================\r\n        with TemporaryDirectory() as tmpdir:\r\n            if accelerator.is_main_process:\r\n                unwrapped_model = accelerator.unwrap_model(model)\r\n                accelerator.save(unwrapped_model, f\"{tmpdir}/ckpt_{epoch}.bin\")\r\n                checkpoint = Checkpoint.from_directory(tmpdir)\r\n            else:\r\n                checkpoint = None\r\n            ray.train.report(metrics=eval_metric, checkpoint=checkpoint)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(config):\r\n    \"\"\"Your training function that launches on each worker.\"\"\"\r\n\r\n    # Unpack training configs\r\n    lr = config[\"lr\"]\r\n    seed = config[\"seed\"]\r\n    num_epochs = config[\"num_epochs\"]\r\n    train_batch_size = config[\"train_batch_size\"]\r\n    eval_batch_size = config[\"eval_batch_size\"]\r\n    train_ds_size = config[\"train_dataset_size\"]\r\n\r\n    set_seed(seed)\r\n\r\n    # Initialize accelerator\r\n    accelerator = Accelerator()\r\n\r\n    # Load datasets and metrics\r\n    metric = evaluate.load(\"glue\", \"mrpc\")\r\n\r\n    # Prepare Ray Data loaders\r\n    # ====================================================\r\n    train_ds = ray.train.get_dataset_shard(\"train\")\r\n    eval_ds = ray.train.get_dataset_shard(\"validation\")\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\n    def collate_fn(batch):\r\n        outputs = tokenizer(\r\n            list(batch[\"sentence1\"]),\r\n            list(batch[\"sentence2\"]),\r\n            truncation=True,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n        )\r\n        outputs[\"labels\"] = torch.LongTensor(batch[\"label\"])\r\n        outputs = {k: v.to(accelerator.device) for k, v in outputs.items()}\r\n        return outputs\r\n\r\n    train_dataloader = train_ds.iter_torch_batches(\r\n        batch_size=train_batch_size, collate_fn=collate_fn\r\n    )\r\n    eval_dataloader = eval_ds.iter_torch_batches(\r\n        batch_size=eval_batch_size, collate_fn=collate_fn\r\n    )\r\n    # ====================================================\r\n\r\n    # Instantiate the model, optimizer, lr_scheduler\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n        \"bert-base-cased\", return_dict=True\r\n    )\r\n\r\n    optimizer = AdamW(params=model.parameters(), lr=lr)\r\n\r\n    steps_per_epoch = train_ds_size // (accelerator.num_processes * train_batch_size)\r\n    lr_scheduler = get_linear_schedule_with_warmup(\r\n        optimizer=optimizer,\r\n        num_warmup_steps=100,\r\n        num_training_steps=(steps_per_epoch * num_epochs),\r\n    )\r\n\r\n    # Prepare everything with accelerator\r\n    model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)\r\n\r\n    for epoch in range(num_epochs):\r\n        # Training\r\n        model.train()\r\n        for batch in train_dataloader:\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        # Evaluation\r\n        model.eval()\r\n        for batch in eval_dataloader:\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            predictions = outputs.logits.argmax(dim=-1)\r\n\r\n            predictions, references = accelerator.gather_for_metrics(\r\n                (predictions, batch[\"labels\"])\r\n            )\r\n            metric.add_batch(\r\n                predictions=predictions,\r\n                references=references,\r\n            )\r\n\r\n        eval_metric = metric.compute()\r\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\r\n\r\n        # Report checkpoint and metrics to Ray Train\r\n        # ==========================================\r\n        with TemporaryDirectory() as tmpdir:\r\n            if accelerator.is_main_process:\r\n                unwrapped_model = accelerator.unwrap_model(model)\r\n                accelerator.save(unwrapped_model, f\"{tmpdir}/ckpt_{epoch}.bin\")\r\n                checkpoint = Checkpoint.from_directory(tmpdir)\r\n            else:\r\n                checkpoint = None\r\n            ray.train.report(metrics=eval_metric, checkpoint=checkpoint)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(config):\r\n    \"\"\"Your training function that will be launched on each worker.\"\"\"\r\n\r\n    # Unpack training configs\r\n    set_seed(config[\"seed\"])\r\n    num_epochs = config[\"num_epochs\"]\r\n    train_batch_size = config[\"train_batch_size\"]\r\n    eval_batch_size = config[\"eval_batch_size\"]\r\n\r\n    # Instantiate the Model\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n        \"bert-base-cased\", return_dict=True\r\n    )\r\n\r\n    # Prepare Ray Data Loaders\r\n    # ====================================================\r\n    train_ds = ray.train.get_dataset_shard(\"train\")\r\n    eval_ds = ray.train.get_dataset_shard(\"validation\")\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\n    def collate_fn(batch):\r\n        outputs = tokenizer(\r\n            list(batch[\"sentence1\"]),\r\n            list(batch[\"sentence2\"]),\r\n            truncation=True,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n        )\r\n        outputs[\"labels\"] = torch.LongTensor(batch[\"label\"])\r\n        return outputs\r\n\r\n    train_dataloader = train_ds.iter_torch_batches(\r\n        batch_size=train_batch_size, collate_fn=collate_fn\r\n    )\r\n    eval_dataloader = eval_ds.iter_torch_batches(\r\n        batch_size=eval_batch_size, collate_fn=collate_fn\r\n    )\r\n    # ====================================================\r\n\r\n    # Initialize DeepSpeed Engine\r\n    model, optimizer, _, lr_scheduler = deepspeed.initialize(\r\n        model=model,\r\n        model_parameters=model.parameters(),\r\n        config=deepspeed_config,\r\n    )\r\n    device = get_accelerator().device_name(model.local_rank)\r\n\r\n    # Initialize Evaluation Metrics\r\n    f1 = BinaryF1Score().to(device)\r\n    accuracy = BinaryAccuracy().to(device)\r\n\r\n    for epoch in range(num_epochs):\r\n        # Training\r\n        model.train()\r\n        for batch in train_dataloader:\r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            model.backward(loss)\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        # Evaluation\r\n        model.eval()\r\n        for batch in eval_dataloader:\r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            predictions = outputs.logits.argmax(dim=-1)\r\n\r\n            f1.update(predictions, batch[\"labels\"])\r\n            accuracy.update(predictions, batch[\"labels\"])\r\n\r\n        # torchmetrics will aggregate the metrics across all workers\r\n        eval_metric = {\r\n            \"f1\": f1.compute().item(),\r\n            \"accuracy\": accuracy.compute().item(),\r\n        }\r\n        f1.reset()\r\n        accuracy.reset()\r\n\r\n        if model.global_rank == 0:\r\n            print(f\"epoch {epoch}:\", eval_metric)\r\n\r\n        # Report checkpoint and metrics to Ray Train\r\n        # ==============================================================\r\n        with TemporaryDirectory() as tmpdir:\r\n            # Each worker saves its own checkpoint shard\r\n            model.save_checkpoint(tmpdir)\r\n\r\n            # Ensure all workers finished saving their checkpoint shard\r\n            torch.distributed.barrier()\r\n\r\n            # Report checkpoint shards from each worker in parallel\r\n            ray.train.report(\r\n                metrics=eval_metric, checkpoint=Checkpoint.from_directory(tmpdir)\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(config):\r\n    \"\"\"Your training function that will be launched on each worker.\"\"\"\r\n\r\n    # Unpack training configs\r\n    set_seed(config[\"seed\"])\r\n    num_epochs = config[\"num_epochs\"]\r\n    eval_batch_size = config[\"eval_batch_size\"]\r\n\r\n    # Instantiate the Model\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n        \"bert-base-cased\", return_dict=True\r\n    )\r\n\r\n    # Prepare PyTorch Data Loaders\r\n    # ====================================================\r\n    hf_datasets = load_dataset(\"glue\", \"mrpc\")\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\n    def collate_fn(batch):\r\n        outputs = tokenizer(\r\n            [sample[\"sentence1\"] for sample in batch],\r\n            [sample[\"sentence2\"] for sample in batch],\r\n            truncation=True,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n        )\r\n        outputs[\"labels\"] = torch.LongTensor([sample[\"label\"] for sample in batch])\r\n        return outputs\r\n\r\n    # Instantiate dataloaders.\r\n    # The train_dataloader already created by `deepspeed.initialize`\r\n    eval_dataloader = DataLoader(\r\n        hf_datasets[\"validation\"],\r\n        shuffle=False,\r\n        collate_fn=collate_fn,\r\n        batch_size=eval_batch_size,\r\n        drop_last=True,\r\n    )\r\n    # ====================================================\r\n\r\n    # Initialize DeepSpeed Engine\r\n    model, optimizer, train_dataloader, lr_scheduler = deepspeed.initialize(\r\n        model=model,\r\n        model_parameters=model.parameters(),\r\n        training_data=hf_datasets[\"train\"],\r\n        collate_fn=collate_fn,\r\n        config=deepspeed_config,\r\n    )\r\n    device = get_accelerator().device_name(model.local_rank)\r\n\r\n    # Initialize Evaluation Metrics\r\n    f1 = BinaryF1Score().to(device)\r\n    accuracy = BinaryAccuracy().to(device)\r\n\r\n    for epoch in range(num_epochs):\r\n        # Training\r\n        model.train()\r\n        for batch in train_dataloader:\r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\n            outputs = model(**batch)\r\n            loss = outputs.loss\r\n            model.backward(loss)\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n\r\n        # Evaluation\r\n        model.eval()\r\n        for batch in eval_dataloader:\r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\n            with torch.no_grad():\r\n                outputs = model(**batch)\r\n            predictions = outputs.logits.argmax(dim=-1)\r\n\r\n            f1.update(predictions, batch[\"labels\"])\r\n            accuracy.update(predictions, batch[\"labels\"])\r\n\r\n        # torchmetrics will aggregate the metrics across all workers\r\n        eval_metric = {\r\n            \"f1\": f1.compute().item(),\r\n            \"accuracy\": accuracy.compute().item(),\r\n        }\r\n        f1.reset()\r\n        accuracy.reset()\r\n\r\n        if model.global_rank == 0:\r\n            print(f\"epoch {epoch}:\", eval_metric)\r\n\r\n        # Report checkpoint and metrics to Ray Train\r\n        # ==============================================================\r\n        with TemporaryDirectory() as tmpdir:\r\n            # Each worker saves its own checkpoint shard\r\n            model.save_checkpoint(tmpdir)\r\n\r\n            # Ensure all workers finished saving their checkpoint shard\r\n            torch.distributed.barrier()\r\n\r\n            # Report checkpoint shards from each worker in parallel\r\n            ray.train.report(\r\n                metrics=eval_metric, checkpoint=Checkpoint.from_directory(tmpdir)\r\n            )",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def get_datasets(split: float = 0.7) -> Tuple[Dataset]:\r\n    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/regression.csv\")\r\n\r\n    def combine_x(batch):\r\n        return pd.DataFrame(\r\n            {\r\n                \"x\": batch[[f\"x{i:03d}\" for i in range(100)]].values.tolist(),\r\n                \"y\": batch[\"y\"],\r\n            }\r\n        )\r\n\r\n    dataset = dataset.map_batches(combine_x, batch_format=\"pandas\")\r\n    train_dataset, validation_dataset = dataset.repartition(\r\n        num_blocks=4\r\n    ).train_test_split(split, shuffle=True)\r\n    return train_dataset, validation_dataset",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train_epoch(epoch, dataloader, model, loss_fn, optimizer):\r\n    if ray.train.get_context().get_world_size() > 1:\r\n        dataloader.sampler.set_epoch(epoch)\r\n\r\n    size = len(dataloader.dataset) // train.get_context().get_world_size()\r\n    model.train()\r\n    for batch, (X, y) in enumerate(dataloader):\r\n        # Compute prediction error\r\n        pred = model(X)\r\n        loss = loss_fn(pred, y)\r\n\r\n        # Backpropagation\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        if batch % 100 == 0:\r\n            loss, current = loss.item(), batch * len(X)\r\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def get_dataset(split_type=\"train\"):\r\n    def dataset_factory():\r\n        return tfds.load(\"mnist\", split=[split_type], as_supervised=True)[0].take(128)\r\n\r\n    dataset = ray.data.read_datasource(\r\n        SimpleTensorFlowDatasource(), dataset_factory=dataset_factory\r\n    )\r\n\r\n    def normalize_images(x):\r\n        x = np.float32(x.numpy()) / 255.0\r\n        x = np.reshape(x, (-1,))\r\n        return x\r\n\r\n    def preprocess_dataset(batch):\r\n        return [\r\n            (normalize_images(image), normalize_images(image)) for image, _ in batch\r\n        ]\r\n\r\n    dataset = dataset.map_batches(preprocess_dataset)\r\n\r\n    def convert_batch_to_pandas(batch):\r\n\r\n        images = [TensorArray(image) for image, _ in batch]\r\n        # because we did autoencoder here\r\n        df = pd.DataFrame({\"image\": images, \"label\": images})\r\n        return df\r\n\r\n    dataset = dataset.map_batches(convert_batch_to_pandas)\r\n    return dataset",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def training_step(self, batch):\r\n        if not self.restored and self.fail_epoch == self.current_epoch:\r\n            raise RuntimeError\r\n\r\n        output = self.forward(batch)\r\n        loss = torch.sum(output)\r\n        self.log(\"loss\", loss)\r\n        return loss",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def training_step(self, batch):\r\n        output = self.forward(batch)\r\n        loss = torch.sum(output)\r\n        self.log(\"loss\", loss)\r\n        return loss",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def train_epoch(dataset, model, device, criterion, optimizer):\r\n    num_correct = 0\r\n    num_total = 0\r\n    running_loss = 0.0\r\n\r\n    for i, (inputs, labels) in enumerate(dataset):\r\n        inputs = inputs.to(device)\r\n        labels = labels.to(device)\r\n\r\n        # Zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # Forward + backward + optimize\r\n        outputs = model(inputs.float())\r\n        loss = criterion(outputs, labels.float())\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # how are we doing?\r\n        predictions = (torch.sigmoid(outputs) > 0.5).int()\r\n        num_correct += (predictions == labels).sum().item()\r\n        num_total += len(outputs)\r\n\r\n        # Save loss to plot\r\n        running_loss += loss.item()\r\n        if i % 100 == 0:\r\n            print(f\"training batch [{i}] loss: {loss.item()}\")\r\n\r\n    return (running_loss, num_correct, num_total)",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def make_and_upload_dataset(dir_path):\r\n\r\n    import os\r\n    import random\r\n\r\n    import pandas as pd\r\n    import sklearn.datasets\r\n\r\n    NUM_EXAMPLES = 2_000_000\r\n    NUM_FEATURES = 20\r\n    PARQUET_FILE_CHUNK_SIZE = 50_000\r\n    NUM_FILES = NUM_EXAMPLES // PARQUET_FILE_CHUNK_SIZE\r\n\r\n    def create_data_chunk(n, d, seed, include_label=False):\r\n        X, y = sklearn.datasets.make_classification(\r\n            n_samples=n,\r\n            n_features=d,\r\n            n_informative=10,\r\n            n_redundant=2,\r\n            n_repeated=0,\r\n            n_classes=2,\r\n            n_clusters_per_class=3,\r\n            weights=None,\r\n            flip_y=0.03,\r\n            class_sep=0.8,\r\n            hypercube=True,\r\n            shift=0.0,\r\n            scale=1.0,\r\n            shuffle=False,\r\n            random_state=seed,\r\n        )\r\n\r\n        # turn into dataframe with column names\r\n        col_names = [\"feature_%0d\" % i for i in range(1, d + 1, 1)]\r\n        df = pd.DataFrame(X)\r\n        df.columns = col_names\r\n\r\n        # add some bogus categorical data columns\r\n        options = [\"apple\", \"banana\", \"orange\"]\r\n        df[\"fruit\"] = df.feature_1.map(\r\n            lambda x: random.choice(options)\r\n        )  # bogus, but nice to test categoricals\r\n\r\n        # add some nullable columns\r\n        options = [None, 1, 2]\r\n        df[\"nullable_feature\"] = df.feature_1.map(\r\n            lambda x: random.choice(options)\r\n        )  # bogus, but nice to test categoricals\r\n\r\n        # add label column\r\n        if include_label:\r\n            df[\"label\"] = y\r\n        return df\r\n\r\n    # create data files\r\n    print(\"Creating synthetic dataset...\")\r\n    data_path = os.path.join(dir_path, \"data\")\r\n    os.makedirs(data_path, exist_ok=True)\r\n    for i in range(NUM_FILES):\r\n        path = os.path.join(data_path, f\"data_{i:05d}.parquet.snappy\")\r\n        if not os.path.exists(path):\r\n            tmp_df = create_data_chunk(\r\n                n=PARQUET_FILE_CHUNK_SIZE, d=NUM_FEATURES, seed=i, include_label=True\r\n            )\r\n            tmp_df.to_parquet(path, compression=\"snappy\", index=False)\r\n        print(f\"Wrote {path} to disk...\")\r\n        # todo: at large enough scale we might want to upload the rest after\r\n        #  first N files rather than write to disk\r\n        # to simulate a user with local copy of subset of data\r\n\r\n    print(\"Creating synthetic inference dataset...\")\r\n    inference_path = os.path.join(dir_path, \"inference\")\r\n    os.makedirs(inference_path, exist_ok=True)\r\n    for i in range(NUM_FILES):\r\n        path = os.path.join(inference_path, f\"data_{i:05d}.parquet.snappy\")\r\n        if not os.path.exists(path):\r\n            tmp_df = create_data_chunk(\r\n                n=PARQUET_FILE_CHUNK_SIZE, d=NUM_FEATURES, seed=i, include_label=False\r\n            )\r\n            tmp_df.to_parquet(path, compression=\"snappy\", index=False)\r\n        print(f\"Wrote {path} to disk...\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _preprocess(\r\n        self, ds: ray.data.Dataset, inferencing: bool\r\n    ) -> Tuple[ray.data.Dataset, ray.data.Dataset]:\r\n        print(\"\\nStep 1: Dropping nulls, creating new_col, updating feature_1\\n\")\r\n\r\n        def batch_transformer(df: pd.DataFrame):\r\n            # Disable chained assignment warning.\r\n            pd.options.mode.chained_assignment = None\r\n\r\n            # Drop nulls.\r\n            df = df.dropna(subset=[\"nullable_feature\"])\r\n\r\n            # Add new column.\r\n            df[\"new_col\"] = (\r\n                df[\"feature_1\"] - 2 * df[\"feature_2\"] + df[\"feature_3\"]\r\n            ) / 3.0\r\n\r\n            # Transform column.\r\n            df[\"feature_1\"] = 2.0 * df[\"feature_1\"] + 0.1\r\n\r\n            return df\r\n\r\n        ds = ds.map_batches(batch_transformer, batch_format=\"pandas\")\r\n\r\n        print(\r\n            \"\\nStep 2: Precalculating fruit-grouped mean for new column and \"\r\n            \"for one-hot encoding (latter only uses fruit groups)\\n\"\r\n        )\r\n        agg_ds = ds.groupby(\"fruit\").mean(\"feature_1\")\r\n        fruit_means = {r[\"fruit\"]: r[\"mean(feature_1)\"] for r in agg_ds.take_all()}\r\n\r\n        print(\r\n            \"\\nStep 3: create mean_by_fruit as mean of feature_1 groupby \"\r\n            \"fruit; one-hot encode fruit column\\n\"\r\n        )\r\n\r\n        if inferencing:\r\n            assert self.fruits is not None\r\n        else:\r\n            assert self.fruits is None\r\n            self.fruits = list(fruit_means.keys())\r\n\r\n        fruit_one_hots = {\r\n            fruit: collections.defaultdict(int, fruit=1) for fruit in self.fruits\r\n        }\r\n\r\n        def batch_transformer(df: pd.DataFrame):\r\n            # Add column containing the feature_1-mean of the fruit groups.\r\n            df[\"mean_by_fruit\"] = df[\"fruit\"].map(fruit_means)\r\n\r\n            # One-hot encode the fruit column.\r\n            for fruit, one_hot in fruit_one_hots.items():\r\n                df[f\"fruit_{fruit}\"] = df[\"fruit\"].map(one_hot)\r\n\r\n            # Drop the fruit column, which is no longer needed.\r\n            df.drop(columns=\"fruit\", inplace=True)\r\n\r\n            return df\r\n\r\n        ds = ds.map_batches(batch_transformer, batch_format=\"pandas\")\r\n\r\n        if inferencing:\r\n            print(\"\\nStep 4: Standardize inference dataset\\n\")\r\n            assert self.standard_stats is not None\r\n        else:\r\n            assert self.standard_stats is None\r\n\r\n            print(\"\\nStep 4a: Split training dataset into train-test split\\n\")\r\n\r\n            # Split into train/test datasets.\r\n            split_index = int(0.9 * ds.count())\r\n            # Split into 90% training set, 10% test set.\r\n            train_ds, test_ds = ds.split_at_indices([split_index])\r\n\r\n            print(\r\n                \"\\nStep 4b: Precalculate training dataset stats for \"\r\n                \"standard scaling\\n\"\r\n            )\r\n            # Calculate stats needed for standard scaling feature columns.\r\n            feature_columns = [col for col in train_ds.schema().names if col != \"label\"]\r\n            standard_aggs = [\r\n                agg(on=col) for col in feature_columns for agg in (Mean, Std)\r\n            ]\r\n            self.standard_stats = train_ds.aggregate(*standard_aggs)\r\n            print(\"\\nStep 4c: Standardize training dataset\\n\")\r\n\r\n        # Standard scaling of feature columns.\r\n        standard_stats = self.standard_stats\r\n\r\n        def batch_standard_scaler(df: pd.DataFrame):\r\n            def column_standard_scaler(s: pd.Series):\r\n                if s.name == \"label\":\r\n                    # Don't scale the label column.\r\n                    return s\r\n                s_mean = standard_stats[f\"mean({s.name})\"]\r\n                s_std = standard_stats[f\"std({s.name})\"]\r\n                return (s - s_mean) / s_std\r\n\r\n            return df.transform(column_standard_scaler)\r\n\r\n        if inferencing:\r\n            # Apply standard scaling to inference dataset.\r\n            inference_ds = ds.map_batches(batch_standard_scaler, batch_format=\"pandas\")\r\n            return inference_ds, None\r\n        else:\r\n            # Apply standard scaling to both training dataset and test dataset.\r\n            train_ds = train_ds.map_batches(\r\n                batch_standard_scaler, batch_format=\"pandas\"\r\n            )\r\n            test_ds = test_ds.map_batches(batch_standard_scaler, batch_format=\"pandas\")\r\n            return train_ds, test_ds",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def create_data_chunk(n, d, seed, include_label=False):\r\n        X, y = sklearn.datasets.make_classification(\r\n            n_samples=n,\r\n            n_features=d,\r\n            n_informative=10,\r\n            n_redundant=2,\r\n            n_repeated=0,\r\n            n_classes=2,\r\n            n_clusters_per_class=3,\r\n            weights=None,\r\n            flip_y=0.03,\r\n            class_sep=0.8,\r\n            hypercube=True,\r\n            shift=0.0,\r\n            scale=1.0,\r\n            shuffle=False,\r\n            random_state=seed,\r\n        )\r\n\r\n        # turn into dataframe with column names\r\n        col_names = [\"feature_%0d\" % i for i in range(1, d + 1, 1)]\r\n        df = pd.DataFrame(X)\r\n        df.columns = col_names\r\n\r\n        # add some bogus categorical data columns\r\n        options = [\"apple\", \"banana\", \"orange\"]\r\n        df[\"fruit\"] = df.feature_1.map(\r\n            lambda x: random.choice(options)\r\n        )  # bogus, but nice to test categoricals\r\n\r\n        # add some nullable columns\r\n        options = [None, 1, 2]\r\n        df[\"nullable_feature\"] = df.feature_1.map(\r\n            lambda x: random.choice(options)\r\n        )  # bogus, but nice to test categoricals\r\n\r\n        # add label column\r\n        if include_label:\r\n            df[\"label\"] = y\r\n        return df",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def __call__(self, batch) -> \"pd.DataFrame\":\r\n            tensor = torch.FloatTensor(batch.values).to(self.device)\r\n            return pd.DataFrame(\r\n                self.model(tensor).cpu().detach().numpy(), columns=[\"value\"]\r\n            )",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def test_predict_dataframe(use_gpu):\r\n    predictor = TorchPredictor(model=DummyModelMultiInput(), use_gpu=use_gpu)\r\n\r\n    data_batch = pd.DataFrame({\"X0\": [0.0, 0.0, 0.0], \"X1\": [1.0, 2.0, 3.0]})\r\n    predictions = predictor.predict(data_batch, dtype=torch.float)\r\n\r\n    assert len(predictions) == 3\r\n    assert predictions.to_numpy().flatten().tolist() == [1.0, 2.0, 3.0]",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predict(batch_type):\r\n    predictor = TorchPredictor(model=DummyModelMultiInput())\r\n\r\n    raw_batch = pd.DataFrame({\"X0\": [0.0, 0.0, 0.0], \"X1\": [1.0, 2.0, 3.0]})\r\n    data_batch = _convert_pandas_to_batch_type(raw_batch, type=TYPE_TO_ENUM[batch_type])\r\n    raw_predictions = predictor.predict(data_batch, dtype=torch.float)\r\n    predictions = _convert_batch_type_to_pandas(raw_predictions)\r\n\r\n    assert len(predictions) == 3\r\n    assert predictions.to_numpy().flatten().tolist() == [1.0, 2.0, 3.0]",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multi_modal_real_model(use_gpu):\r\n    class CustomModule(torch.nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.linear1 = torch.nn.Linear(1, 1)\r\n            self.linear2 = torch.nn.Linear(1, 1)\r\n\r\n        def forward(self, input_dict: dict):\r\n            # Add feature dimension, expanding (batch_size,) to (batch_size, 1).\r\n            input_dict[\"A\"] = input_dict[\"A\"].unsqueeze(1)\r\n            input_dict[\"B\"] = input_dict[\"B\"].unsqueeze(1)\r\n            out1 = self.linear1(input_dict[\"A\"])\r\n            out2 = self.linear2(input_dict[\"B\"])\r\n            return out1 + out2\r\n\r\n    predictor = TorchPredictor(model=CustomModule(), use_gpu=use_gpu)\r\n\r\n    data = pd.DataFrame([[1, 2], [3, 4]], columns=[\"A\", \"B\"])\r\n\r\n    predictions = predictor.predict(data, dtype=torch.float)\r\n    assert len(predictions) == 2\r\n    if use_gpu:\r\n        assert next(\r\n            predictor.model.parameters()\r\n        ).is_cuda, \"Model should be moved to GPU if use_gpu is True\"\r\n    else:\r\n        assert not next(\r\n            predictor.model.parameters()\r\n        ).is_cuda, \"Model should not be on GPU if use_gpu is False\"",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _reset(self, tensordict: Optional[TensorDictBase] = None, **kwargs):\r\n        if tensordict is not None:\r\n            # We must avoid modifying the original tensordict so a shallow copy is necessary.\r\n            # We just select the input data and reset signal, which is all we need.\r\n            tensordict = tensordict.select(\r\n                *self.reset_keys, *self.state_spec.keys(True, True), strict=False\r\n            )\r\n        tensordict_reset = self.base_env._reset(tensordict, **kwargs)\r\n        if tensordict is None:\r\n            # make sure all transforms see a source tensordict\r\n            tensordict = tensordict_reset.empty()\r\n        self.base_env._complete_done(self.base_env.full_done_spec, tensordict_reset)\r\n        tensordict_reset = self.transform._reset(tensordict, tensordict_reset)\r\n        return tensordict_reset",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _predict_pandas(\r\n        self,\r\n        data: \"pd.DataFrame\",\r\n        feature_columns: Optional[Union[List[str], List[int]]] = None,\r\n        dmatrix_kwargs: Optional[Dict[str, Any]] = None,\r\n        **predict_kwargs,\r\n    ) -> \"pd.DataFrame\":\r\n        dmatrix_kwargs = dmatrix_kwargs or {}\r\n\r\n        feature_names = None\r\n        if TENSOR_COLUMN_NAME in data:\r\n            data = data[TENSOR_COLUMN_NAME].to_numpy()\r\n            data = _unwrap_ndarray_object_type_if_needed(data)\r\n            if feature_columns:\r\n                # In this case feature_columns is a list of integers\r\n                data = data[:, feature_columns]\r\n        elif feature_columns:\r\n            # feature_columns is a list of integers or strings\r\n            data = data[feature_columns].to_numpy()\r\n            # Only set the feature names if they are strings\r\n            if all(isinstance(fc, str) for fc in feature_columns):\r\n                feature_names = feature_columns\r\n        else:\r\n            feature_columns = data.columns.tolist()\r\n            data = data.to_numpy()\r\n\r\n            if all(isinstance(fc, str) for fc in feature_columns):\r\n                feature_names = feature_columns\r\n\r\n        if feature_names:\r\n            dmatrix_kwargs[\"feature_names\"] = feature_names\r\n\r\n        matrix = xgboost.DMatrix(data, **dmatrix_kwargs)\r\n        df = pd.DataFrame(self.model.predict(matrix, **predict_kwargs))\r\n        df.columns = (\r\n            [\"predictions\"]\r\n            if len(df.columns) == 1\r\n            else [f\"predictions_{i}\" for i in range(len(df.columns))]\r\n        )\r\n        return df",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train_func(model, optimizer, train_loader, device=None):\r\n    device = device or torch.device(\"cpu\")\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        if batch_idx * len(data) > EPOCH_SIZE:\r\n            return\r\n        data, target = data.to(device), target.to(device)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(\r\n    netD,\r\n    netG,\r\n    optimG,\r\n    optimD,\r\n    criterion,\r\n    dataloader,\r\n    iteration,\r\n    device,\r\n    mnist_model_ref,\r\n):\r\n    real_label = 1\r\n    fake_label = 0\r\n\r\n    for i, data in enumerate(dataloader, 0):\r\n        if i >= train_iterations_per_step:\r\n            break\r\n\r\n        netD.zero_grad()\r\n        real_cpu = data[0].to(device)\r\n        b_size = real_cpu.size(0)\r\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\r\n        output = netD(real_cpu).view(-1)\r\n        errD_real = criterion(output, label)\r\n        errD_real.backward()\r\n        D_x = output.mean().item()\r\n\r\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\r\n        fake = netG(noise)\r\n        label.fill_(fake_label)\r\n        output = netD(fake.detach()).view(-1)\r\n        errD_fake = criterion(output, label)\r\n        errD_fake.backward()\r\n        D_G_z1 = output.mean().item()\r\n        errD = errD_real + errD_fake\r\n        optimD.step()\r\n\r\n        netG.zero_grad()\r\n        label.fill_(real_label)\r\n        output = netD(fake).view(-1)\r\n        errG = criterion(output, label)\r\n        errG.backward()\r\n        D_G_z2 = output.mean().item()\r\n        optimG.step()\r\n\r\n        is_score, is_std = inception_score(fake, mnist_model_ref)\r\n\r\n        # Output training stats\r\n        if iteration % 10 == 0:\r\n            print(\r\n                \"[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z))\"\r\n                \": %.4f / %.4f \\tInception score: %.4f\"\r\n                % (\r\n                    iteration,\r\n                    len(dataloader),\r\n                    errD.item(),\r\n                    errG.item(),\r\n                    D_x,\r\n                    D_G_z1,\r\n                    D_G_z2,\r\n                    is_score,\r\n                )\r\n            )\r\n\r\n    return errG.item(), errD.item(), is_score",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_func(model, optimizer, train_loader):\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        # We set this just for the example to run quickly.\r\n        if batch_idx * len(data) > EPOCH_SIZE:\r\n            return\r\n        data, target = data.to(device), target.to(device)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def load_dataset_files(client, data_dir, file_path, nbytes, npartitions):\r\n    num_bytes_per_partition = nbytes // npartitions\r\n    filenames = []\r\n\r\n    @dask.delayed\r\n    def generate_file(i, data_dir, file_path):\r\n        key = \"{}/df-{}-{}.parquet.gzip\".format(file_path, num_bytes_per_partition, i)\r\n        from os import path\r\n\r\n        if path.exists(key):\r\n            print(f\"The file {key} already exists. Do nothing\")\r\n            return\r\n\r\n        filename = os.path.join(data_dir, key)\r\n        if not os.path.exists(filename):\r\n            print(\"Generating partition\", filename)\r\n            nrows = num_bytes_per_partition // 8\r\n            dataset = pd.DataFrame(\r\n                np.random.randint(\r\n                    0, np.iinfo(np.int64).max, size=(nrows, 1), dtype=np.int64\r\n                ),\r\n                columns=[\"a\"],\r\n            )\r\n            dataset.to_parquet(filename, compression=\"gzip\")\r\n        print(\"Writing partition to a file\", filename)\r\n\r\n    x = []\r\n    for i in range(npartitions):\r\n        x.append(generate_file(i, data_dir, file_path))\r\n    # from ray.util.dask import ProgressBarCallback\r\n    # with ProgressBarCallback():\r\n    #     dask.compute(x, _ray_enable_progress_bar=True)\r\n    dask.compute(x)\r\n\r\n    filenames = [\r\n        f\"{file_path}/df-{num_bytes_per_partition}-{i}.parquet.gzip\"\r\n        for i in range(npartitions)\r\n    ]\r\n    df = dd.read_parquet(filenames)\r\n    return df",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def load_dataset(client, data_dir, s3_bucket, nbytes, npartitions):\r\n    num_bytes_per_partition = nbytes // npartitions\r\n    filenames = []\r\n\r\n    @dask.delayed\r\n    def generate_s3_file(i, data_dir, s3_bucket):\r\n        s3 = boto3.client(\"s3\")\r\n        key = \"df-{}-{}.parquet.gzip\".format(num_bytes_per_partition, i)\r\n        contents = s3.list_objects(Bucket=s3_bucket, Prefix=key)\r\n        for obj in contents.get(\"Contents\", []):\r\n            if obj[\"Key\"] == key:\r\n                print(f\"S3 partition {i} exists\")\r\n                return\r\n\r\n        filename = os.path.join(data_dir, key)\r\n        if not os.path.exists(filename):\r\n            print(\"Generating partition\", filename)\r\n            nrows = num_bytes_per_partition // 8\r\n            dataset = pd.DataFrame(\r\n                np.random.randint(\r\n                    0, np.iinfo(np.int64).max, size=(nrows, 1), dtype=np.int64\r\n                ),\r\n                columns=[\"a\"],\r\n            )\r\n            dataset.to_parquet(filename, compression=\"gzip\")\r\n        print(\"Writing partition to S3\", filename)\r\n        with open(filename, \"rb\") as f:\r\n            s3.put_object(Bucket=s3_bucket, Key=key, Body=f)\r\n\r\n    x = []\r\n    for i in range(npartitions):\r\n        x.append(generate_s3_file(i, data_dir, s3_bucket))\r\n    # from ray.util.dask import ProgressBarCallback\r\n    # with ProgressBarCallback():\r\n    # dask.compute(x, _ray_enable_progress_bar=True)\r\n    dask.compute(x)\r\n\r\n    filenames = [\r\n        f\"s3://{s3_bucket}/df-{num_bytes_per_partition}-{i}.parquet.gzip\"\r\n        for i in range(npartitions)\r\n    ]\r\n\r\n    df = None\r\n    max_retry = 3\r\n    retry = 0\r\n    while retry < max_retry:\r\n        try:\r\n            df = dd.read_parquet(filenames)\r\n            break\r\n        except FileNotFoundError as e:\r\n            print(traceback.format_exc())\r\n            print(f\"Failed to load a file. {e}\")\r\n            # Wait a little bit before retrying.\r\n            time.sleep(30)\r\n            retry += 1\r\n\r\n    return df",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def train_epoch(epoch, dataloader, model, loss_fn, optimizer):\r\n    if ray.train.get_context().get_world_size() > 1:\r\n        dataloader.sampler.set_epoch(epoch)\r\n\r\n    for X, y in dataloader:\r\n        # Compute prediction error\r\n        pred = model(X)\r\n        loss = loss_fn(pred, y)\r\n\r\n        # Backpropagation\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def generate_s3_file(i, data_dir, s3_bucket):\r\n        s3 = boto3.client(\"s3\")\r\n        key = \"df-{}-{}.parquet.gzip\".format(num_bytes_per_partition, i)\r\n        contents = s3.list_objects(Bucket=s3_bucket, Prefix=key)\r\n        for obj in contents.get(\"Contents\", []):\r\n            if obj[\"Key\"] == key:\r\n                print(f\"S3 partition {i} exists\")\r\n                return\r\n\r\n        filename = os.path.join(data_dir, key)\r\n        if not os.path.exists(filename):\r\n            print(\"Generating partition\", filename)\r\n            nrows = num_bytes_per_partition // 8\r\n            dataset = pd.DataFrame(\r\n                np.random.randint(\r\n                    0, np.iinfo(np.int64).max, size=(nrows, 1), dtype=np.int64\r\n                ),\r\n                columns=[\"a\"],\r\n            )\r\n            dataset.to_parquet(filename, compression=\"gzip\")\r\n        print(\"Writing partition to S3\", filename)\r\n        with open(filename, \"rb\") as f:\r\n            s3.put_object(Bucket=s3_bucket, Key=key, Body=f)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _reset(self, tensordict: Optional[TensorDictBase] = None, **kwargs):\r\n        if tensordict is not None:\r\n            # We must avoid modifying the original tensordict so a shallow copy is necessary.\r\n            # We just select the input data and reset signal, which is all we need.\r\n            tensordict = tensordict.select(\r\n                *self.reset_keys, *self.state_spec.keys(True, True), strict=False\r\n            )\r\n        for reset_key in self.base_env.reset_keys:\r\n            if tensordict is not None and reset_key in tensordict.keys(True):\r\n                tensordict_reset = tensordict.exclude(*self.base_env.reset_keys)\r\n            else:\r\n                tensordict_reset = self.base_env._reset(tensordict, **kwargs)\r\n            break\r\n        if tensordict is None:\r\n            # make sure all transforms see a source tensordict\r\n            tensordict = tensordict_reset.empty()\r\n        self.base_env._complete_done(self.base_env.full_done_spec, tensordict_reset)\r\n        tensordict_reset = self.transform._reset(tensordict, tensordict_reset)\r\n        return tensordict_reset",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_custom_multi_action_distribution(self):\r\n        class Model:\r\n            pass\r\n\r\n        ray.init(\r\n            object_store_memory=1000 * 1024 * 1024, ignore_reinit_error=True\r\n        )  # otherwise fails sometimes locally\r\n        # registration\r\n        ModelCatalog.register_custom_action_dist(\"test\", CustomMultiActionDistribution)\r\n        s1 = Discrete(5)\r\n        s2 = Box(0, 1, shape=(3,), dtype=np.float32)\r\n        spaces = dict(action_1=s1, action_2=s2)\r\n        action_space = Dict(spaces)\r\n        # test retrieving it\r\n        model_config = MODEL_DEFAULTS.copy()\r\n        model_config[\"custom_action_dist\"] = \"test\"\r\n        dist_cls, param_shape = ModelCatalog.get_action_dist(action_space, model_config)\r\n        self.assertIsInstance(dist_cls, partial)\r\n        self.assertEqual(param_shape, s1.n + 2 * s2.shape[0])\r\n\r\n        # test the class works as a distribution\r\n        dist_input = tf1.placeholder(tf.float32, (None, param_shape))\r\n        model = Model()\r\n        model.model_config = model_config\r\n        dist = dist_cls(dist_input, model=model)\r\n        self.assertIsInstance(dist.sample(), dict)\r\n        self.assertIn(\"action_1\", dist.sample())\r\n        self.assertIn(\"action_2\", dist.sample())\r\n        self.assertEqual(dist.sample()[\"action_1\"].dtype, tf.int64)\r\n        self.assertEqual(dist.sample()[\"action_2\"].shape[1:], s2.shape)\r\n\r\n        with self.assertRaises(NotImplementedError):\r\n            dist.entropy()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_custom_action_distribution(self):\r\n        class Model:\r\n            pass\r\n\r\n        ray.init(\r\n            object_store_memory=1000 * 1024 * 1024, ignore_reinit_error=True\r\n        )  # otherwise fails sometimes locally\r\n        # registration\r\n        ModelCatalog.register_custom_action_dist(\"test\", CustomActionDistribution)\r\n        action_space = Box(0, 1, shape=(5, 3), dtype=np.float32)\r\n\r\n        # test retrieving it\r\n        model_config = MODEL_DEFAULTS.copy()\r\n        model_config[\"custom_action_dist\"] = \"test\"\r\n        dist_cls, param_shape = ModelCatalog.get_action_dist(action_space, model_config)\r\n        self.assertEqual(str(dist_cls), str(CustomActionDistribution))\r\n        self.assertEqual(param_shape, action_space.shape)\r\n\r\n        # test the class works as a distribution\r\n        dist_input = tf1.placeholder(tf.float32, (None,) + param_shape)\r\n        model = Model()\r\n        model.model_config = model_config\r\n        dist = dist_cls(dist_input, model=model)\r\n        self.assertEqual(dist.sample().shape[1:], dist_input.shape[1:])\r\n        self.assertIsInstance(dist.sample(), tf.Tensor)\r\n        with self.assertRaises(NotImplementedError):\r\n            dist.entropy()\r\n\r\n        # test passing the options to it\r\n        model_config[\"custom_model_config\"].update({\"output_dim\": (3,)})\r\n        dist_cls, param_shape = ModelCatalog.get_action_dist(action_space, model_config)\r\n        self.assertEqual(param_shape, (3,))\r\n        dist_input = tf1.placeholder(tf.float32, (None,) + param_shape)\r\n        model.model_config = model_config\r\n        dist = dist_cls(dist_input, model=model)\r\n        self.assertEqual(dist.sample().shape[1:], dist_input.shape[1:])\r\n        self.assertIsInstance(dist.sample(), tf.Tensor)\r\n        with self.assertRaises(NotImplementedError):\r\n            dist.entropy()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def prepare_training_svd(train, test):\r\n    reader = surprise.Reader(\"ml-100k\", rating_scale=(1, 5))\r\n    return surprise.Dataset.load_from_df(\r\n        train.drop(DEFAULT_TIMESTAMP_COL, axis=1), reader=reader\r\n    ).build_full_trainset()",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def prepare_training_cornac(train, test):\r\n    return cornac.data.Dataset.from_uir(\r\n        train.drop(DEFAULT_TIMESTAMP_COL, axis=1).itertuples(index=False), seed=SEED\r\n    )",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def score(\r\n    learner,\r\n    test_df,\r\n    user_col=cc.DEFAULT_USER_COL,\r\n    item_col=cc.DEFAULT_ITEM_COL,\r\n    prediction_col=cc.DEFAULT_PREDICTION_COL,\r\n    top_k=None,\r\n):\r\n    \"\"\"Score all users+items provided and reduce to top_k items per user if top_k>0\r\n\r\n    Args:\r\n        learner (object): Model.\r\n        test_df (pandas.DataFrame): Test dataframe.\r\n        user_col (str): User column name.\r\n        item_col (str): Item column name.\r\n        prediction_col (str): Prediction column name.\r\n        top_k (int): Number of top items to recommend.\r\n\r\n    Returns:\r\n        pandas.DataFrame: Result of recommendation\r\n    \"\"\"\r\n    # replace values not known to the model with NaN\r\n    total_users, total_items = learner.dls.classes.values()\r\n    test_df.loc[~test_df[user_col].isin(total_users), user_col] = np.nan\r\n    test_df.loc[~test_df[item_col].isin(total_items), item_col] = np.nan\r\n\r\n    # map ids to embedding ids\r\n    u = learner._get_idx(test_df[user_col], is_item=False)\r\n    m = learner._get_idx(test_df[item_col], is_item=True)\r\n\r\n    # score the pytorch model\r\n    x = torch.column_stack((u, m))\r\n\r\n    if torch.cuda.is_available():\r\n        x = x.to(\"cuda\")\r\n        learner.model = learner.model.to(\"cuda\")\r\n\r\n    pred = learner.model.forward(x).detach().cpu().numpy()\r\n    scores = pd.DataFrame(\r\n        {user_col: test_df[user_col], item_col: test_df[item_col], prediction_col: pred}\r\n    )\r\n    scores = scores.sort_values([user_col, prediction_col], ascending=[True, False])\r\n\r\n    if top_k is not None:\r\n        top_scores = scores.groupby(user_col).head(top_k).reset_index(drop=True)\r\n    else:\r\n        top_scores = scores\r\n\r\n    return top_scores",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def ncf_training(params):\r\n    \"\"\"\r\n    Train NCF using the given hyper-parameters\r\n    \"\"\"\r\n    logger.debug(\"Start training...\")\r\n    train_data = pd.read_pickle(\r\n        path=os.path.join(params[\"datastore\"], params[\"train_datapath\"])\r\n    )\r\n    validation_data = pd.read_pickle(\r\n        path=os.path.join(params[\"datastore\"], params[\"validation_datapath\"])\r\n    )\r\n\r\n    data = NCFDataset(train=train_data, test=validation_data, seed=DEFAULT_SEED)\r\n\r\n    model = NCF(\r\n        n_users=data.n_users,\r\n        n_items=data.n_items,\r\n        model_type=\"NeuMF\",\r\n        n_factors=params[\"n_factors\"],\r\n        layer_sizes=[16, 8, 4],\r\n        n_epochs=params[\"n_epochs\"],\r\n        learning_rate=params[\"learning_rate\"],\r\n        verbose=params[\"verbose\"],\r\n        seed=DEFAULT_SEED,\r\n    )\r\n\r\n    model.fit(data)\r\n\r\n    logger.debug(\"Evaluating...\")\r\n\r\n    metrics_dict = {}\r\n    rating_metrics = params[\"rating_metrics\"]\r\n    if len(rating_metrics) > 0:\r\n        predictions = [\r\n            [row.userID, row.itemID, model.predict(row.userID, row.itemID)]\r\n            for (_, row) in validation_data.iterrows()\r\n        ]\r\n\r\n        predictions = pd.DataFrame(\r\n            predictions, columns=[\"userID\", \"itemID\", \"prediction\"]\r\n        )\r\n        predictions = predictions.astype(\r\n            {\"userID\": \"int64\", \"itemID\": \"int64\", \"prediction\": \"float64\"}\r\n        )\r\n\r\n        for metric in rating_metrics:\r\n            result = getattr(evaluation, metric)(validation_data, predictions)\r\n            metrics_dict = _update_metrics(metrics_dict, metric, params, result)\r\n\r\n    ranking_metrics = params[\"ranking_metrics\"]\r\n    if len(ranking_metrics) > 0:\r\n        users, items, preds = [], [], []\r\n        item = list(train_data.itemID.unique())\r\n        for user in train_data.userID.unique():\r\n            user = [user] * len(item)\r\n            users.extend(user)\r\n            items.extend(item)\r\n            preds.extend(list(model.predict(user, item, is_list=True)))\r\n\r\n        all_predictions = pd.DataFrame(\r\n            data={\"userID\": users, \"itemID\": items, \"prediction\": preds}\r\n        )\r\n\r\n        merged = pd.merge(\r\n            train_data, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\"\r\n        )\r\n        all_predictions = merged[merged.rating.isnull()].drop(\"rating\", axis=1)\r\n        for metric in ranking_metrics:\r\n            result = getattr(evaluation, metric)(\r\n                validation_data,\r\n                all_predictions,\r\n                col_prediction=\"prediction\",\r\n                k=params[\"k\"],\r\n            )\r\n            metrics_dict = _update_metrics(metrics_dict, metric, params, result)\r\n\r\n    if len(ranking_metrics) == 0 and len(rating_metrics) == 0:\r\n        raise ValueError(\"No metrics were specified.\")\r\n\r\n    # Report the metrics\r\n    nni.report_final_result(metrics_dict)\r\n\r\n    # Save the metrics in a JSON file\r\n    output_dir = os.environ.get(\"NNI_OUTPUT_DIR\")\r\n    with open(os.path.join(output_dir, \"metrics.json\"), \"w\") as fp:\r\n        temp_dict = metrics_dict.copy()\r\n        temp_dict[params[\"primary_metric\"]] = temp_dict.pop(\"default\")\r\n        json.dump(temp_dict, fp)\r\n\r\n    return model",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_train_loader(tmp_path, dataset_ncf_files_sorted):\r\n    train_path, _, _ = dataset_ncf_files_sorted\r\n    train = pd.read_csv(train_path)\r\n    users = train[DEFAULT_USER_COL].unique()\r\n    items = train[DEFAULT_ITEM_COL].unique()\r\n\r\n    n_neg = 1\r\n    dataset = Dataset(train_path, n_neg=n_neg)\r\n    assert dataset.n_users == len(users)\r\n    assert dataset.n_items == len(items)\r\n    assert set(dataset.user2id.keys()) == set(users)\r\n    assert set(dataset.item2id.keys()) == set(items)\r\n    assert len(set(dataset.user2id.values())) == len(users)\r\n    assert len(set(dataset.item2id.values())) == len(items)\r\n\r\n    # test number of batches and data size is as expected after loading all training data\r\n    full_data_len = train.shape[0] * 2\r\n    batch_size = full_data_len // 10\r\n    expected_batches = full_data_len // batch_size\r\n    train_save_path = os.path.join(tmp_path, \"train_full.csv\")\r\n    batch_records = []\r\n    for batch in dataset.train_loader(\r\n        batch_size, shuffle_size=batch_size, yield_id=True, write_to=train_save_path\r\n    ):\r\n        assert type(batch[0][0]) == int\r\n        assert type(batch[1][0]) == int\r\n        assert type(batch[2][0]) == float\r\n        batch_data = {\r\n            DEFAULT_USER_COL: [dataset.id2user[user] for user in batch[0]],\r\n            DEFAULT_ITEM_COL: [dataset.id2item[item] for item in batch[1]],\r\n            DEFAULT_RATING_COL: batch[2],\r\n        }\r\n        batch_records.append(pd.DataFrame(batch_data))\r\n\r\n    assert len(batch_records) == expected_batches\r\n    train_loader_df = pd.concat(batch_records).reset_index(drop=True)\r\n    assert train_loader_df.shape[0] == expected_batches * batch_size\r\n    assert set(train_loader_df[DEFAULT_USER_COL]) == set(users)\r\n    assert set(train_loader_df[DEFAULT_ITEM_COL]) == set(items)\r\n\r\n    # test that data is successfully saved\r\n    assert os.path.exists(train_save_path)\r\n    train_file_data = pd.read_csv(train_save_path)\r\n    assert train_file_data.equals(train_loader_df)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_csv_to_libffm():\r\n    df_feature = pd.DataFrame(\r\n        {\r\n            \"rating\": [1, 0, 0, 1, 1],\r\n            \"field1\": [\"xxx1\", \"xxx2\", \"xxx4\", \"xxx4\", \"xxx4\"],\r\n            \"field2\": [3, 4, 5, 6, 7],\r\n            \"field3\": [1.0, 2.0, 3.0, 4.0, 5.0],\r\n            \"field4\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\r\n        }\r\n    )\r\n\r\n    with TemporaryDirectory() as td:\r\n        filepath = os.path.join(td, \"test\")\r\n\r\n        converter = LibffmConverter(filepath=filepath).fit(df_feature)\r\n        df_feature_libffm = converter.transform(df_feature)\r\n\r\n        # Check the input column types. For example, a bool type is not allowed.\r\n        df_feature_wrong_type = df_feature.copy()\r\n        df_feature_wrong_type[\"field4\"] = True\r\n        with pytest.raises(TypeError) as e:\r\n            LibffmConverter().fit(df_feature_wrong_type)\r\n            assert (\r\n                e.value == \"Input columns should be only object and/or numeric types.\"\r\n            )\r\n\r\n        # Check if the dim is the same.\r\n        assert df_feature_libffm.shape == df_feature.shape\r\n\r\n        # Check if the columns are converted successfully.\r\n        assert df_feature_libffm.iloc[0, :].values.tolist() == [\r\n            1,\r\n            \"1:1:1\",\r\n            \"2:4:3\",\r\n            \"3:5:1.0\",\r\n            \"4:6:1\",\r\n        ]\r\n\r\n        # Check if the duplicated column entries are indexed correctly.\r\n        # It should skip counting the duplicated features in a field column.\r\n        assert df_feature_libffm.iloc[-1, :].values.tolist() == [\r\n            1,\r\n            \"1:3:1\",\r\n            \"2:4:7\",\r\n            \"3:5:5.0\",\r\n            \"4:10:1\",\r\n        ]\r\n\r\n        # Check if the file is written successfully.\r\n        assert os.path.isfile(filepath)\r\n\r\n        with open(filepath, \"r\") as f:\r\n            line = f.readline()\r\n            assert line == \"1 1:1:1 2:4:3 3:5:1.0 4:6:1\\n\"\r\n\r\n        # Parameters in the transformation should be reported correctly.\r\n        params = converter.get_params()\r\n        assert params == {\"field count\": 4, \"feature count\": 10, \"file path\": filepath}\r\n\r\n        # Dataset with the same columns should be transformable with a fitted converter.\r\n        df_feature_new = pd.DataFrame(\r\n            {\r\n                \"rating\": [1, 0, 0, 1, 1, 1],\r\n                \"field1\": [\"xxx1\", \"xxx2\", \"xxx4\", \"xxx4\", \"xxx4\", \"xxx3\"],\r\n                \"field2\": [3, 4, 5, 6, 7, 8],\r\n                \"field3\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\r\n                \"field4\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"],\r\n            }\r\n        )\r\n        df_feature_new_libffm = converter.transform(df_feature_new)\r\n\r\n        assert df_feature_new_libffm.iloc[0, :].values.tolist() == [\r\n            1,\r\n            \"1:1:1\",\r\n            \"2:5:3\",\r\n            \"3:6:1.0\",\r\n            \"4:7:1\",\r\n        ]\r\n        assert df_feature_new_libffm.iloc[-1, :].values.tolist() == [\r\n            1,\r\n            \"1:4:1\",\r\n            \"2:5:8\",\r\n            \"3:6:6.0\",\r\n            \"4:12:1\",\r\n        ]",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predict(rating_true):\r\n    svd = surprise.SVD()\r\n    train_set = surprise.Dataset.load_from_df(\r\n        rating_true, reader=surprise.Reader()\r\n    ).build_full_trainset()\r\n    svd.fit(train_set)\r\n\r\n    preds = predict(svd, rating_true)\r\n    assert set(preds.columns) == {\"userID\", \"itemID\", \"prediction\"}\r\n    assert preds[\"userID\"].dtypes == rating_true[\"userID\"].dtypes\r\n    assert preds[\"itemID\"].dtypes == rating_true[\"itemID\"].dtypes\r\n    user = rating_true.iloc[0][\"userID\"]\r\n    item = rating_true.iloc[0][\"itemID\"]\r\n    assert preds[(preds[\"userID\"] == user) & (preds[\"itemID\"] == item)][\r\n        \"prediction\"\r\n    ].values == pytest.approx(svd.predict(user, item).est, rel=TOL)\r\n\r\n    preds = predict(\r\n        svd,\r\n        rating_true.rename(columns={\"userID\": \"uid\", \"itemID\": \"iid\"}),\r\n        usercol=\"uid\",\r\n        itemcol=\"iid\",\r\n        predcol=\"pred\",\r\n    )\r\n    assert set(preds.columns) == {\"uid\", \"iid\", \"pred\"}\r\n    assert preds[\"uid\"].dtypes == rating_true[\"userID\"].dtypes\r\n    assert preds[\"iid\"].dtypes == rating_true[\"itemID\"].dtypes\r\n    user = rating_true.iloc[1][\"userID\"]\r\n    item = rating_true.iloc[1][\"itemID\"]\r\n    assert preds[(preds[\"uid\"] == user) & (preds[\"iid\"] == item)][\r\n        \"pred\"\r\n    ].values == pytest.approx(svd.predict(user, item).est, rel=TOL)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_recommend_k_items(rating_true):\r\n    n_users = len(rating_true[\"userID\"].unique())\r\n    n_items = len(rating_true[\"itemID\"].unique())\r\n    svd = surprise.SVD()\r\n    train_set = surprise.Dataset.load_from_df(\r\n        rating_true, reader=surprise.Reader()\r\n    ).build_full_trainset()\r\n    svd.fit(train_set)\r\n\r\n    preds = compute_ranking_predictions(svd, rating_true, remove_seen=True)\r\n    assert set(preds.columns) == {\"userID\", \"itemID\", \"prediction\"}\r\n    assert preds[\"userID\"].dtypes == rating_true[\"userID\"].dtypes\r\n    assert preds[\"itemID\"].dtypes == rating_true[\"itemID\"].dtypes\r\n    user = preds.iloc[0][\"userID\"]\r\n    item = preds.iloc[0][\"itemID\"]\r\n    assert preds[(preds[\"userID\"] == user) & (preds[\"itemID\"] == item)][\r\n        \"prediction\"\r\n    ].values == pytest.approx(svd.predict(user, item).est, rel=TOL)\r\n    # Test default remove_seen=True\r\n    assert pd.merge(rating_true, preds, on=[\"userID\", \"itemID\"]).shape[0] == 0\r\n    assert preds.shape[0] == (n_users * n_items - rating_true.shape[0])\r\n\r\n    preds = compute_ranking_predictions(\r\n        svd,\r\n        rating_true.rename(columns={\"userID\": \"uid\", \"itemID\": \"iid\", \"rating\": \"r\"}),\r\n        usercol=\"uid\",\r\n        itemcol=\"iid\",\r\n        predcol=\"pred\",\r\n        remove_seen=False,\r\n    )\r\n    assert set(preds.columns) == {\"uid\", \"iid\", \"pred\"}\r\n    assert preds[\"uid\"].dtypes == rating_true[\"userID\"].dtypes\r\n    assert preds[\"iid\"].dtypes == rating_true[\"itemID\"].dtypes\r\n    user = preds.iloc[1][\"uid\"]\r\n    item = preds.iloc[1][\"iid\"]\r\n    assert preds[(preds[\"uid\"] == user) & (preds[\"iid\"] == item)][\r\n        \"pred\"\r\n    ].values == pytest.approx(svd.predict(user, item).est, rel=TOL)\r\n\r\n    # Test remove_seen=False\r\n    assert (\r\n        pd.merge(\r\n            rating_true, preds, left_on=[\"userID\", \"itemID\"], right_on=[\"uid\", \"iid\"]\r\n        ).shape[0]\r\n        == rating_true.shape[0]\r\n    )\r\n    assert preds.shape[0] == n_users * n_items",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def test_dataset_with_duplicates(header):\r\n    model = SAR(**header)\r\n    train = pd.DataFrame(\r\n        {\r\n            header[\"col_user\"]: [1, 1, 2, 2, 2],\r\n            header[\"col_item\"]: [1, 2, 1, 2, 2],\r\n            header[\"col_rating\"]: [3.0, 4.0, 3.0, 4.0, 4.0],\r\n        }\r\n    )\r\n    with pytest.raises(ValueError):\r\n        model.fit(train)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predict_all_items(train_test_dummy_timestamp, header):\r\n    model = SAR(**header)\r\n    trainset, _ = train_test_dummy_timestamp\r\n    model.fit(trainset)\r\n\r\n    user_items = itertools.product(\r\n        trainset[header[\"col_user\"]].unique(), trainset[header[\"col_item\"]].unique()\r\n    )\r\n    testset = pd.DataFrame(user_items, columns=[header[\"col_user\"], header[\"col_item\"]])\r\n    preds = model.predict(testset)\r\n\r\n    assert len(preds) == len(testset)\r\n    assert isinstance(preds, pd.DataFrame)\r\n    assert preds[header[\"col_user\"]].dtype == trainset[header[\"col_user\"]].dtype\r\n    assert preds[header[\"col_item\"]].dtype == trainset[header[\"col_item\"]].dtype\r\n    assert preds[DEFAULT_PREDICTION_COL].dtype == trainset[header[\"col_rating\"]].dtype",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_match_similarity_type_from_json_file(header):\r\n    # store parameters in json\r\n    params_str = json.dumps({\"similarity_type\": \"lift\"})\r\n    # load parameters in json\r\n    params = json.loads(params_str)\r\n\r\n    params.update(header)\r\n\r\n    model = SAR(**params)\r\n\r\n    train = pd.DataFrame(\r\n        {\r\n            header[\"col_user\"]: [1, 1, 1, 1, 2, 2, 2, 2],\r\n            header[\"col_item\"]: [1, 2, 3, 4, 1, 5, 6, 7],\r\n            header[\"col_rating\"]: [3.0, 4.0, 5.0, 4.0, 3.0, 2.0, 1.0, 5.0],\r\n            header[\"col_timestamp\"]: [1, 20, 30, 400, 50, 60, 70, 800],\r\n        }\r\n    )\r\n\r\n    # make sure fit still works when similarity type is loaded from a json file\r\n    model.fit(train)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_logbook_saver_callback(caplog):\r\n    callback = LogbookSaver(\"./logbook.pkl\")\r\n    assert check_callback(callback) == [callback]\r\n\r\n    clf = DecisionTreeClassifier()\r\n    evolved_estimator = GASearchCV(\r\n        clf,\r\n        cv=3,\r\n        scoring=\"accuracy\",\r\n        generations=2,\r\n        param_grid={\r\n            \"min_weight_fraction_leaf\": Continuous(0, 0.5),\r\n            \"max_depth\": Integer(2, 20),\r\n            \"max_leaf_nodes\": Integer(2, 30),\r\n        },\r\n        verbose=False,\r\n    )\r\n\r\n    evolved_estimator.fit(X_train, y_train, callbacks=callback)\r\n\r\n    assert os.path.exists(\"./logbook.pkl\")\r\n\r\n    os.remove(\"./logbook.pkl\")\r\n\r\n    with caplog.at_level(logging.ERROR):\r\n        callback = LogbookSaver(checkpoint_path=\"./no_folder/logbook.pkl\", estimator=4)\r\n        callback()\r\n    assert \"Could not save the Logbook in the checkpoint\" in caplog.text",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_datafile_init(dataset_ncf_files_sorted):\r\n    train_path, _, _ = dataset_ncf_files_sorted\r\n    train = pd.read_csv(train_path)\r\n    users = train[DEFAULT_USER_COL].unique()\r\n    items = train[DEFAULT_ITEM_COL].unique()\r\n    datafile = DataFile(\r\n        train_path,\r\n        DEFAULT_USER_COL,\r\n        DEFAULT_ITEM_COL,\r\n        DEFAULT_RATING_COL,\r\n        col_test_batch=None,\r\n        binary=True,\r\n    )\r\n    assert set(datafile.users) == set(users)\r\n    assert set(datafile.items) == set(items)\r\n    assert set(datafile.user2id.keys()) == set(users)\r\n    assert set(datafile.item2id.keys()) == set(items)\r\n    assert len(set(datafile.user2id.values())) == len(users)\r\n    assert len(set(datafile.item2id.values())) == len(items)\r\n    assert datafile.data_len == train.shape[0]\r\n\r\n    datafile_records = []\r\n    with datafile as f:\r\n        for line in f:\r\n            datafile_records.append(\r\n                {\r\n                    DEFAULT_USER_COL: line[DEFAULT_USER_COL],\r\n                    DEFAULT_ITEM_COL: line[DEFAULT_ITEM_COL],\r\n                    DEFAULT_RATING_COL: line[DEFAULT_RATING_COL],\r\n                }\r\n            )\r\n    datafile_df = pd.DataFrame.from_records(datafile_records)\r\n    assert datafile_df.shape[0] == train.shape[0]\r\n\r\n    # test the data loaded from the file is the same as original data\r\n    datafile_df = datafile_df.sort_values(by=[DEFAULT_USER_COL, DEFAULT_ITEM_COL])\r\n    train = train.sort_values(by=[DEFAULT_USER_COL, DEFAULT_ITEM_COL])\r\n    train[DEFAULT_RATING_COL] = train[DEFAULT_RATING_COL].apply(lambda x: float(x > 0))\r\n    train = train.drop(DEFAULT_TIMESTAMP_COL, axis=1)\r\n    assert train.equals(datafile_df)\r\n\r\n    # test data can be loaded for a valid user and it throws exception for invalid user\r\n    user = train[DEFAULT_USER_COL].iloc[0]\r\n    missing_user = train[DEFAULT_USER_COL].iloc[-1] + 1\r\n    with datafile as f:\r\n        user_data = f.load_data(user)\r\n        assert user_data[DEFAULT_USER_COL].iloc[0] == user\r\n        with pytest.raises(MissingUserException):\r\n            user_data == f.load_data(missing_user)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_tensorboard_callback(callback, path):\r\n    assert check_callback(callback) == [callback]\r\n\r\n    clf = DecisionTreeClassifier()\r\n    evolved_estimator = GASearchCV(\r\n        clf,\r\n        cv=3,\r\n        scoring=\"accuracy\",\r\n        generations=2,\r\n        param_grid={\r\n            \"min_weight_fraction_leaf\": Continuous(0, 0.5),\r\n            \"max_depth\": Integer(2, 20),\r\n            \"max_leaf_nodes\": Integer(2, 30),\r\n        },\r\n        verbose=False,\r\n    )\r\n\r\n    evolved_estimator.fit(X_train, y_train, callbacks=callback)\r\n\r\n    assert os.path.exists(path)\r\n\r\n    shutil.rmtree(path)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def varimp(self, use_pandas=False):\r\n        \"\"\"\r\n        Return the Importance of components associcated with a pca model.\r\n\r\n        use_pandas: ``bool``  (default: ``False``).\r\n        \"\"\"\r\n        model = self._model_json[\"output\"]\r\n        if \"importance\" in list(model.keys()) and model[\"importance\"]:\r\n            vals = model[\"importance\"].cell_values\r\n            header = model[\"importance\"].col_header\r\n            if use_pandas and can_use_pandas():\r\n                import pandas\r\n                return pandas.DataFrame(vals, columns=header)\r\n            else:\r\n                return vals\r\n        else:\r\n            print(\"Warning: This model doesn't have importances of components.\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def handler(event, context):\r\n    # fix random seed for reproducibility\r\n    seed = 7\r\n    numpy.random.seed(seed)\r\n    # load pima indians dataset\r\n    dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n    # split into input (X) and output (Y) variables\r\n    X = dataset[:,0:8]\r\n    Y = dataset[:,8]\r\n    # create model\r\n    model = Sequential()\r\n    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\r\n    model.add(Dense(8, init='uniform', activation='relu'))\r\n    model.add(Dense(1, init='uniform', activation='sigmoid'))\r\n    # Compile model\r\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n    # Fit the model\r\n    model.fit(X, Y, nb_epoch=150, batch_size=10,  verbose=2)\r\n    # calculate predictions\r\n    predictions = model.predict(X)\r\n    # round predictions\r\n    rounded = [round(x[0]) for x in predictions]\r\n    print(rounded)\r\n    return rounded",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def varimp(self, use_pandas=False):\r\n        \"\"\"\r\n        Pretty print the variable importances, or return them in a list.\r\n\r\n        :param use_pandas: If True, then the variable importances will be returned as a pandas data frame.\r\n\r\n        :returns: A list or Pandas DataFrame.\r\n        \"\"\"\r\n        model = self._model_json[\"output\"]\r\n        if \"variable_importances\" in list(model.keys()) and model[\"variable_importances\"]:\r\n            vals = model[\"variable_importances\"].cell_values\r\n            header = model[\"variable_importances\"].col_header\r\n            if use_pandas and can_use_pandas():\r\n                import pandas\r\n                return pandas.DataFrame(vals, columns=header)\r\n            else:\r\n                return vals\r\n        else:\r\n            print(\"Warning: This model doesn't have variable importances\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\r\n        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\r\n        jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-dimensional real function of n real\r\n    variables) and the loss function rho(s) (a scalar function), `least_squares`\r\n    finds a local minimum of the cost function F(x)::\r\n\r\n        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\r\n        subject to lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must return a 1-d array_like of shape (m,) or a scalar. If the\r\n        argument ``x`` is complex or the function ``fun`` returns complex\r\n        residuals, it must be wrapped in a real function of real arguments,\r\n        as shown at the end of the Examples section.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-d array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as much operations compared to '2-point' (default). The\r\n        scheme 'cs' uses complex steps, and while potentially the most\r\n        accurate, it is applicable only when `fun` correctly handles\r\n        complex inputs and can be analytically continued to the complex\r\n        plane. Method 'lm' always uses the '2-point' scheme. If callable,\r\n        it is used as ``jac(x, *args, **kwargs)`` and should return a\r\n        good approximation (or the exact value) for the Jacobian as an\r\n        array_like (np.atleast_2d is applied), a sparse matrix or a\r\n        `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float, optional\r\n        Tolerance for termination by the change of the cost function. Default\r\n        is 1e-8. The optimization process is stopped when  ``dF < ftol * F``,\r\n        and there was an adequate agreement between a local quadratic model and\r\n        the true model in the last step.\r\n    xtol : float, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is 1e-8. The exact condition depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n    gtol : float, optional\r\n        Tolerance for termination by the norm of the gradient. Default is 1e-8.\r\n        The exact condition depends on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust region along j-th\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given size\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-d ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default) the solver is chosen based on the type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True) which adds a regularization term to the\r\n              normal equation, which improves convergence if the Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        difference estimation, its shape must be (m, n). If the Jacobian has\r\n        only few non-zero elements in *each* row, providing the sparsity\r\n        structure will greatly speed up the computations [Curtis]_. A zero\r\n        entry means that a corresponding element in the Jacobian is identically\r\n        zero. If provided, forces the use of 'lsmr' trust-region solver.\r\n        If None (default) then dense differencing will be used. Has no effect\r\n        for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    `OptimizeResult` with the following fields defined:\r\n    x : ndarray, shape (n,)\r\n        Solution found.\r\n    cost : float\r\n        Value of the cost function at the solution.\r\n    fun : ndarray, shape (m,)\r\n        Vector of residuals at the solution.\r\n    jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n        Modified Jacobian matrix at the solution, in the sense that J^T J\r\n        is a Gauss-Newton approximation of the Hessian of the cost function.\r\n        The type is the same as the one used by the algorithm.\r\n    grad : ndarray, shape (m,)\r\n        Gradient of the cost function at the solution.\r\n    optimality : float\r\n        First-order optimality measure. In unconstrained problems, it is always\r\n        the uniform norm of the gradient. In constrained problems, it is the\r\n        quantity which was compared with `gtol` during iterations.\r\n    active_mask : ndarray of int, shape (n,)\r\n        Each component shows whether a corresponding constraint is active\r\n        (that is, whether a variable is at the bound):\r\n\r\n            *  0 : a constraint is not active.\r\n            * -1 : a lower bound is active.\r\n            *  1 : an upper bound is active.\r\n\r\n        Might be somewhat arbitrary for 'trf' method as it generates a sequence\r\n        of strictly feasible iterates and `active_mask` is determined within a\r\n        tolerance threshold.\r\n    nfev : int\r\n        Number of function evaluations done. Methods 'trf' and 'dogbox' do not\r\n        count function calls for numerical Jacobian approximation, as opposed\r\n        to 'lm' method.\r\n    njev : int or None\r\n        Number of Jacobian evaluations done. If numerical Jacobian\r\n        approximation is used in 'lm' method, it is set to None.\r\n    status : int\r\n        The reason for algorithm termination:\r\n\r\n            * -1 : improper input parameters status returned from MINPACK.\r\n            *  0 : the maximum number of function evaluations is exceeded.\r\n            *  1 : `gtol` termination condition is satisfied.\r\n            *  2 : `ftol` termination condition is satisfied.\r\n            *  3 : `xtol` termination condition is satisfied.\r\n            *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n    message : str\r\n        Verbal description of the termination reason.\r\n    success : bool\r\n        True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e. robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independed variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    9.8669242910846867e-30\r\n    >>> res_1.optimality\r\n    8.8928864934219529e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687069299604613e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     rnd = np.random.RandomState(random_state)\r\n    ...     error = noise * rnd.randn(t.size)\r\n    ...     outliers = rnd.randint(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And finally plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    In the next example, we show how complex-valued residual functions of\r\n    complex variables can be optimized with ``least_squares()``. Consider the\r\n    following function:\r\n\r\n    >>> def f(z):\r\n    ...     return z - (0.5 + 0.5j)\r\n\r\n    We wrap it into a function of real variables that returns real residuals\r\n    by simply handling the real and imaginary parts as independent variables:\r\n\r\n    >>> def f_wrap(x):\r\n    ...     fx = f(x[0] + 1j*x[1])\r\n    ...     return np.array([fx.real, fx.imag])\r\n\r\n    Thus, instead of the original m-dimensional complex function of n complex\r\n    variables we optimize a 2m-dimensional real function of 2n real variables:\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\r\n    >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\r\n    >>> z\r\n    (0.49999999999925893+0.49999999999925893j)\r\n\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    if np.iscomplexobj(x0):\r\n        raise ValueError(\"`x0` must be real.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound must be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like.\")\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = csr_matrix(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return csr_matrix(jac(x, *args, **kwargs))\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, string_types) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False,\r\n              check_finite=True, bounds=(-np.inf, np.inf), method=None,\r\n              jac=None, **kwargs):\r\n    \"\"\"\r\n    Use non-linear least squares to fit a function, f, to data.\r\n\r\n    Assumes ``ydata = f(xdata, *params) + eps``\r\n\r\n    Parameters\r\n    ----------\r\n    f : callable\r\n        The model function, f(x, ...).  It must take the independent\r\n        variable as the first argument and the parameters to fit as\r\n        separate remaining arguments.\r\n    xdata : An M-length sequence or an (k,M)-shaped array for functions with k predictors\r\n        The independent variable where the data is measured.\r\n    ydata : M-length sequence\r\n        The dependent data --- nominally f(xdata, ...)\r\n    p0 : None, scalar, or N-length sequence, optional\r\n        Initial guess for the parameters.  If None, then the initial\r\n        values will all be 1 (if the number of parameters for the function\r\n        can be determined using introspection, otherwise a ValueError\r\n        is raised).\r\n    sigma : None or M-length sequence or MxM array, optional\r\n        Determines the uncertainty in `ydata`. If we define residuals as\r\n        ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\r\n        depends on its number of dimensions:\r\n\r\n            - A 1-d `sigma` should contain values of standard deviations of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = sum((r / sigma) ** 2)``.\r\n\r\n            - A 2-d `sigma` should contain the covariance matrix of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = r.T @ inv(sigma) @ r``.\r\n\r\n              .. versionadded:: 0.19\r\n\r\n        None (default) is equivalent of 1-d `sigma` filled with ones.\r\n    absolute_sigma : bool, optional\r\n        If True, `sigma` is used in an absolute sense and the estimated parameter\r\n        covariance `pcov` reflects these absolute values.\r\n\r\n        If False, only the relative magnitudes of the `sigma` values matter.\r\n        The returned parameter covariance matrix `pcov` is based on scaling\r\n        `sigma` by a constant factor. This constant is set by demanding that the\r\n        reduced `chisq` for the optimal parameters `popt` when using the\r\n        *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\r\n        match the sample variance of the residuals after the fit.\r\n        Mathematically,\r\n        ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\r\n    check_finite : bool, optional\r\n        If True, check that the input arrays do not contain nans of infs,\r\n        and raise a ValueError if they do. Setting this parameter to\r\n        False may silently produce nonsensical results if the input arrays\r\n        do contain nans. Default is True.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each element of the tuple must be either an array with the length equal\r\n        to the number of parameters, or a scalar (in which case the bound is\r\n        taken to be the same for all parameters.) Use ``np.inf`` with an\r\n        appropriate sign to disable bounds on all or some parameters.\r\n\r\n        .. versionadded:: 0.17\r\n    method : {'lm', 'trf', 'dogbox'}, optional\r\n        Method to use for optimization.  See `least_squares` for more details.\r\n        Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\r\n        provided. The method 'lm' won't work when the number of observations\r\n        is less than the number of variables, use 'trf' or 'dogbox' in this\r\n        case.\r\n\r\n        .. versionadded:: 0.17\r\n    jac : callable, string or None, optional\r\n        Function with signature ``jac(x, ...)`` which computes the Jacobian\r\n        matrix of the model function with respect to parameters as a dense\r\n        array_like structure. It will be scaled according to provided `sigma`.\r\n        If None (default), the Jacobian will be estimated numerically.\r\n        String keywords for 'trf' and 'dogbox' methods can be used to select\r\n        a finite difference scheme, see `least_squares`.\r\n\r\n        .. versionadded:: 0.18\r\n    kwargs\r\n        Keyword arguments passed to `leastsq` for ``method='lm'`` or\r\n        `least_squares` otherwise.\r\n\r\n    Returns\r\n    -------\r\n    popt : array\r\n        Optimal values for the parameters so that the sum of the squared\r\n        residuals of ``f(xdata, *popt) - ydata`` is minimized\r\n    pcov : 2d array\r\n        The estimated covariance of popt. The diagonals provide the variance\r\n        of the parameter estimate. To compute one standard deviation errors\r\n        on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\r\n\r\n        How the `sigma` parameter affects the estimated covariance\r\n        depends on `absolute_sigma` argument, as described above.\r\n\r\n        If the Jacobian matrix at the solution doesn't have a full rank, then\r\n        'lm' method returns a matrix filled with ``np.inf``, on the other hand\r\n        'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\r\n        the covariance matrix.\r\n\r\n    Raises\r\n    ------\r\n    ValueError\r\n        if either `ydata` or `xdata` contain NaNs, or if incompatible options\r\n        are used.\r\n\r\n    RuntimeError\r\n        if the least-squares minimization fails.\r\n\r\n    OptimizeWarning\r\n        if covariance of the parameters can not be estimated.\r\n\r\n    See Also\r\n    --------\r\n    least_squares : Minimize the sum of squares of nonlinear functions.\r\n    scipy.stats.linregress : Calculate a linear least squares regression for\r\n                             two sets of measurements.\r\n\r\n    Notes\r\n    -----\r\n    With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\r\n    through `leastsq`. Note that this algorithm can only deal with\r\n    unconstrained problems.\r\n\r\n    Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\r\n    the docstring of `least_squares` for more information.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> from scipy.optimize import curve_fit\r\n\r\n    >>> def func(x, a, b, c):\r\n    ...     return a * np.exp(-b * x) + c\r\n\r\n    Define the data to be fit with some noise:\r\n\r\n    >>> xdata = np.linspace(0, 4, 50)\r\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\r\n    >>> np.random.seed(1729)\r\n    >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\r\n    >>> ydata = y + y_noise\r\n    >>> plt.plot(xdata, ydata, 'b-', label='data')\r\n\r\n    Fit for the parameters a, b, c of the function `func`:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata)\r\n    >>> popt\r\n    array([ 2.55423706,  1.35190947,  0.47450618])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'r-',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    Constrain the optimization to the region of ``0 <= a <= 3``,\r\n    ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\r\n    >>> popt\r\n    array([ 2.43708906,  1.        ,  0.35015434])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'g--',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    >>> plt.xlabel('x')\r\n    >>> plt.ylabel('y')\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    \"\"\"\r\n    if p0 is None:\r\n        # determine number of parameters by inspecting the function\r\n        from scipy._lib._util import getargspec_no_self as _getargspec\r\n        args, varargs, varkw, defaults = _getargspec(f)\r\n        if len(args) < 2:\r\n            raise ValueError(\"Unable to determine number of fit parameters.\")\r\n        n = len(args) - 1\r\n    else:\r\n        p0 = np.atleast_1d(p0)\r\n        n = p0.size\r\n\r\n    lb, ub = prepare_bounds(bounds, n)\r\n    if p0 is None:\r\n        p0 = _initialize_feasible(lb, ub)\r\n\r\n    bounded_problem = np.any((lb > -np.inf) | (ub < np.inf))\r\n    if method is None:\r\n        if bounded_problem:\r\n            method = 'trf'\r\n        else:\r\n            method = 'lm'\r\n\r\n    if method == 'lm' and bounded_problem:\r\n        raise ValueError(\"Method 'lm' only works for unconstrained problems. \"\r\n                         \"Use 'trf' or 'dogbox' instead.\")\r\n\r\n    # NaNs can not be handled\r\n    if check_finite:\r\n        ydata = np.asarray_chkfinite(ydata)\r\n    else:\r\n        ydata = np.asarray(ydata)\r\n\r\n    if isinstance(xdata, (list, tuple, np.ndarray)):\r\n        # `xdata` is passed straight to the user-defined `f`, so allow\r\n        # non-array_like `xdata`.\r\n        if check_finite:\r\n            xdata = np.asarray_chkfinite(xdata)\r\n        else:\r\n            xdata = np.asarray(xdata)\r\n\r\n    # Determine type of sigma\r\n    if sigma is not None:\r\n        sigma = np.asarray(sigma)\r\n\r\n        # if 1-d, sigma are errors, define transform = 1/sigma\r\n        if sigma.shape == (ydata.size, ):\r\n            transform = 1.0 / sigma\r\n        # if 2-d, sigma is the covariance matrix,\r\n        # define transform = L such that L L^T = C\r\n        elif sigma.shape == (ydata.size, ydata.size):\r\n            try:\r\n                # scipy.linalg.cholesky requires lower=True to return L L^T = A\r\n                transform = cholesky(sigma, lower=True)\r\n            except LinAlgError:\r\n                raise ValueError(\"`sigma` must be positive definite.\")\r\n        else:\r\n            raise ValueError(\"`sigma` has incorrect shape.\")\r\n    else:\r\n        transform = None\r\n\r\n    func = _wrap_func(f, xdata, ydata, transform)\r\n    if callable(jac):\r\n        jac = _wrap_jac(jac, xdata, transform)\r\n    elif jac is None and method != 'lm':\r\n        jac = '2-point'\r\n\r\n    if method == 'lm':\r\n        # Remove full_output from kwargs, otherwise we're passing it in twice.\r\n        return_full = kwargs.pop('full_output', False)\r\n        res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\r\n        popt, pcov, infodict, errmsg, ier = res\r\n        cost = np.sum(infodict['fvec'] ** 2)\r\n        if ier not in [1, 2, 3, 4]:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\r\n    else:\r\n        # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.\r\n        if 'max_nfev' not in kwargs:\r\n            kwargs['max_nfev'] = kwargs.pop('maxfev', None)\r\n\r\n        res = least_squares(func, p0, jac=jac, bounds=bounds, method=method,\r\n                            **kwargs)\r\n\r\n        if not res.success:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + res.message)\r\n\r\n        cost = 2 * res.cost  # res.cost is half sum of squares!\r\n        popt = res.x\r\n\r\n        # Do Moore-Penrose inverse discarding zero singular values.\r\n        _, s, VT = svd(res.jac, full_matrices=False)\r\n        threshold = np.finfo(float).eps * max(res.jac.shape) * s[0]\r\n        s = s[s > threshold]\r\n        VT = VT[:s.size]\r\n        pcov = np.dot(VT.T / s**2, VT)\r\n        return_full = False\r\n\r\n    warn_cov = False\r\n    if pcov is None:\r\n        # indeterminate covariance\r\n        pcov = zeros((len(popt), len(popt)), dtype=float)\r\n        pcov.fill(inf)\r\n        warn_cov = True\r\n    elif not absolute_sigma:\r\n        if ydata.size > p0.size:\r\n            s_sq = cost / (ydata.size - p0.size)\r\n            pcov = pcov * s_sq\r\n        else:\r\n            pcov.fill(inf)\r\n            warn_cov = True\r\n\r\n    if warn_cov:\r\n        warnings.warn('Covariance of the parameters could not be estimated',\r\n                      category=OptimizeWarning)\r\n\r\n    if return_full:\r\n        return popt, pcov, infodict, errmsg, ier\r\n    else:\r\n        return popt, pcov",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_fastica_simple(add_noise=False):\r\n    # Test the FastICA algorithm on very simple data.\r\n    rng = np.random.RandomState(0)\r\n    # scipy.stats uses the global RNG:\r\n    np.random.seed(0)\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\n    s2 = stats.t.rvs(1, size=n_samples)\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n    s1, s2 = s\r\n\r\n    # Mixing angle\r\n    phi = 0.6\r\n    mixing = np.array([[np.cos(phi), np.sin(phi)],\r\n                       [np.sin(phi), -np.cos(phi)]])\r\n    m = np.dot(mixing, s)\r\n\r\n    if add_noise:\r\n        m += 0.1 * rng.randn(2, 1000)\r\n\r\n    center_and_norm(m)\r\n\r\n    # function as fun arg\r\n    def g_test(x):\r\n        return x ** 3, (3 * x ** 2).mean(axis=-1)\r\n\r\n    algos = ['parallel', 'deflation']\r\n    nls = ['logcosh', 'exp', 'cube', g_test]\r\n    whitening = [True, False]\r\n    for algo, nl, whiten in itertools.product(algos, nls, whitening):\r\n        if whiten:\r\n            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo)\r\n            assert_raises(ValueError, fastica, m.T, fun=np.tanh,\r\n                          algorithm=algo)\r\n        else:\r\n            X = PCA(n_components=2, whiten=True).fit_transform(m.T)\r\n            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False)\r\n            assert_raises(ValueError, fastica, X, fun=np.tanh,\r\n                          algorithm=algo)\r\n        s_ = s_.T\r\n        # Check that the mixing model described in the docstring holds:\r\n        if whiten:\r\n            assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))\r\n\r\n        center_and_norm(s_)\r\n        s1_, s2_ = s_\r\n        # Check to see if the sources have been estimated\r\n        # in the wrong order\r\n        if abs(np.dot(s1_, s2)) > abs(np.dot(s1_, s1)):\r\n            s2_, s1_ = s_\r\n        s1_ *= np.sign(np.dot(s1_, s1))\r\n        s2_ *= np.sign(np.dot(s2_, s2))\r\n\r\n        # Check that we have estimated the original sources\r\n        if not add_noise:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=2)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=2)\r\n        else:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=1)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=1)\r\n\r\n    # Test FastICA class\r\n    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo, random_state=0)\r\n    ica = FastICA(fun=nl, algorithm=algo, random_state=0)\r\n    sources = ica.fit_transform(m.T)\r\n    assert_equal(ica.components_.shape, (2, 2))\r\n    assert_equal(sources.shape, (1000, 2))\r\n\r\n    assert_array_almost_equal(sources_fun, sources)\r\n    assert_array_almost_equal(sources, ica.transform(m.T))\r\n\r\n    assert_equal(ica.mixing_.shape, (2, 2))\r\n\r\n    for fn in [np.tanh, \"exp(-.5(x^2))\"]:\r\n        ica = FastICA(fun=fn, algorithm=algo, random_state=0)\r\n        assert_raises(ValueError, ica.fit, m.T)\r\n\r\n    assert_raises(TypeError, FastICA(fun=moves.xrange(10)).fit, m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def fit(self, X, y):\r\n        \"\"\"Fit the model using X, y as training data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like, shape (n_samples, n_features)\r\n            Training data.\r\n\r\n        y : array-like, shape (n_samples,)\r\n            Target values.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            returns an instance of self.\r\n        \"\"\"\r\n        X, y = check_X_y(X, y, y_numeric=True)\r\n        X = as_float_array(X, copy=self.copy_X)\r\n        y = as_float_array(y, copy=self.copy_X)\r\n\r\n        # init cross-validation generator\r\n        cv = check_cv(self.cv, classifier=False)\r\n\r\n        # As we use cross-validation, the Gram matrix is not precomputed here\r\n        Gram = self.precompute\r\n        if hasattr(Gram, '__array__'):\r\n            warnings.warn(\"Parameter 'precompute' cannot be an array in \"\r\n                          \"%s. Automatically switch to 'auto' instead.\"\r\n                          % self.__class__.__name__)\r\n            Gram = 'auto'\r\n\r\n        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\r\n            delayed(_lars_path_residues)(\r\n                X[train], y[train], X[test], y[test], Gram=Gram, copy=False,\r\n                method=self.method, verbose=max(0, self.verbose - 1),\r\n                normalize=self.normalize, fit_intercept=self.fit_intercept,\r\n                max_iter=self.max_iter, eps=self.eps, positive=self.positive)\r\n            for train, test in cv.split(X, y))\r\n        all_alphas = np.concatenate(list(zip(*cv_paths))[0])\r\n        # Unique also sorts\r\n        all_alphas = np.unique(all_alphas)\r\n        # Take at most max_n_alphas values\r\n        stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))\r\n        all_alphas = all_alphas[::stride]\r\n\r\n        mse_path = np.empty((len(all_alphas), len(cv_paths)))\r\n        for index, (alphas, active, coefs, residues) in enumerate(cv_paths):\r\n            alphas = alphas[::-1]\r\n            residues = residues[::-1]\r\n            if alphas[0] != 0:\r\n                alphas = np.r_[0, alphas]\r\n                residues = np.r_[residues[0, np.newaxis], residues]\r\n            if alphas[-1] != all_alphas[-1]:\r\n                alphas = np.r_[alphas, all_alphas[-1]]\r\n                residues = np.r_[residues, residues[-1, np.newaxis]]\r\n            this_residues = interpolate.interp1d(alphas,\r\n                                                 residues,\r\n                                                 axis=0)(all_alphas)\r\n            this_residues **= 2\r\n            mse_path[:, index] = np.mean(this_residues, axis=-1)\r\n\r\n        mask = np.all(np.isfinite(mse_path), axis=-1)\r\n        all_alphas = all_alphas[mask]\r\n        mse_path = mse_path[mask]\r\n        # Select the alpha that minimizes left-out error\r\n        i_best_alpha = np.argmin(mse_path.mean(axis=-1))\r\n        best_alpha = all_alphas[i_best_alpha]\r\n\r\n        # Store our parameters\r\n        self.alpha_ = best_alpha\r\n        self.cv_alphas_ = all_alphas\r\n        self.mse_path_ = mse_path\r\n\r\n        # Now compute the full model\r\n        # it will call a lasso internally when self if LassoLarsCV\r\n        # as self.method == 'lasso'\r\n        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,\r\n                  Xy=None, fit_path=True)\r\n        return self",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def fit(self, X, y, sample_weight=None):\r\n        \"\"\"Fit estimator using RANSAC algorithm.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like or sparse matrix, shape [n_samples, n_features]\r\n            Training data.\r\n\r\n        y : array-like, shape = [n_samples] or [n_samples, n_targets]\r\n            Target values.\r\n\r\n        sample_weight : array-like, shape = [n_samples]\r\n            Individual weights for each sample\r\n            raises error if sample_weight is passed and base_estimator\r\n            fit method does not support it.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If no valid consensus set could be found. This occurs if\r\n            `is_data_valid` and `is_model_valid` return False for all\r\n            `max_trials` randomly chosen sub-samples.\r\n\r\n        \"\"\"\r\n        X = check_array(X, accept_sparse='csr')\r\n        y = check_array(y, ensure_2d=False)\r\n        check_consistent_length(X, y)\r\n\r\n        if self.base_estimator is not None:\r\n            base_estimator = clone(self.base_estimator)\r\n        else:\r\n            base_estimator = LinearRegression()\r\n\r\n        if self.min_samples is None:\r\n            # assume linear model by default\r\n            min_samples = X.shape[1] + 1\r\n        elif 0 < self.min_samples < 1:\r\n            min_samples = np.ceil(self.min_samples * X.shape[0])\r\n        elif self.min_samples >= 1:\r\n            if self.min_samples % 1 != 0:\r\n                raise ValueError(\"Absolute number of samples must be an \"\r\n                                 \"integer value.\")\r\n            min_samples = self.min_samples\r\n        else:\r\n            raise ValueError(\"Value for `min_samples` must be scalar and \"\r\n                             \"positive.\")\r\n        if min_samples > X.shape[0]:\r\n            raise ValueError(\"`min_samples` may not be larger than number \"\r\n                             \"of samples ``X.shape[0]``.\")\r\n\r\n        if self.stop_probability < 0 or self.stop_probability > 1:\r\n            raise ValueError(\"`stop_probability` must be in range [0, 1].\")\r\n\r\n        if self.residual_threshold is None:\r\n            # MAD (median absolute deviation)\r\n            residual_threshold = np.median(np.abs(y - np.median(y)))\r\n        else:\r\n            residual_threshold = self.residual_threshold\r\n\r\n        if self.residual_metric is not None:\r\n            warnings.warn(\r\n                \"'residual_metric' was deprecated in version 0.18 and \"\r\n                \"will be removed in version 0.20. Use 'loss' instead.\",\r\n                DeprecationWarning)\r\n\r\n        if self.loss == \"absolute_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)\r\n\r\n        elif self.loss == \"squared_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)\r\n\r\n        elif callable(self.loss):\r\n            loss_function = self.loss\r\n\r\n        else:\r\n            raise ValueError(\r\n                \"loss should be 'absolute_loss', 'squared_loss' or a callable.\"\r\n                \"Got %s. \" % self.loss)\r\n\r\n\r\n        random_state = check_random_state(self.random_state)\r\n\r\n        try:  # Not all estimator accept a random_state\r\n            base_estimator.set_params(random_state=random_state)\r\n        except ValueError:\r\n            pass\r\n\r\n        estimator_fit_has_sample_weight = has_fit_parameter(base_estimator,\r\n                                                            \"sample_weight\")\r\n        estimator_name = type(base_estimator).__name__\r\n        if (sample_weight is not None and not\r\n                estimator_fit_has_sample_weight):\r\n            raise ValueError(\"%s does not support sample_weight. Samples\"\r\n                             \" weights are only used for the calibration\"\r\n                             \" itself.\" % estimator_name)\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n\r\n        n_inliers_best = 1\r\n        score_best = -np.inf\r\n        inlier_mask_best = None\r\n        X_inlier_best = None\r\n        y_inlier_best = None\r\n        self.n_skips_no_inliers_ = 0\r\n        self.n_skips_invalid_data_ = 0\r\n        self.n_skips_invalid_model_ = 0\r\n\r\n        # number of data samples\r\n        n_samples = X.shape[0]\r\n        sample_idxs = np.arange(n_samples)\r\n\r\n        n_samples, _ = X.shape\r\n\r\n        self.n_trials_ = 0\r\n        max_trials = self.max_trials\r\n        while self.n_trials_ < max_trials:\r\n            self.n_trials_ += 1\r\n\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                break\r\n\r\n            # choose random sample set\r\n            subset_idxs = sample_without_replacement(n_samples, min_samples,\r\n                                                     random_state=random_state)\r\n            X_subset = X[subset_idxs]\r\n            y_subset = y[subset_idxs]\r\n\r\n            # check if random sample set is valid\r\n            if (self.is_data_valid is not None\r\n                    and not self.is_data_valid(X_subset, y_subset)):\r\n                self.n_skips_invalid_data_ += 1\r\n                continue\r\n\r\n            # fit model for current random sample set\r\n            if sample_weight is None:\r\n                base_estimator.fit(X_subset, y_subset)\r\n            else:\r\n                base_estimator.fit(X_subset, y_subset,\r\n                                   sample_weight=sample_weight[subset_idxs])\r\n\r\n            # check if estimated model is valid\r\n            if (self.is_model_valid is not None and not\r\n                    self.is_model_valid(base_estimator, X_subset, y_subset)):\r\n                self.n_skips_invalid_model_ += 1\r\n                continue\r\n\r\n            # residuals of all data for current random sample model\r\n            y_pred = base_estimator.predict(X)\r\n\r\n            # XXX: Deprecation: Remove this if block in 0.20\r\n            if self.residual_metric is not None:\r\n                diff = y_pred - y\r\n                if diff.ndim == 1:\r\n                    diff = diff.reshape(-1, 1)\r\n                residuals_subset = self.residual_metric(diff)\r\n            else:\r\n                residuals_subset = loss_function(y, y_pred)\r\n\r\n            # classify data into inliers and outliers\r\n            inlier_mask_subset = residuals_subset < residual_threshold\r\n            n_inliers_subset = np.sum(inlier_mask_subset)\r\n\r\n            # less inliers -> skip current random sample\r\n            if n_inliers_subset < n_inliers_best:\r\n                self.n_skips_no_inliers_ += 1\r\n                continue\r\n\r\n            # extract inlier data set\r\n            inlier_idxs_subset = sample_idxs[inlier_mask_subset]\r\n            X_inlier_subset = X[inlier_idxs_subset]\r\n            y_inlier_subset = y[inlier_idxs_subset]\r\n\r\n            # score of inlier data set\r\n            score_subset = base_estimator.score(X_inlier_subset,\r\n                                                y_inlier_subset)\r\n\r\n            # same number of inliers but worse score -> skip current random\r\n            # sample\r\n            if (n_inliers_subset == n_inliers_best\r\n                    and score_subset < score_best):\r\n                continue\r\n\r\n            # save current random sample as best sample\r\n            n_inliers_best = n_inliers_subset\r\n            score_best = score_subset\r\n            inlier_mask_best = inlier_mask_subset\r\n            X_inlier_best = X_inlier_subset\r\n            y_inlier_best = y_inlier_subset\r\n\r\n            max_trials = min(\r\n                max_trials,\r\n                _dynamic_max_trials(n_inliers_best, n_samples,\r\n                                    min_samples, self.stop_probability))\r\n\r\n            # break if sufficient number of inliers or score is reached\r\n            if n_inliers_best >= self.stop_n_inliers or \\\r\n                            score_best >= self.stop_score:\r\n                break\r\n\r\n        # if none of the iterations met the required criteria\r\n        if inlier_mask_best is None:\r\n            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips):\r\n                raise ValueError(\r\n                    \"RANSAC skipped more iterations than `max_skips` without\"\r\n                    \" finding a valid consensus set. Iterations were skipped\"\r\n                    \" because each randomly chosen sub-sample failed the\"\r\n                    \" passing criteria. See estimator attributes for\"\r\n                    \" diagnostics (n_skips*).\")\r\n            else:\r\n                raise ValueError(\r\n                    \"RANSAC could not find a valid consensus set. All\"\r\n                    \" `max_trials` iterations were skipped because each\"\r\n                    \" randomly chosen sub-sample failed the passing criteria.\"\r\n                    \" See estimator attributes for diagnostics (n_skips*).\")\r\n        else:\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                warnings.warn(\"RANSAC found a valid consensus set but exited\"\r\n                              \" early due to skipping more iterations than\"\r\n                              \" `max_skips`. See estimator attributes for\"\r\n                              \" diagnostics (n_skips*).\",\r\n                              UserWarning)\r\n\r\n        # estimate final model using all inliers\r\n        base_estimator.fit(X_inlier_best, y_inlier_best)\r\n\r\n        self.estimator_ = base_estimator\r\n        self.inlier_mask_ = inlier_mask_best\r\n        return self",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_staged_functions_defensive():\r\n    # test that staged_functions make defensive copies\r\n    rng = np.random.RandomState(0)\r\n    X = rng.uniform(size=(10, 3))\r\n    y = (4 * X[:, 0]).astype(np.int) + 1  # don't predict zeros\r\n    for estimator in [GradientBoostingRegressor(),\r\n                      GradientBoostingClassifier()]:\r\n        estimator.fit(X, y)\r\n        for func in ['predict', 'decision_function', 'predict_proba']:\r\n            staged_func = getattr(estimator, \"staged_\" + func, None)\r\n            if staged_func is None:\r\n                # regressor has no staged_predict_proba\r\n                continue\r\n            with warnings.catch_warnings(record=True):\r\n                staged_result = list(staged_func(X))\r\n            staged_result[1][:] = 0\r\n            assert_true(np.all(staged_result[0] != 0))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pickle_version_warning_is_not_raised_with_matching_version():\r\n    iris = datasets.load_iris()\r\n    tree = DecisionTreeClassifier().fit(iris.data, iris.target)\r\n    tree_pickle = pickle.dumps(tree)\r\n    assert_true(b\"version\" in tree_pickle)\r\n    tree_restored = assert_no_warnings(pickle.loads, tree_pickle)\r\n\r\n    # test that we can predict with the restored decision tree classifier\r\n    score_of_original = tree.score(iris.data, iris.target)\r\n    score_of_restored = tree_restored.score(iris.data, iris.target)\r\n    assert_equal(score_of_original, score_of_restored)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\r\n        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\r\n        jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-dimensional function of n variables) and\r\n    the loss function rho(s) (a scalar function), `least_squares` finds a\r\n    local minimum of the cost function F(x)::\r\n\r\n        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\r\n        subject to lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must return a 1-d array_like of shape (m,) or a scalar.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-d array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as much operations compared to '2-point' (default). The\r\n        scheme 'cs' uses complex steps, and while potentially the most\r\n        accurate, it is applicable only when `fun` correctly handles\r\n        complex inputs and can be analytically continued to the complex\r\n        plane. Method 'lm' always uses the '2-point' scheme. If callable,\r\n        it is used as ``jac(x, *args, **kwargs)`` and should return a\r\n        good approximation (or the exact value) for the Jacobian as an\r\n        array_like (np.atleast_2d is applied), a sparse matrix or a\r\n        `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float, optional\r\n        Tolerance for termination by the change of the cost function. Default\r\n        is 1e-8. The optimization process is stopped when  ``dF < ftol * F``,\r\n        and there was an adequate agreement between a local quadratic model and\r\n        the true model in the last step.\r\n    xtol : float, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is 1e-8. The exact condition depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n    gtol : float, optional\r\n        Tolerance for termination by the norm of the gradient. Default is 1e-8.\r\n        The exact condition depends on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust region along j-th\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given size\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-d ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default) the solver is chosen based on the type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True) which adds a regularization term to the\r\n              normal equation, which improves convergence if the Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        difference estimation, its shape must be (m, n). If the Jacobian has\r\n        only few non-zero elements in *each* row, providing the sparsity\r\n        structure will greatly speed up the computations [Curtis]_. A zero\r\n        entry means that a corresponding element in the Jacobian is identically\r\n        zero. If provided, forces the use of 'lsmr' trust-region solver.\r\n        If None (default) then dense differencing will be used. Has no effect\r\n        for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    `OptimizeResult` with the following fields defined:\r\n    x : ndarray, shape (n,)\r\n        Solution found.\r\n    cost : float\r\n        Value of the cost function at the solution.\r\n    fun : ndarray, shape (m,)\r\n        Vector of residuals at the solution.\r\n    jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n        Modified Jacobian matrix at the solution, in the sense that J^T J\r\n        is a Gauss-Newton approximation of the Hessian of the cost function.\r\n        The type is the same as the one used by the algorithm.\r\n    grad : ndarray, shape (m,)\r\n        Gradient of the cost function at the solution.\r\n    optimality : float\r\n        First-order optimality measure. In unconstrained problems, it is always\r\n        the uniform norm of the gradient. In constrained problems, it is the\r\n        quantity which was compared with `gtol` during iterations.\r\n    active_mask : ndarray of int, shape (n,)\r\n        Each component shows whether a corresponding constraint is active\r\n        (that is, whether a variable is at the bound):\r\n\r\n            *  0 : a constraint is not active.\r\n            * -1 : a lower bound is active.\r\n            *  1 : an upper bound is active.\r\n\r\n        Might be somewhat arbitrary for 'trf' method as it generates a sequence\r\n        of strictly feasible iterates and `active_mask` is determined within a\r\n        tolerance threshold.\r\n    nfev : int\r\n        Number of function evaluations done. Methods 'trf' and 'dogbox' do not\r\n        count function calls for numerical Jacobian approximation, as opposed\r\n        to 'lm' method.\r\n    njev : int or None\r\n        Number of Jacobian evaluations done. If numerical Jacobian\r\n        approximation is used in 'lm' method, it is set to None.\r\n    status : int\r\n        The reason for algorithm termination:\r\n\r\n            * -1 : improper input parameters status returned from MINPACK.\r\n            *  0 : the maximum number of function evaluations is exceeded.\r\n            *  1 : `gtol` termination condition is satisfied.\r\n            *  2 : `ftol` termination condition is satisfied.\r\n            *  3 : `xtol` termination condition is satisfied.\r\n            *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n    message : str\r\n        Verbal description of the termination reason.\r\n    success : bool\r\n        True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e. robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independed variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    9.8669242910846867e-30\r\n    >>> res_1.optimality\r\n    8.8928864934219529e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687069299604613e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     rnd = np.random.RandomState(random_state)\r\n    ...     error = noise * rnd.randn(t.size)\r\n    ...     outliers = rnd.randint(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And finally plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound must be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like.\")\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = csr_matrix(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return csr_matrix(jac(x, *args, **kwargs))\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, string_types) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\r\n        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\r\n        jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-D real function of n real\r\n    variables) and the loss function rho(s) (a scalar function), `least_squares`\r\n    finds a local minimum of the cost function F(x)::\r\n\r\n        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\r\n        subject to lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must allocate and return a 1-D array_like of shape (m,) or a scalar.\r\n        If the argument ``x`` is complex or the function ``fun`` returns\r\n        complex residuals, it must be wrapped in a real function of real\r\n        arguments, as shown at the end of the Examples section.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-D array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as many operations as '2-point' (default). The scheme 'cs'\r\n        uses complex steps, and while potentially the most accurate, it is\r\n        applicable only when `fun` correctly handles complex inputs and\r\n        can be analytically continued to the complex plane. Method 'lm'\r\n        always uses the '2-point' scheme. If callable, it is used as\r\n        ``jac(x, *args, **kwargs)`` and should return a good approximation\r\n        (or the exact value) for the Jacobian as an array_like (np.atleast_2d\r\n        is applied), a sparse matrix (csr_matrix preferred for performance) or\r\n        a `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float or None, optional\r\n        Tolerance for termination by the change of the cost function. Default\r\n        is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\r\n        and there was an adequate agreement between a local quadratic model and\r\n        the true model in the last step.\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    xtol : float or None, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is 1e-8. The exact condition depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    gtol : float or None, optional\r\n        Tolerance for termination by the norm of the gradient. Default is 1e-8.\r\n        The exact condition depends on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust region along jth\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given size\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-D ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default), the solver is chosen based on the type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally,  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True), which adds a regularization term to the\r\n              normal equation, which improves convergence if the Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        difference estimation, its shape must be (m, n). If the Jacobian has\r\n        only few non-zero elements in *each* row, providing the sparsity\r\n        structure will greatly speed up the computations [Curtis]_. A zero\r\n        entry means that a corresponding element in the Jacobian is identically\r\n        zero. If provided, forces the use of 'lsmr' trust-region solver.\r\n        If None (default), then dense differencing will be used. Has no effect\r\n        for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    result : OptimizeResult\r\n        `OptimizeResult` with the following fields defined:\r\n\r\n            x : ndarray, shape (n,)\r\n                Solution found.\r\n            cost : float\r\n                Value of the cost function at the solution.\r\n            fun : ndarray, shape (m,)\r\n                Vector of residuals at the solution.\r\n            jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n                Modified Jacobian matrix at the solution, in the sense that J^T J\r\n                is a Gauss-Newton approximation of the Hessian of the cost function.\r\n                The type is the same as the one used by the algorithm.\r\n            grad : ndarray, shape (m,)\r\n                Gradient of the cost function at the solution.\r\n            optimality : float\r\n                First-order optimality measure. In unconstrained problems, it is\r\n                always the uniform norm of the gradient. In constrained problems,\r\n                it is the quantity which was compared with `gtol` during iterations.\r\n            active_mask : ndarray of int, shape (n,)\r\n                Each component shows whether a corresponding constraint is active\r\n                (that is, whether a variable is at the bound):\r\n\r\n                    *  0 : a constraint is not active.\r\n                    * -1 : a lower bound is active.\r\n                    *  1 : an upper bound is active.\r\n\r\n                Might be somewhat arbitrary for 'trf' method as it generates a\r\n                sequence of strictly feasible iterates and `active_mask` is\r\n                determined within a tolerance threshold.\r\n            nfev : int\r\n                Number of function evaluations done. Methods 'trf' and 'dogbox' do\r\n                not count function calls for numerical Jacobian approximation, as\r\n                opposed to 'lm' method.\r\n            njev : int or None\r\n                Number of Jacobian evaluations done. If numerical Jacobian\r\n                approximation is used in 'lm' method, it is set to None.\r\n            status : int\r\n                The reason for algorithm termination:\r\n\r\n                    * -1 : improper input parameters status returned from MINPACK.\r\n                    *  0 : the maximum number of function evaluations is exceeded.\r\n                    *  1 : `gtol` termination condition is satisfied.\r\n                    *  2 : `ftol` termination condition is satisfied.\r\n                    *  3 : `xtol` termination condition is satisfied.\r\n                    *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n            message : str\r\n                Verbal description of the termination reason.\r\n            success : bool\r\n                True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve-fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also,\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e., robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independent variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    9.8669242910846867e-30\r\n    >>> res_1.optimality\r\n    8.8928864934219529e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687069299604613e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> from numpy.random import default_rng\r\n    >>> rng = default_rng()\r\n    >>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\r\n    ...     rng = default_rng(seed)\r\n    ...\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     error = noise * rng.standard_normal(t.size)\r\n    ...     outliers = rng.integers(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And, finally, plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    In the next example, we show how complex-valued residual functions of\r\n    complex variables can be optimized with ``least_squares()``. Consider the\r\n    following function:\r\n\r\n    >>> def f(z):\r\n    ...     return z - (0.5 + 0.5j)\r\n\r\n    We wrap it into a function of real variables that returns real residuals\r\n    by simply handling the real and imaginary parts as independent variables:\r\n\r\n    >>> def f_wrap(x):\r\n    ...     fx = f(x[0] + 1j*x[1])\r\n    ...     return np.array([fx.real, fx.imag])\r\n\r\n    Thus, instead of the original m-D complex function of n complex\r\n    variables we optimize a 2m-D real function of 2n real variables:\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\r\n    >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\r\n    >>> z\r\n    (0.49999999999925893+0.49999999999925893j)\r\n\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    if np.iscomplexobj(x0):\r\n        raise ValueError(\"`x0` must be real.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound must be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol, method)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like. \"\r\n                         \"f0.shape: {0}\".format(f0.shape))\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = J0.tocsr()\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs).tocsr()\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, str) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False,\r\n              check_finite=True, bounds=(-np.inf, np.inf), method=None,\r\n              jac=None, **kwargs):\r\n    \"\"\"\r\n    Use non-linear least squares to fit a function, f, to data.\r\n\r\n    Assumes ``ydata = f(xdata, *params) + eps``.\r\n\r\n    Parameters\r\n    ----------\r\n    f : callable\r\n        The model function, f(x, ...). It must take the independent\r\n        variable as the first argument and the parameters to fit as\r\n        separate remaining arguments.\r\n    xdata : array_like or object\r\n        The independent variable where the data is measured.\r\n        Should usually be an M-length sequence or an (k,M)-shaped array for\r\n        functions with k predictors, but can actually be any object.\r\n    ydata : array_like\r\n        The dependent data, a length M array - nominally ``f(xdata, ...)``.\r\n    p0 : array_like, optional\r\n        Initial guess for the parameters (length N). If None, then the\r\n        initial values will all be 1 (if the number of parameters for the\r\n        function can be determined using introspection, otherwise a\r\n        ValueError is raised).\r\n    sigma : None or M-length sequence or MxM array, optional\r\n        Determines the uncertainty in `ydata`. If we define residuals as\r\n        ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\r\n        depends on its number of dimensions:\r\n\r\n            - A 1-D `sigma` should contain values of standard deviations of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = sum((r / sigma) ** 2)``.\r\n\r\n            - A 2-D `sigma` should contain the covariance matrix of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = r.T @ inv(sigma) @ r``.\r\n\r\n              .. versionadded:: 0.19\r\n\r\n        None (default) is equivalent of 1-D `sigma` filled with ones.\r\n    absolute_sigma : bool, optional\r\n        If True, `sigma` is used in an absolute sense and the estimated parameter\r\n        covariance `pcov` reflects these absolute values.\r\n\r\n        If False (default), only the relative magnitudes of the `sigma` values matter.\r\n        The returned parameter covariance matrix `pcov` is based on scaling\r\n        `sigma` by a constant factor. This constant is set by demanding that the\r\n        reduced `chisq` for the optimal parameters `popt` when using the\r\n        *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\r\n        match the sample variance of the residuals after the fit. Default is False.\r\n        Mathematically,\r\n        ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\r\n    check_finite : bool, optional\r\n        If True, check that the input arrays do not contain nans of infs,\r\n        and raise a ValueError if they do. Setting this parameter to\r\n        False may silently produce nonsensical results if the input arrays\r\n        do contain nans. Default is True.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on parameters. Defaults to no bounds.\r\n        Each element of the tuple must be either an array with the length equal\r\n        to the number of parameters, or a scalar (in which case the bound is\r\n        taken to be the same for all parameters). Use ``np.inf`` with an\r\n        appropriate sign to disable bounds on all or some parameters.\r\n\r\n        .. versionadded:: 0.17\r\n    method : {'lm', 'trf', 'dogbox'}, optional\r\n        Method to use for optimization. See `least_squares` for more details.\r\n        Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\r\n        provided. The method 'lm' won't work when the number of observations\r\n        is less than the number of variables, use 'trf' or 'dogbox' in this\r\n        case.\r\n\r\n        .. versionadded:: 0.17\r\n    jac : callable, string or None, optional\r\n        Function with signature ``jac(x, ...)`` which computes the Jacobian\r\n        matrix of the model function with respect to parameters as a dense\r\n        array_like structure. It will be scaled according to provided `sigma`.\r\n        If None (default), the Jacobian will be estimated numerically.\r\n        String keywords for 'trf' and 'dogbox' methods can be used to select\r\n        a finite difference scheme, see `least_squares`.\r\n\r\n        .. versionadded:: 0.18\r\n    kwargs\r\n        Keyword arguments passed to `leastsq` for ``method='lm'`` or\r\n        `least_squares` otherwise.\r\n\r\n    Returns\r\n    -------\r\n    popt : array\r\n        Optimal values for the parameters so that the sum of the squared\r\n        residuals of ``f(xdata, *popt) - ydata`` is minimized.\r\n    pcov : 2-D array\r\n        The estimated covariance of popt. The diagonals provide the variance\r\n        of the parameter estimate. To compute one standard deviation errors\r\n        on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\r\n\r\n        How the `sigma` parameter affects the estimated covariance\r\n        depends on `absolute_sigma` argument, as described above.\r\n\r\n        If the Jacobian matrix at the solution doesn't have a full rank, then\r\n        'lm' method returns a matrix filled with ``np.inf``, on the other hand\r\n        'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\r\n        the covariance matrix.\r\n\r\n    Raises\r\n    ------\r\n    ValueError\r\n        if either `ydata` or `xdata` contain NaNs, or if incompatible options\r\n        are used.\r\n\r\n    RuntimeError\r\n        if the least-squares minimization fails.\r\n\r\n    OptimizeWarning\r\n        if covariance of the parameters can not be estimated.\r\n\r\n    See Also\r\n    --------\r\n    least_squares : Minimize the sum of squares of nonlinear functions.\r\n    scipy.stats.linregress : Calculate a linear least squares regression for\r\n                             two sets of measurements.\r\n\r\n    Notes\r\n    -----\r\n    With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\r\n    through `leastsq`. Note that this algorithm can only deal with\r\n    unconstrained problems.\r\n\r\n    Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\r\n    the docstring of `least_squares` for more information.\r\n\r\n    Examples\r\n    --------\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> from scipy.optimize import curve_fit\r\n\r\n    >>> def func(x, a, b, c):\r\n    ...     return a * np.exp(-b * x) + c\r\n\r\n    Define the data to be fit with some noise:\r\n\r\n    >>> xdata = np.linspace(0, 4, 50)\r\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\r\n    >>> rng = np.random.default_rng()\r\n    >>> y_noise = 0.2 * rng.normal(size=xdata.size)\r\n    >>> ydata = y + y_noise\r\n    >>> plt.plot(xdata, ydata, 'b-', label='data')\r\n\r\n    Fit for the parameters a, b, c of the function `func`:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata)\r\n    >>> popt\r\n    array([2.56274217, 1.37268521, 0.47427475])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'r-',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    Constrain the optimization to the region of ``0 <= a <= 3``,\r\n    ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\r\n    >>> popt\r\n    array([2.43736712, 1.        , 0.34463856])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'g--',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    >>> plt.xlabel('x')\r\n    >>> plt.ylabel('y')\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    \"\"\"\r\n    if p0 is None:\r\n        # determine number of parameters by inspecting the function\r\n        sig = _getfullargspec(f)\r\n        args = sig.args\r\n        if len(args) < 2:\r\n            raise ValueError(\"Unable to determine number of fit parameters.\")\r\n        n = len(args) - 1\r\n    else:\r\n        p0 = np.atleast_1d(p0)\r\n        n = p0.size\r\n\r\n    lb, ub = prepare_bounds(bounds, n)\r\n    if p0 is None:\r\n        p0 = _initialize_feasible(lb, ub)\r\n\r\n    bounded_problem = np.any((lb > -np.inf) | (ub < np.inf))\r\n    if method is None:\r\n        if bounded_problem:\r\n            method = 'trf'\r\n        else:\r\n            method = 'lm'\r\n\r\n    if method == 'lm' and bounded_problem:\r\n        raise ValueError(\"Method 'lm' only works for unconstrained problems. \"\r\n                         \"Use 'trf' or 'dogbox' instead.\")\r\n\r\n    # optimization may produce garbage for float32 inputs, cast them to float64\r\n\r\n    # NaNs cannot be handled\r\n    if check_finite:\r\n        ydata = np.asarray_chkfinite(ydata, float)\r\n    else:\r\n        ydata = np.asarray(ydata, float)\r\n\r\n    if isinstance(xdata, (list, tuple, np.ndarray)):\r\n        # `xdata` is passed straight to the user-defined `f`, so allow\r\n        # non-array_like `xdata`.\r\n        if check_finite:\r\n            xdata = np.asarray_chkfinite(xdata, float)\r\n        else:\r\n            xdata = np.asarray(xdata, float)\r\n\r\n    if ydata.size == 0:\r\n        raise ValueError(\"`ydata` must not be empty!\")\r\n\r\n    # Determine type of sigma\r\n    if sigma is not None:\r\n        sigma = np.asarray(sigma)\r\n\r\n        # if 1-D, sigma are errors, define transform = 1/sigma\r\n        if sigma.shape == (ydata.size, ):\r\n            transform = 1.0 / sigma\r\n        # if 2-D, sigma is the covariance matrix,\r\n        # define transform = L such that L L^T = C\r\n        elif sigma.shape == (ydata.size, ydata.size):\r\n            try:\r\n                # scipy.linalg.cholesky requires lower=True to return L L^T = A\r\n                transform = cholesky(sigma, lower=True)\r\n            except LinAlgError as e:\r\n                raise ValueError(\"`sigma` must be positive definite.\") from e\r\n        else:\r\n            raise ValueError(\"`sigma` has incorrect shape.\")\r\n    else:\r\n        transform = None\r\n\r\n    func = _wrap_func(f, xdata, ydata, transform)\r\n    if callable(jac):\r\n        jac = _wrap_jac(jac, xdata, transform)\r\n    elif jac is None and method != 'lm':\r\n        jac = '2-point'\r\n\r\n    if 'args' in kwargs:\r\n        # The specification for the model function `f` does not support\r\n        # additional arguments. Refer to the `curve_fit` docstring for\r\n        # acceptable call signatures of `f`.\r\n        raise ValueError(\"'args' is not a supported keyword argument.\")\r\n\r\n    if method == 'lm':\r\n        # if ydata.size == 1, this might be used for broadcast.\r\n        if ydata.size != 1 and n > ydata.size:\r\n            raise TypeError(f\"The number of func parameters={n} must not\"\r\n                            f\" exceed the number of data points={ydata.size}\")\r\n        # Remove full_output from kwargs, otherwise we're passing it in twice.\r\n        return_full = kwargs.pop('full_output', False)\r\n        res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\r\n        popt, pcov, infodict, errmsg, ier = res\r\n        ysize = len(infodict['fvec'])\r\n        cost = np.sum(infodict['fvec'] ** 2)\r\n        if ier not in [1, 2, 3, 4]:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\r\n    else:\r\n        # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.\r\n        if 'max_nfev' not in kwargs:\r\n            kwargs['max_nfev'] = kwargs.pop('maxfev', None)\r\n\r\n        res = least_squares(func, p0, jac=jac, bounds=bounds, method=method,\r\n                            **kwargs)\r\n\r\n        if not res.success:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + res.message)\r\n\r\n        ysize = len(res.fun)\r\n        cost = 2 * res.cost  # res.cost is half sum of squares!\r\n        popt = res.x\r\n\r\n        # Do Moore-Penrose inverse discarding zero singular values.\r\n        _, s, VT = svd(res.jac, full_matrices=False)\r\n        threshold = np.finfo(float).eps * max(res.jac.shape) * s[0]\r\n        s = s[s > threshold]\r\n        VT = VT[:s.size]\r\n        pcov = np.dot(VT.T / s**2, VT)\r\n        return_full = False\r\n\r\n    warn_cov = False\r\n    if pcov is None:\r\n        # indeterminate covariance\r\n        pcov = zeros((len(popt), len(popt)), dtype=float)\r\n        pcov.fill(inf)\r\n        warn_cov = True\r\n    elif not absolute_sigma:\r\n        if ysize > p0.size:\r\n            s_sq = cost / (ysize - p0.size)\r\n            pcov = pcov * s_sq\r\n        else:\r\n            pcov.fill(inf)\r\n            warn_cov = True\r\n\r\n    if warn_cov:\r\n        warnings.warn('Covariance of the parameters could not be estimated',\r\n                      category=OptimizeWarning)\r\n\r\n    if return_full:\r\n        return popt, pcov, infodict, errmsg, ier\r\n    else:\r\n        return popt, pcov",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_affinity_propagation_predict_error():\r\n    # Test exception in AffinityPropagation.predict\r\n    # Not fitted.\r\n    af = AffinityPropagation(affinity=\"euclidean\")\r\n    with pytest.raises(ValueError):\r\n        af.predict(X)\r\n\r\n    # Predict not supported when affinity=\"precomputed\".\r\n    S = np.dot(X, X.T)\r\n    af = AffinityPropagation(affinity=\"precomputed\", random_state=57)\r\n    af.fit(S)\r\n    with pytest.raises(ValueError):\r\n        af.predict(X)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_n_clusters():\r\n    # Test that n_clusters param works properly\r\n    X, y = make_blobs(n_samples=100, centers=10)\r\n    brc1 = Birch(n_clusters=10)\r\n    brc1.fit(X)\r\n    assert len(brc1.subcluster_centers_) > 10\r\n    assert len(np.unique(brc1.labels_)) == 10\r\n\r\n    # Test that n_clusters = Agglomerative Clustering gives\r\n    # the same results.\r\n    gc = AgglomerativeClustering(n_clusters=10)\r\n    brc2 = Birch(n_clusters=gc)\r\n    brc2.fit(X)\r\n    assert_array_equal(brc1.subcluster_labels_, brc2.subcluster_labels_)\r\n    assert_array_equal(brc1.labels_, brc2.labels_)\r\n\r\n    # Test that the wrong global clustering step raises an Error.\r\n    clf = ElasticNet()\r\n    brc3 = Birch(n_clusters=clf)\r\n    with pytest.raises(ValueError):\r\n        brc3.fit(X)\r\n\r\n    # Test that a small number of clusters raises a warning.\r\n    brc4 = Birch(threshold=10000.)\r\n    assert_warns(ConvergenceWarning, brc4.fit, X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_target_regressor_error():\r\n    X, y = friedman\r\n    # provide a transformer and functions at the same time\r\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\r\n                                      transformer=StandardScaler(),\r\n                                      func=np.exp, inverse_func=np.log)\r\n    with pytest.raises(ValueError,\r\n                       match=\"'transformer' and functions\"\r\n                       \" 'func'/'inverse_func' cannot both be set.\"):\r\n        regr.fit(X, y)\r\n    # fit with sample_weight with a regressor which does not support it\r\n    sample_weight = np.ones((y.shape[0],))\r\n    regr = TransformedTargetRegressor(regressor=OrthogonalMatchingPursuit(),\r\n                                      transformer=StandardScaler())\r\n    with pytest.raises(TypeError, match=r\"fit\\(\\) got an unexpected \"\r\n                       \"keyword argument 'sample_weight'\"):\r\n        regr.fit(X, y, sample_weight=sample_weight)\r\n    # func is given but inverse_func is not\r\n    regr = TransformedTargetRegressor(func=np.exp)\r\n    with pytest.raises(ValueError, match=\"When 'func' is provided, \"\r\n                       \"'inverse_func' must also be provided\"):\r\n        regr.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fastica_simple(add_noise, seed):\r\n    # Test the FastICA algorithm on very simple data.\r\n    rng = np.random.RandomState(seed)\r\n    # scipy.stats uses the global RNG:\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\n    s2 = stats.t.rvs(1, size=n_samples)\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n    s1, s2 = s\r\n\r\n    # Mixing angle\r\n    phi = 0.6\r\n    mixing = np.array([[np.cos(phi), np.sin(phi)],\r\n                       [np.sin(phi), -np.cos(phi)]])\r\n    m = np.dot(mixing, s)\r\n\r\n    if add_noise:\r\n        m += 0.1 * rng.randn(2, 1000)\r\n\r\n    center_and_norm(m)\r\n\r\n    # function as fun arg\r\n    def g_test(x):\r\n        return x ** 3, (3 * x ** 2).mean(axis=-1)\r\n\r\n    algos = ['parallel', 'deflation']\r\n    nls = ['logcosh', 'exp', 'cube', g_test]\r\n    whitening = [True, False]\r\n    for algo, nl, whiten in itertools.product(algos, nls, whitening):\r\n        if whiten:\r\n            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo,\r\n                                      random_state=rng)\r\n            with pytest.raises(ValueError):\r\n                fastica(m.T, fun=np.tanh, algorithm=algo)\r\n        else:\r\n            pca = PCA(n_components=2, whiten=True, random_state=rng)\r\n            X = pca.fit_transform(m.T)\r\n            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False,\r\n                                      random_state=rng)\r\n            with pytest.raises(ValueError):\r\n                fastica(X, fun=np.tanh, algorithm=algo)\r\n        s_ = s_.T\r\n        # Check that the mixing model described in the docstring holds:\r\n        if whiten:\r\n            assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))\r\n\r\n        center_and_norm(s_)\r\n        s1_, s2_ = s_\r\n        # Check to see if the sources have been estimated\r\n        # in the wrong order\r\n        if abs(np.dot(s1_, s2)) > abs(np.dot(s1_, s1)):\r\n            s2_, s1_ = s_\r\n        s1_ *= np.sign(np.dot(s1_, s1))\r\n        s2_ *= np.sign(np.dot(s2_, s2))\r\n\r\n        # Check that we have estimated the original sources\r\n        if not add_noise:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=2)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=2)\r\n        else:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=1)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=1)\r\n\r\n    # Test FastICA class\r\n    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo,\r\n                                random_state=seed)\r\n    ica = FastICA(fun=nl, algorithm=algo, random_state=seed)\r\n    sources = ica.fit_transform(m.T)\r\n    assert ica.components_.shape == (2, 2)\r\n    assert sources.shape == (1000, 2)\r\n\r\n    assert_array_almost_equal(sources_fun, sources)\r\n    assert_array_almost_equal(sources, ica.transform(m.T))\r\n\r\n    assert ica.mixing_.shape == (2, 2)\r\n\r\n    for fn in [np.tanh, \"exp(-.5(x^2))\"]:\r\n        ica = FastICA(fun=fn, algorithm=algo)\r\n        with pytest.raises(ValueError):\r\n            ica.fit(m.T)\r\n\r\n    with pytest.raises(TypeError):\r\n        FastICA(fun=range(10)).fit(m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_factor_analysis():\r\n    # Test FactorAnalysis ability to recover the data covariance structure\r\n    rng = np.random.RandomState(0)\r\n    n_samples, n_features, n_components = 20, 5, 3\r\n\r\n    # Some random settings for the generative model\r\n    W = rng.randn(n_components, n_features)\r\n    # latent variable of dim 3, 20 of it\r\n    h = rng.randn(n_samples, n_components)\r\n    # using gamma to model different noise variance\r\n    # per component\r\n    noise = rng.gamma(1, size=n_features) * rng.randn(n_samples, n_features)\r\n\r\n    # generate observations\r\n    # wlog, mean is 0\r\n    X = np.dot(h, W) + noise\r\n\r\n    with pytest.raises(ValueError):\r\n        FactorAnalysis(svd_method='foo')\r\n    fa_fail = FactorAnalysis()\r\n    fa_fail.svd_method = 'foo'\r\n    with pytest.raises(ValueError):\r\n        fa_fail.fit(X)\r\n    fas = []\r\n    for method in ['randomized', 'lapack']:\r\n        fa = FactorAnalysis(n_components=n_components, svd_method=method)\r\n        fa.fit(X)\r\n        fas.append(fa)\r\n\r\n        X_t = fa.transform(X)\r\n        assert X_t.shape == (n_samples, n_components)\r\n\r\n        assert_almost_equal(fa.loglike_[-1], fa.score_samples(X).sum())\r\n        assert_almost_equal(fa.score_samples(X).mean(), fa.score(X))\r\n\r\n        diff = np.all(np.diff(fa.loglike_))\r\n        assert diff > 0., 'Log likelihood dif not increase'\r\n\r\n        # Sample Covariance\r\n        scov = np.cov(X, rowvar=0., bias=1.)\r\n\r\n        # Model Covariance\r\n        mcov = fa.get_covariance()\r\n        diff = np.sum(np.abs(scov - mcov)) / W.size\r\n        assert diff < 0.1, \"Mean absolute difference is %f\" % diff\r\n        fa = FactorAnalysis(n_components=n_components,\r\n                            noise_variance_init=np.ones(n_features))\r\n        with pytest.raises(ValueError):\r\n            fa.fit(X[:, :2])\r\n\r\n    f = lambda x, y: np.abs(getattr(x, y))  # sign will not be equal\r\n    fa1, fa2 = fas\r\n    for attr in ['loglike_', 'components_', 'noise_variance_']:\r\n        assert_almost_equal(f(fa1, attr), f(fa2, attr))\r\n\r\n    fa1.max_iter = 1\r\n    fa1.verbose = True\r\n    assert_warns(ConvergenceWarning, fa1.fit, X)\r\n\r\n    # Test get_covariance and get_precision with n_components == n_features\r\n    # with n_components < n_features and with n_components == 0\r\n    for n_components in [0, 2, X.shape[1]]:\r\n        fa.n_components = n_components\r\n        fa.fit(X)\r\n        cov = fa.get_covariance()\r\n        precision = fa.get_precision()\r\n        assert_array_almost_equal(np.dot(cov, precision),\r\n                                  np.eye(X.shape[1]), 12)\r\n\r\n    # test rotation\r\n    n_components = 2\r\n\r\n    results, projections = {}, {}\r\n    for method in (None, \"varimax\", 'quartimax'):\r\n        fa_var = FactorAnalysis(n_components=n_components,\r\n                                rotation=method)\r\n        results[method] = fa_var.fit_transform(X)\r\n        projections[method] = fa_var.get_covariance()\r\n    for rot1, rot2 in combinations([None, 'varimax', 'quartimax'], 2):\r\n        assert not np.allclose(results[rot1], results[rot2])\r\n        assert np.allclose(projections[rot1], projections[rot2], atol=3)\r\n\r\n    assert_raises(ValueError,\r\n                  FactorAnalysis(rotation='not_implemented').fit_transform, X)\r\n\r\n    # test against R's psych::principal with rotate=\"varimax\"\r\n    # (i.e., the values below stem from rotating the components in R)\r\n    # R's factor analysis returns quite different values; therefore, we only\r\n    # test the rotation itself\r\n    factors = np.array(\r\n        [[0.89421016, -0.35854928, -0.27770122, 0.03773647],\r\n         [-0.45081822, -0.89132754, 0.0932195, -0.01787973],\r\n         [0.99500666, -0.02031465, 0.05426497, -0.11539407],\r\n         [0.96822861, -0.06299656, 0.24411001, 0.07540887]])\r\n    r_solution = np.array([[0.962, 0.052], [-0.141, 0.989],\r\n                           [0.949, -0.300], [0.937, -0.251]])\r\n    rotated = _ortho_rotation(factors[:, :n_components], method='varimax').T\r\n    assert_array_almost_equal(np.abs(rotated), np.abs(r_solution), decimal=3)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_incremental_pca_sparse(matrix_class):\r\n    # Incremental PCA on sparse arrays.\r\n    X = iris.data\r\n    pca = PCA(n_components=2)\r\n    pca.fit_transform(X)\r\n    X_sparse = matrix_class(X)\r\n    batch_size = X_sparse.shape[0] // 3\r\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\r\n\r\n    X_transformed = ipca.fit_transform(X_sparse)\r\n\r\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\r\n    np.testing.assert_allclose(ipca.explained_variance_ratio_.sum(),\r\n                               pca.explained_variance_ratio_.sum(), rtol=1e-3)\r\n\r\n    for n_components in [1, 2, X.shape[1]]:\r\n        ipca = IncrementalPCA(n_components, batch_size=batch_size)\r\n        ipca.fit(X_sparse)\r\n        cov = ipca.get_covariance()\r\n        precision = ipca.get_precision()\r\n        np.testing.assert_allclose(np.dot(cov, precision),\r\n                                   np.eye(X_sparse.shape[1]), atol=1e-13)\r\n\r\n    with pytest.raises(\r\n            TypeError,\r\n            match=\"IncrementalPCA.partial_fit does not support \"\r\n            \"sparse input. Either convert data to dense \"\r\n            \"or use IncrementalPCA.fit to do so in batches.\"):\r\n        ipca.partial_fit(X_sparse)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_type(Ensemble):\r\n    # check that ensemble will fail during validation if the underlying\r\n    # estimators are not of the same type (i.e. classifier or regressor)\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        X, y = make_classification(n_samples=10)\r\n        estimators = [('lr', LinearRegression())]\r\n        ensemble_type = 'classifier'\r\n    else:\r\n        X, y = make_regression(n_samples=10)\r\n        estimators = [('lr', LogisticRegression())]\r\n        ensemble_type = 'regressor'\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"should be a {}\".format(ensemble_type)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_name_validation(X, y, Ensemble):\r\n    # raise an error when the name contains dunder\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('lr__', LogisticRegression())]\r\n    else:\r\n        estimators = [('lr__', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Estimator names must not contain __: got \\['lr__'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name is not unique\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('lr', LogisticRegression()),\r\n                      ('lr', LogisticRegression())]\r\n    else:\r\n        estimators = [('lr', LinearRegression()),\r\n                      ('lr', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Names provided are not unique: \\['lr', 'lr'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name conflicts with the parameters\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('estimators', LogisticRegression())]\r\n    else:\r\n        estimators = [('estimators', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"Estimator names conflict with constructor arguments\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_all_dropped(X, y, estimator):\r\n    # check that we raise a consistent error when all estimators are\r\n    # dropped\r\n    estimator.set_params(lr='drop')\r\n    with pytest.raises(ValueError, match=\"All estimators are dropped.\"):\r\n        estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_gradient_boosting_with_init_pipeline():\r\n    # Check that the init estimator can be a pipeline (see issue #13466)\r\n\r\n    X, y = make_regression(random_state=0)\r\n    init = make_pipeline(LinearRegression())\r\n    gb = GradientBoostingRegressor(init=init)\r\n    gb.fit(X, y)  # pipeline without sample_weight works fine\r\n\r\n    with pytest.raises(\r\n            ValueError,\r\n            match='The initial estimator Pipeline does not support sample '\r\n                  'weights'):\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))\r\n\r\n    # Passing sample_weight to a pipeline raises a ValueError. This test makes\r\n    # sure we make the distinction between ValueError raised by a pipeline that\r\n    # was passed sample_weight, and a ValueError raised by a regular estimator\r\n    # whose input checking failed.\r\n    with pytest.raises(\r\n            ValueError,\r\n            match='nu <= 0 or nu > 1'):\r\n        # Note that NuSVR properly supports sample_weight\r\n        init = NuSVR(gamma='auto', nu=1.5)\r\n        gb = GradientBoostingRegressor(init=init)\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_gradient_boosting_init_wrong_methods(estimator, missing_method):\r\n    # Make sure error is raised if init estimators don't have the required\r\n    # methods (fit, predict, predict_proba)\r\n\r\n    message = (\"The init parameter must be a valid estimator and support \"\r\n               \"both fit and \" + missing_method)\r\n    with pytest.raises(ValueError, match=message):\r\n        estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_classifier_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        clf = StackingClassifier(**params, cv=3)\r\n        clf.fit(\r\n            scale(X_iris), y, sample_weight=np.ones(X_iris.shape[0])\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_regressor_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        reg = StackingRegressor(**params, cv=3)\r\n        reg.fit(\r\n            scale(X_diabetes), y, sample_weight=np.ones(X_diabetes.shape[0])\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_cv_influence(stacker, X, y):\r\n    # check that the stacking affects the fit of the final estimator but not\r\n    # the fit of the base estimators\r\n    # note: ConvergenceWarning are catch since we are not worrying about the\r\n    # convergence here\r\n    stacker_cv_3 = clone(stacker)\r\n    stacker_cv_5 = clone(stacker)\r\n\r\n    stacker_cv_3.set_params(cv=3)\r\n    stacker_cv_5.set_params(cv=5)\r\n\r\n    stacker_cv_3.fit(X, y)\r\n    stacker_cv_5.fit(X, y)\r\n\r\n    # the base estimators should be identical\r\n    for est_cv_3, est_cv_5 in zip(stacker_cv_3.estimators_,\r\n                                  stacker_cv_5.estimators_):\r\n        assert_allclose(est_cv_3.coef_, est_cv_5.coef_)\r\n\r\n    # the final estimator should be different\r\n    with pytest.raises(AssertionError, match='Not equal'):\r\n        assert_allclose(stacker_cv_3.final_estimator_.coef_,\r\n                        stacker_cv_5.final_estimator_.coef_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_forest_y_sparse():\r\n    X = [[1, 2, 3]]\r\n    y = csr_matrix([4, 5, 6])\r\n    est = RandomForestClassifier()\r\n    msg = \"sparse multilabel-indicator for y is not supported.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        est.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_classifier_estimator_init(params, err_msg):\r\n    ensemble = VotingClassifier(**params)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_verbose(estimator, capsys):\r\n\r\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\r\n    y = np.array([1, 1, 2, 2])\r\n\r\n    pattern = (r'\\[Voting\\].*\\(1 of 2\\) Processing lr, total=.*\\n'\r\n               r'\\[Voting\\].*\\(2 of 2\\) Processing rf, total=.*\\n$')\r\n\r\n    estimator.fit(X, y)\r\n    assert re.match(pattern, capsys.readouterr()[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predictproba_hardvoting():\r\n    eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),\r\n                                        ('lr2', LogisticRegression())],\r\n                            voting='hard')\r\n    msg = \"predict_proba is not available when voting='hard'\"\r\n    with pytest.raises(AttributeError, match=msg):\r\n        eclf.predict_proba\r\n\r\n    assert not hasattr(eclf, \"predict_proba\")\r\n    eclf.fit(X, y)\r\n    assert not hasattr(eclf, \"predict_proba\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sample_weight():\r\n    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"\r\n    clf1 = LogisticRegression(random_state=123)\r\n    clf2 = RandomForestClassifier(random_state=123)\r\n    clf3 = SVC(probability=True, random_state=123)\r\n    eclf1 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\r\n        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))\r\n    eclf2 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\r\n        voting='soft').fit(X, y)\r\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\r\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\r\n\r\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))\r\n    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\r\n    eclf3.fit(X, y, sample_weight)\r\n    clf1.fit(X, y, sample_weight)\r\n    assert_array_equal(eclf3.predict(X), clf1.predict(X))\r\n    assert_array_almost_equal(eclf3.predict_proba(X), clf1.predict_proba(X))\r\n\r\n    # check that an error is raised and indicative if sample_weight is not\r\n    # supported.\r\n    clf4 = KNeighborsClassifier()\r\n    eclf3 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('svc', clf3), ('knn', clf4)],\r\n        voting='soft')\r\n    msg = ('Underlying estimator KNeighborsClassifier does not support '\r\n           'sample weights.')\r\n    with pytest.raises(TypeError, match=msg):\r\n        eclf3.fit(X, y, sample_weight)\r\n\r\n    # check that _fit_single_estimator will raise the right error\r\n    # it should raise the original error if this is not linked to sample_weight\r\n    class ClassifierErrorFit(ClassifierMixin, BaseEstimator):\r\n        def fit(self, X, y, sample_weight):\r\n            raise TypeError('Error unrelated to sample_weight.')\r\n    clf = ClassifierErrorFit()\r\n    with pytest.raises(TypeError, match='Error unrelated to sample_weight'):\r\n        clf.fit(X, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboost_negative_weight_error(model, X, y):\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = -10\r\n\r\n    err_msg = \"sample_weight cannot contain negative weight\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        model.fit(X, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_none_estimator_with_weights(X, y, voter):\r\n    # check that an estimator can be set to 'drop' and passing some weight\r\n    # regression test for\r\n    # https://github.com/scikit-learn/scikit-learn/issues/13777\r\n    voter = clone(voter)\r\n    voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n    voter.set_params(lr='drop')\r\n    with pytest.warns(None) as record:\r\n        voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n    assert not record\r\n    y_pred = voter.predict(X)\r\n    assert y_pred.shape == y.shape",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboostregressor_sample_weight():\r\n    # check that giving weight will have an influence on the error computed\r\n    # for a weak learner\r\n    rng = np.random.RandomState(42)\r\n    X = np.linspace(0, 100, num=1000)\r\n    y = (.8 * X + 0.2) + (rng.rand(X.shape[0]) * 0.0001)\r\n    X = X.reshape(-1, 1)\r\n\r\n    # add an arbitrary outlier\r\n    X[-1] *= 10\r\n    y[-1] = 10000\r\n\r\n    # random_state=0 ensure that the underlying bootstrap will use the outlier\r\n    regr_no_outlier = AdaBoostRegressor(\r\n        base_estimator=LinearRegression(), n_estimators=1, random_state=0\r\n    )\r\n    regr_with_weight = clone(regr_no_outlier)\r\n    regr_with_outlier = clone(regr_no_outlier)\r\n\r\n    # fit 3 models:\r\n    # - a model containing the outlier\r\n    # - a model without the outlier\r\n    # - a model containing the outlier but with a null sample-weight\r\n    regr_with_outlier.fit(X, y)\r\n    regr_no_outlier.fit(X[:-1], y[:-1])\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = 0\r\n    regr_with_weight.fit(X, y, sample_weight=sample_weight)\r\n\r\n    score_with_outlier = regr_with_outlier.score(X[:-1], y[:-1])\r\n    score_no_outlier = regr_no_outlier.score(X[:-1], y[:-1])\r\n    score_with_weight = regr_with_weight.score(X[:-1], y[:-1])\r\n\r\n    assert score_with_outlier < score_no_outlier\r\n    assert score_with_outlier < score_with_weight\r\n    assert score_no_outlier == pytest.approx(score_with_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparse_support():\r\n    # Make sure sparse data is supported\r\n\r\n    X, y = make_regression(n_features=10)\r\n    X = scipy.sparse.csr_matrix(X)\r\n    sfs = SequentialFeatureSelector(LinearRegression(), cv=2)\r\n    sfs.fit(X, y)\r\n    sfs.transform(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nan_support():\r\n    # Make sure nans are OK if the underlying estimator supports nans\r\n\r\n    rng = np.random.RandomState(0)\r\n    n_samples, n_features = 100, 10\r\n    X, y = make_regression(n_samples, n_features, random_state=0)\r\n    nan_mask = rng.randint(0, 2, size=(n_samples, n_features), dtype=bool)\r\n    X[nan_mask] = np.nan\r\n    sfs = SequentialFeatureSelector(HistGradientBoostingRegressor(), cv=2)\r\n    sfs.fit(X, y)\r\n    sfs.transform(X)\r\n\r\n    with pytest.raises(ValueError, match='Input contains NaN'):\r\n        # LinearRegression does not support nans\r\n        SequentialFeatureSelector(LinearRegression(), cv=2).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_n_features_to_select(n_features_to_select):\r\n    X, y = make_regression(n_features=5)\r\n    sfs = SequentialFeatureSelector(LinearRegression(),\r\n                                    n_features_to_select=n_features_to_select)\r\n    with pytest.raises(ValueError, match=\"must be either None\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_direction():\r\n    X, y = make_regression(n_features=5)\r\n    sfs = SequentialFeatureSelector(LinearRegression(), direction='bad')\r\n    with pytest.raises(ValueError, match=\"must be either 'forward' or\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_warning_for_kind_legacy():\r\n    est = LogisticRegression()\r\n    (X, y), n_targets = binary_classification_data\r\n    est.fit(X, y)\r\n\r\n    err_msg = (\"A Bunch will be returned in place of 'predictions' from \"\r\n               \"version 1.1\")\r\n    with pytest.warns(FutureWarning, match=err_msg):\r\n        partial_dependence(est, X=X, features=[1, 2])\r\n\r\n    with pytest.warns(FutureWarning, match=err_msg):\r\n        partial_dependence(est, X=X, features=[1, 2], kind='legacy')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_error(estimator, params, err_msg):\r\n    X, y = make_classification(random_state=0)\r\n    estimator.fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_slice_error(with_dataframe, err_msg):\r\n    X, y = make_classification(random_state=0)\r\n    if with_dataframe:\r\n        pd = pytest.importorskip('pandas')\r\n        X = pd.DataFrame(X)\r\n    estimator = LogisticRegression().fit(X, y)\r\n\r\n    with pytest.raises(TypeError, match=err_msg):\r\n        partial_dependence(estimator, X, features=slice(0, 2, 1))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_permutation_importance_no_weights_scoring_function():\r\n    # Creating a scorer function that does not takes sample_weight\r\n    def my_scorer(estimator, X, y):\r\n        return 1\r\n\r\n    # Creating some data and estimator for the permutation test\r\n    x = np.array([[1, 2], [3, 4]])\r\n    y = np.array([1, 2])\r\n    w = np.array([1, 1])\r\n    lr = LinearRegression()\r\n    lr.fit(x, y)\r\n\r\n    # test that permutation_importance does not return error when\r\n    # sample_weight is None\r\n    try:\r\n        permutation_importance(lr, x, y, random_state=1,\r\n                               scoring=my_scorer,\r\n                               n_repeats=1)\r\n    except TypeError:\r\n        pytest.fail(\"permutation_test raised an error when using a scorer \"\r\n                    \"function that does not accept sample_weight even though \"\r\n                    \"sample_weight was None\")\r\n\r\n    # test that permutation_importance raise exception when sample_weight is\r\n    # not None\r\n    with pytest.raises(TypeError):\r\n        permutation_importance(lr, x, y, random_state=1,\r\n                               scoring=my_scorer,\r\n                               n_repeats=1,\r\n                               sample_weight=w)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_indices(estimator, features):\r\n    X, y = make_classification(random_state=0)\r\n    estimator.fit(X, y)\r\n\r\n    err_msg = 'all features must be in'\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, [features])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_partial_dependence_error(pyplot, data, params, err_msg):\r\n    X, y = data\r\n    estimator = LinearRegression().fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        plot_partial_dependence(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_string(estimator):\r\n    pd = pytest.importorskip(\"pandas\")\r\n    X, y = make_classification(random_state=0)\r\n    df = pd.DataFrame(X)\r\n    estimator.fit(df, y)\r\n\r\n    features = ['random']\r\n    err_msg = 'A given column is not a column of the dataframe'\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, df, features)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit(self, X, y, sample_weight=None):\r\n        \"\"\"Fit estimator using RANSAC algorithm.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like or sparse matrix, shape [n_samples, n_features]\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\r\n            Target values.\r\n\r\n        sample_weight : array-like of shape (n_samples,), default=None\r\n            Individual weights for each sample\r\n            raises error if sample_weight is passed and base_estimator\r\n            fit method does not support it.\r\n\r\n            .. versionadded:: 0.18\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If no valid consensus set could be found. This occurs if\r\n            `is_data_valid` and `is_model_valid` return False for all\r\n            `max_trials` randomly chosen sub-samples.\r\n\r\n        \"\"\"\r\n        # Need to validate separately here.\r\n        # We can't pass multi_ouput=True because that would allow y to be csr.\r\n        check_X_params = dict(accept_sparse='csr')\r\n        check_y_params = dict(ensure_2d=False)\r\n        X, y = self._validate_data(X, y, validate_separately=(check_X_params,\r\n                                                              check_y_params))\r\n        check_consistent_length(X, y)\r\n\r\n        if self.base_estimator is not None:\r\n            base_estimator = clone(self.base_estimator)\r\n        else:\r\n            base_estimator = LinearRegression()\r\n\r\n        if self.min_samples is None:\r\n            # assume linear model by default\r\n            min_samples = X.shape[1] + 1\r\n        elif 0 < self.min_samples < 1:\r\n            min_samples = np.ceil(self.min_samples * X.shape[0])\r\n        elif self.min_samples >= 1:\r\n            if self.min_samples % 1 != 0:\r\n                raise ValueError(\"Absolute number of samples must be an \"\r\n                                 \"integer value.\")\r\n            min_samples = self.min_samples\r\n        else:\r\n            raise ValueError(\"Value for `min_samples` must be scalar and \"\r\n                             \"positive.\")\r\n        if min_samples > X.shape[0]:\r\n            raise ValueError(\"`min_samples` may not be larger than number \"\r\n                             \"of samples: n_samples = %d.\" % (X.shape[0]))\r\n\r\n        if self.stop_probability < 0 or self.stop_probability > 1:\r\n            raise ValueError(\"`stop_probability` must be in range [0, 1].\")\r\n\r\n        if self.residual_threshold is None:\r\n            # MAD (median absolute deviation)\r\n            residual_threshold = np.median(np.abs(y - np.median(y)))\r\n        else:\r\n            residual_threshold = self.residual_threshold\r\n\r\n        if self.loss == \"absolute_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)\r\n\r\n        elif self.loss == \"squared_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)\r\n\r\n        elif callable(self.loss):\r\n            loss_function = self.loss\r\n\r\n        else:\r\n            raise ValueError(\r\n                \"loss should be 'absolute_loss', 'squared_loss' or a callable.\"\r\n                \"Got %s. \" % self.loss)\r\n\r\n\r\n        random_state = check_random_state(self.random_state)\r\n\r\n        try:  # Not all estimator accept a random_state\r\n            base_estimator.set_params(random_state=random_state)\r\n        except ValueError:\r\n            pass\r\n\r\n        estimator_fit_has_sample_weight = has_fit_parameter(base_estimator,\r\n                                                            \"sample_weight\")\r\n        estimator_name = type(base_estimator).__name__\r\n        if (sample_weight is not None and not\r\n                estimator_fit_has_sample_weight):\r\n            raise ValueError(\"%s does not support sample_weight. Samples\"\r\n                             \" weights are only used for the calibration\"\r\n                             \" itself.\" % estimator_name)\r\n        if sample_weight is not None:\r\n            sample_weight = _check_sample_weight(sample_weight, X)\r\n\r\n        n_inliers_best = 1\r\n        score_best = -np.inf\r\n        inlier_mask_best = None\r\n        X_inlier_best = None\r\n        y_inlier_best = None\r\n        inlier_best_idxs_subset = None\r\n        self.n_skips_no_inliers_ = 0\r\n        self.n_skips_invalid_data_ = 0\r\n        self.n_skips_invalid_model_ = 0\r\n\r\n        # number of data samples\r\n        n_samples = X.shape[0]\r\n        sample_idxs = np.arange(n_samples)\r\n\r\n        self.n_trials_ = 0\r\n        max_trials = self.max_trials\r\n        while self.n_trials_ < max_trials:\r\n            self.n_trials_ += 1\r\n\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                break\r\n\r\n            # choose random sample set\r\n            subset_idxs = sample_without_replacement(n_samples, min_samples,\r\n                                                     random_state=random_state)\r\n            X_subset = X[subset_idxs]\r\n            y_subset = y[subset_idxs]\r\n\r\n            # check if random sample set is valid\r\n            if (self.is_data_valid is not None\r\n                    and not self.is_data_valid(X_subset, y_subset)):\r\n                self.n_skips_invalid_data_ += 1\r\n                continue\r\n\r\n            # fit model for current random sample set\r\n            if sample_weight is None:\r\n                base_estimator.fit(X_subset, y_subset)\r\n            else:\r\n                base_estimator.fit(X_subset, y_subset,\r\n                                   sample_weight=sample_weight[subset_idxs])\r\n\r\n            # check if estimated model is valid\r\n            if (self.is_model_valid is not None and not\r\n                    self.is_model_valid(base_estimator, X_subset, y_subset)):\r\n                self.n_skips_invalid_model_ += 1\r\n                continue\r\n\r\n            # residuals of all data for current random sample model\r\n            y_pred = base_estimator.predict(X)\r\n            residuals_subset = loss_function(y, y_pred)\r\n\r\n            # classify data into inliers and outliers\r\n            inlier_mask_subset = residuals_subset < residual_threshold\r\n            n_inliers_subset = np.sum(inlier_mask_subset)\r\n\r\n            # less inliers -> skip current random sample\r\n            if n_inliers_subset < n_inliers_best:\r\n                self.n_skips_no_inliers_ += 1\r\n                continue\r\n\r\n            # extract inlier data set\r\n            inlier_idxs_subset = sample_idxs[inlier_mask_subset]\r\n            X_inlier_subset = X[inlier_idxs_subset]\r\n            y_inlier_subset = y[inlier_idxs_subset]\r\n\r\n            # score of inlier data set\r\n            score_subset = base_estimator.score(X_inlier_subset,\r\n                                                y_inlier_subset)\r\n\r\n            # same number of inliers but worse score -> skip current random\r\n            # sample\r\n            if (n_inliers_subset == n_inliers_best\r\n                    and score_subset < score_best):\r\n                continue\r\n\r\n            # save current random sample as best sample\r\n            n_inliers_best = n_inliers_subset\r\n            score_best = score_subset\r\n            inlier_mask_best = inlier_mask_subset\r\n            X_inlier_best = X_inlier_subset\r\n            y_inlier_best = y_inlier_subset\r\n            inlier_best_idxs_subset = inlier_idxs_subset\r\n\r\n            max_trials = min(\r\n                max_trials,\r\n                _dynamic_max_trials(n_inliers_best, n_samples,\r\n                                    min_samples, self.stop_probability))\r\n\r\n            # break if sufficient number of inliers or score is reached\r\n            if n_inliers_best >= self.stop_n_inliers or \\\r\n                            score_best >= self.stop_score:\r\n                break\r\n\r\n        # if none of the iterations met the required criteria\r\n        if inlier_mask_best is None:\r\n            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips):\r\n                raise ValueError(\r\n                    \"RANSAC skipped more iterations than `max_skips` without\"\r\n                    \" finding a valid consensus set. Iterations were skipped\"\r\n                    \" because each randomly chosen sub-sample failed the\"\r\n                    \" passing criteria. See estimator attributes for\"\r\n                    \" diagnostics (n_skips*).\")\r\n            else:\r\n                raise ValueError(\r\n                    \"RANSAC could not find a valid consensus set. All\"\r\n                    \" `max_trials` iterations were skipped because each\"\r\n                    \" randomly chosen sub-sample failed the passing criteria.\"\r\n                    \" See estimator attributes for diagnostics (n_skips*).\")\r\n        else:\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                warnings.warn(\"RANSAC found a valid consensus set but exited\"\r\n                              \" early due to skipping more iterations than\"\r\n                              \" `max_skips`. See estimator attributes for\"\r\n                              \" diagnostics (n_skips*).\",\r\n                              ConvergenceWarning)\r\n\r\n        # estimate final model using all inliers\r\n        if sample_weight is None:\r\n            base_estimator.fit(X_inlier_best, y_inlier_best)\r\n        else:\r\n            base_estimator.fit(\r\n                X_inlier_best,\r\n                y_inlier_best,\r\n                sample_weight=sample_weight[inlier_best_idxs_subset])\r\n\r\n        self.estimator_ = base_estimator\r\n        self.inlier_mask_ = inlier_mask_best\r\n        return self",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit(self, X, y):\r\n        \"\"\"Fit the model using X, y as training data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,)\r\n            Target values.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            returns an instance of self.\r\n        \"\"\"\r\n        X, y = self._validate_data(X, y, y_numeric=True)\r\n        X = as_float_array(X, copy=self.copy_X)\r\n        y = as_float_array(y, copy=self.copy_X)\r\n\r\n        # init cross-validation generator\r\n        cv = check_cv(self.cv, classifier=False)\r\n\r\n        # As we use cross-validation, the Gram matrix is not precomputed here\r\n        Gram = self.precompute\r\n        if hasattr(Gram, '__array__'):\r\n            warnings.warn('Parameter \"precompute\" cannot be an array in '\r\n                          '%s. Automatically switch to \"auto\" instead.'\r\n                          % self.__class__.__name__)\r\n            Gram = 'auto'\r\n\r\n        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\r\n            delayed(_lars_path_residues)(\r\n                X[train], y[train], X[test], y[test], Gram=Gram, copy=False,\r\n                method=self.method, verbose=max(0, self.verbose - 1),\r\n                normalize=self.normalize, fit_intercept=self.fit_intercept,\r\n                max_iter=self.max_iter, eps=self.eps, positive=self.positive)\r\n            for train, test in cv.split(X, y))\r\n        all_alphas = np.concatenate(list(zip(*cv_paths))[0])\r\n        # Unique also sorts\r\n        all_alphas = np.unique(all_alphas)\r\n        # Take at most max_n_alphas values\r\n        stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))\r\n        all_alphas = all_alphas[::stride]\r\n\r\n        mse_path = np.empty((len(all_alphas), len(cv_paths)))\r\n        for index, (alphas, _, _, residues) in enumerate(cv_paths):\r\n            alphas = alphas[::-1]\r\n            residues = residues[::-1]\r\n            if alphas[0] != 0:\r\n                alphas = np.r_[0, alphas]\r\n                residues = np.r_[residues[0, np.newaxis], residues]\r\n            if alphas[-1] != all_alphas[-1]:\r\n                alphas = np.r_[alphas, all_alphas[-1]]\r\n                residues = np.r_[residues, residues[-1, np.newaxis]]\r\n            this_residues = interpolate.interp1d(alphas,\r\n                                                 residues,\r\n                                                 axis=0)(all_alphas)\r\n            this_residues **= 2\r\n            mse_path[:, index] = np.mean(this_residues, axis=-1)\r\n\r\n        mask = np.all(np.isfinite(mse_path), axis=-1)\r\n        all_alphas = all_alphas[mask]\r\n        mse_path = mse_path[mask]\r\n        # Select the alpha that minimizes left-out error\r\n        i_best_alpha = np.argmin(mse_path.mean(axis=-1))\r\n        best_alpha = all_alphas[i_best_alpha]\r\n\r\n        # Store our parameters\r\n        self.alpha_ = best_alpha\r\n        self.cv_alphas_ = all_alphas\r\n        self.mse_path_ = mse_path\r\n\r\n        # Now compute the full model\r\n        # it will call a lasso internally when self if LassoLarsCV\r\n        # as self.method == 'lasso'\r\n        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,\r\n                  Xy=None, fit_path=True)\r\n        return self",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_linear_regression_pd_sparse_dataframe_warning():\r\n    pd = pytest.importorskip('pandas')\r\n    # restrict the pd versions < '0.24.0' as they have a bug in is_sparse func\r\n    if parse_version(pd.__version__) < parse_version('0.24.0'):\r\n        pytest.skip(\"pandas 0.24+ required.\")\r\n\r\n    # Warning is raised only when some of the columns is sparse\r\n    df = pd.DataFrame({'0': np.random.randn(10)})\r\n    for col in range(1, 4):\r\n        arr = np.random.randn(10)\r\n        arr[:8] = 0\r\n        # all columns but the first column is sparse\r\n        if col != 0:\r\n            arr = pd.arrays.SparseArray(arr, fill_value=0)\r\n        df[str(col)] = arr\r\n\r\n    msg = \"pandas.DataFrame with sparse columns found.\"\r\n    with pytest.warns(UserWarning, match=msg):\r\n        reg = LinearRegression()\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])\r\n\r\n    # does not warn when the whole dataframe is sparse\r\n    df['0'] = pd.arrays.SparseArray(df['0'], fill_value=0)\r\n    assert hasattr(df, \"sparse\")\r\n\r\n    with pytest.warns(None) as record:\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])\r\n    assert not record",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_enet_sample_weight_sparse():\r\n    reg = ElasticNet()\r\n    X = sparse.csc_matrix(np.zeros((3, 2)))\r\n    y = np.array([-1, 0, 1])\r\n    sw = np.array([1, 2, 3])\r\n    with pytest.raises(ValueError, match=\"Sample weights do not.*support \"\r\n                                         \"sparse matrices\"):\r\n        reg.fit(X, y, sample_weight=sw, check_input=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_scorer_select_proba_error(scorer):\r\n    # check that we raise the the proper error when passing an unknown\r\n    # pos_label\r\n    X, y = make_classification(\r\n        n_classes=2, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n    assert scorer._kwargs[\"pos_label\"] not in np.unique(y).tolist()\r\n\r\n    err_msg = \"is not a valid label\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        scorer(lr, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_raises_on_score_list():\r\n    # Test that when a list of scores is returned, we raise proper errors.\r\n    X, y = make_blobs(random_state=0)\r\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\r\n    clf = DecisionTreeClassifier()\r\n    with pytest.raises(ValueError):\r\n        cross_val_score(clf, X, y, scoring=f1_scorer_no_average)\r\n    grid_search = GridSearchCV(clf, scoring=f1_scorer_no_average,\r\n                               param_grid={'max_depth': [1, 2]})\r\n    with pytest.raises(ValueError):\r\n        grid_search.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_confusion_matrix_pipeline(pyplot, clf, data, n_classes):\r\n    X, y = data\r\n    with pytest.raises(NotFittedError):\r\n        plot_confusion_matrix(clf, X, y)\r\n    clf.fit(X, y)\r\n    y_pred = clf.predict(X)\r\n\r\n    disp = plot_confusion_matrix(clf, X, y)\r\n    cm = confusion_matrix(y, y_pred)\r\n\r\n    assert_allclose(disp.confusion_matrix, cm)\r\n    assert disp.text_.shape == (n_classes, n_classes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_curve_error_non_binary(pyplot, data, plot_func):\r\n    X, y = data\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X, y)\r\n\r\n    msg = \"DecisionTreeClassifier should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_func(clf, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_errors(pyplot):\r\n    X, y_multiclass = make_classification(n_classes=3, n_samples=50,\r\n                                          n_informative=3,\r\n                                          random_state=0)\r\n    y_binary = y_multiclass == 0\r\n\r\n    # Unfitted classifer\r\n    binary_clf = DecisionTreeClassifier()\r\n    with pytest.raises(NotFittedError):\r\n        plot_precision_recall_curve(binary_clf, X, y_binary)\r\n    binary_clf.fit(X, y_binary)\r\n\r\n    multi_clf = DecisionTreeClassifier().fit(X, y_multiclass)\r\n\r\n    # Fitted multiclass classifier with binary data\r\n    msg = \"DecisionTreeClassifier should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_precision_recall_curve(multi_clf, X, y_binary)\r\n\r\n    reg = DecisionTreeRegressor().fit(X, y_multiclass)\r\n    msg = \"DecisionTreeRegressor should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_precision_recall_curve(reg, X, y_binary)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_precision_recall(pyplot, response_method, with_sample_weight):\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n\r\n    lr = LogisticRegression().fit(X, y)\r\n\r\n    if with_sample_weight:\r\n        rng = np.random.RandomState(42)\r\n        sample_weight = rng.randint(0, 4, size=X.shape[0])\r\n    else:\r\n        sample_weight = None\r\n\r\n    disp = plot_precision_recall_curve(lr, X, y, alpha=0.8,\r\n                                       response_method=response_method,\r\n                                       sample_weight=sample_weight)\r\n\r\n    y_score = getattr(lr, response_method)(X)\r\n    if response_method == 'predict_proba':\r\n        y_score = y_score[:, 1]\r\n\r\n    prec, recall, _ = precision_recall_curve(y, y_score,\r\n                                             sample_weight=sample_weight)\r\n    avg_prec = average_precision_score(y, y_score, sample_weight=sample_weight)\r\n\r\n    assert_allclose(disp.precision, prec)\r\n    assert_allclose(disp.recall, recall)\r\n    assert disp.average_precision == pytest.approx(avg_prec)\r\n\r\n    assert disp.estimator_name == \"LogisticRegression\"\r\n\r\n    # cannot fail thanks to pyplot fixture\r\n    import matplotlib as mpl  # noqa\r\n    assert isinstance(disp.line_, mpl.lines.Line2D)\r\n    assert disp.line_.get_alpha() == 0.8\r\n    assert isinstance(disp.ax_, mpl.axes.Axes)\r\n    assert isinstance(disp.figure_, mpl.figure.Figure)\r\n\r\n    expected_label = \"LogisticRegression (AP = {:0.2f})\".format(avg_prec)\r\n    assert disp.line_.get_label() == expected_label\r\n    assert disp.ax_.get_xlabel() == \"Recall (Positive label: 1)\"\r\n    assert disp.ax_.get_ylabel() == \"Precision (Positive label: 1)\"\r\n\r\n    # draw again with another label\r\n    disp.plot(name=\"MySpecialEstimator\")\r\n    expected_label = \"MySpecialEstimator (AP = {:0.2f})\".format(avg_prec)\r\n    assert disp.line_.get_label() == expected_label",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_curve_pipeline(pyplot, clf):\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n    with pytest.raises(NotFittedError):\r\n        plot_precision_recall_curve(clf, X, y)\r\n    clf.fit(X, y)\r\n    disp = plot_precision_recall_curve(clf, X, y)\r\n    assert disp.estimator_name == clf.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_curve_string_labels(pyplot):\r\n    # regression test #15738\r\n    cancer = load_breast_cancer()\r\n    X = cancer.data\r\n    y = cancer.target_names[cancer.target]\r\n\r\n    lr = make_pipeline(StandardScaler(), LogisticRegression())\r\n    lr.fit(X, y)\r\n    for klass in cancer.target_names:\r\n        assert klass in lr.classes_\r\n    disp = plot_precision_recall_curve(lr, X, y)\r\n\r\n    y_pred = lr.predict_proba(X)[:, 1]\r\n    avg_prec = average_precision_score(y, y_pred,\r\n                                       pos_label=lr.classes_[1])\r\n\r\n    assert disp.average_precision == pytest.approx(avg_prec)\r\n    assert disp.estimator_name == lr.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_thresholded_scorers():\r\n    # Test scorers that take thresholds.\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = LogisticRegression(random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\r\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n    assert_almost_equal(score1, score3)\r\n\r\n    logscore = get_scorer('neg_log_loss')(clf, X_test, y_test)\r\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\r\n    assert_almost_equal(-logscore, logloss)\r\n\r\n    # same for an estimator without decision_function\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n\r\n    # test with a regressor (no decision_function)\r\n    reg = DecisionTreeRegressor()\r\n    reg.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(reg, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, reg.predict(X_test))\r\n    assert_almost_equal(score1, score2)\r\n\r\n    # Test that an exception is raised on more than two classes\r\n    X, y = make_blobs(random_state=0, centers=3)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    with pytest.raises(ValueError, match=\"multiclass format is not supported\"):\r\n        get_scorer('roc_auc')(clf, X_test, y_test)\r\n\r\n    # test error is raised with a single class present in model\r\n    # (predict_proba shape is not suitable for binary auc)\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, np.zeros_like(y_train))\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer('roc_auc')(clf, X_test, y_test)\r\n\r\n    # for proba scorers\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer('neg_log_loss')(clf, X_test, y_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error_on_regressor(pyplot, data):\r\n    X, y = data\r\n    est = SVR().fit(X, y)\r\n\r\n    msg = \"plot_confusion_matrix only supports classifiers\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_confusion_matrix(est, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_roc_curve_not_fitted_errors(pyplot, data_binary, clf):\r\n    X, y = data_binary\r\n    with pytest.raises(NotFittedError):\r\n        plot_roc_curve(clf, X, y)\r\n    clf.fit(X, y)\r\n    disp = plot_roc_curve(clf, X, y)\r\n    assert clf.__class__.__name__ in disp.line_.get_label()\r\n    assert disp.estimator_name == clf.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_det_curve_not_fitted_errors(pyplot, data_binary, clf, plot_func):\r\n    X, y = data_binary\r\n    # clone since we parametrize the test and the classifier will be fitted\r\n    # when testing the second and subsequent plotting function\r\n    model = clone(clf)\r\n    with pytest.raises(NotFittedError):\r\n        plot_func(model, X, y)\r\n    model.fit(X, y)\r\n    disp = plot_func(model, X, y)\r\n    assert model.__class__.__name__ in disp.line_.get_label()\r\n    assert disp.estimator_name == model.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_roc_curve_pos_label(pyplot, response_method):\r\n    # check that we can provide the positive label and display the proper\r\n    # statistics\r\n    X, y = load_breast_cancer(return_X_y=True)\r\n    # create an highly imbalanced\r\n    idx_positive = np.flatnonzero(y == 1)\r\n    idx_negative = np.flatnonzero(y == 0)\r\n    idx_selected = np.hstack([idx_negative, idx_positive[:25]])\r\n    X, y = X[idx_selected], y[idx_selected]\r\n    X, y = shuffle(X, y, random_state=42)\r\n    # only use 2 features to make the problem even harder\r\n    X = X[:, :2]\r\n    y = np.array(\r\n        [\"cancer\" if c == 1 else \"not cancer\" for c in y], dtype=object\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, stratify=y, random_state=0,\r\n    )\r\n\r\n    classifier = LogisticRegression()\r\n    classifier.fit(X_train, y_train)\r\n\r\n    # sanity check to be sure the positive class is classes_[0] and that we\r\n    # are betrayed by the class imbalance\r\n    assert classifier.classes_.tolist() == [\"cancer\", \"not cancer\"]\r\n\r\n    disp = plot_roc_curve(\r\n        classifier, X_test, y_test, pos_label=\"cancer\",\r\n        response_method=response_method\r\n    )\r\n\r\n    roc_auc_limit = 0.95679\r\n\r\n    assert disp.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert np.trapz(disp.tpr, disp.fpr) == pytest.approx(roc_auc_limit)\r\n\r\n    disp = plot_roc_curve(\r\n        classifier, X_test, y_test,\r\n        response_method=response_method,\r\n    )\r\n\r\n    assert disp.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert np.trapz(disp.tpr, disp.fpr) == pytest.approx(roc_auc_limit)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_searchcv_raise_warning_with_non_finite_score(\r\n        SearchCV, specialized_params, return_train_score):\r\n    # Non-regression test for:\r\n    # https://github.com/scikit-learn/scikit-learn/issues/10529\r\n    # Check that we raise a UserWarning when a non-finite score is\r\n    # computed in the SearchCV\r\n    X, y = make_classification(n_classes=2, random_state=0)\r\n\r\n    class FailingScorer:\r\n        \"\"\"Scorer that will fail for some split but not all.\"\"\"\r\n\r\n        def __init__(self):\r\n            self.n_counts = 0\r\n\r\n        def __call__(self, estimator, X, y):\r\n            self.n_counts += 1\r\n            if self.n_counts % 5 == 0:\r\n                return np.nan\r\n            return 1\r\n\r\n    grid = SearchCV(\r\n        DecisionTreeClassifier(),\r\n        scoring=FailingScorer(),\r\n        cv=3,\r\n        return_train_score=return_train_score,\r\n        **specialized_params\r\n    )\r\n\r\n    with pytest.warns(UserWarning) as warn_msg:\r\n        grid.fit(X, y)\r\n\r\n    set_with_warning = [\"test\", \"train\"] if return_train_score else [\"test\"]\r\n    assert len(warn_msg) == len(set_with_warning)\r\n    for msg, dataset in zip(warn_msg, set_with_warning):\r\n        assert (f\"One or more of the {dataset} scores are non-finite\" in\r\n                str(msg.message))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_default_iid(SearchCV, specialized_params):\r\n    # Test the IID parameter  TODO: Clearly this test does something else???\r\n    # noise-free simple 2d-data\r\n    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,\r\n                      cluster_std=0.1, shuffle=False, n_samples=80)\r\n    # split dataset into two folds that are not iid\r\n    # first one contains data of all 4 blobs, second only from two.\r\n    mask = np.ones(X.shape[0], dtype=bool)\r\n    mask[np.where(y == 1)[0][::2]] = 0\r\n    mask[np.where(y == 2)[0][::2]] = 0\r\n    # this leads to perfect classification on one fold and a score of 1/3 on\r\n    # the other\r\n    # create \"cv\" for splits\r\n    cv = [[mask, ~mask], [~mask, mask]]\r\n\r\n    common_params = {'estimator': SVC(), 'cv': cv,\r\n                     'return_train_score': True}\r\n    search = SearchCV(**common_params, **specialized_params)\r\n    search.fit(X, y)\r\n\r\n    test_cv_scores = np.array(\r\n        [search.cv_results_['split%d_test_score' % s][0]\r\n         for s in range(search.n_splits_)]\r\n    )\r\n    test_mean = search.cv_results_['mean_test_score'][0]\r\n    test_std = search.cv_results_['std_test_score'][0]\r\n\r\n    train_cv_scores = np.array(\r\n        [search.cv_results_['split%d_train_score' % s][0]\r\n         for s in range(search.n_splits_)]\r\n    )\r\n    train_mean = search.cv_results_['mean_train_score'][0]\r\n    train_std = search.cv_results_['std_train_score'][0]\r\n\r\n    assert search.cv_results_['param_C'][0] == 1\r\n    # scores are the same as above\r\n    assert_allclose(test_cv_scores, [1, 1. / 3.])\r\n    assert_allclose(train_cv_scores, [1, 1])\r\n    # Unweighted mean/std is used\r\n    assert test_mean == pytest.approx(np.mean(test_cv_scores))\r\n    assert test_std == pytest.approx(np.std(test_cv_scores))\r\n\r\n    # For the train scores, we do not take a weighted mean irrespective of\r\n    # i.i.d. or not\r\n    assert train_mean == pytest.approx(1)\r\n    assert train_std == pytest.approx(0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_empty_cv_iterator_error():\r\n    # Use global X, y\r\n\r\n    # create cv\r\n    cv = KFold(n_splits=3).split(X)\r\n\r\n    # pop all of it, this should cause the expected ValueError\r\n    [u for u in cv]\r\n    # cv is empty now\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},\r\n                               cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(ValueError,\r\n                       match='No fits were performed. '\r\n                             'Was the CV iterator empty\\\\? '\r\n                             'Were there no candidates\\\\?'):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_cv_score_samples_error(search_cv):\r\n    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)\r\n    search_cv.fit(X, y)\r\n\r\n    # Make sure to error out when underlying estimator does not implement\r\n    # the method `score_samples`\r\n    err_msg = (\"'DecisionTreeClassifier' object has no attribute \"\r\n               \"'score_samples'\")\r\n\r\n    with pytest.raises(AttributeError, match=err_msg):\r\n        search_cv.score_samples(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test__custom_fit_no_run_search():\r\n    class NoRunSearchSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n        def fit(self, X, y=None, groups=None, **fit_params):\r\n            return self\r\n\r\n    # this should not raise any exceptions\r\n    NoRunSearchSearchCV(SVC()).fit(X, y)\r\n\r\n    class BadSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n    with pytest.raises(NotImplementedError,\r\n                       match=\"_run_search not implemented.\"):\r\n        # this should raise a NotImplementedError\r\n        BadSearchCV(SVC()).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _loss_grad_lbfgs(self, transformation, X, same_class_mask, sign=1.0):\r\n        \"\"\"Compute the loss and the loss gradient w.r.t. ``transformation``.\r\n\r\n        Parameters\r\n        ----------\r\n        transformation : ndarray of shape (n_components * n_features,)\r\n            The raveled linear transformation on which to compute loss and\r\n            evaluate gradient.\r\n\r\n        X : ndarray of shape (n_samples, n_features)\r\n            The training samples.\r\n\r\n        same_class_mask : ndarray of shape (n_samples, n_samples)\r\n            A mask where ``mask[i, j] == 1`` if ``X[i]`` and ``X[j]`` belong\r\n            to the same class, and ``0`` otherwise.\r\n\r\n        Returns\r\n        -------\r\n        loss : float\r\n            The loss computed for the given transformation.\r\n\r\n        gradient : ndarray of shape (n_components * n_features,)\r\n            The new (flattened) gradient of the loss.\r\n        \"\"\"\r\n\r\n        if self.n_iter_ == 0:\r\n            self.n_iter_ += 1\r\n            if self.verbose:\r\n                header_fields = ['Iteration', 'Objective Value', 'Time(s)']\r\n                header_fmt = '{:>10} {:>20} {:>10}'\r\n                header = header_fmt.format(*header_fields)\r\n                cls_name = self.__class__.__name__\r\n                print('[{}]'.format(cls_name))\r\n                print('[{}] {}\\n[{}] {}'.format(cls_name, header,\r\n                                                cls_name, '-' * len(header)))\r\n\r\n        t_funcall = time.time()\r\n\r\n        transformation = transformation.reshape(-1, X.shape[1])\r\n        X_embedded = np.dot(X, transformation.T)  # (n_samples, n_components)\r\n\r\n        # Compute softmax distances\r\n        p_ij = pairwise_distances(X_embedded, squared=True)\r\n        np.fill_diagonal(p_ij, np.inf)\r\n        p_ij = softmax(-p_ij)  # (n_samples, n_samples)\r\n\r\n        # Compute loss\r\n        masked_p_ij = p_ij * same_class_mask\r\n        p = np.sum(masked_p_ij, axis=1, keepdims=True)  # (n_samples, 1)\r\n        loss = np.sum(p)\r\n\r\n        # Compute gradient of loss w.r.t. `transform`\r\n        weighted_p_ij = masked_p_ij - p_ij * p\r\n        weighted_p_ij_sym = weighted_p_ij + weighted_p_ij.T\r\n        np.fill_diagonal(weighted_p_ij_sym, -weighted_p_ij.sum(axis=0))\r\n        gradient = 2 * X_embedded.T.dot(weighted_p_ij_sym).dot(X)\r\n        # time complexity of the gradient: O(n_components x n_samples x (\r\n        # n_samples + n_features))\r\n\r\n        if self.verbose:\r\n            t_funcall = time.time() - t_funcall\r\n            values_fmt = '[{}] {:>10} {:>20.6e} {:>10.2f}'\r\n            print(values_fmt.format(self.__class__.__name__, self.n_iter_,\r\n                                    loss, t_funcall))\r\n            sys.stdout.flush()\r\n\r\n        return sign * loss, sign * gradient.ravel()",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_random_search_bad_cv():\r\n    # Use global X, y\r\n\r\n    class BrokenKFold(KFold):\r\n        def get_n_splits(self, *args, **kw):\r\n            return 1\r\n\r\n    # create bad cv\r\n    cv = BrokenKFold(n_splits=3)\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},\r\n                               cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(ValueError,\r\n                       match='cv.split and cv.get_n_splits returned '\r\n                             'inconsistent results. Expected \\\\d+ '\r\n                             'splits, got \\\\d+'):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_neighbors_metrics(n_samples=20, n_features=3,\r\n                           n_query_pts=2, n_neighbors=5):\r\n    # Test computing the neighbors for various metrics\r\n    # create a symmetric matrix\r\n    V = rng.rand(n_features, n_features)\r\n    VI = np.dot(V, V.T)\r\n\r\n    metrics = [('euclidean', {}),\r\n               ('manhattan', {}),\r\n               ('minkowski', dict(p=1)),\r\n               ('minkowski', dict(p=2)),\r\n               ('minkowski', dict(p=3)),\r\n               ('minkowski', dict(p=np.inf)),\r\n               ('chebyshev', {}),\r\n               ('seuclidean', dict(V=rng.rand(n_features))),\r\n               ('wminkowski', dict(p=3, w=rng.rand(n_features))),\r\n               ('mahalanobis', dict(VI=VI)),\r\n               ('haversine', {})]\r\n    algorithms = ['brute', 'ball_tree', 'kd_tree']\r\n    X = rng.rand(n_samples, n_features)\r\n\r\n    test = rng.rand(n_query_pts, n_features)\r\n\r\n    for metric, metric_params in metrics:\r\n        if metric == \"wminkowski\" and sp_version >= parse_version(\"1.8.0\"):\r\n            # wminkowski will be removed in SciPy 1.8.0\r\n            continue\r\n        results = {}\r\n        p = metric_params.pop('p', 2)\r\n        for algorithm in algorithms:\r\n            # KD tree doesn't support all metrics\r\n            if (algorithm == 'kd_tree' and\r\n                    metric not in neighbors.KDTree.valid_metrics):\r\n                assert_raises(ValueError,\r\n                              neighbors.NearestNeighbors,\r\n                              algorithm=algorithm,\r\n                              metric=metric, metric_params=metric_params)\r\n                continue\r\n            neigh = neighbors.NearestNeighbors(n_neighbors=n_neighbors,\r\n                                               algorithm=algorithm,\r\n                                               metric=metric, p=p,\r\n                                               metric_params=metric_params)\r\n\r\n            # Haversine distance only accepts 2D data\r\n            feature_sl = (slice(None, 2)\r\n                          if metric == 'haversine' else slice(None))\r\n\r\n            neigh.fit(X[:, feature_sl])\r\n\r\n            # wminkoski is deprecated in SciPy 1.6.0 and removed in 1.8.0\r\n            ExceptionToAssert = None\r\n            if (metric == \"wminkowski\" and algorithm == 'brute'\r\n                    and sp_version >= parse_version(\"1.6.0\")):\r\n                ExceptionToAssert = DeprecationWarning\r\n\r\n            with pytest.warns(ExceptionToAssert):\r\n                results[algorithm] = neigh.kneighbors(test[:, feature_sl],\r\n                                                      return_distance=True)\r\n\r\n        assert_array_almost_equal(results['brute'][0], results['ball_tree'][0])\r\n        assert_array_almost_equal(results['brute'][1], results['ball_tree'][1])\r\n        if 'kd_tree' in results:\r\n            assert_array_almost_equal(results['brute'][0],\r\n                                      results['kd_tree'][0])\r\n            assert_array_almost_equal(results['brute'][1],\r\n                                      results['kd_tree'][1])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_strings_dtype():\r\n    clf = SelfTrainingClassifier(KNeighborsClassifier())\r\n    X, y = make_blobs(n_samples=30, random_state=0,\r\n                      cluster_std=0.1)\r\n    labels_multiclass = [\"one\", \"two\", \"three\"]\r\n\r\n    y_strings = np.take(labels_multiclass, y)\r\n\r\n    with pytest.raises(ValueError, match=\"dtype\"):\r\n        clf.fit(X, y_strings)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_no_unlabeled():\r\n    # Test that training on a fully labeled dataset produces the same results\r\n    # as training the classifier by itself.\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.warns(UserWarning, match=\"y contains no unlabeled samples\"):\r\n        st.fit(X_train, y_train)\r\n    assert_array_equal(knn.predict(X_test), st.predict(X_test))\r\n    # Assert that all samples were labeled in iteration 0 (since there were no\r\n    # unlabeled samples).\r\n    assert np.all(st.labeled_iter_ == 0)\r\n    assert st.termination_condition_ == \"all_labeled\"",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_warns_k_best():\r\n    st = SelfTrainingClassifier(KNeighborsClassifier(),\r\n                                criterion='k_best',\r\n                                k_best=1000)\r\n    with pytest.warns(UserWarning, match=\"k_best is larger than\"):\r\n        st.fit(X_train, y_train_missing_labels)\r\n\r\n    assert st.termination_condition_ == 'all_labeled'",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_invalid_params_selection_crit():\r\n    st = SelfTrainingClassifier(KNeighborsClassifier(),\r\n                                criterion='foo')\r\n\r\n    with pytest.raises(ValueError, match=\"criterion must be either\"):\r\n        st.fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_prefitted_throws_error():\r\n    # Test that passing a pre-fitted classifier and calling predict throws an\r\n    # error\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.raises(NotFittedError, match=\"This SelfTrainingClassifier\"\r\n                       \" instance is not fitted yet\"):\r\n        st.predict(X_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error():\r\n    # Test that it gives proper exception on deficient input\r\n    # impossible value of C\r\n    with pytest.raises(ValueError):\r\n        svm.SVC(C=-1).fit(X, Y)\r\n\r\n    # impossible value of nu\r\n    clf = svm.NuSVC(nu=0.0)\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X_sp, Y)\r\n\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X_sp, Y2)\r\n\r\n    clf = svm.SVC()\r\n    clf.fit(X_sp, Y)\r\n    assert_array_equal(clf.predict(T), true_result)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unfitted():\r\n    X = \"foo!\"  # input validation not required when SVM not fitted\r\n\r\n    clf = svm.SVC()\r\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)\r\n\r\n    clf = svm.NuSVR()\r\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pickle_version_warning_is_not_raised_with_matching_version():\r\n    iris = datasets.load_iris()\r\n    tree = DecisionTreeClassifier().fit(iris.data, iris.target)\r\n    tree_pickle = pickle.dumps(tree)\r\n    assert b\"version\" in tree_pickle\r\n    tree_restored = assert_no_warnings(pickle.loads, tree_pickle)\r\n\r\n    # test that we can predict with the restored decision tree classifier\r\n    score_of_original = tree.score(iris.data, iris.target)\r\n    score_of_restored = tree_restored.score(iris.data, iris.target)\r\n    assert score_of_original == score_of_restored",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precomputed():\r\n    # SVC with a precomputed kernel.\r\n    # We test it with a toy dataset and with iris.\r\n    clf = svm.SVC(kernel='precomputed')\r\n    # Gram matrix for train data (square matrix)\r\n    # (we use just a linear kernel)\r\n    K = np.dot(X, np.array(X).T)\r\n    clf.fit(K, Y)\r\n    # Gram matrix for test data (rectangular matrix)\r\n    KT = np.dot(T, np.array(X).T)\r\n    pred = clf.predict(KT)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(KT.T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])\r\n    assert_array_equal(clf.support_, [1, 3])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    KT = np.zeros_like(KT)\r\n    for i in range(len(T)):\r\n        for j in clf.support_:\r\n            KT[i, j] = np.dot(T[i], X[j])\r\n\r\n    pred = clf.predict(KT)\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # same as before, but using a callable function instead of the kernel\r\n    # matrix. kernel is just a linear kernel\r\n\r\n    kfunc = lambda x, y: np.dot(x, y.T)\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(np.array(X), Y)\r\n    pred = clf.predict(T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # test a precomputed kernel with the iris dataset\r\n    # and check parameters against a linear SVC\r\n    clf = svm.SVC(kernel='precomputed')\r\n    clf2 = svm.SVC(kernel='linear')\r\n    K = np.dot(iris.data, iris.data.T)\r\n    clf.fit(K, iris.target)\r\n    clf2.fit(iris.data, iris.target)\r\n    pred = clf.predict(K)\r\n    assert_array_almost_equal(clf.support_, clf2.support_)\r\n    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)\r\n    assert_array_almost_equal(clf.intercept_, clf2.intercept_)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    K = np.zeros_like(K)\r\n    for i in range(len(iris.data)):\r\n        for j in clf.support_:\r\n            K[i, j] = np.dot(iris.data[i], iris.data[j])\r\n\r\n    pred = clf.predict(K)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)\r\n\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(iris.data, iris.target)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_calibration_bad_method(data, ensemble):\r\n    # Check only \"isotonic\" and \"sigmoid\" are accepted as methods\r\n    X, y = data\r\n    clf = LinearSVC()\r\n    clf_invalid_method = CalibratedClassifierCV(\r\n        clf, method=\"foo\", ensemble=ensemble\r\n    )\r\n    with pytest.raises(ValueError):\r\n        clf_invalid_method.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_input():\r\n    # Test that it gives proper exception on deficient input\r\n    # impossible value of C\r\n    with pytest.raises(ValueError):\r\n        svm.SVC(C=-1).fit(X, Y)\r\n\r\n    # impossible value of nu\r\n    clf = svm.NuSVC(nu=0.0)\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y)\r\n\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y2)\r\n\r\n    # Test with arrays that are non-contiguous.\r\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\r\n        Xf = np.asfortranarray(X)\r\n        assert not Xf.flags['C_CONTIGUOUS']\r\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\r\n        yf = yf[:, -1]\r\n        assert not yf.flags['F_CONTIGUOUS']\r\n        assert not yf.flags['C_CONTIGUOUS']\r\n        clf.fit(Xf, yf)\r\n        assert_array_equal(clf.predict(T), true_result)\r\n\r\n    # error for precomputed kernelsx\r\n    clf = svm.SVC(kernel='precomputed')\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y)\r\n\r\n    # predict with sparse input when trained with dense\r\n    clf = svm.SVC().fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(sparse.lil_matrix(X))\r\n\r\n    Xt = np.array(X).T\r\n    clf.fit(np.dot(X, Xt), Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(X)\r\n\r\n    clf = svm.SVC()\r\n    clf.fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(Xt)",
        "labels": [
            "Matrix Multiplication API Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_lda_predict_proba(solver, n_classes):\r\n    def generate_dataset(n_samples, centers, covariances, random_state=None):\r\n        \"\"\"Generate a multivariate normal data given some centers and\r\n        covariances\"\"\"\r\n        rng = check_random_state(random_state)\r\n        X = np.vstack([rng.multivariate_normal(mean, cov,\r\n                                               size=n_samples // len(centers))\r\n                       for mean, cov in zip(centers, covariances)])\r\n        y = np.hstack([[clazz] * (n_samples // len(centers))\r\n                       for clazz in range(len(centers))])\r\n        return X, y\r\n\r\n    blob_centers = np.array([[0, 0], [-10, 40], [-30, 30]])[:n_classes]\r\n    blob_stds = np.array([[[10, 10], [10, 100]]] * len(blob_centers))\r\n    X, y = generate_dataset(\r\n        n_samples=90000, centers=blob_centers, covariances=blob_stds,\r\n        random_state=42\r\n    )\r\n    lda = LinearDiscriminantAnalysis(solver=solver, store_covariance=True,\r\n                                     shrinkage=None).fit(X, y)\r\n    # check that the empirical means and covariances are close enough to the\r\n    # one used to generate the data\r\n    assert_allclose(lda.means_, blob_centers, atol=1e-1)\r\n    assert_allclose(lda.covariance_, blob_stds[0], atol=1)\r\n\r\n    # implement the method to compute the probability given in The Elements\r\n    # of Statistical Learning (cf. p.127, Sect. 4.4.5 \"Logistic Regression\r\n    # or LDA?\")\r\n    precision = linalg.inv(blob_stds[0])\r\n    alpha_k = []\r\n    alpha_k_0 = []\r\n    for clazz in range(len(blob_centers) - 1):\r\n        alpha_k.append(\r\n            np.dot(precision,\r\n                   (blob_centers[clazz] - blob_centers[-1])[:, np.newaxis]))\r\n        alpha_k_0.append(\r\n            np.dot(- 0.5 * (blob_centers[clazz] +\r\n                            blob_centers[-1])[np.newaxis, :], alpha_k[-1]))\r\n\r\n    sample = np.array([[-22, 22]])\r\n\r\n    def discriminant_func(sample, coef, intercept, clazz):\r\n        return np.exp(intercept[clazz] + np.dot(sample, coef[clazz]))\r\n\r\n    prob = np.array([float(\r\n        discriminant_func(sample, alpha_k, alpha_k_0, clazz) /\r\n        (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                  for clazz in range(n_classes - 1)]))) for clazz in range(\r\n                      n_classes - 1)])\r\n\r\n    prob_ref = 1 - np.sum(prob)\r\n\r\n    # check the consistency of the computed probability\r\n    # all probabilities should sum to one\r\n    prob_ref_2 = float(\r\n        1 / (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                      for clazz in range(n_classes - 1)]))\r\n    )\r\n\r\n    assert prob_ref == pytest.approx(prob_ref_2)\r\n    # check that the probability of LDA are close to the theoretical\r\n    # probabilties\r\n    assert_allclose(lda.predict_proba(sample),\r\n                    np.hstack([prob, prob_ref])[np.newaxis],\r\n                    atol=1e-2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_nystroem_precomputed_kernel():\r\n    # Non-regression: test Nystroem on precomputed kernel.\r\n    # PR - 14706\r\n    rnd = np.random.RandomState(12)\r\n    X = rnd.uniform(size=(10, 4))\r\n\r\n    K = polynomial_kernel(X, degree=2, coef0=.1)\r\n    nystroem = Nystroem(kernel='precomputed', n_components=X.shape[0])\r\n    X_transformed = nystroem.fit_transform(K)\r\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\r\n\r\n    # if degree, gamma or coef0 is passed, we raise a ValueError\r\n    msg = \"Don't pass gamma, coef0 or degree to Nystroem\"\r\n    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})\r\n    for param in params:\r\n        ny = Nystroem(kernel='precomputed', n_components=X.shape[0],\r\n                      **param)\r\n        with pytest.raises(ValueError, match=msg):\r\n            ny.fit(K)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_classifier_chain_tuple_invalid_order():\r\n    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\r\n    y = [[3, 2], [2, 3], [3, 2]]\r\n    order = tuple([1, 2])\r\n\r\n    chain = ClassifierChain(RandomForestClassifier(), order=order)\r\n\r\n    with pytest.raises(ValueError, match='invalid order'):\r\n        chain.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_tree_rotate_deprecation(pyplot):\r\n    tree = DecisionTreeClassifier()\r\n    tree.fit(X, y)\r\n    # test that a warning is raised when rotate is used.\r\n    match = (r\"'rotate' has no effect and is deprecated in 0.23. \"\r\n             r\"It will be removed in 1.0 \\(renaming of 0.25\\).\")\r\n    with pytest.warns(FutureWarning, match=match):\r\n        plot_tree(tree, rotate=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_score_samples_on_pipeline_without_score_samples():\r\n    X = np.array([[1], [2]])\r\n    y = np.array([1, 2])\r\n    # Test that a pipeline does not have score_samples method when the final\r\n    # step of the pipeline does not have score_samples defined.\r\n    pipe = make_pipeline(LogisticRegression())\r\n    pipe.fit(X, y)\r\n    with pytest.raises(AttributeError,\r\n                       match=\"'LogisticRegression' object has no attribute \"\r\n                             \"'score_samples'\"):\r\n        pipe.score_samples(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pipeline_param_error():\r\n    clf = make_pipeline(LogisticRegression())\r\n    with pytest.raises(ValueError, match=\"Pipeline.fit does not accept \"\r\n                                         \"the sample_weight parameter\"):\r\n        clf.fit([[0], [0]], [0, 1], sample_weight=[1, 1])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error():\r\n    # Test that it gives proper exception on deficient input.\r\n    for name, TreeEstimator in CLF_TREES.items():\r\n        # predict before fit\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict_proba(X)\r\n\r\n        est.fit(X, y)\r\n        X2 = [[-2, -1, 1]]  # wrong feature shape for sample\r\n        with pytest.raises(ValueError):\r\n            est.predict_proba(X2)\r\n\r\n    for name, TreeEstimator in ALL_TREES.items():\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=.6).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=0.).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=3.).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_weight_fraction_leaf=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_weight_fraction_leaf=0.51).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=0.0).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=1.1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=2.5).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(max_depth=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(max_features=42).fit(X, y)\r\n        # min_impurity_split warning\r\n        with ignore_warnings(category=FutureWarning):\r\n            with pytest.raises(ValueError):\r\n                TreeEstimator(min_impurity_split=-1.0).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_impurity_decrease=-1.0).fit(X, y)\r\n\r\n        # Wrong dimensions\r\n        est = TreeEstimator()\r\n        y2 = y[:-1]\r\n        with pytest.raises(ValueError):\r\n            est.fit(X, y2)\r\n\r\n        # Test with arrays that are non-contiguous.\r\n        Xf = np.asfortranarray(X)\r\n        est = TreeEstimator()\r\n        est.fit(Xf, y)\r\n        assert_almost_equal(est.predict(T), true_result)\r\n\r\n        # predict before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict(T)\r\n\r\n        # predict on vector with different dims\r\n        est.fit(X, y)\r\n        t = np.asarray(T)\r\n        with pytest.raises(ValueError):\r\n            est.predict(t[:, 1:])\r\n\r\n        # wrong sample shape\r\n        Xt = np.array(X).T\r\n\r\n        est = TreeEstimator()\r\n        est.fit(np.dot(X, Xt), y)\r\n        with pytest.raises(ValueError):\r\n            est.predict(X)\r\n        with pytest.raises(ValueError):\r\n            est.apply(X)\r\n\r\n        clf = TreeEstimator()\r\n        clf.fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            clf.predict(Xt)\r\n        with pytest.raises(ValueError):\r\n            clf.apply(Xt)\r\n\r\n        # apply before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.apply(T)\r\n\r\n    # non positive target for Poisson splitting Criterion\r\n    est = DecisionTreeRegressor(criterion=\"poisson\")\r\n    with pytest.raises(ValueError, match=\"y is not positive.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [0, 0, 0])\r\n    with pytest.raises(ValueError, match=\"Some.*y are negative.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [5, -0.1, 2])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_prune_tree_raises_negative_ccp_alpha():\r\n    clf = DecisionTreeClassifier()\r\n    msg = \"ccp_alpha must be greater than or equal to 0\"\r\n\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.set_params(ccp_alpha=-1.0)\r\n        clf.fit(X, y)\r\n\r\n    clf.set_params(ccp_alpha=0.0)\r\n    clf.fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.set_params(ccp_alpha=-1.0)\r\n        clf._prune_tree()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=EPS**0.5, xtol=EPS**0.5, gtol=EPS**0.5, x_scale=1.0,\r\n        loss='linear', f_scale=1.0, diff_step=None, tr_solver=None,\r\n        tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(),\r\n        kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-dimensional function of n variables) and\r\n    the loss function rho(s) (a scalar function), `least_squares` finds a\r\n    local minimum of the cost function F(x)::\r\n\r\n        F(x) = 0.5 * sum(rho(f_i(x)**2), i = 1, ..., m), lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must return a 1-d array_like of shape (m,) or a scalar.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-d array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as much operations compared to '2-point' (default). The\r\n        scheme 'cs' uses complex steps, and while potentially the most\r\n        accurate, it is applicable only when `fun` correctly handles\r\n        complex inputs and can be analytically continued to the complex\r\n        plane. Method 'lm' always uses the '2-point' scheme. If callable,\r\n        it is used as ``jac(x, *args, **kwargs)`` and should return a\r\n        good approximation (or the exact value) for the Jacobian as an\r\n        array_like (np.atleast_2d is applied), a sparse matrix or a\r\n        `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float, optional\r\n        Tolerance for termination by the change of the cost function.\r\n        Default is the square root of machine epsilon. The optimization process\r\n        is stopped when ``dF < ftol * F``, and there was an adequate agreement\r\n        between a local quadratic model and the true model in the last step.\r\n    xtol : float, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is the square root of machine epsilon. The exact condition\r\n        checked depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n    gtol : float, optional\r\n        Tolerance for termination by the norm of the gradient. Default is\r\n        the square root of machine epsilon. The exact condition depends\r\n        on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust-region along j-th\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given length\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-d ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default) the solver is chosen based on type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True) which adds a regularization term to the\r\n              normal equations, which improves convergence if Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        differences. If the Jacobian has only few non-zeros in *each* row,\r\n        providing the sparsity structure will greatly speed up the computations\r\n        [Curtis]_. Should have shape (m, n). A zero entry means that a\r\n        corresponding element in the Jacobian is identically zero. If provided,\r\n        forces the use of 'lsmr' trust-region solver. If None (default) then\r\n        dense differencing will be used. Has no effect for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    `OptimizeResult` with the following fields defined:\r\n    x : ndarray, shape (n,)\r\n        Solution found.\r\n    cost : float\r\n        Value of the cost function at the solution.\r\n    fun : ndarray, shape (m,)\r\n        Vector of residuals at the solution.\r\n    jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n        Modified Jacobian matrix at the solution, in the sense that J^T J\r\n        is a Gauss-Newton approximation of the Hessian of the cost function.\r\n        The type is the same as the one used by the algorithm.\r\n    grad : ndarray, shape (m,)\r\n        Gradient of the cost function at the solution.\r\n    optimality : float\r\n        First-order optimality measure. In unconstrained problems, it is always\r\n        the uniform norm of the gradient. In constrained problems, it is the\r\n        quantity which was compared with `gtol` during iterations.\r\n    active_mask : ndarray of int, shape (n,)\r\n        Each component shows whether a corresponding constraint is active\r\n        (that is, whether a variable is at the bound):\r\n\r\n            *  0 : a constraint is not active.\r\n            * -1 : a lower bound is active.\r\n            *  1 : an upper bound is active.\r\n\r\n        Might be somewhat arbitrary for 'trf' method as it generates a sequence\r\n        of strictly feasible iterates and `active_mask` is determined within a\r\n        tolerance threshold.\r\n    nfev : int\r\n        Number of function evaluations done. Methods 'trf' and 'dogbox' do not\r\n        count function calls for numerical Jacobian approximation, as opposed\r\n        to 'lm' method.\r\n    njev : int or None\r\n        Number of Jacobian evaluations done. If numerical Jacobian\r\n        approximation is used in 'lm' method, it is set to None.\r\n    status : int\r\n        The reason for algorithm termination:\r\n\r\n            * -1 : improper input parameters status returned from MINPACK.\r\n            *  0 : the maximum number of function evaluations is exceeded.\r\n            *  1 : `gtol` termination condition is satisfied.\r\n            *  2 : `ftol` termination condition is satisfied.\r\n            *  3 : `xtol` termination condition is satisfied.\r\n            *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n    message : str\r\n        Verbal description of the termination reason.\r\n    success : bool\r\n        True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e. robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independed variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    2.4651903288156619e-30\r\n    >>> res_1.optimality\r\n    4.4408921315878507e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687161966109073e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     rnd = np.random.RandomState(random_state)\r\n    ...     error = noise * rnd.randn(t.size)\r\n    ...     outliers = rnd.randint(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And finally plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound mush be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like.\")\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = csr_matrix(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return csr_matrix(jac(x, *args, **kwargs))\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, string_types) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations: {0}, initial cost: {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_check_is_fitted():\r\n    # Check is TypeError raised when non estimator instance passed\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(ARDRegression)\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(\"SVR\")\r\n\r\n    ard = ARDRegression()\r\n    svr = SVR()\r\n\r\n    try:\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(ard)\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(svr)\r\n    except ValueError:\r\n        assert False, \"check_is_fitted failed with ValueError\"\r\n\r\n    # NotFittedError is a subclass of both ValueError and AttributeError\r\n    try:\r\n        check_is_fitted(ard, msg=\"Random message %(name)s, %(name)s\")\r\n    except ValueError as e:\r\n        assert str(e) == \"Random message ARDRegression, ARDRegression\"\r\n\r\n    try:\r\n        check_is_fitted(svr, msg=\"Another message %(name)s, %(name)s\")\r\n    except AttributeError as e:\r\n        assert str(e) == \"Another message SVR, SVR\"\r\n\r\n    ard.fit(*make_blobs())\r\n    svr.fit(*make_blobs())\r\n\r\n    assert check_is_fitted(ard) is None\r\n    assert check_is_fitted(svr) is None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fastica_simple(add_noise=False):\r\n    # Test the FastICA algorithm on very simple data.\r\n    rng = np.random.RandomState(0)\r\n    # scipy.stats uses the global RNG:\r\n    np.random.seed(0)\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\n    s2 = stats.t.rvs(1, size=n_samples)\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n    s1, s2 = s\r\n\r\n    # Mixing angle\r\n    phi = 0.6\r\n    mixing = np.array([[np.cos(phi), np.sin(phi)],\r\n                       [np.sin(phi), -np.cos(phi)]])\r\n    m = np.dot(mixing, s)\r\n\r\n    if add_noise:\r\n        m += 0.1 * rng.randn(2, 1000)\r\n\r\n    center_and_norm(m)\r\n\r\n    # function as fun arg\r\n    def g_test(x):\r\n        return x ** 3, (3 * x ** 2).mean(axis=-1)\r\n\r\n    algos = ['parallel', 'deflation']\r\n    nls = ['logcosh', 'exp', 'cube', g_test]\r\n    whitening = [True, False]\r\n    for algo, nl, whiten in itertools.product(algos, nls, whitening):\r\n        if whiten:\r\n            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo)\r\n            assert_raises(ValueError, fastica, m.T, fun=np.tanh,\r\n                          algorithm=algo)\r\n        else:\r\n            X = PCA(n_components=2, whiten=True).fit_transform(m.T)\r\n            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False)\r\n            assert_raises(ValueError, fastica, X, fun=np.tanh,\r\n                          algorithm=algo)\r\n        s_ = s_.T\r\n        # Check that the mixing model described in the docstring holds:\r\n        if whiten:\r\n            assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))\r\n\r\n        center_and_norm(s_)\r\n        s1_, s2_ = s_\r\n        # Check to see if the sources have been estimated\r\n        # in the wrong order\r\n        if abs(np.dot(s1_, s2)) > abs(np.dot(s1_, s1)):\r\n            s2_, s1_ = s_\r\n        s1_ *= np.sign(np.dot(s1_, s1))\r\n        s2_ *= np.sign(np.dot(s2_, s2))\r\n\r\n        # Check that we have estimated the original sources\r\n        if not add_noise:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=2)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=2)\r\n        else:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=1)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=1)\r\n\r\n    # Test FastICA class\r\n    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo, random_state=0)\r\n    ica = FastICA(fun=nl, algorithm=algo, random_state=0)\r\n    sources = ica.fit_transform(m.T)\r\n    assert_equal(ica.components_.shape, (2, 2))\r\n    assert_equal(sources.shape, (1000, 2))\r\n\r\n    assert_array_almost_equal(sources_fun, sources)\r\n    assert_array_almost_equal(sources, ica.transform(m.T))\r\n\r\n    assert_equal(ica.mixing_.shape, (2, 2))\r\n\r\n    for fn in [np.tanh, \"exp(-.5(x^2))\"]:\r\n        ica = FastICA(fun=fn, algorithm=algo, random_state=0)\r\n        assert_raises(ValueError, ica.fit, m.T)\r\n\r\n    assert_raises(TypeError, FastICA(fun=moves.xrange(10)).fit, m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_rfe_set_params():\r\n    generator = check_random_state(0)\r\n    iris = load_iris()\r\n    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]\r\n    y = iris.target\r\n    clf = SVC(kernel=\"linear\")\r\n    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)\r\n    y_pred = rfe.fit(X, y).predict(X)\r\n\r\n    clf = SVC()\r\n    with warnings.catch_warnings(record=True):\r\n        # estimator_params is deprecated\r\n        rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1,\r\n                  estimator_params={'kernel': 'linear'})\r\n        y_pred2 = rfe.fit(X, y).predict(X)\r\n    assert_array_equal(y_pred, y_pred2)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_staged_functions_defensive():\r\n    # test that staged_functions make defensive copies\r\n    rng = np.random.RandomState(0)\r\n    X = rng.uniform(size=(10, 3))\r\n    y = (4 * X[:, 0]).astype(np.int) + 1  # don't predict zeros\r\n    for estimator in [GradientBoostingRegressor(),\r\n                      GradientBoostingClassifier()]:\r\n        estimator.fit(X, y)\r\n        for func in ['predict', 'decision_function', 'predict_proba']:\r\n            staged_func = getattr(estimator, \"staged_\" + func, None)\r\n            if staged_func is None:\r\n                # regressor has no staged_predict_proba\r\n                continue\r\n            with warnings.catch_warnings(record=True):\r\n                staged_result = list(staged_func(X))\r\n            staged_result[1][:] = 0\r\n            assert_true(np.all(staged_result[0] != 0))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\r\n        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\r\n        jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-D real function of n real\r\n    variables) and the loss function rho(s) (a scalar function), `least_squares`\r\n    finds a local minimum of the cost function F(x)::\r\n\r\n        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\r\n        subject to lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must allocate and return a 1-D array_like of shape (m,) or a scalar.\r\n        If the argument ``x`` is complex or the function ``fun`` returns\r\n        complex residuals, it must be wrapped in a real function of real\r\n        arguments, as shown at the end of the Examples section.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-D array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as many operations as '2-point' (default). The scheme 'cs'\r\n        uses complex steps, and while potentially the most accurate, it is\r\n        applicable only when `fun` correctly handles complex inputs and\r\n        can be analytically continued to the complex plane. Method 'lm'\r\n        always uses the '2-point' scheme. If callable, it is used as\r\n        ``jac(x, *args, **kwargs)`` and should return a good approximation\r\n        (or the exact value) for the Jacobian as an array_like (np.atleast_2d\r\n        is applied), a sparse matrix (csr_matrix preferred for performance) or\r\n        a `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float or None, optional\r\n        Tolerance for termination by the change of the cost function. Default\r\n        is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\r\n        and there was an adequate agreement between a local quadratic model and\r\n        the true model in the last step.\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    xtol : float or None, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is 1e-8. The exact condition depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    gtol : float or None, optional\r\n        Tolerance for termination by the norm of the gradient. Default is 1e-8.\r\n        The exact condition depends on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n        If None and 'method' is not 'lm', the termination by this condition is\r\n        disabled. If 'method' is 'lm', this tolerance must be higher than\r\n        machine epsilon.\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust region along jth\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given size\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-D ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default), the solver is chosen based on the type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally,  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True), which adds a regularization term to the\r\n              normal equation, which improves convergence if the Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        difference estimation, its shape must be (m, n). If the Jacobian has\r\n        only few non-zero elements in *each* row, providing the sparsity\r\n        structure will greatly speed up the computations [Curtis]_. A zero\r\n        entry means that a corresponding element in the Jacobian is identically\r\n        zero. If provided, forces the use of 'lsmr' trust-region solver.\r\n        If None (default), then dense differencing will be used. Has no effect\r\n        for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    result : OptimizeResult\r\n        `OptimizeResult` with the following fields defined:\r\n\r\n            x : ndarray, shape (n,)\r\n                Solution found.\r\n            cost : float\r\n                Value of the cost function at the solution.\r\n            fun : ndarray, shape (m,)\r\n                Vector of residuals at the solution.\r\n            jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n                Modified Jacobian matrix at the solution, in the sense that J^T J\r\n                is a Gauss-Newton approximation of the Hessian of the cost function.\r\n                The type is the same as the one used by the algorithm.\r\n            grad : ndarray, shape (m,)\r\n                Gradient of the cost function at the solution.\r\n            optimality : float\r\n                First-order optimality measure. In unconstrained problems, it is\r\n                always the uniform norm of the gradient. In constrained problems,\r\n                it is the quantity which was compared with `gtol` during iterations.\r\n            active_mask : ndarray of int, shape (n,)\r\n                Each component shows whether a corresponding constraint is active\r\n                (that is, whether a variable is at the bound):\r\n\r\n                    *  0 : a constraint is not active.\r\n                    * -1 : a lower bound is active.\r\n                    *  1 : an upper bound is active.\r\n\r\n                Might be somewhat arbitrary for 'trf' method as it generates a\r\n                sequence of strictly feasible iterates and `active_mask` is\r\n                determined within a tolerance threshold.\r\n            nfev : int\r\n                Number of function evaluations done. Methods 'trf' and 'dogbox' do\r\n                not count function calls for numerical Jacobian approximation, as\r\n                opposed to 'lm' method.\r\n            njev : int or None\r\n                Number of Jacobian evaluations done. If numerical Jacobian\r\n                approximation is used in 'lm' method, it is set to None.\r\n            status : int\r\n                The reason for algorithm termination:\r\n\r\n                    * -1 : improper input parameters status returned from MINPACK.\r\n                    *  0 : the maximum number of function evaluations is exceeded.\r\n                    *  1 : `gtol` termination condition is satisfied.\r\n                    *  2 : `ftol` termination condition is satisfied.\r\n                    *  3 : `xtol` termination condition is satisfied.\r\n                    *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n            message : str\r\n                Verbal description of the termination reason.\r\n            success : bool\r\n                True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve-fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also,\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e., robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independent variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    9.8669242910846867e-30\r\n    >>> res_1.optimality\r\n    8.8928864934219529e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687069299604613e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> from numpy.random import default_rng\r\n    >>> rng = default_rng()\r\n    >>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\r\n    ...     rng = default_rng(seed)\r\n    ...\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     error = noise * rng.standard_normal(t.size)\r\n    ...     outliers = rng.integers(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And, finally, plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    In the next example, we show how complex-valued residual functions of\r\n    complex variables can be optimized with ``least_squares()``. Consider the\r\n    following function:\r\n\r\n    >>> def f(z):\r\n    ...     return z - (0.5 + 0.5j)\r\n\r\n    We wrap it into a function of real variables that returns real residuals\r\n    by simply handling the real and imaginary parts as independent variables:\r\n\r\n    >>> def f_wrap(x):\r\n    ...     fx = f(x[0] + 1j*x[1])\r\n    ...     return np.array([fx.real, fx.imag])\r\n\r\n    Thus, instead of the original m-D complex function of n complex\r\n    variables we optimize a 2m-D real function of 2n real variables:\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\r\n    >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\r\n    >>> z\r\n    (0.49999999999925893+0.49999999999925893j)\r\n\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    if np.iscomplexobj(x0):\r\n        raise ValueError(\"`x0` must be real.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound must be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol, method)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like. \"\r\n                         \"f0.shape: {0}\".format(f0.shape))\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = J0.tocsr()\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs).tocsr()\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, str) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False,\r\n              check_finite=True, bounds=(-np.inf, np.inf), method=None,\r\n              jac=None, **kwargs):\r\n    \"\"\"\r\n    Use non-linear least squares to fit a function, f, to data.\r\n\r\n    Assumes ``ydata = f(xdata, *params) + eps``.\r\n\r\n    Parameters\r\n    ----------\r\n    f : callable\r\n        The model function, f(x, ...). It must take the independent\r\n        variable as the first argument and the parameters to fit as\r\n        separate remaining arguments.\r\n    xdata : array_like or object\r\n        The independent variable where the data is measured.\r\n        Should usually be an M-length sequence or an (k,M)-shaped array for\r\n        functions with k predictors, but can actually be any object.\r\n    ydata : array_like\r\n        The dependent data, a length M array - nominally ``f(xdata, ...)``.\r\n    p0 : array_like, optional\r\n        Initial guess for the parameters (length N). If None, then the\r\n        initial values will all be 1 (if the number of parameters for the\r\n        function can be determined using introspection, otherwise a\r\n        ValueError is raised).\r\n    sigma : None or M-length sequence or MxM array, optional\r\n        Determines the uncertainty in `ydata`. If we define residuals as\r\n        ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\r\n        depends on its number of dimensions:\r\n\r\n            - A 1-D `sigma` should contain values of standard deviations of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = sum((r / sigma) ** 2)``.\r\n\r\n            - A 2-D `sigma` should contain the covariance matrix of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = r.T @ inv(sigma) @ r``.\r\n\r\n              .. versionadded:: 0.19\r\n\r\n        None (default) is equivalent of 1-D `sigma` filled with ones.\r\n    absolute_sigma : bool, optional\r\n        If True, `sigma` is used in an absolute sense and the estimated parameter\r\n        covariance `pcov` reflects these absolute values.\r\n\r\n        If False (default), only the relative magnitudes of the `sigma` values matter.\r\n        The returned parameter covariance matrix `pcov` is based on scaling\r\n        `sigma` by a constant factor. This constant is set by demanding that the\r\n        reduced `chisq` for the optimal parameters `popt` when using the\r\n        *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\r\n        match the sample variance of the residuals after the fit. Default is False.\r\n        Mathematically,\r\n        ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\r\n    check_finite : bool, optional\r\n        If True, check that the input arrays do not contain nans of infs,\r\n        and raise a ValueError if they do. Setting this parameter to\r\n        False may silently produce nonsensical results if the input arrays\r\n        do contain nans. Default is True.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on parameters. Defaults to no bounds.\r\n        Each element of the tuple must be either an array with the length equal\r\n        to the number of parameters, or a scalar (in which case the bound is\r\n        taken to be the same for all parameters). Use ``np.inf`` with an\r\n        appropriate sign to disable bounds on all or some parameters.\r\n\r\n        .. versionadded:: 0.17\r\n    method : {'lm', 'trf', 'dogbox'}, optional\r\n        Method to use for optimization. See `least_squares` for more details.\r\n        Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\r\n        provided. The method 'lm' won't work when the number of observations\r\n        is less than the number of variables, use 'trf' or 'dogbox' in this\r\n        case.\r\n\r\n        .. versionadded:: 0.17\r\n    jac : callable, string or None, optional\r\n        Function with signature ``jac(x, ...)`` which computes the Jacobian\r\n        matrix of the model function with respect to parameters as a dense\r\n        array_like structure. It will be scaled according to provided `sigma`.\r\n        If None (default), the Jacobian will be estimated numerically.\r\n        String keywords for 'trf' and 'dogbox' methods can be used to select\r\n        a finite difference scheme, see `least_squares`.\r\n\r\n        .. versionadded:: 0.18\r\n    kwargs\r\n        Keyword arguments passed to `leastsq` for ``method='lm'`` or\r\n        `least_squares` otherwise.\r\n\r\n    Returns\r\n    -------\r\n    popt : array\r\n        Optimal values for the parameters so that the sum of the squared\r\n        residuals of ``f(xdata, *popt) - ydata`` is minimized.\r\n    pcov : 2-D array\r\n        The estimated covariance of popt. The diagonals provide the variance\r\n        of the parameter estimate. To compute one standard deviation errors\r\n        on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\r\n\r\n        How the `sigma` parameter affects the estimated covariance\r\n        depends on `absolute_sigma` argument, as described above.\r\n\r\n        If the Jacobian matrix at the solution doesn't have a full rank, then\r\n        'lm' method returns a matrix filled with ``np.inf``, on the other hand\r\n        'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\r\n        the covariance matrix.\r\n\r\n    Raises\r\n    ------\r\n    ValueError\r\n        if either `ydata` or `xdata` contain NaNs, or if incompatible options\r\n        are used.\r\n\r\n    RuntimeError\r\n        if the least-squares minimization fails.\r\n\r\n    OptimizeWarning\r\n        if covariance of the parameters can not be estimated.\r\n\r\n    See Also\r\n    --------\r\n    least_squares : Minimize the sum of squares of nonlinear functions.\r\n    scipy.stats.linregress : Calculate a linear least squares regression for\r\n                             two sets of measurements.\r\n\r\n    Notes\r\n    -----\r\n    With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\r\n    through `leastsq`. Note that this algorithm can only deal with\r\n    unconstrained problems.\r\n\r\n    Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\r\n    the docstring of `least_squares` for more information.\r\n\r\n    Examples\r\n    --------\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> from scipy.optimize import curve_fit\r\n\r\n    >>> def func(x, a, b, c):\r\n    ...     return a * np.exp(-b * x) + c\r\n\r\n    Define the data to be fit with some noise:\r\n\r\n    >>> xdata = np.linspace(0, 4, 50)\r\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\r\n    >>> rng = np.random.default_rng()\r\n    >>> y_noise = 0.2 * rng.normal(size=xdata.size)\r\n    >>> ydata = y + y_noise\r\n    >>> plt.plot(xdata, ydata, 'b-', label='data')\r\n\r\n    Fit for the parameters a, b, c of the function `func`:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata)\r\n    >>> popt\r\n    array([2.56274217, 1.37268521, 0.47427475])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'r-',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    Constrain the optimization to the region of ``0 <= a <= 3``,\r\n    ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\r\n    >>> popt\r\n    array([2.43736712, 1.        , 0.34463856])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'g--',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    >>> plt.xlabel('x')\r\n    >>> plt.ylabel('y')\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    \"\"\"\r\n    if p0 is None:\r\n        # determine number of parameters by inspecting the function\r\n        sig = _getfullargspec(f)\r\n        args = sig.args\r\n        if len(args) < 2:\r\n            raise ValueError(\"Unable to determine number of fit parameters.\")\r\n        n = len(args) - 1\r\n    else:\r\n        p0 = np.atleast_1d(p0)\r\n        n = p0.size\r\n\r\n    lb, ub = prepare_bounds(bounds, n)\r\n    if p0 is None:\r\n        p0 = _initialize_feasible(lb, ub)\r\n\r\n    bounded_problem = np.any((lb > -np.inf) | (ub < np.inf))\r\n    if method is None:\r\n        if bounded_problem:\r\n            method = 'trf'\r\n        else:\r\n            method = 'lm'\r\n\r\n    if method == 'lm' and bounded_problem:\r\n        raise ValueError(\"Method 'lm' only works for unconstrained problems. \"\r\n                         \"Use 'trf' or 'dogbox' instead.\")\r\n\r\n    # optimization may produce garbage for float32 inputs, cast them to float64\r\n\r\n    # NaNs cannot be handled\r\n    if check_finite:\r\n        ydata = np.asarray_chkfinite(ydata, float)\r\n    else:\r\n        ydata = np.asarray(ydata, float)\r\n\r\n    if isinstance(xdata, (list, tuple, np.ndarray)):\r\n        # `xdata` is passed straight to the user-defined `f`, so allow\r\n        # non-array_like `xdata`.\r\n        if check_finite:\r\n            xdata = np.asarray_chkfinite(xdata, float)\r\n        else:\r\n            xdata = np.asarray(xdata, float)\r\n\r\n    if ydata.size == 0:\r\n        raise ValueError(\"`ydata` must not be empty!\")\r\n\r\n    # Determine type of sigma\r\n    if sigma is not None:\r\n        sigma = np.asarray(sigma)\r\n\r\n        # if 1-D, sigma are errors, define transform = 1/sigma\r\n        if sigma.shape == (ydata.size, ):\r\n            transform = 1.0 / sigma\r\n        # if 2-D, sigma is the covariance matrix,\r\n        # define transform = L such that L L^T = C\r\n        elif sigma.shape == (ydata.size, ydata.size):\r\n            try:\r\n                # scipy.linalg.cholesky requires lower=True to return L L^T = A\r\n                transform = cholesky(sigma, lower=True)\r\n            except LinAlgError as e:\r\n                raise ValueError(\"`sigma` must be positive definite.\") from e\r\n        else:\r\n            raise ValueError(\"`sigma` has incorrect shape.\")\r\n    else:\r\n        transform = None\r\n\r\n    func = _wrap_func(f, xdata, ydata, transform)\r\n    if callable(jac):\r\n        jac = _wrap_jac(jac, xdata, transform)\r\n    elif jac is None and method != 'lm':\r\n        jac = '2-point'\r\n\r\n    if 'args' in kwargs:\r\n        # The specification for the model function `f` does not support\r\n        # additional arguments. Refer to the `curve_fit` docstring for\r\n        # acceptable call signatures of `f`.\r\n        raise ValueError(\"'args' is not a supported keyword argument.\")\r\n\r\n    if method == 'lm':\r\n        # if ydata.size == 1, this might be used for broadcast.\r\n        if ydata.size != 1 and n > ydata.size:\r\n            raise TypeError(f\"The number of func parameters={n} must not\"\r\n                            f\" exceed the number of data points={ydata.size}\")\r\n        # Remove full_output from kwargs, otherwise we're passing it in twice.\r\n        return_full = kwargs.pop('full_output', False)\r\n        res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\r\n        popt, pcov, infodict, errmsg, ier = res\r\n        ysize = len(infodict['fvec'])\r\n        cost = np.sum(infodict['fvec'] ** 2)\r\n        if ier not in [1, 2, 3, 4]:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\r\n    else:\r\n        # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.\r\n        if 'max_nfev' not in kwargs:\r\n            kwargs['max_nfev'] = kwargs.pop('maxfev', None)\r\n\r\n        res = least_squares(func, p0, jac=jac, bounds=bounds, method=method,\r\n                            **kwargs)\r\n\r\n        if not res.success:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + res.message)\r\n\r\n        ysize = len(res.fun)\r\n        cost = 2 * res.cost  # res.cost is half sum of squares!\r\n        popt = res.x\r\n\r\n        # Do Moore-Penrose inverse discarding zero singular values.\r\n        _, s, VT = svd(res.jac, full_matrices=False)\r\n        threshold = np.finfo(float).eps * max(res.jac.shape) * s[0]\r\n        s = s[s > threshold]\r\n        VT = VT[:s.size]\r\n        pcov = np.dot(VT.T / s**2, VT)\r\n        return_full = False\r\n\r\n    warn_cov = False\r\n    if pcov is None:\r\n        # indeterminate covariance\r\n        pcov = zeros((len(popt), len(popt)), dtype=float)\r\n        pcov.fill(inf)\r\n        warn_cov = True\r\n    elif not absolute_sigma:\r\n        if ysize > p0.size:\r\n            s_sq = cost / (ysize - p0.size)\r\n            pcov = pcov * s_sq\r\n        else:\r\n            pcov.fill(inf)\r\n            warn_cov = True\r\n\r\n    if warn_cov:\r\n        warnings.warn('Covariance of the parameters could not be estimated',\r\n                      category=OptimizeWarning)\r\n\r\n    if return_full:\r\n        return popt, pcov, infodict, errmsg, ier\r\n    else:\r\n        return popt, pcov",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_affinity_propagation_predict_error():\r\n    # Test exception in AffinityPropagation.predict\r\n    # Not fitted.\r\n    af = AffinityPropagation(affinity=\"euclidean\")\r\n    with pytest.raises(ValueError):\r\n        af.predict(X)\r\n\r\n    # Predict not supported when affinity=\"precomputed\".\r\n    S = np.dot(X, X.T)\r\n    af = AffinityPropagation(affinity=\"precomputed\", random_state=57)\r\n    af.fit(S)\r\n    with pytest.raises(ValueError):\r\n        af.predict(X)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_n_clusters():\r\n    # Test that n_clusters param works properly\r\n    X, y = make_blobs(n_samples=100, centers=10)\r\n    brc1 = Birch(n_clusters=10)\r\n    brc1.fit(X)\r\n    assert len(brc1.subcluster_centers_) > 10\r\n    assert len(np.unique(brc1.labels_)) == 10\r\n\r\n    # Test that n_clusters = Agglomerative Clustering gives\r\n    # the same results.\r\n    gc = AgglomerativeClustering(n_clusters=10)\r\n    brc2 = Birch(n_clusters=gc)\r\n    brc2.fit(X)\r\n    assert_array_equal(brc1.subcluster_labels_, brc2.subcluster_labels_)\r\n    assert_array_equal(brc1.labels_, brc2.labels_)\r\n\r\n    # Test that the wrong global clustering step raises an Error.\r\n    clf = ElasticNet()\r\n    brc3 = Birch(n_clusters=clf)\r\n    with pytest.raises(ValueError):\r\n        brc3.fit(X)\r\n\r\n    # Test that a small number of clusters raises a warning.\r\n    brc4 = Birch(threshold=10000.)\r\n    assert_warns(ConvergenceWarning, brc4.fit, X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_target_regressor_error():\r\n    X, y = friedman\r\n    # provide a transformer and functions at the same time\r\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\r\n                                      transformer=StandardScaler(),\r\n                                      func=np.exp, inverse_func=np.log)\r\n    with pytest.raises(ValueError,\r\n                       match=\"'transformer' and functions\"\r\n                       \" 'func'/'inverse_func' cannot both be set.\"):\r\n        regr.fit(X, y)\r\n    # fit with sample_weight with a regressor which does not support it\r\n    sample_weight = np.ones((y.shape[0],))\r\n    regr = TransformedTargetRegressor(regressor=OrthogonalMatchingPursuit(),\r\n                                      transformer=StandardScaler())\r\n    with pytest.raises(TypeError, match=r\"fit\\(\\) got an unexpected \"\r\n                       \"keyword argument 'sample_weight'\"):\r\n        regr.fit(X, y, sample_weight=sample_weight)\r\n    # func is given but inverse_func is not\r\n    regr = TransformedTargetRegressor(func=np.exp)\r\n    with pytest.raises(ValueError, match=\"When 'func' is provided, \"\r\n                       \"'inverse_func' must also be provided\"):\r\n        regr.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_factor_analysis():\r\n    # Test FactorAnalysis ability to recover the data covariance structure\r\n    rng = np.random.RandomState(0)\r\n    n_samples, n_features, n_components = 20, 5, 3\r\n\r\n    # Some random settings for the generative model\r\n    W = rng.randn(n_components, n_features)\r\n    # latent variable of dim 3, 20 of it\r\n    h = rng.randn(n_samples, n_components)\r\n    # using gamma to model different noise variance\r\n    # per component\r\n    noise = rng.gamma(1, size=n_features) * rng.randn(n_samples, n_features)\r\n\r\n    # generate observations\r\n    # wlog, mean is 0\r\n    X = np.dot(h, W) + noise\r\n\r\n    with pytest.raises(ValueError):\r\n        FactorAnalysis(svd_method='foo')\r\n    fa_fail = FactorAnalysis()\r\n    fa_fail.svd_method = 'foo'\r\n    with pytest.raises(ValueError):\r\n        fa_fail.fit(X)\r\n    fas = []\r\n    for method in ['randomized', 'lapack']:\r\n        fa = FactorAnalysis(n_components=n_components, svd_method=method)\r\n        fa.fit(X)\r\n        fas.append(fa)\r\n\r\n        X_t = fa.transform(X)\r\n        assert X_t.shape == (n_samples, n_components)\r\n\r\n        assert_almost_equal(fa.loglike_[-1], fa.score_samples(X).sum())\r\n        assert_almost_equal(fa.score_samples(X).mean(), fa.score(X))\r\n\r\n        diff = np.all(np.diff(fa.loglike_))\r\n        assert diff > 0., 'Log likelihood dif not increase'\r\n\r\n        # Sample Covariance\r\n        scov = np.cov(X, rowvar=0., bias=1.)\r\n\r\n        # Model Covariance\r\n        mcov = fa.get_covariance()\r\n        diff = np.sum(np.abs(scov - mcov)) / W.size\r\n        assert diff < 0.1, \"Mean absolute difference is %f\" % diff\r\n        fa = FactorAnalysis(n_components=n_components,\r\n                            noise_variance_init=np.ones(n_features))\r\n        with pytest.raises(ValueError):\r\n            fa.fit(X[:, :2])\r\n\r\n    f = lambda x, y: np.abs(getattr(x, y))  # sign will not be equal\r\n    fa1, fa2 = fas\r\n    for attr in ['loglike_', 'components_', 'noise_variance_']:\r\n        assert_almost_equal(f(fa1, attr), f(fa2, attr))\r\n\r\n    fa1.max_iter = 1\r\n    fa1.verbose = True\r\n    assert_warns(ConvergenceWarning, fa1.fit, X)\r\n\r\n    # Test get_covariance and get_precision with n_components == n_features\r\n    # with n_components < n_features and with n_components == 0\r\n    for n_components in [0, 2, X.shape[1]]:\r\n        fa.n_components = n_components\r\n        fa.fit(X)\r\n        cov = fa.get_covariance()\r\n        precision = fa.get_precision()\r\n        assert_array_almost_equal(np.dot(cov, precision),\r\n                                  np.eye(X.shape[1]), 12)\r\n\r\n    # test rotation\r\n    n_components = 2\r\n\r\n    results, projections = {}, {}\r\n    for method in (None, \"varimax\", 'quartimax'):\r\n        fa_var = FactorAnalysis(n_components=n_components,\r\n                                rotation=method)\r\n        results[method] = fa_var.fit_transform(X)\r\n        projections[method] = fa_var.get_covariance()\r\n    for rot1, rot2 in combinations([None, 'varimax', 'quartimax'], 2):\r\n        assert not np.allclose(results[rot1], results[rot2])\r\n        assert np.allclose(projections[rot1], projections[rot2], atol=3)\r\n\r\n    assert_raises(ValueError,\r\n                  FactorAnalysis(rotation='not_implemented').fit_transform, X)\r\n\r\n    # test against R's psych::principal with rotate=\"varimax\"\r\n    # (i.e., the values below stem from rotating the components in R)\r\n    # R's factor analysis returns quite different values; therefore, we only\r\n    # test the rotation itself\r\n    factors = np.array(\r\n        [[0.89421016, -0.35854928, -0.27770122, 0.03773647],\r\n         [-0.45081822, -0.89132754, 0.0932195, -0.01787973],\r\n         [0.99500666, -0.02031465, 0.05426497, -0.11539407],\r\n         [0.96822861, -0.06299656, 0.24411001, 0.07540887]])\r\n    r_solution = np.array([[0.962, 0.052], [-0.141, 0.989],\r\n                           [0.949, -0.300], [0.937, -0.251]])\r\n    rotated = _ortho_rotation(factors[:, :n_components], method='varimax').T\r\n    assert_array_almost_equal(np.abs(rotated), np.abs(r_solution), decimal=3)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_fastica_simple(add_noise, seed):\r\n    # Test the FastICA algorithm on very simple data.\r\n    rng = np.random.RandomState(seed)\r\n    # scipy.stats uses the global RNG:\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\n    s2 = stats.t.rvs(1, size=n_samples)\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n    s1, s2 = s\r\n\r\n    # Mixing angle\r\n    phi = 0.6\r\n    mixing = np.array([[np.cos(phi), np.sin(phi)],\r\n                       [np.sin(phi), -np.cos(phi)]])\r\n    m = np.dot(mixing, s)\r\n\r\n    if add_noise:\r\n        m += 0.1 * rng.randn(2, 1000)\r\n\r\n    center_and_norm(m)\r\n\r\n    # function as fun arg\r\n    def g_test(x):\r\n        return x ** 3, (3 * x ** 2).mean(axis=-1)\r\n\r\n    algos = ['parallel', 'deflation']\r\n    nls = ['logcosh', 'exp', 'cube', g_test]\r\n    whitening = [True, False]\r\n    for algo, nl, whiten in itertools.product(algos, nls, whitening):\r\n        if whiten:\r\n            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo,\r\n                                      random_state=rng)\r\n            with pytest.raises(ValueError):\r\n                fastica(m.T, fun=np.tanh, algorithm=algo)\r\n        else:\r\n            pca = PCA(n_components=2, whiten=True, random_state=rng)\r\n            X = pca.fit_transform(m.T)\r\n            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False,\r\n                                      random_state=rng)\r\n            with pytest.raises(ValueError):\r\n                fastica(X, fun=np.tanh, algorithm=algo)\r\n        s_ = s_.T\r\n        # Check that the mixing model described in the docstring holds:\r\n        if whiten:\r\n            assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))\r\n\r\n        center_and_norm(s_)\r\n        s1_, s2_ = s_\r\n        # Check to see if the sources have been estimated\r\n        # in the wrong order\r\n        if abs(np.dot(s1_, s2)) > abs(np.dot(s1_, s1)):\r\n            s2_, s1_ = s_\r\n        s1_ *= np.sign(np.dot(s1_, s1))\r\n        s2_ *= np.sign(np.dot(s2_, s2))\r\n\r\n        # Check that we have estimated the original sources\r\n        if not add_noise:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=2)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=2)\r\n        else:\r\n            assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=1)\r\n            assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=1)\r\n\r\n    # Test FastICA class\r\n    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo,\r\n                                random_state=seed)\r\n    ica = FastICA(fun=nl, algorithm=algo, random_state=seed)\r\n    sources = ica.fit_transform(m.T)\r\n    assert ica.components_.shape == (2, 2)\r\n    assert sources.shape == (1000, 2)\r\n\r\n    assert_array_almost_equal(sources_fun, sources)\r\n    assert_array_almost_equal(sources, ica.transform(m.T))\r\n\r\n    assert ica.mixing_.shape == (2, 2)\r\n\r\n    for fn in [np.tanh, \"exp(-.5(x^2))\"]:\r\n        ica = FastICA(fun=fn, algorithm=algo)\r\n        with pytest.raises(ValueError):\r\n            ica.fit(m.T)\r\n\r\n    with pytest.raises(TypeError):\r\n        FastICA(fun=range(10)).fit(m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_incremental_pca_sparse(matrix_class):\r\n    # Incremental PCA on sparse arrays.\r\n    X = iris.data\r\n    pca = PCA(n_components=2)\r\n    pca.fit_transform(X)\r\n    X_sparse = matrix_class(X)\r\n    batch_size = X_sparse.shape[0] // 3\r\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\r\n\r\n    X_transformed = ipca.fit_transform(X_sparse)\r\n\r\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\r\n    np.testing.assert_allclose(ipca.explained_variance_ratio_.sum(),\r\n                               pca.explained_variance_ratio_.sum(), rtol=1e-3)\r\n\r\n    for n_components in [1, 2, X.shape[1]]:\r\n        ipca = IncrementalPCA(n_components, batch_size=batch_size)\r\n        ipca.fit(X_sparse)\r\n        cov = ipca.get_covariance()\r\n        precision = ipca.get_precision()\r\n        np.testing.assert_allclose(np.dot(cov, precision),\r\n                                   np.eye(X_sparse.shape[1]), atol=1e-13)\r\n\r\n    with pytest.raises(\r\n            TypeError,\r\n            match=\"IncrementalPCA.partial_fit does not support \"\r\n            \"sparse input. Either convert data to dense \"\r\n            \"or use IncrementalPCA.fit to do so in batches.\"):\r\n        ipca.partial_fit(X_sparse)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_type(Ensemble):\r\n    # check that ensemble will fail during validation if the underlying\r\n    # estimators are not of the same type (i.e. classifier or regressor)\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        X, y = make_classification(n_samples=10)\r\n        estimators = [('lr', LinearRegression())]\r\n        ensemble_type = 'classifier'\r\n    else:\r\n        X, y = make_regression(n_samples=10)\r\n        estimators = [('lr', LogisticRegression())]\r\n        ensemble_type = 'regressor'\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"should be a {}\".format(ensemble_type)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_name_validation(X, y, Ensemble):\r\n    # raise an error when the name contains dunder\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('lr__', LogisticRegression())]\r\n    else:\r\n        estimators = [('lr__', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Estimator names must not contain __: got \\['lr__'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name is not unique\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('lr', LogisticRegression()),\r\n                      ('lr', LogisticRegression())]\r\n    else:\r\n        estimators = [('lr', LinearRegression()),\r\n                      ('lr', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Names provided are not unique: \\['lr', 'lr'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name conflicts with the parameters\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [('estimators', LogisticRegression())]\r\n    else:\r\n        estimators = [('estimators', LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"Estimator names conflict with constructor arguments\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_all_dropped(X, y, estimator):\r\n    # check that we raise a consistent error when all estimators are\r\n    # dropped\r\n    estimator.set_params(lr='drop')\r\n    with pytest.raises(ValueError, match=\"All estimators are dropped.\"):\r\n        estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_gradient_boosting_init_wrong_methods(estimator, missing_method):\r\n    # Make sure error is raised if init estimators don't have the required\r\n    # methods (fit, predict, predict_proba)\r\n\r\n    message = (\"The init parameter must be a valid estimator and support \"\r\n               \"both fit and \" + missing_method)\r\n    with pytest.raises(ValueError, match=message):\r\n        estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_forest_y_sparse():\r\n    X = [[1, 2, 3]]\r\n    y = csr_matrix([4, 5, 6])\r\n    est = RandomForestClassifier()\r\n    msg = \"sparse multilabel-indicator for y is not supported.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        est.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_classifier_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        clf = StackingClassifier(**params, cv=3)\r\n        clf.fit(\r\n            scale(X_iris), y, sample_weight=np.ones(X_iris.shape[0])\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_regressor_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        reg = StackingRegressor(**params, cv=3)\r\n        reg.fit(\r\n            scale(X_diabetes), y, sample_weight=np.ones(X_diabetes.shape[0])\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_classifier_estimator_init(params, err_msg):\r\n    ensemble = VotingClassifier(**params)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_cv_influence(stacker, X, y):\r\n    # check that the stacking affects the fit of the final estimator but not\r\n    # the fit of the base estimators\r\n    # note: ConvergenceWarning are catch since we are not worrying about the\r\n    # convergence here\r\n    stacker_cv_3 = clone(stacker)\r\n    stacker_cv_5 = clone(stacker)\r\n\r\n    stacker_cv_3.set_params(cv=3)\r\n    stacker_cv_5.set_params(cv=5)\r\n\r\n    stacker_cv_3.fit(X, y)\r\n    stacker_cv_5.fit(X, y)\r\n\r\n    # the base estimators should be identical\r\n    for est_cv_3, est_cv_5 in zip(stacker_cv_3.estimators_,\r\n                                  stacker_cv_5.estimators_):\r\n        assert_allclose(est_cv_3.coef_, est_cv_5.coef_)\r\n\r\n    # the final estimator should be different\r\n    with pytest.raises(AssertionError, match='Not equal'):\r\n        assert_allclose(stacker_cv_3.final_estimator_.coef_,\r\n                        stacker_cv_5.final_estimator_.coef_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_none_estimator_with_weights(X, y, voter):\r\n    # check that an estimator can be set to 'drop' and passing some weight\r\n    # regression test for\r\n    # https://github.com/scikit-learn/scikit-learn/issues/13777\r\n    voter = clone(voter)\r\n    voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n    voter.set_params(lr='drop')\r\n    with pytest.warns(None) as record:\r\n        voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n    assert not record\r\n    y_pred = voter.predict(X)\r\n    assert y_pred.shape == y.shape",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboostregressor_sample_weight():\r\n    # check that giving weight will have an influence on the error computed\r\n    # for a weak learner\r\n    rng = np.random.RandomState(42)\r\n    X = np.linspace(0, 100, num=1000)\r\n    y = (.8 * X + 0.2) + (rng.rand(X.shape[0]) * 0.0001)\r\n    X = X.reshape(-1, 1)\r\n\r\n    # add an arbitrary outlier\r\n    X[-1] *= 10\r\n    y[-1] = 10000\r\n\r\n    # random_state=0 ensure that the underlying bootstrap will use the outlier\r\n    regr_no_outlier = AdaBoostRegressor(\r\n        base_estimator=LinearRegression(), n_estimators=1, random_state=0\r\n    )\r\n    regr_with_weight = clone(regr_no_outlier)\r\n    regr_with_outlier = clone(regr_no_outlier)\r\n\r\n    # fit 3 models:\r\n    # - a model containing the outlier\r\n    # - a model without the outlier\r\n    # - a model containing the outlier but with a null sample-weight\r\n    regr_with_outlier.fit(X, y)\r\n    regr_no_outlier.fit(X[:-1], y[:-1])\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = 0\r\n    regr_with_weight.fit(X, y, sample_weight=sample_weight)\r\n\r\n    score_with_outlier = regr_with_outlier.score(X[:-1], y[:-1])\r\n    score_no_outlier = regr_no_outlier.score(X[:-1], y[:-1])\r\n    score_with_weight = regr_with_weight.score(X[:-1], y[:-1])\r\n\r\n    assert score_with_outlier < score_no_outlier\r\n    assert score_with_outlier < score_with_weight\r\n    assert score_no_outlier == pytest.approx(score_with_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predictproba_hardvoting():\r\n    eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),\r\n                                        ('lr2', LogisticRegression())],\r\n                            voting='hard')\r\n    msg = \"predict_proba is not available when voting='hard'\"\r\n    with pytest.raises(AttributeError, match=msg):\r\n        eclf.predict_proba\r\n\r\n    assert not hasattr(eclf, \"predict_proba\")\r\n    eclf.fit(X, y)\r\n    assert not hasattr(eclf, \"predict_proba\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_gradient_boosting_with_init_pipeline():\r\n    # Check that the init estimator can be a pipeline (see issue #13466)\r\n\r\n    X, y = make_regression(random_state=0)\r\n    init = make_pipeline(LinearRegression())\r\n    gb = GradientBoostingRegressor(init=init)\r\n    gb.fit(X, y)  # pipeline without sample_weight works fine\r\n\r\n    with pytest.raises(\r\n            ValueError,\r\n            match='The initial estimator Pipeline does not support sample '\r\n                  'weights'):\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))\r\n\r\n    # Passing sample_weight to a pipeline raises a ValueError. This test makes\r\n    # sure we make the distinction between ValueError raised by a pipeline that\r\n    # was passed sample_weight, and a ValueError raised by a regular estimator\r\n    # whose input checking failed.\r\n    with pytest.raises(\r\n            ValueError,\r\n            match='nu <= 0 or nu > 1'):\r\n        # Note that NuSVR properly supports sample_weight\r\n        init = NuSVR(gamma='auto', nu=1.5)\r\n        gb = GradientBoostingRegressor(init=init)\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sample_weight():\r\n    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"\r\n    clf1 = LogisticRegression(random_state=123)\r\n    clf2 = RandomForestClassifier(random_state=123)\r\n    clf3 = SVC(probability=True, random_state=123)\r\n    eclf1 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\r\n        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))\r\n    eclf2 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\r\n        voting='soft').fit(X, y)\r\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\r\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\r\n\r\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))\r\n    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\r\n    eclf3.fit(X, y, sample_weight)\r\n    clf1.fit(X, y, sample_weight)\r\n    assert_array_equal(eclf3.predict(X), clf1.predict(X))\r\n    assert_array_almost_equal(eclf3.predict_proba(X), clf1.predict_proba(X))\r\n\r\n    # check that an error is raised and indicative if sample_weight is not\r\n    # supported.\r\n    clf4 = KNeighborsClassifier()\r\n    eclf3 = VotingClassifier(estimators=[\r\n        ('lr', clf1), ('svc', clf3), ('knn', clf4)],\r\n        voting='soft')\r\n    msg = ('Underlying estimator KNeighborsClassifier does not support '\r\n           'sample weights.')\r\n    with pytest.raises(TypeError, match=msg):\r\n        eclf3.fit(X, y, sample_weight)\r\n\r\n    # check that _fit_single_estimator will raise the right error\r\n    # it should raise the original error if this is not linked to sample_weight\r\n    class ClassifierErrorFit(ClassifierMixin, BaseEstimator):\r\n        def fit(self, X, y, sample_weight):\r\n            raise TypeError('Error unrelated to sample_weight.')\r\n    clf = ClassifierErrorFit()\r\n    with pytest.raises(TypeError, match='Error unrelated to sample_weight'):\r\n        clf.fit(X, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_verbose(estimator, capsys):\r\n\r\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\r\n    y = np.array([1, 1, 2, 2])\r\n\r\n    pattern = (r'\\[Voting\\].*\\(1 of 2\\) Processing lr, total=.*\\n'\r\n               r'\\[Voting\\].*\\(2 of 2\\) Processing rf, total=.*\\n$')\r\n\r\n    estimator.fit(X, y)\r\n    assert re.match(pattern, capsys.readouterr()[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboost_negative_weight_error(model, X, y):\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = -10\r\n\r\n    err_msg = \"sample_weight cannot contain negative weight\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        model.fit(X, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_direction():\r\n    X, y = make_regression(n_features=5)\r\n    sfs = SequentialFeatureSelector(LinearRegression(), direction='bad')\r\n    with pytest.raises(ValueError, match=\"must be either 'forward' or\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_n_features_to_select(n_features_to_select):\r\n    X, y = make_regression(n_features=5)\r\n    sfs = SequentialFeatureSelector(LinearRegression(),\r\n                                    n_features_to_select=n_features_to_select)\r\n    with pytest.raises(ValueError, match=\"must be either None\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nan_support():\r\n    # Make sure nans are OK if the underlying estimator supports nans\r\n\r\n    rng = np.random.RandomState(0)\r\n    n_samples, n_features = 100, 10\r\n    X, y = make_regression(n_samples, n_features, random_state=0)\r\n    nan_mask = rng.randint(0, 2, size=(n_samples, n_features), dtype=bool)\r\n    X[nan_mask] = np.nan\r\n    sfs = SequentialFeatureSelector(HistGradientBoostingRegressor(), cv=2)\r\n    sfs.fit(X, y)\r\n    sfs.transform(X)\r\n\r\n    with pytest.raises(ValueError, match='Input contains NaN'):\r\n        # LinearRegression does not support nans\r\n        SequentialFeatureSelector(LinearRegression(), cv=2).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparse_support():\r\n    # Make sure sparse data is supported\r\n\r\n    X, y = make_regression(n_features=10)\r\n    X = scipy.sparse.csr_matrix(X)\r\n    sfs = SequentialFeatureSelector(LinearRegression(), cv=2)\r\n    sfs.fit(X, y)\r\n    sfs.transform(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_error(estimator, params, err_msg):\r\n    X, y = make_classification(random_state=0)\r\n    estimator.fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_string(estimator):\r\n    pd = pytest.importorskip(\"pandas\")\r\n    X, y = make_classification(random_state=0)\r\n    df = pd.DataFrame(X)\r\n    estimator.fit(df, y)\r\n\r\n    features = ['random']\r\n    err_msg = 'A given column is not a column of the dataframe'\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, df, features)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_warning_for_kind_legacy():\r\n    est = LogisticRegression()\r\n    (X, y), n_targets = binary_classification_data\r\n    est.fit(X, y)\r\n\r\n    err_msg = (\"A Bunch will be returned in place of 'predictions' from \"\r\n               \"version 1.1\")\r\n    with pytest.warns(FutureWarning, match=err_msg):\r\n        partial_dependence(est, X=X, features=[1, 2])\r\n\r\n    with pytest.warns(FutureWarning, match=err_msg):\r\n        partial_dependence(est, X=X, features=[1, 2], kind='legacy')",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_indices(estimator, features):\r\n    X, y = make_classification(random_state=0)\r\n    estimator.fit(X, y)\r\n\r\n    err_msg = 'all features must be in'\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, [features])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_slice_error(with_dataframe, err_msg):\r\n    X, y = make_classification(random_state=0)\r\n    if with_dataframe:\r\n        pd = pytest.importorskip('pandas')\r\n        X = pd.DataFrame(X)\r\n    estimator = LogisticRegression().fit(X, y)\r\n\r\n    with pytest.raises(TypeError, match=err_msg):\r\n        partial_dependence(estimator, X, features=slice(0, 2, 1))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_permutation_importance_no_weights_scoring_function():\r\n    # Creating a scorer function that does not takes sample_weight\r\n    def my_scorer(estimator, X, y):\r\n        return 1\r\n\r\n    # Creating some data and estimator for the permutation test\r\n    x = np.array([[1, 2], [3, 4]])\r\n    y = np.array([1, 2])\r\n    w = np.array([1, 1])\r\n    lr = LinearRegression()\r\n    lr.fit(x, y)\r\n\r\n    # test that permutation_importance does not return error when\r\n    # sample_weight is None\r\n    try:\r\n        permutation_importance(lr, x, y, random_state=1,\r\n                               scoring=my_scorer,\r\n                               n_repeats=1)\r\n    except TypeError:\r\n        pytest.fail(\"permutation_test raised an error when using a scorer \"\r\n                    \"function that does not accept sample_weight even though \"\r\n                    \"sample_weight was None\")\r\n\r\n    # test that permutation_importance raise exception when sample_weight is\r\n    # not None\r\n    with pytest.raises(TypeError):\r\n        permutation_importance(lr, x, y, random_state=1,\r\n                               scoring=my_scorer,\r\n                               n_repeats=1,\r\n                               sample_weight=w)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_partial_dependence_error(pyplot, data, params, err_msg):\r\n    X, y = data\r\n    estimator = LinearRegression().fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        plot_partial_dependence(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit(self, X, y, sample_weight=None):\r\n        \"\"\"Fit estimator using RANSAC algorithm.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like or sparse matrix, shape [n_samples, n_features]\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\r\n            Target values.\r\n\r\n        sample_weight : array-like of shape (n_samples,), default=None\r\n            Individual weights for each sample\r\n            raises error if sample_weight is passed and base_estimator\r\n            fit method does not support it.\r\n\r\n            .. versionadded:: 0.18\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If no valid consensus set could be found. This occurs if\r\n            `is_data_valid` and `is_model_valid` return False for all\r\n            `max_trials` randomly chosen sub-samples.\r\n\r\n        \"\"\"\r\n        # Need to validate separately here.\r\n        # We can't pass multi_ouput=True because that would allow y to be csr.\r\n        check_X_params = dict(accept_sparse='csr')\r\n        check_y_params = dict(ensure_2d=False)\r\n        X, y = self._validate_data(X, y, validate_separately=(check_X_params,\r\n                                                              check_y_params))\r\n        check_consistent_length(X, y)\r\n\r\n        if self.base_estimator is not None:\r\n            base_estimator = clone(self.base_estimator)\r\n        else:\r\n            base_estimator = LinearRegression()\r\n\r\n        if self.min_samples is None:\r\n            # assume linear model by default\r\n            min_samples = X.shape[1] + 1\r\n        elif 0 < self.min_samples < 1:\r\n            min_samples = np.ceil(self.min_samples * X.shape[0])\r\n        elif self.min_samples >= 1:\r\n            if self.min_samples % 1 != 0:\r\n                raise ValueError(\"Absolute number of samples must be an \"\r\n                                 \"integer value.\")\r\n            min_samples = self.min_samples\r\n        else:\r\n            raise ValueError(\"Value for `min_samples` must be scalar and \"\r\n                             \"positive.\")\r\n        if min_samples > X.shape[0]:\r\n            raise ValueError(\"`min_samples` may not be larger than number \"\r\n                             \"of samples: n_samples = %d.\" % (X.shape[0]))\r\n\r\n        if self.stop_probability < 0 or self.stop_probability > 1:\r\n            raise ValueError(\"`stop_probability` must be in range [0, 1].\")\r\n\r\n        if self.residual_threshold is None:\r\n            # MAD (median absolute deviation)\r\n            residual_threshold = np.median(np.abs(y - np.median(y)))\r\n        else:\r\n            residual_threshold = self.residual_threshold\r\n\r\n        if self.loss == \"absolute_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)\r\n\r\n        elif self.loss == \"squared_loss\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2\r\n            else:\r\n                loss_function = lambda \\\r\n                    y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)\r\n\r\n        elif callable(self.loss):\r\n            loss_function = self.loss\r\n\r\n        else:\r\n            raise ValueError(\r\n                \"loss should be 'absolute_loss', 'squared_loss' or a callable.\"\r\n                \"Got %s. \" % self.loss)\r\n\r\n\r\n        random_state = check_random_state(self.random_state)\r\n\r\n        try:  # Not all estimator accept a random_state\r\n            base_estimator.set_params(random_state=random_state)\r\n        except ValueError:\r\n            pass\r\n\r\n        estimator_fit_has_sample_weight = has_fit_parameter(base_estimator,\r\n                                                            \"sample_weight\")\r\n        estimator_name = type(base_estimator).__name__\r\n        if (sample_weight is not None and not\r\n                estimator_fit_has_sample_weight):\r\n            raise ValueError(\"%s does not support sample_weight. Samples\"\r\n                             \" weights are only used for the calibration\"\r\n                             \" itself.\" % estimator_name)\r\n        if sample_weight is not None:\r\n            sample_weight = _check_sample_weight(sample_weight, X)\r\n\r\n        n_inliers_best = 1\r\n        score_best = -np.inf\r\n        inlier_mask_best = None\r\n        X_inlier_best = None\r\n        y_inlier_best = None\r\n        inlier_best_idxs_subset = None\r\n        self.n_skips_no_inliers_ = 0\r\n        self.n_skips_invalid_data_ = 0\r\n        self.n_skips_invalid_model_ = 0\r\n\r\n        # number of data samples\r\n        n_samples = X.shape[0]\r\n        sample_idxs = np.arange(n_samples)\r\n\r\n        self.n_trials_ = 0\r\n        max_trials = self.max_trials\r\n        while self.n_trials_ < max_trials:\r\n            self.n_trials_ += 1\r\n\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                break\r\n\r\n            # choose random sample set\r\n            subset_idxs = sample_without_replacement(n_samples, min_samples,\r\n                                                     random_state=random_state)\r\n            X_subset = X[subset_idxs]\r\n            y_subset = y[subset_idxs]\r\n\r\n            # check if random sample set is valid\r\n            if (self.is_data_valid is not None\r\n                    and not self.is_data_valid(X_subset, y_subset)):\r\n                self.n_skips_invalid_data_ += 1\r\n                continue\r\n\r\n            # fit model for current random sample set\r\n            if sample_weight is None:\r\n                base_estimator.fit(X_subset, y_subset)\r\n            else:\r\n                base_estimator.fit(X_subset, y_subset,\r\n                                   sample_weight=sample_weight[subset_idxs])\r\n\r\n            # check if estimated model is valid\r\n            if (self.is_model_valid is not None and not\r\n                    self.is_model_valid(base_estimator, X_subset, y_subset)):\r\n                self.n_skips_invalid_model_ += 1\r\n                continue\r\n\r\n            # residuals of all data for current random sample model\r\n            y_pred = base_estimator.predict(X)\r\n            residuals_subset = loss_function(y, y_pred)\r\n\r\n            # classify data into inliers and outliers\r\n            inlier_mask_subset = residuals_subset < residual_threshold\r\n            n_inliers_subset = np.sum(inlier_mask_subset)\r\n\r\n            # less inliers -> skip current random sample\r\n            if n_inliers_subset < n_inliers_best:\r\n                self.n_skips_no_inliers_ += 1\r\n                continue\r\n\r\n            # extract inlier data set\r\n            inlier_idxs_subset = sample_idxs[inlier_mask_subset]\r\n            X_inlier_subset = X[inlier_idxs_subset]\r\n            y_inlier_subset = y[inlier_idxs_subset]\r\n\r\n            # score of inlier data set\r\n            score_subset = base_estimator.score(X_inlier_subset,\r\n                                                y_inlier_subset)\r\n\r\n            # same number of inliers but worse score -> skip current random\r\n            # sample\r\n            if (n_inliers_subset == n_inliers_best\r\n                    and score_subset < score_best):\r\n                continue\r\n\r\n            # save current random sample as best sample\r\n            n_inliers_best = n_inliers_subset\r\n            score_best = score_subset\r\n            inlier_mask_best = inlier_mask_subset\r\n            X_inlier_best = X_inlier_subset\r\n            y_inlier_best = y_inlier_subset\r\n            inlier_best_idxs_subset = inlier_idxs_subset\r\n\r\n            max_trials = min(\r\n                max_trials,\r\n                _dynamic_max_trials(n_inliers_best, n_samples,\r\n                                    min_samples, self.stop_probability))\r\n\r\n            # break if sufficient number of inliers or score is reached\r\n            if n_inliers_best >= self.stop_n_inliers or \\\r\n                            score_best >= self.stop_score:\r\n                break\r\n\r\n        # if none of the iterations met the required criteria\r\n        if inlier_mask_best is None:\r\n            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips):\r\n                raise ValueError(\r\n                    \"RANSAC skipped more iterations than `max_skips` without\"\r\n                    \" finding a valid consensus set. Iterations were skipped\"\r\n                    \" because each randomly chosen sub-sample failed the\"\r\n                    \" passing criteria. See estimator attributes for\"\r\n                    \" diagnostics (n_skips*).\")\r\n            else:\r\n                raise ValueError(\r\n                    \"RANSAC could not find a valid consensus set. All\"\r\n                    \" `max_trials` iterations were skipped because each\"\r\n                    \" randomly chosen sub-sample failed the passing criteria.\"\r\n                    \" See estimator attributes for diagnostics (n_skips*).\")\r\n        else:\r\n            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\r\n                    self.n_skips_invalid_model_) > self.max_skips:\r\n                warnings.warn(\"RANSAC found a valid consensus set but exited\"\r\n                              \" early due to skipping more iterations than\"\r\n                              \" `max_skips`. See estimator attributes for\"\r\n                              \" diagnostics (n_skips*).\",\r\n                              ConvergenceWarning)\r\n\r\n        # estimate final model using all inliers\r\n        if sample_weight is None:\r\n            base_estimator.fit(X_inlier_best, y_inlier_best)\r\n        else:\r\n            base_estimator.fit(\r\n                X_inlier_best,\r\n                y_inlier_best,\r\n                sample_weight=sample_weight[inlier_best_idxs_subset])\r\n\r\n        self.estimator_ = base_estimator\r\n        self.inlier_mask_ = inlier_mask_best\r\n        return self",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit(self, X, y):\r\n        \"\"\"Fit the model using X, y as training data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,)\r\n            Target values.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            returns an instance of self.\r\n        \"\"\"\r\n        X, y = self._validate_data(X, y, y_numeric=True)\r\n        X = as_float_array(X, copy=self.copy_X)\r\n        y = as_float_array(y, copy=self.copy_X)\r\n\r\n        # init cross-validation generator\r\n        cv = check_cv(self.cv, classifier=False)\r\n\r\n        # As we use cross-validation, the Gram matrix is not precomputed here\r\n        Gram = self.precompute\r\n        if hasattr(Gram, '__array__'):\r\n            warnings.warn('Parameter \"precompute\" cannot be an array in '\r\n                          '%s. Automatically switch to \"auto\" instead.'\r\n                          % self.__class__.__name__)\r\n            Gram = 'auto'\r\n\r\n        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\r\n            delayed(_lars_path_residues)(\r\n                X[train], y[train], X[test], y[test], Gram=Gram, copy=False,\r\n                method=self.method, verbose=max(0, self.verbose - 1),\r\n                normalize=self.normalize, fit_intercept=self.fit_intercept,\r\n                max_iter=self.max_iter, eps=self.eps, positive=self.positive)\r\n            for train, test in cv.split(X, y))\r\n        all_alphas = np.concatenate(list(zip(*cv_paths))[0])\r\n        # Unique also sorts\r\n        all_alphas = np.unique(all_alphas)\r\n        # Take at most max_n_alphas values\r\n        stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))\r\n        all_alphas = all_alphas[::stride]\r\n\r\n        mse_path = np.empty((len(all_alphas), len(cv_paths)))\r\n        for index, (alphas, _, _, residues) in enumerate(cv_paths):\r\n            alphas = alphas[::-1]\r\n            residues = residues[::-1]\r\n            if alphas[0] != 0:\r\n                alphas = np.r_[0, alphas]\r\n                residues = np.r_[residues[0, np.newaxis], residues]\r\n            if alphas[-1] != all_alphas[-1]:\r\n                alphas = np.r_[alphas, all_alphas[-1]]\r\n                residues = np.r_[residues, residues[-1, np.newaxis]]\r\n            this_residues = interpolate.interp1d(alphas,\r\n                                                 residues,\r\n                                                 axis=0)(all_alphas)\r\n            this_residues **= 2\r\n            mse_path[:, index] = np.mean(this_residues, axis=-1)\r\n\r\n        mask = np.all(np.isfinite(mse_path), axis=-1)\r\n        all_alphas = all_alphas[mask]\r\n        mse_path = mse_path[mask]\r\n        # Select the alpha that minimizes left-out error\r\n        i_best_alpha = np.argmin(mse_path.mean(axis=-1))\r\n        best_alpha = all_alphas[i_best_alpha]\r\n\r\n        # Store our parameters\r\n        self.alpha_ = best_alpha\r\n        self.cv_alphas_ = all_alphas\r\n        self.mse_path_ = mse_path\r\n\r\n        # Now compute the full model\r\n        # it will call a lasso internally when self if LassoLarsCV\r\n        # as self.method == 'lasso'\r\n        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,\r\n                  Xy=None, fit_path=True)\r\n        return self",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_linear_regression_pd_sparse_dataframe_warning():\r\n    pd = pytest.importorskip('pandas')\r\n    # restrict the pd versions < '0.24.0' as they have a bug in is_sparse func\r\n    if parse_version(pd.__version__) < parse_version('0.24.0'):\r\n        pytest.skip(\"pandas 0.24+ required.\")\r\n\r\n    # Warning is raised only when some of the columns is sparse\r\n    df = pd.DataFrame({'0': np.random.randn(10)})\r\n    for col in range(1, 4):\r\n        arr = np.random.randn(10)\r\n        arr[:8] = 0\r\n        # all columns but the first column is sparse\r\n        if col != 0:\r\n            arr = pd.arrays.SparseArray(arr, fill_value=0)\r\n        df[str(col)] = arr\r\n\r\n    msg = \"pandas.DataFrame with sparse columns found.\"\r\n    with pytest.warns(UserWarning, match=msg):\r\n        reg = LinearRegression()\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])\r\n\r\n    # does not warn when the whole dataframe is sparse\r\n    df['0'] = pd.arrays.SparseArray(df['0'], fill_value=0)\r\n    assert hasattr(df, \"sparse\")\r\n\r\n    with pytest.warns(None) as record:\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])\r\n    assert not record",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_enet_sample_weight_sparse():\r\n    reg = ElasticNet()\r\n    X = sparse.csc_matrix(np.zeros((3, 2)))\r\n    y = np.array([-1, 0, 1])\r\n    sw = np.array([1, 2, 3])\r\n    with pytest.raises(ValueError, match=\"Sample weights do not.*support \"\r\n                                         \"sparse matrices\"):\r\n        reg.fit(X, y, sample_weight=sw, check_input=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_scorer_select_proba_error(scorer):\r\n    # check that we raise the the proper error when passing an unknown\r\n    # pos_label\r\n    X, y = make_classification(\r\n        n_classes=2, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n    assert scorer._kwargs[\"pos_label\"] not in np.unique(y).tolist()\r\n\r\n    err_msg = \"is not a valid label\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        scorer(lr, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error_on_regressor(pyplot, data):\r\n    X, y = data\r\n    est = SVR().fit(X, y)\r\n\r\n    msg = \"plot_confusion_matrix only supports classifiers\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_confusion_matrix(est, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_raises_on_score_list():\r\n    # Test that when a list of scores is returned, we raise proper errors.\r\n    X, y = make_blobs(random_state=0)\r\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\r\n    clf = DecisionTreeClassifier()\r\n    with pytest.raises(ValueError):\r\n        cross_val_score(clf, X, y, scoring=f1_scorer_no_average)\r\n    grid_search = GridSearchCV(clf, scoring=f1_scorer_no_average,\r\n                               param_grid={'max_depth': [1, 2]})\r\n    with pytest.raises(ValueError):\r\n        grid_search.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_confusion_matrix_pipeline(pyplot, clf, data, n_classes):\r\n    X, y = data\r\n    with pytest.raises(NotFittedError):\r\n        plot_confusion_matrix(clf, X, y)\r\n    clf.fit(X, y)\r\n    y_pred = clf.predict(X)\r\n\r\n    disp = plot_confusion_matrix(clf, X, y)\r\n    cm = confusion_matrix(y, y_pred)\r\n\r\n    assert_allclose(disp.confusion_matrix, cm)\r\n    assert disp.text_.shape == (n_classes, n_classes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_thresholded_scorers():\r\n    # Test scorers that take thresholds.\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = LogisticRegression(random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\r\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n    assert_almost_equal(score1, score3)\r\n\r\n    logscore = get_scorer('neg_log_loss')(clf, X_test, y_test)\r\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\r\n    assert_almost_equal(-logscore, logloss)\r\n\r\n    # same for an estimator without decision_function\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n\r\n    # test with a regressor (no decision_function)\r\n    reg = DecisionTreeRegressor()\r\n    reg.fit(X_train, y_train)\r\n    score1 = get_scorer('roc_auc')(reg, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, reg.predict(X_test))\r\n    assert_almost_equal(score1, score2)\r\n\r\n    # Test that an exception is raised on more than two classes\r\n    X, y = make_blobs(random_state=0, centers=3)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    with pytest.raises(ValueError, match=\"multiclass format is not supported\"):\r\n        get_scorer('roc_auc')(clf, X_test, y_test)\r\n\r\n    # test error is raised with a single class present in model\r\n    # (predict_proba shape is not suitable for binary auc)\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, np.zeros_like(y_train))\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer('roc_auc')(clf, X_test, y_test)\r\n\r\n    # for proba scorers\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer('neg_log_loss')(clf, X_test, y_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_curve_error_non_binary(pyplot, data, plot_func):\r\n    X, y = data\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X, y)\r\n\r\n    msg = \"DecisionTreeClassifier should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_func(clf, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_det_curve_not_fitted_errors(pyplot, data_binary, clf, plot_func):\r\n    X, y = data_binary\r\n    # clone since we parametrize the test and the classifier will be fitted\r\n    # when testing the second and subsequent plotting function\r\n    model = clone(clf)\r\n    with pytest.raises(NotFittedError):\r\n        plot_func(model, X, y)\r\n    model.fit(X, y)\r\n    disp = plot_func(model, X, y)\r\n    assert model.__class__.__name__ in disp.line_.get_label()\r\n    assert disp.estimator_name == model.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_curve_string_labels(pyplot):\r\n    # regression test #15738\r\n    cancer = load_breast_cancer()\r\n    X = cancer.data\r\n    y = cancer.target_names[cancer.target]\r\n\r\n    lr = make_pipeline(StandardScaler(), LogisticRegression())\r\n    lr.fit(X, y)\r\n    for klass in cancer.target_names:\r\n        assert klass in lr.classes_\r\n    disp = plot_precision_recall_curve(lr, X, y)\r\n\r\n    y_pred = lr.predict_proba(X)[:, 1]\r\n    avg_prec = average_precision_score(y, y_pred,\r\n                                       pos_label=lr.classes_[1])\r\n\r\n    assert disp.average_precision == pytest.approx(avg_prec)\r\n    assert disp.estimator_name == lr.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_roc_curve_not_fitted_errors(pyplot, data_binary, clf):\r\n    X, y = data_binary\r\n    with pytest.raises(NotFittedError):\r\n        plot_roc_curve(clf, X, y)\r\n    clf.fit(X, y)\r\n    disp = plot_roc_curve(clf, X, y)\r\n    assert clf.__class__.__name__ in disp.line_.get_label()\r\n    assert disp.estimator_name == clf.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_errors(pyplot):\r\n    X, y_multiclass = make_classification(n_classes=3, n_samples=50,\r\n                                          n_informative=3,\r\n                                          random_state=0)\r\n    y_binary = y_multiclass == 0\r\n\r\n    # Unfitted classifer\r\n    binary_clf = DecisionTreeClassifier()\r\n    with pytest.raises(NotFittedError):\r\n        plot_precision_recall_curve(binary_clf, X, y_binary)\r\n    binary_clf.fit(X, y_binary)\r\n\r\n    multi_clf = DecisionTreeClassifier().fit(X, y_multiclass)\r\n\r\n    # Fitted multiclass classifier with binary data\r\n    msg = \"DecisionTreeClassifier should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_precision_recall_curve(multi_clf, X, y_binary)\r\n\r\n    reg = DecisionTreeRegressor().fit(X, y_multiclass)\r\n    msg = \"DecisionTreeRegressor should be a binary classifier\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        plot_precision_recall_curve(reg, X, y_binary)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_curve_pipeline(pyplot, clf):\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n    with pytest.raises(NotFittedError):\r\n        plot_precision_recall_curve(clf, X, y)\r\n    clf.fit(X, y)\r\n    disp = plot_precision_recall_curve(clf, X, y)\r\n    assert disp.estimator_name == clf.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_roc_curve_pos_label(pyplot, response_method):\r\n    # check that we can provide the positive label and display the proper\r\n    # statistics\r\n    X, y = load_breast_cancer(return_X_y=True)\r\n    # create an highly imbalanced\r\n    idx_positive = np.flatnonzero(y == 1)\r\n    idx_negative = np.flatnonzero(y == 0)\r\n    idx_selected = np.hstack([idx_negative, idx_positive[:25]])\r\n    X, y = X[idx_selected], y[idx_selected]\r\n    X, y = shuffle(X, y, random_state=42)\r\n    # only use 2 features to make the problem even harder\r\n    X = X[:, :2]\r\n    y = np.array(\r\n        [\"cancer\" if c == 1 else \"not cancer\" for c in y], dtype=object\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, stratify=y, random_state=0,\r\n    )\r\n\r\n    classifier = LogisticRegression()\r\n    classifier.fit(X_train, y_train)\r\n\r\n    # sanity check to be sure the positive class is classes_[0] and that we\r\n    # are betrayed by the class imbalance\r\n    assert classifier.classes_.tolist() == [\"cancer\", \"not cancer\"]\r\n\r\n    disp = plot_roc_curve(\r\n        classifier, X_test, y_test, pos_label=\"cancer\",\r\n        response_method=response_method\r\n    )\r\n\r\n    roc_auc_limit = 0.95679\r\n\r\n    assert disp.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert np.trapz(disp.tpr, disp.fpr) == pytest.approx(roc_auc_limit)\r\n\r\n    disp = plot_roc_curve(\r\n        classifier, X_test, y_test,\r\n        response_method=response_method,\r\n    )\r\n\r\n    assert disp.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert np.trapz(disp.tpr, disp.fpr) == pytest.approx(roc_auc_limit)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_precision_recall(pyplot, response_method, with_sample_weight):\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n\r\n    lr = LogisticRegression().fit(X, y)\r\n\r\n    if with_sample_weight:\r\n        rng = np.random.RandomState(42)\r\n        sample_weight = rng.randint(0, 4, size=X.shape[0])\r\n    else:\r\n        sample_weight = None\r\n\r\n    disp = plot_precision_recall_curve(lr, X, y, alpha=0.8,\r\n                                       response_method=response_method,\r\n                                       sample_weight=sample_weight)\r\n\r\n    y_score = getattr(lr, response_method)(X)\r\n    if response_method == 'predict_proba':\r\n        y_score = y_score[:, 1]\r\n\r\n    prec, recall, _ = precision_recall_curve(y, y_score,\r\n                                             sample_weight=sample_weight)\r\n    avg_prec = average_precision_score(y, y_score, sample_weight=sample_weight)\r\n\r\n    assert_allclose(disp.precision, prec)\r\n    assert_allclose(disp.recall, recall)\r\n    assert disp.average_precision == pytest.approx(avg_prec)\r\n\r\n    assert disp.estimator_name == \"LogisticRegression\"\r\n\r\n    # cannot fail thanks to pyplot fixture\r\n    import matplotlib as mpl  # noqa\r\n    assert isinstance(disp.line_, mpl.lines.Line2D)\r\n    assert disp.line_.get_alpha() == 0.8\r\n    assert isinstance(disp.ax_, mpl.axes.Axes)\r\n    assert isinstance(disp.figure_, mpl.figure.Figure)\r\n\r\n    expected_label = \"LogisticRegression (AP = {:0.2f})\".format(avg_prec)\r\n    assert disp.line_.get_label() == expected_label\r\n    assert disp.ax_.get_xlabel() == \"Recall (Positive label: 1)\"\r\n    assert disp.ax_.get_ylabel() == \"Precision (Positive label: 1)\"\r\n\r\n    # draw again with another label\r\n    disp.plot(name=\"MySpecialEstimator\")\r\n    expected_label = \"MySpecialEstimator (AP = {:0.2f})\".format(avg_prec)\r\n    assert disp.line_.get_label() == expected_label",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_searchcv_raise_warning_with_non_finite_score(\r\n        SearchCV, specialized_params, return_train_score):\r\n    # Non-regression test for:\r\n    # https://github.com/scikit-learn/scikit-learn/issues/10529\r\n    # Check that we raise a UserWarning when a non-finite score is\r\n    # computed in the SearchCV\r\n    X, y = make_classification(n_classes=2, random_state=0)\r\n\r\n    class FailingScorer:\r\n        \"\"\"Scorer that will fail for some split but not all.\"\"\"\r\n\r\n        def __init__(self):\r\n            self.n_counts = 0\r\n\r\n        def __call__(self, estimator, X, y):\r\n            self.n_counts += 1\r\n            if self.n_counts % 5 == 0:\r\n                return np.nan\r\n            return 1\r\n\r\n    grid = SearchCV(\r\n        DecisionTreeClassifier(),\r\n        scoring=FailingScorer(),\r\n        cv=3,\r\n        return_train_score=return_train_score,\r\n        **specialized_params\r\n    )\r\n\r\n    with pytest.warns(UserWarning) as warn_msg:\r\n        grid.fit(X, y)\r\n\r\n    set_with_warning = [\"test\", \"train\"] if return_train_score else [\"test\"]\r\n    assert len(warn_msg) == len(set_with_warning)\r\n    for msg, dataset in zip(warn_msg, set_with_warning):\r\n        assert (f\"One or more of the {dataset} scores are non-finite\" in\r\n                str(msg.message))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_cv_score_samples_error(search_cv):\r\n    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)\r\n    search_cv.fit(X, y)\r\n\r\n    # Make sure to error out when underlying estimator does not implement\r\n    # the method `score_samples`\r\n    err_msg = (\"'DecisionTreeClassifier' object has no attribute \"\r\n               \"'score_samples'\")\r\n\r\n    with pytest.raises(AttributeError, match=err_msg):\r\n        search_cv.score_samples(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_random_search_bad_cv():\r\n    # Use global X, y\r\n\r\n    class BrokenKFold(KFold):\r\n        def get_n_splits(self, *args, **kw):\r\n            return 1\r\n\r\n    # create bad cv\r\n    cv = BrokenKFold(n_splits=3)\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},\r\n                               cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(ValueError,\r\n                       match='cv.split and cv.get_n_splits returned '\r\n                             'inconsistent results. Expected \\\\d+ '\r\n                             'splits, got \\\\d+'):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_default_iid(SearchCV, specialized_params):\r\n    # Test the IID parameter  TODO: Clearly this test does something else???\r\n    # noise-free simple 2d-data\r\n    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,\r\n                      cluster_std=0.1, shuffle=False, n_samples=80)\r\n    # split dataset into two folds that are not iid\r\n    # first one contains data of all 4 blobs, second only from two.\r\n    mask = np.ones(X.shape[0], dtype=bool)\r\n    mask[np.where(y == 1)[0][::2]] = 0\r\n    mask[np.where(y == 2)[0][::2]] = 0\r\n    # this leads to perfect classification on one fold and a score of 1/3 on\r\n    # the other\r\n    # create \"cv\" for splits\r\n    cv = [[mask, ~mask], [~mask, mask]]\r\n\r\n    common_params = {'estimator': SVC(), 'cv': cv,\r\n                     'return_train_score': True}\r\n    search = SearchCV(**common_params, **specialized_params)\r\n    search.fit(X, y)\r\n\r\n    test_cv_scores = np.array(\r\n        [search.cv_results_['split%d_test_score' % s][0]\r\n         for s in range(search.n_splits_)]\r\n    )\r\n    test_mean = search.cv_results_['mean_test_score'][0]\r\n    test_std = search.cv_results_['std_test_score'][0]\r\n\r\n    train_cv_scores = np.array(\r\n        [search.cv_results_['split%d_train_score' % s][0]\r\n         for s in range(search.n_splits_)]\r\n    )\r\n    train_mean = search.cv_results_['mean_train_score'][0]\r\n    train_std = search.cv_results_['std_train_score'][0]\r\n\r\n    assert search.cv_results_['param_C'][0] == 1\r\n    # scores are the same as above\r\n    assert_allclose(test_cv_scores, [1, 1. / 3.])\r\n    assert_allclose(train_cv_scores, [1, 1])\r\n    # Unweighted mean/std is used\r\n    assert test_mean == pytest.approx(np.mean(test_cv_scores))\r\n    assert test_std == pytest.approx(np.std(test_cv_scores))\r\n\r\n    # For the train scores, we do not take a weighted mean irrespective of\r\n    # i.i.d. or not\r\n    assert train_mean == pytest.approx(1)\r\n    assert train_std == pytest.approx(0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_empty_cv_iterator_error():\r\n    # Use global X, y\r\n\r\n    # create cv\r\n    cv = KFold(n_splits=3).split(X)\r\n\r\n    # pop all of it, this should cause the expected ValueError\r\n    [u for u in cv]\r\n    # cv is empty now\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},\r\n                               cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(ValueError,\r\n                       match='No fits were performed. '\r\n                             'Was the CV iterator empty\\\\? '\r\n                             'Were there no candidates\\\\?'):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test__custom_fit_no_run_search():\r\n    class NoRunSearchSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n        def fit(self, X, y=None, groups=None, **fit_params):\r\n            return self\r\n\r\n    # this should not raise any exceptions\r\n    NoRunSearchSearchCV(SVC()).fit(X, y)\r\n\r\n    class BadSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n    with pytest.raises(NotImplementedError,\r\n                       match=\"_run_search not implemented.\"):\r\n        # this should raise a NotImplementedError\r\n        BadSearchCV(SVC()).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _loss_grad_lbfgs(self, transformation, X, same_class_mask, sign=1.0):\r\n        \"\"\"Compute the loss and the loss gradient w.r.t. ``transformation``.\r\n\r\n        Parameters\r\n        ----------\r\n        transformation : ndarray of shape (n_components * n_features,)\r\n            The raveled linear transformation on which to compute loss and\r\n            evaluate gradient.\r\n\r\n        X : ndarray of shape (n_samples, n_features)\r\n            The training samples.\r\n\r\n        same_class_mask : ndarray of shape (n_samples, n_samples)\r\n            A mask where ``mask[i, j] == 1`` if ``X[i]`` and ``X[j]`` belong\r\n            to the same class, and ``0`` otherwise.\r\n\r\n        Returns\r\n        -------\r\n        loss : float\r\n            The loss computed for the given transformation.\r\n\r\n        gradient : ndarray of shape (n_components * n_features,)\r\n            The new (flattened) gradient of the loss.\r\n        \"\"\"\r\n\r\n        if self.n_iter_ == 0:\r\n            self.n_iter_ += 1\r\n            if self.verbose:\r\n                header_fields = ['Iteration', 'Objective Value', 'Time(s)']\r\n                header_fmt = '{:>10} {:>20} {:>10}'\r\n                header = header_fmt.format(*header_fields)\r\n                cls_name = self.__class__.__name__\r\n                print('[{}]'.format(cls_name))\r\n                print('[{}] {}\\n[{}] {}'.format(cls_name, header,\r\n                                                cls_name, '-' * len(header)))\r\n\r\n        t_funcall = time.time()\r\n\r\n        transformation = transformation.reshape(-1, X.shape[1])\r\n        X_embedded = np.dot(X, transformation.T)  # (n_samples, n_components)\r\n\r\n        # Compute softmax distances\r\n        p_ij = pairwise_distances(X_embedded, squared=True)\r\n        np.fill_diagonal(p_ij, np.inf)\r\n        p_ij = softmax(-p_ij)  # (n_samples, n_samples)\r\n\r\n        # Compute loss\r\n        masked_p_ij = p_ij * same_class_mask\r\n        p = np.sum(masked_p_ij, axis=1, keepdims=True)  # (n_samples, 1)\r\n        loss = np.sum(p)\r\n\r\n        # Compute gradient of loss w.r.t. `transform`\r\n        weighted_p_ij = masked_p_ij - p_ij * p\r\n        weighted_p_ij_sym = weighted_p_ij + weighted_p_ij.T\r\n        np.fill_diagonal(weighted_p_ij_sym, -weighted_p_ij.sum(axis=0))\r\n        gradient = 2 * X_embedded.T.dot(weighted_p_ij_sym).dot(X)\r\n        # time complexity of the gradient: O(n_components x n_samples x (\r\n        # n_samples + n_features))\r\n\r\n        if self.verbose:\r\n            t_funcall = time.time() - t_funcall\r\n            values_fmt = '[{}] {:>10} {:>20.6e} {:>10.2f}'\r\n            print(values_fmt.format(self.__class__.__name__, self.n_iter_,\r\n                                    loss, t_funcall))\r\n            sys.stdout.flush()\r\n\r\n        return sign * loss, sign * gradient.ravel()",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_neighbors_metrics(n_samples=20, n_features=3,\r\n                           n_query_pts=2, n_neighbors=5):\r\n    # Test computing the neighbors for various metrics\r\n    # create a symmetric matrix\r\n    V = rng.rand(n_features, n_features)\r\n    VI = np.dot(V, V.T)\r\n\r\n    metrics = [('euclidean', {}),\r\n               ('manhattan', {}),\r\n               ('minkowski', dict(p=1)),\r\n               ('minkowski', dict(p=2)),\r\n               ('minkowski', dict(p=3)),\r\n               ('minkowski', dict(p=np.inf)),\r\n               ('chebyshev', {}),\r\n               ('seuclidean', dict(V=rng.rand(n_features))),\r\n               ('wminkowski', dict(p=3, w=rng.rand(n_features))),\r\n               ('mahalanobis', dict(VI=VI)),\r\n               ('haversine', {})]\r\n    algorithms = ['brute', 'ball_tree', 'kd_tree']\r\n    X = rng.rand(n_samples, n_features)\r\n\r\n    test = rng.rand(n_query_pts, n_features)\r\n\r\n    for metric, metric_params in metrics:\r\n        if metric == \"wminkowski\" and sp_version >= parse_version(\"1.8.0\"):\r\n            # wminkowski will be removed in SciPy 1.8.0\r\n            continue\r\n        results = {}\r\n        p = metric_params.pop('p', 2)\r\n        for algorithm in algorithms:\r\n            # KD tree doesn't support all metrics\r\n            if (algorithm == 'kd_tree' and\r\n                    metric not in neighbors.KDTree.valid_metrics):\r\n                assert_raises(ValueError,\r\n                              neighbors.NearestNeighbors,\r\n                              algorithm=algorithm,\r\n                              metric=metric, metric_params=metric_params)\r\n                continue\r\n            neigh = neighbors.NearestNeighbors(n_neighbors=n_neighbors,\r\n                                               algorithm=algorithm,\r\n                                               metric=metric, p=p,\r\n                                               metric_params=metric_params)\r\n\r\n            # Haversine distance only accepts 2D data\r\n            feature_sl = (slice(None, 2)\r\n                          if metric == 'haversine' else slice(None))\r\n\r\n            neigh.fit(X[:, feature_sl])\r\n\r\n            # wminkoski is deprecated in SciPy 1.6.0 and removed in 1.8.0\r\n            ExceptionToAssert = None\r\n            if (metric == \"wminkowski\" and algorithm == 'brute'\r\n                    and sp_version >= parse_version(\"1.6.0\")):\r\n                ExceptionToAssert = DeprecationWarning\r\n\r\n            with pytest.warns(ExceptionToAssert):\r\n                results[algorithm] = neigh.kneighbors(test[:, feature_sl],\r\n                                                      return_distance=True)\r\n\r\n        assert_array_almost_equal(results['brute'][0], results['ball_tree'][0])\r\n        assert_array_almost_equal(results['brute'][1], results['ball_tree'][1])\r\n        if 'kd_tree' in results:\r\n            assert_array_almost_equal(results['brute'][0],\r\n                                      results['kd_tree'][0])\r\n            assert_array_almost_equal(results['brute'][1],\r\n                                      results['kd_tree'][1])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_no_unlabeled():\r\n    # Test that training on a fully labeled dataset produces the same results\r\n    # as training the classifier by itself.\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.warns(UserWarning, match=\"y contains no unlabeled samples\"):\r\n        st.fit(X_train, y_train)\r\n    assert_array_equal(knn.predict(X_test), st.predict(X_test))\r\n    # Assert that all samples were labeled in iteration 0 (since there were no\r\n    # unlabeled samples).\r\n    assert np.all(st.labeled_iter_ == 0)\r\n    assert st.termination_condition_ == \"all_labeled\"",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_strings_dtype():\r\n    clf = SelfTrainingClassifier(KNeighborsClassifier())\r\n    X, y = make_blobs(n_samples=30, random_state=0,\r\n                      cluster_std=0.1)\r\n    labels_multiclass = [\"one\", \"two\", \"three\"]\r\n\r\n    y_strings = np.take(labels_multiclass, y)\r\n\r\n    with pytest.raises(ValueError, match=\"dtype\"):\r\n        clf.fit(X, y_strings)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_prefitted_throws_error():\r\n    # Test that passing a pre-fitted classifier and calling predict throws an\r\n    # error\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.raises(NotFittedError, match=\"This SelfTrainingClassifier\"\r\n                       \" instance is not fitted yet\"):\r\n        st.predict(X_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_warns_k_best():\r\n    st = SelfTrainingClassifier(KNeighborsClassifier(),\r\n                                criterion='k_best',\r\n                                k_best=1000)\r\n    with pytest.warns(UserWarning, match=\"k_best is larger than\"):\r\n        st.fit(X_train, y_train_missing_labels)\r\n\r\n    assert st.termination_condition_ == 'all_labeled'",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_invalid_params_selection_crit():\r\n    st = SelfTrainingClassifier(KNeighborsClassifier(),\r\n                                criterion='foo')\r\n\r\n    with pytest.raises(ValueError, match=\"criterion must be either\"):\r\n        st.fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error():\r\n    # Test that it gives proper exception on deficient input\r\n    # impossible value of C\r\n    with pytest.raises(ValueError):\r\n        svm.SVC(C=-1).fit(X, Y)\r\n\r\n    # impossible value of nu\r\n    clf = svm.NuSVC(nu=0.0)\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X_sp, Y)\r\n\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X_sp, Y2)\r\n\r\n    clf = svm.SVC()\r\n    clf.fit(X_sp, Y)\r\n    assert_array_equal(clf.predict(T), true_result)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precomputed():\r\n    # SVC with a precomputed kernel.\r\n    # We test it with a toy dataset and with iris.\r\n    clf = svm.SVC(kernel='precomputed')\r\n    # Gram matrix for train data (square matrix)\r\n    # (we use just a linear kernel)\r\n    K = np.dot(X, np.array(X).T)\r\n    clf.fit(K, Y)\r\n    # Gram matrix for test data (rectangular matrix)\r\n    KT = np.dot(T, np.array(X).T)\r\n    pred = clf.predict(KT)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(KT.T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])\r\n    assert_array_equal(clf.support_, [1, 3])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    KT = np.zeros_like(KT)\r\n    for i in range(len(T)):\r\n        for j in clf.support_:\r\n            KT[i, j] = np.dot(T[i], X[j])\r\n\r\n    pred = clf.predict(KT)\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # same as before, but using a callable function instead of the kernel\r\n    # matrix. kernel is just a linear kernel\r\n\r\n    kfunc = lambda x, y: np.dot(x, y.T)\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(np.array(X), Y)\r\n    pred = clf.predict(T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # test a precomputed kernel with the iris dataset\r\n    # and check parameters against a linear SVC\r\n    clf = svm.SVC(kernel='precomputed')\r\n    clf2 = svm.SVC(kernel='linear')\r\n    K = np.dot(iris.data, iris.data.T)\r\n    clf.fit(K, iris.target)\r\n    clf2.fit(iris.data, iris.target)\r\n    pred = clf.predict(K)\r\n    assert_array_almost_equal(clf.support_, clf2.support_)\r\n    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)\r\n    assert_array_almost_equal(clf.intercept_, clf2.intercept_)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    K = np.zeros_like(K)\r\n    for i in range(len(iris.data)):\r\n        for j in clf.support_:\r\n            K[i, j] = np.dot(iris.data[i], iris.data[j])\r\n\r\n    pred = clf.predict(K)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)\r\n\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(iris.data, iris.target)\r\n    assert_almost_equal(np.mean(pred == iris.target), .99, decimal=2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_unfitted():\r\n    X = \"foo!\"  # input validation not required when SVM not fitted\r\n\r\n    clf = svm.SVC()\r\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)\r\n\r\n    clf = svm.NuSVR()\r\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_input():\r\n    # Test that it gives proper exception on deficient input\r\n    # impossible value of C\r\n    with pytest.raises(ValueError):\r\n        svm.SVC(C=-1).fit(X, Y)\r\n\r\n    # impossible value of nu\r\n    clf = svm.NuSVC(nu=0.0)\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y)\r\n\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y2)\r\n\r\n    # Test with arrays that are non-contiguous.\r\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\r\n        Xf = np.asfortranarray(X)\r\n        assert not Xf.flags['C_CONTIGUOUS']\r\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\r\n        yf = yf[:, -1]\r\n        assert not yf.flags['F_CONTIGUOUS']\r\n        assert not yf.flags['C_CONTIGUOUS']\r\n        clf.fit(Xf, yf)\r\n        assert_array_equal(clf.predict(T), true_result)\r\n\r\n    # error for precomputed kernelsx\r\n    clf = svm.SVC(kernel='precomputed')\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y)\r\n\r\n    # predict with sparse input when trained with dense\r\n    clf = svm.SVC().fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(sparse.lil_matrix(X))\r\n\r\n    Xt = np.array(X).T\r\n    clf.fit(np.dot(X, Xt), Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(X)\r\n\r\n    clf = svm.SVC()\r\n    clf.fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(Xt)",
        "labels": [
            "Matrix Multiplication API Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_calibration_bad_method(data, ensemble):\r\n    # Check only \"isotonic\" and \"sigmoid\" are accepted as methods\r\n    X, y = data\r\n    clf = LinearSVC()\r\n    clf_invalid_method = CalibratedClassifierCV(\r\n        clf, method=\"foo\", ensemble=ensemble\r\n    )\r\n    with pytest.raises(ValueError):\r\n        clf_invalid_method.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pickle_version_warning_is_not_raised_with_matching_version():\r\n    iris = datasets.load_iris()\r\n    tree = DecisionTreeClassifier().fit(iris.data, iris.target)\r\n    tree_pickle = pickle.dumps(tree)\r\n    assert b\"version\" in tree_pickle\r\n    tree_restored = assert_no_warnings(pickle.loads, tree_pickle)\r\n\r\n    # test that we can predict with the restored decision tree classifier\r\n    score_of_original = tree.score(iris.data, iris.target)\r\n    score_of_restored = tree_restored.score(iris.data, iris.target)\r\n    assert score_of_original == score_of_restored",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_lda_predict_proba(solver, n_classes):\r\n    def generate_dataset(n_samples, centers, covariances, random_state=None):\r\n        \"\"\"Generate a multivariate normal data given some centers and\r\n        covariances\"\"\"\r\n        rng = check_random_state(random_state)\r\n        X = np.vstack([rng.multivariate_normal(mean, cov,\r\n                                               size=n_samples // len(centers))\r\n                       for mean, cov in zip(centers, covariances)])\r\n        y = np.hstack([[clazz] * (n_samples // len(centers))\r\n                       for clazz in range(len(centers))])\r\n        return X, y\r\n\r\n    blob_centers = np.array([[0, 0], [-10, 40], [-30, 30]])[:n_classes]\r\n    blob_stds = np.array([[[10, 10], [10, 100]]] * len(blob_centers))\r\n    X, y = generate_dataset(\r\n        n_samples=90000, centers=blob_centers, covariances=blob_stds,\r\n        random_state=42\r\n    )\r\n    lda = LinearDiscriminantAnalysis(solver=solver, store_covariance=True,\r\n                                     shrinkage=None).fit(X, y)\r\n    # check that the empirical means and covariances are close enough to the\r\n    # one used to generate the data\r\n    assert_allclose(lda.means_, blob_centers, atol=1e-1)\r\n    assert_allclose(lda.covariance_, blob_stds[0], atol=1)\r\n\r\n    # implement the method to compute the probability given in The Elements\r\n    # of Statistical Learning (cf. p.127, Sect. 4.4.5 \"Logistic Regression\r\n    # or LDA?\")\r\n    precision = linalg.inv(blob_stds[0])\r\n    alpha_k = []\r\n    alpha_k_0 = []\r\n    for clazz in range(len(blob_centers) - 1):\r\n        alpha_k.append(\r\n            np.dot(precision,\r\n                   (blob_centers[clazz] - blob_centers[-1])[:, np.newaxis]))\r\n        alpha_k_0.append(\r\n            np.dot(- 0.5 * (blob_centers[clazz] +\r\n                            blob_centers[-1])[np.newaxis, :], alpha_k[-1]))\r\n\r\n    sample = np.array([[-22, 22]])\r\n\r\n    def discriminant_func(sample, coef, intercept, clazz):\r\n        return np.exp(intercept[clazz] + np.dot(sample, coef[clazz]))\r\n\r\n    prob = np.array([float(\r\n        discriminant_func(sample, alpha_k, alpha_k_0, clazz) /\r\n        (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                  for clazz in range(n_classes - 1)]))) for clazz in range(\r\n                      n_classes - 1)])\r\n\r\n    prob_ref = 1 - np.sum(prob)\r\n\r\n    # check the consistency of the computed probability\r\n    # all probabilities should sum to one\r\n    prob_ref_2 = float(\r\n        1 / (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                      for clazz in range(n_classes - 1)]))\r\n    )\r\n\r\n    assert prob_ref == pytest.approx(prob_ref_2)\r\n    # check that the probability of LDA are close to the theoretical\r\n    # probabilties\r\n    assert_allclose(lda.predict_proba(sample),\r\n                    np.hstack([prob, prob_ref])[np.newaxis],\r\n                    atol=1e-2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_nystroem_precomputed_kernel():\r\n    # Non-regression: test Nystroem on precomputed kernel.\r\n    # PR - 14706\r\n    rnd = np.random.RandomState(12)\r\n    X = rnd.uniform(size=(10, 4))\r\n\r\n    K = polynomial_kernel(X, degree=2, coef0=.1)\r\n    nystroem = Nystroem(kernel='precomputed', n_components=X.shape[0])\r\n    X_transformed = nystroem.fit_transform(K)\r\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\r\n\r\n    # if degree, gamma or coef0 is passed, we raise a ValueError\r\n    msg = \"Don't pass gamma, coef0 or degree to Nystroem\"\r\n    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})\r\n    for param in params:\r\n        ny = Nystroem(kernel='precomputed', n_components=X.shape[0],\r\n                      **param)\r\n        with pytest.raises(ValueError, match=msg):\r\n            ny.fit(K)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_classifier_chain_tuple_invalid_order():\r\n    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\r\n    y = [[3, 2], [2, 3], [3, 2]]\r\n    order = tuple([1, 2])\r\n\r\n    chain = ClassifierChain(RandomForestClassifier(), order=order)\r\n\r\n    with pytest.raises(ValueError, match='invalid order'):\r\n        chain.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_tree_rotate_deprecation(pyplot):\r\n    tree = DecisionTreeClassifier()\r\n    tree.fit(X, y)\r\n    # test that a warning is raised when rotate is used.\r\n    match = (r\"'rotate' has no effect and is deprecated in 0.23. \"\r\n             r\"It will be removed in 1.0 \\(renaming of 0.25\\).\")\r\n    with pytest.warns(FutureWarning, match=match):\r\n        plot_tree(tree, rotate=True)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_score_samples_on_pipeline_without_score_samples():\r\n    X = np.array([[1], [2]])\r\n    y = np.array([1, 2])\r\n    # Test that a pipeline does not have score_samples method when the final\r\n    # step of the pipeline does not have score_samples defined.\r\n    pipe = make_pipeline(LogisticRegression())\r\n    pipe.fit(X, y)\r\n    with pytest.raises(AttributeError,\r\n                       match=\"'LogisticRegression' object has no attribute \"\r\n                             \"'score_samples'\"):\r\n        pipe.score_samples(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pipeline_param_error():\r\n    clf = make_pipeline(LogisticRegression())\r\n    with pytest.raises(ValueError, match=\"Pipeline.fit does not accept \"\r\n                                         \"the sample_weight parameter\"):\r\n        clf.fit([[0], [0]], [0, 1], sample_weight=[1, 1])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error():\r\n    # Test that it gives proper exception on deficient input.\r\n    for name, TreeEstimator in CLF_TREES.items():\r\n        # predict before fit\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict_proba(X)\r\n\r\n        est.fit(X, y)\r\n        X2 = [[-2, -1, 1]]  # wrong feature shape for sample\r\n        with pytest.raises(ValueError):\r\n            est.predict_proba(X2)\r\n\r\n    for name, TreeEstimator in ALL_TREES.items():\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=.6).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=0.).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_leaf=3.).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_weight_fraction_leaf=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_weight_fraction_leaf=0.51).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=0.0).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=1.1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_samples_split=2.5).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(max_depth=-1).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(max_features=42).fit(X, y)\r\n        # min_impurity_split warning\r\n        with ignore_warnings(category=FutureWarning):\r\n            with pytest.raises(ValueError):\r\n                TreeEstimator(min_impurity_split=-1.0).fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            TreeEstimator(min_impurity_decrease=-1.0).fit(X, y)\r\n\r\n        # Wrong dimensions\r\n        est = TreeEstimator()\r\n        y2 = y[:-1]\r\n        with pytest.raises(ValueError):\r\n            est.fit(X, y2)\r\n\r\n        # Test with arrays that are non-contiguous.\r\n        Xf = np.asfortranarray(X)\r\n        est = TreeEstimator()\r\n        est.fit(Xf, y)\r\n        assert_almost_equal(est.predict(T), true_result)\r\n\r\n        # predict before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict(T)\r\n\r\n        # predict on vector with different dims\r\n        est.fit(X, y)\r\n        t = np.asarray(T)\r\n        with pytest.raises(ValueError):\r\n            est.predict(t[:, 1:])\r\n\r\n        # wrong sample shape\r\n        Xt = np.array(X).T\r\n\r\n        est = TreeEstimator()\r\n        est.fit(np.dot(X, Xt), y)\r\n        with pytest.raises(ValueError):\r\n            est.predict(X)\r\n        with pytest.raises(ValueError):\r\n            est.apply(X)\r\n\r\n        clf = TreeEstimator()\r\n        clf.fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            clf.predict(Xt)\r\n        with pytest.raises(ValueError):\r\n            clf.apply(Xt)\r\n\r\n        # apply before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.apply(T)\r\n\r\n    # non positive target for Poisson splitting Criterion\r\n    est = DecisionTreeRegressor(criterion=\"poisson\")\r\n    with pytest.raises(ValueError, match=\"y is not positive.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [0, 0, 0])\r\n    with pytest.raises(ValueError, match=\"Some.*y are negative.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [5, -0.1, 2])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_prune_tree_raises_negative_ccp_alpha():\r\n    clf = DecisionTreeClassifier()\r\n    msg = \"ccp_alpha must be greater than or equal to 0\"\r\n\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.set_params(ccp_alpha=-1.0)\r\n        clf.fit(X, y)\r\n\r\n    clf.set_params(ccp_alpha=0.0)\r\n    clf.fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.set_params(ccp_alpha=-1.0)\r\n        clf._prune_tree()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_is_fitted():\r\n    # Check is TypeError raised when non estimator instance passed\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(ARDRegression)\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(\"SVR\")\r\n\r\n    ard = ARDRegression()\r\n    svr = SVR()\r\n\r\n    try:\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(ard)\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(svr)\r\n    except ValueError:\r\n        assert False, \"check_is_fitted failed with ValueError\"\r\n\r\n    # NotFittedError is a subclass of both ValueError and AttributeError\r\n    try:\r\n        check_is_fitted(ard, msg=\"Random message %(name)s, %(name)s\")\r\n    except ValueError as e:\r\n        assert str(e) == \"Random message ARDRegression, ARDRegression\"\r\n\r\n    try:\r\n        check_is_fitted(svr, msg=\"Another message %(name)s, %(name)s\")\r\n    except AttributeError as e:\r\n        assert str(e) == \"Another message SVR, SVR\"\r\n\r\n    ard.fit(*make_blobs())\r\n    svr.fit(*make_blobs())\r\n\r\n    assert check_is_fitted(ard) is None\r\n    assert check_is_fitted(svr) is None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def least_squares(\r\n        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\r\n        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\r\n        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\r\n        jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={}):\r\n    \"\"\"Solve a nonlinear least-squares problem with bounds on the variables.\r\n\r\n    Given the residuals f(x) (an m-dimensional real function of n real\r\n    variables) and the loss function rho(s) (a scalar function), `least_squares`\r\n    finds a local minimum of the cost function F(x)::\r\n\r\n        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\r\n        subject to lb <= x <= ub\r\n\r\n    The purpose of the loss function rho(s) is to reduce the influence of\r\n    outliers on the solution.\r\n\r\n    Parameters\r\n    ----------\r\n    fun : callable\r\n        Function which computes the vector of residuals, with the signature\r\n        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\r\n        respect to its first argument. The argument ``x`` passed to this\r\n        function is an ndarray of shape (n,) (never a scalar, even for n=1).\r\n        It must return a 1-d array_like of shape (m,) or a scalar. If the\r\n        argument ``x`` is complex or the function ``fun`` returns complex\r\n        residuals, it must be wrapped in a real function of real arguments,\r\n        as shown at the end of the Examples section.\r\n    x0 : array_like with shape (n,) or float\r\n        Initial guess on independent variables. If float, it will be treated\r\n        as a 1-d array with one element.\r\n    jac : {'2-point', '3-point', 'cs', callable}, optional\r\n        Method of computing the Jacobian matrix (an m-by-n matrix, where\r\n        element (i, j) is the partial derivative of f[i] with respect to\r\n        x[j]). The keywords select a finite difference scheme for numerical\r\n        estimation. The scheme '3-point' is more accurate, but requires\r\n        twice as much operations compared to '2-point' (default). The\r\n        scheme 'cs' uses complex steps, and while potentially the most\r\n        accurate, it is applicable only when `fun` correctly handles\r\n        complex inputs and can be analytically continued to the complex\r\n        plane. Method 'lm' always uses the '2-point' scheme. If callable,\r\n        it is used as ``jac(x, *args, **kwargs)`` and should return a\r\n        good approximation (or the exact value) for the Jacobian as an\r\n        array_like (np.atleast_2d is applied), a sparse matrix or a\r\n        `scipy.sparse.linalg.LinearOperator`.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each array must match the size of `x0` or be a scalar, in the latter\r\n        case a bound will be the same for all variables. Use ``np.inf`` with\r\n        an appropriate sign to disable bounds on all or some variables.\r\n    method : {'trf', 'dogbox', 'lm'}, optional\r\n        Algorithm to perform minimization.\r\n\r\n            * 'trf' : Trust Region Reflective algorithm, particularly suitable\r\n              for large sparse problems with bounds. Generally robust method.\r\n            * 'dogbox' : dogleg algorithm with rectangular trust regions,\r\n              typical use case is small problems with bounds. Not recommended\r\n              for problems with rank-deficient Jacobian.\r\n            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\r\n              Doesn't handle bounds and sparse Jacobians. Usually the most\r\n              efficient method for small unconstrained problems.\r\n\r\n        Default is 'trf'. See Notes for more information.\r\n    ftol : float, optional\r\n        Tolerance for termination by the change of the cost function. Default\r\n        is 1e-8. The optimization process is stopped when  ``dF < ftol * F``,\r\n        and there was an adequate agreement between a local quadratic model and\r\n        the true model in the last step.\r\n    xtol : float, optional\r\n        Tolerance for termination by the change of the independent variables.\r\n        Default is 1e-8. The exact condition depends on the `method` used:\r\n\r\n            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\r\n            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\r\n              a trust-region radius and ``xs`` is the value of ``x``\r\n              scaled according to `x_scale` parameter (see below).\r\n\r\n    gtol : float, optional\r\n        Tolerance for termination by the norm of the gradient. Default is 1e-8.\r\n        The exact condition depends on a `method` used:\r\n\r\n            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\r\n              ``g_scaled`` is the value of the gradient scaled to account for\r\n              the presence of the bounds [STIR]_.\r\n            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\r\n              ``g_free`` is the gradient with respect to the variables which\r\n              are not in the optimal state on the boundary.\r\n            * For 'lm' : the maximum absolute value of the cosine of angles\r\n              between columns of the Jacobian and the residual vector is less\r\n              than `gtol`, or the residual vector is zero.\r\n\r\n    x_scale : array_like or 'jac', optional\r\n        Characteristic scale of each variable. Setting `x_scale` is equivalent\r\n        to reformulating the problem in scaled variables ``xs = x / x_scale``.\r\n        An alternative view is that the size of a trust region along j-th\r\n        dimension is proportional to ``x_scale[j]``. Improved convergence may\r\n        be achieved by setting `x_scale` such that a step of a given size\r\n        along any of the scaled variables has a similar effect on the cost\r\n        function. If set to 'jac', the scale is iteratively updated using the\r\n        inverse norms of the columns of the Jacobian matrix (as described in\r\n        [JJMore]_).\r\n    loss : str or callable, optional\r\n        Determines the loss function. The following keyword values are allowed:\r\n\r\n            * 'linear' (default) : ``rho(z) = z``. Gives a standard\r\n              least-squares problem.\r\n            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\r\n              approximation of l1 (absolute value) loss. Usually a good\r\n              choice for robust least squares.\r\n            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\r\n              similarly to 'soft_l1'.\r\n            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\r\n              influence, but may cause difficulties in optimization process.\r\n            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\r\n              a single residual, has properties similar to 'cauchy'.\r\n\r\n        If callable, it must take a 1-d ndarray ``z=f**2`` and return an\r\n        array_like with shape (3, m) where row 0 contains function values,\r\n        row 1 contains first derivatives and row 2 contains second\r\n        derivatives. Method 'lm' supports only 'linear' loss.\r\n    f_scale : float, optional\r\n        Value of soft margin between inlier and outlier residuals, default\r\n        is 1.0. The loss function is evaluated as follows\r\n        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\r\n        and ``rho`` is determined by `loss` parameter. This parameter has\r\n        no effect with ``loss='linear'``, but for other `loss` values it is\r\n        of crucial importance.\r\n    max_nfev : None or int, optional\r\n        Maximum number of function evaluations before the termination.\r\n        If None (default), the value is chosen automatically:\r\n\r\n            * For 'trf' and 'dogbox' : 100 * n.\r\n            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\r\n              otherwise (because 'lm' counts function calls in Jacobian\r\n              estimation).\r\n\r\n    diff_step : None or array_like, optional\r\n        Determines the relative step size for the finite difference\r\n        approximation of the Jacobian. The actual step is computed as\r\n        ``x * diff_step``. If None (default), then `diff_step` is taken to be\r\n        a conventional \"optimal\" power of machine epsilon for the finite\r\n        difference scheme used [NR]_.\r\n    tr_solver : {None, 'exact', 'lsmr'}, optional\r\n        Method for solving trust-region subproblems, relevant only for 'trf'\r\n        and 'dogbox' methods.\r\n\r\n            * 'exact' is suitable for not very large problems with dense\r\n              Jacobian matrices. The computational complexity per iteration is\r\n              comparable to a singular value decomposition of the Jacobian\r\n              matrix.\r\n            * 'lsmr' is suitable for problems with sparse and large Jacobian\r\n              matrices. It uses the iterative procedure\r\n              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\r\n              least-squares problem and only requires matrix-vector product\r\n              evaluations.\r\n\r\n        If None (default) the solver is chosen based on the type of Jacobian\r\n        returned on the first iteration.\r\n    tr_options : dict, optional\r\n        Keyword options passed to trust-region solver.\r\n\r\n            * ``tr_solver='exact'``: `tr_options` are ignored.\r\n            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\r\n              Additionally  ``method='trf'`` supports  'regularize' option\r\n              (bool, default is True) which adds a regularization term to the\r\n              normal equation, which improves convergence if the Jacobian is\r\n              rank-deficient [Byrd]_ (eq. 3.4).\r\n\r\n    jac_sparsity : {None, array_like, sparse matrix}, optional\r\n        Defines the sparsity structure of the Jacobian matrix for finite\r\n        difference estimation, its shape must be (m, n). If the Jacobian has\r\n        only few non-zero elements in *each* row, providing the sparsity\r\n        structure will greatly speed up the computations [Curtis]_. A zero\r\n        entry means that a corresponding element in the Jacobian is identically\r\n        zero. If provided, forces the use of 'lsmr' trust-region solver.\r\n        If None (default) then dense differencing will be used. Has no effect\r\n        for 'lm' method.\r\n    verbose : {0, 1, 2}, optional\r\n        Level of algorithm's verbosity:\r\n\r\n            * 0 (default) : work silently.\r\n            * 1 : display a termination report.\r\n            * 2 : display progress during iterations (not supported by 'lm'\r\n              method).\r\n\r\n    args, kwargs : tuple and dict, optional\r\n        Additional arguments passed to `fun` and `jac`. Both empty by default.\r\n        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\r\n        `jac`.\r\n\r\n    Returns\r\n    -------\r\n    `OptimizeResult` with the following fields defined:\r\n    x : ndarray, shape (n,)\r\n        Solution found.\r\n    cost : float\r\n        Value of the cost function at the solution.\r\n    fun : ndarray, shape (m,)\r\n        Vector of residuals at the solution.\r\n    jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\r\n        Modified Jacobian matrix at the solution, in the sense that J^T J\r\n        is a Gauss-Newton approximation of the Hessian of the cost function.\r\n        The type is the same as the one used by the algorithm.\r\n    grad : ndarray, shape (m,)\r\n        Gradient of the cost function at the solution.\r\n    optimality : float\r\n        First-order optimality measure. In unconstrained problems, it is always\r\n        the uniform norm of the gradient. In constrained problems, it is the\r\n        quantity which was compared with `gtol` during iterations.\r\n    active_mask : ndarray of int, shape (n,)\r\n        Each component shows whether a corresponding constraint is active\r\n        (that is, whether a variable is at the bound):\r\n\r\n            *  0 : a constraint is not active.\r\n            * -1 : a lower bound is active.\r\n            *  1 : an upper bound is active.\r\n\r\n        Might be somewhat arbitrary for 'trf' method as it generates a sequence\r\n        of strictly feasible iterates and `active_mask` is determined within a\r\n        tolerance threshold.\r\n    nfev : int\r\n        Number of function evaluations done. Methods 'trf' and 'dogbox' do not\r\n        count function calls for numerical Jacobian approximation, as opposed\r\n        to 'lm' method.\r\n    njev : int or None\r\n        Number of Jacobian evaluations done. If numerical Jacobian\r\n        approximation is used in 'lm' method, it is set to None.\r\n    status : int\r\n        The reason for algorithm termination:\r\n\r\n            * -1 : improper input parameters status returned from MINPACK.\r\n            *  0 : the maximum number of function evaluations is exceeded.\r\n            *  1 : `gtol` termination condition is satisfied.\r\n            *  2 : `ftol` termination condition is satisfied.\r\n            *  3 : `xtol` termination condition is satisfied.\r\n            *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\r\n\r\n    message : str\r\n        Verbal description of the termination reason.\r\n    success : bool\r\n        True if one of the convergence criteria is satisfied (`status` > 0).\r\n\r\n    See Also\r\n    --------\r\n    leastsq : A legacy wrapper for the MINPACK implementation of the\r\n              Levenberg-Marquadt algorithm.\r\n    curve_fit : Least-squares minimization applied to a curve fitting problem.\r\n\r\n    Notes\r\n    -----\r\n    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\r\n    algorithms implemented in MINPACK (lmder, lmdif). It runs the\r\n    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\r\n    The implementation is based on paper [JJMore]_, it is very robust and\r\n    efficient with a lot of smart tricks. It should be your first choice\r\n    for unconstrained problems. Note that it doesn't support bounds. Also\r\n    it doesn't work when m < n.\r\n\r\n    Method 'trf' (Trust Region Reflective) is motivated by the process of\r\n    solving a system of equations, which constitute the first-order optimality\r\n    condition for a bound-constrained minimization problem as formulated in\r\n    [STIR]_. The algorithm iteratively solves trust-region subproblems\r\n    augmented by a special diagonal quadratic term and with trust-region shape\r\n    determined by the distance from the bounds and the direction of the\r\n    gradient. This enhancements help to avoid making steps directly into bounds\r\n    and efficiently explore the whole space of variables. To further improve\r\n    convergence, the algorithm considers search directions reflected from the\r\n    bounds. To obey theoretical requirements, the algorithm keeps iterates\r\n    strictly feasible. With dense Jacobians trust-region subproblems are\r\n    solved by an exact method very similar to the one described in [JJMore]_\r\n    (and implemented in MINPACK). The difference from the MINPACK\r\n    implementation is that a singular value decomposition of a Jacobian\r\n    matrix is done once per iteration, instead of a QR decomposition and series\r\n    of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\r\n    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\r\n    The subspace is spanned by a scaled gradient and an approximate\r\n    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\r\n    constraints are imposed the algorithm is very similar to MINPACK and has\r\n    generally comparable performance. The algorithm works quite robust in\r\n    unbounded and bounded problems, thus it is chosen as a default algorithm.\r\n\r\n    Method 'dogbox' operates in a trust-region framework, but considers\r\n    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\r\n    The intersection of a current trust region and initial bounds is again\r\n    rectangular, so on each iteration a quadratic minimization problem subject\r\n    to bound constraints is solved approximately by Powell's dogleg method\r\n    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\r\n    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\r\n    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\r\n    the rank of Jacobian is less than the number of variables. The algorithm\r\n    often outperforms 'trf' in bounded problems with a small number of\r\n    variables.\r\n\r\n    Robust loss functions are implemented as described in [BA]_. The idea\r\n    is to modify a residual vector and a Jacobian matrix on each iteration\r\n    such that computed gradient and Gauss-Newton Hessian approximation match\r\n    the true gradient and Hessian approximation of the cost function. Then\r\n    the algorithm proceeds in a normal way, i.e. robust loss functions are\r\n    implemented as a simple wrapper over standard least-squares algorithms.\r\n\r\n    .. versionadded:: 0.17.0\r\n\r\n    References\r\n    ----------\r\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\r\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\r\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\r\n              Vol. 21, Number 1, pp 1-23, 1999.\r\n    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\r\n            Computing. 3rd edition\", Sec. 5.7.\r\n    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\r\n              solution of the trust region problem by minimization over\r\n              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\r\n              1988.\r\n    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\r\n                sparse Jacobian matrices\", Journal of the Institute of\r\n                Mathematics and its Applications, 13, pp. 117-120, 1974.\r\n    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\r\n                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\r\n                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\r\n    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\r\n                Dogleg Approach for Unconstrained and Bound Constrained\r\n                Nonlinear Optimization\", WSEAS International Conference on\r\n                Applied Mathematics, Corfu, Greece, 2004.\r\n    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\r\n                2nd edition\", Chapter 4.\r\n    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\r\n            Proceedings of the International Workshop on Vision Algorithms:\r\n            Theory and Practice, pp. 298-372, 1999.\r\n\r\n    Examples\r\n    --------\r\n    In this example we find a minimum of the Rosenbrock function without bounds\r\n    on independed variables.\r\n\r\n    >>> def fun_rosenbrock(x):\r\n    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\r\n\r\n    Notice that we only provide the vector of the residuals. The algorithm\r\n    constructs the cost function as a sum of squares of the residuals, which\r\n    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> x0_rosenbrock = np.array([2, 2])\r\n    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\r\n    >>> res_1.x\r\n    array([ 1.,  1.])\r\n    >>> res_1.cost\r\n    9.8669242910846867e-30\r\n    >>> res_1.optimality\r\n    8.8928864934219529e-14\r\n\r\n    We now constrain the variables, in such a way that the previous solution\r\n    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\r\n    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\r\n    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\r\n\r\n    We also provide the analytic Jacobian:\r\n\r\n    >>> def jac_rosenbrock(x):\r\n    ...     return np.array([\r\n    ...         [-20 * x[0], 10],\r\n    ...         [-1, 0]])\r\n\r\n    Putting this all together, we see that the new solution lies on the bound:\r\n\r\n    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\r\n    ...                       bounds=([-np.inf, 1.5], np.inf))\r\n    >>> res_2.x\r\n    array([ 1.22437075,  1.5       ])\r\n    >>> res_2.cost\r\n    0.025213093946805685\r\n    >>> res_2.optimality\r\n    1.5885401433157753e-07\r\n\r\n    Now we solve a system of equations (i.e., the cost function should be zero\r\n    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\r\n    variables:\r\n\r\n    >>> def fun_broyden(x):\r\n    ...     f = (3 - x) * x + 1\r\n    ...     f[1:] -= x[:-1]\r\n    ...     f[:-1] -= 2 * x[1:]\r\n    ...     return f\r\n\r\n    The corresponding Jacobian matrix is sparse. We tell the algorithm to\r\n    estimate it by finite differences and provide the sparsity structure of\r\n    Jacobian to significantly speed up this process.\r\n\r\n    >>> from scipy.sparse import lil_matrix\r\n    >>> def sparsity_broyden(n):\r\n    ...     sparsity = lil_matrix((n, n), dtype=int)\r\n    ...     i = np.arange(n)\r\n    ...     sparsity[i, i] = 1\r\n    ...     i = np.arange(1, n)\r\n    ...     sparsity[i, i - 1] = 1\r\n    ...     i = np.arange(n - 1)\r\n    ...     sparsity[i, i + 1] = 1\r\n    ...     return sparsity\r\n    ...\r\n    >>> n = 100000\r\n    >>> x0_broyden = -np.ones(n)\r\n    ...\r\n    >>> res_3 = least_squares(fun_broyden, x0_broyden,\r\n    ...                       jac_sparsity=sparsity_broyden(n))\r\n    >>> res_3.cost\r\n    4.5687069299604613e-23\r\n    >>> res_3.optimality\r\n    1.1650454296851518e-11\r\n\r\n    Let's also solve a curve fitting problem using robust loss function to\r\n    take care of outliers in the data. Define the model function as\r\n    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\r\n    observation and a, b, c are parameters to estimate.\r\n\r\n    First, define the function which generates the data with noise and\r\n    outliers, define the model parameters, and generate data:\r\n\r\n    >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\r\n    ...     y = a + b * np.exp(t * c)\r\n    ...\r\n    ...     rnd = np.random.RandomState(random_state)\r\n    ...     error = noise * rnd.randn(t.size)\r\n    ...     outliers = rnd.randint(0, t.size, n_outliers)\r\n    ...     error[outliers] *= 10\r\n    ...\r\n    ...     return y + error\r\n    ...\r\n    >>> a = 0.5\r\n    >>> b = 2.0\r\n    >>> c = -1\r\n    >>> t_min = 0\r\n    >>> t_max = 10\r\n    >>> n_points = 15\r\n    ...\r\n    >>> t_train = np.linspace(t_min, t_max, n_points)\r\n    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\r\n\r\n    Define function for computing residuals and initial estimate of\r\n    parameters.\r\n\r\n    >>> def fun(x, t, y):\r\n    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\r\n    ...\r\n    >>> x0 = np.array([1.0, 1.0, 0.0])\r\n\r\n    Compute a standard least-squares solution:\r\n\r\n    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\r\n\r\n    Now compute two solutions with two different robust loss functions. The\r\n    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\r\n    not significantly exceed 0.1 (the noise level used).\r\n\r\n    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\r\n    ...                             args=(t_train, y_train))\r\n    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\r\n    ...                         args=(t_train, y_train))\r\n\r\n    And finally plot all the curves. We see that by selecting an appropriate\r\n    `loss`  we can get estimates close to optimal even in the presence of\r\n    strong outliers. But keep in mind that generally it is recommended to try\r\n    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\r\n    options may cause difficulties in optimization process.\r\n\r\n    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\r\n    >>> y_true = gen_data(t_test, a, b, c)\r\n    >>> y_lsq = gen_data(t_test, *res_lsq.x)\r\n    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\r\n    >>> y_log = gen_data(t_test, *res_log.x)\r\n    ...\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> plt.plot(t_train, y_train, 'o')\r\n    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\r\n    >>> plt.plot(t_test, y_lsq, label='linear loss')\r\n    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\r\n    >>> plt.plot(t_test, y_log, label='cauchy loss')\r\n    >>> plt.xlabel(\"t\")\r\n    >>> plt.ylabel(\"y\")\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    In the next example, we show how complex-valued residual functions of\r\n    complex variables can be optimized with ``least_squares()``. Consider the\r\n    following function:\r\n\r\n    >>> def f(z):\r\n    ...     return z - (0.5 + 0.5j)\r\n\r\n    We wrap it into a function of real variables that returns real residuals\r\n    by simply handling the real and imaginary parts as independent variables:\r\n\r\n    >>> def f_wrap(x):\r\n    ...     fx = f(x[0] + 1j*x[1])\r\n    ...     return np.array([fx.real, fx.imag])\r\n\r\n    Thus, instead of the original m-dimensional complex function of n complex\r\n    variables we optimize a 2m-dimensional real function of 2n real variables:\r\n\r\n    >>> from scipy.optimize import least_squares\r\n    >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\r\n    >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\r\n    >>> z\r\n    (0.49999999999925893+0.49999999999925893j)\r\n\r\n    \"\"\"\r\n    if method not in ['trf', 'dogbox', 'lm']:\r\n        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\r\n\r\n    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\r\n        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\r\n                         \"callable.\")\r\n\r\n    if tr_solver not in [None, 'exact', 'lsmr']:\r\n        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\r\n\r\n    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\r\n        raise ValueError(\"`loss` must be one of {0} or a callable.\"\r\n                         .format(IMPLEMENTED_LOSSES.keys()))\r\n\r\n    if method == 'lm' and loss != 'linear':\r\n        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\r\n\r\n    if verbose not in [0, 1, 2]:\r\n        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\r\n\r\n    if len(bounds) != 2:\r\n        raise ValueError(\"`bounds` must contain 2 elements.\")\r\n\r\n    if max_nfev is not None and max_nfev <= 0:\r\n        raise ValueError(\"`max_nfev` must be None or positive integer.\")\r\n\r\n    if np.iscomplexobj(x0):\r\n        raise ValueError(\"`x0` must be real.\")\r\n\r\n    x0 = np.atleast_1d(x0).astype(float)\r\n\r\n    if x0.ndim > 1:\r\n        raise ValueError(\"`x0` must have at most 1 dimension.\")\r\n\r\n    lb, ub = prepare_bounds(bounds, x0.shape[0])\r\n\r\n    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\r\n        raise ValueError(\"Method 'lm' doesn't support bounds.\")\r\n\r\n    if lb.shape != x0.shape or ub.shape != x0.shape:\r\n        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\r\n\r\n    if np.any(lb >= ub):\r\n        raise ValueError(\"Each lower bound must be strictly less than each \"\r\n                         \"upper bound.\")\r\n\r\n    if not in_bounds(x0, lb, ub):\r\n        raise ValueError(\"`x0` is infeasible.\")\r\n\r\n    x_scale = check_x_scale(x_scale, x0)\r\n\r\n    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol)\r\n\r\n    def fun_wrapped(x):\r\n        return np.atleast_1d(fun(x, *args, **kwargs))\r\n\r\n    if method == 'trf':\r\n        x0 = make_strictly_feasible(x0, lb, ub)\r\n\r\n    f0 = fun_wrapped(x0)\r\n\r\n    if f0.ndim != 1:\r\n        raise ValueError(\"`fun` must return at most 1-d array_like.\")\r\n\r\n    if not np.all(np.isfinite(f0)):\r\n        raise ValueError(\"Residuals are not finite in the initial point.\")\r\n\r\n    n = x0.size\r\n    m = f0.size\r\n\r\n    if method == 'lm' and m < n:\r\n        raise ValueError(\"Method 'lm' doesn't work when the number of \"\r\n                         \"residuals is less than the number of variables.\")\r\n\r\n    loss_function = construct_loss_function(m, loss, f_scale)\r\n    if callable(loss):\r\n        rho = loss_function(f0)\r\n        if rho.shape != (3, m):\r\n            raise ValueError(\"The return value of `loss` callable has wrong \"\r\n                             \"shape.\")\r\n        initial_cost = 0.5 * np.sum(rho[0])\r\n    elif loss_function is not None:\r\n        initial_cost = loss_function(f0, cost_only=True)\r\n    else:\r\n        initial_cost = 0.5 * np.dot(f0, f0)\r\n\r\n    if callable(jac):\r\n        J0 = jac(x0, *args, **kwargs)\r\n\r\n        if issparse(J0):\r\n            J0 = csr_matrix(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return csr_matrix(jac(x, *args, **kwargs))\r\n\r\n        elif isinstance(J0, LinearOperator):\r\n            def jac_wrapped(x, _=None):\r\n                return jac(x, *args, **kwargs)\r\n\r\n        else:\r\n            J0 = np.atleast_2d(J0)\r\n\r\n            def jac_wrapped(x, _=None):\r\n                return np.atleast_2d(jac(x, *args, **kwargs))\r\n\r\n    else:  # Estimate Jacobian by finite differences.\r\n        if method == 'lm':\r\n            if jac_sparsity is not None:\r\n                raise ValueError(\"method='lm' does not support \"\r\n                                 \"`jac_sparsity`.\")\r\n\r\n            if jac != '2-point':\r\n                warn(\"jac='{0}' works equivalently to '2-point' \"\r\n                     \"for method='lm'.\".format(jac))\r\n\r\n            J0 = jac_wrapped = None\r\n        else:\r\n            if jac_sparsity is not None and tr_solver == 'exact':\r\n                raise ValueError(\"tr_solver='exact' is incompatible \"\r\n                                 \"with `jac_sparsity`.\")\r\n\r\n            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\r\n\r\n            def jac_wrapped(x, f):\r\n                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\r\n                                      f0=f, bounds=bounds, args=args,\r\n                                      kwargs=kwargs, sparsity=jac_sparsity)\r\n                if J.ndim != 2:  # J is guaranteed not sparse.\r\n                    J = np.atleast_2d(J)\r\n\r\n                return J\r\n\r\n            J0 = jac_wrapped(x0, f0)\r\n\r\n    if J0 is not None:\r\n        if J0.shape != (m, n):\r\n            raise ValueError(\r\n                \"The return value of `jac` has wrong shape: expected {0}, \"\r\n                \"actual {1}.\".format((m, n), J0.shape))\r\n\r\n        if not isinstance(J0, np.ndarray):\r\n            if method == 'lm':\r\n                raise ValueError(\"method='lm' works only with dense \"\r\n                                 \"Jacobian matrices.\")\r\n\r\n            if tr_solver == 'exact':\r\n                raise ValueError(\r\n                    \"tr_solver='exact' works only with dense \"\r\n                    \"Jacobian matrices.\")\r\n\r\n        jac_scale = isinstance(x_scale, string_types) and x_scale == 'jac'\r\n        if isinstance(J0, LinearOperator) and jac_scale:\r\n            raise ValueError(\"x_scale='jac' can't be used when `jac` \"\r\n                             \"returns LinearOperator.\")\r\n\r\n        if tr_solver is None:\r\n            if isinstance(J0, np.ndarray):\r\n                tr_solver = 'exact'\r\n            else:\r\n                tr_solver = 'lsmr'\r\n\r\n    if method == 'lm':\r\n        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\r\n                              max_nfev, x_scale, diff_step)\r\n\r\n    elif method == 'trf':\r\n        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\r\n                     gtol, max_nfev, x_scale, loss_function, tr_solver,\r\n                     tr_options.copy(), verbose)\r\n\r\n    elif method == 'dogbox':\r\n        if tr_solver == 'lsmr' and 'regularize' in tr_options:\r\n            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\r\n                 \"for 'dogbox' method.\")\r\n            tr_options = tr_options.copy()\r\n            del tr_options['regularize']\r\n\r\n        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\r\n                        xtol, gtol, max_nfev, x_scale, loss_function,\r\n                        tr_solver, tr_options, verbose)\r\n\r\n    result.message = TERMINATION_MESSAGES[result.status]\r\n    result.success = result.status > 0\r\n\r\n    if verbose >= 1:\r\n        print(result.message)\r\n        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\r\n              \"{2:.4e}, first-order optimality {3:.2e}.\"\r\n              .format(result.nfev, initial_cost, result.cost,\r\n                      result.optimality))\r\n\r\n    return result",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False,\r\n              check_finite=True, bounds=(-np.inf, np.inf), method=None,\r\n              jac=None, **kwargs):\r\n    \"\"\"\r\n    Use non-linear least squares to fit a function, f, to data.\r\n\r\n    Assumes ``ydata = f(xdata, *params) + eps``\r\n\r\n    Parameters\r\n    ----------\r\n    f : callable\r\n        The model function, f(x, ...).  It must take the independent\r\n        variable as the first argument and the parameters to fit as\r\n        separate remaining arguments.\r\n    xdata : An M-length sequence or an (k,M)-shaped array for functions with k predictors\r\n        The independent variable where the data is measured.\r\n    ydata : M-length sequence\r\n        The dependent data --- nominally f(xdata, ...)\r\n    p0 : None, scalar, or N-length sequence, optional\r\n        Initial guess for the parameters.  If None, then the initial\r\n        values will all be 1 (if the number of parameters for the function\r\n        can be determined using introspection, otherwise a ValueError\r\n        is raised).\r\n    sigma : None or M-length sequence or MxM array, optional\r\n        Determines the uncertainty in `ydata`. If we define residuals as\r\n        ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\r\n        depends on its number of dimensions:\r\n\r\n            - A 1-d `sigma` should contain values of standard deviations of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = sum((r / sigma) ** 2)``.\r\n\r\n            - A 2-d `sigma` should contain the covariance matrix of\r\n              errors in `ydata`. In this case, the optimized function is\r\n              ``chisq = r.T @ inv(sigma) @ r``.\r\n\r\n              .. versionadded:: 0.19\r\n\r\n        None (default) is equivalent of 1-d `sigma` filled with ones.\r\n    absolute_sigma : bool, optional\r\n        If True, `sigma` is used in an absolute sense and the estimated parameter\r\n        covariance `pcov` reflects these absolute values.\r\n\r\n        If False, only the relative magnitudes of the `sigma` values matter.\r\n        The returned parameter covariance matrix `pcov` is based on scaling\r\n        `sigma` by a constant factor. This constant is set by demanding that the\r\n        reduced `chisq` for the optimal parameters `popt` when using the\r\n        *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\r\n        match the sample variance of the residuals after the fit.\r\n        Mathematically,\r\n        ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\r\n    check_finite : bool, optional\r\n        If True, check that the input arrays do not contain nans of infs,\r\n        and raise a ValueError if they do. Setting this parameter to\r\n        False may silently produce nonsensical results if the input arrays\r\n        do contain nans. Default is True.\r\n    bounds : 2-tuple of array_like, optional\r\n        Lower and upper bounds on independent variables. Defaults to no bounds.\r\n        Each element of the tuple must be either an array with the length equal\r\n        to the number of parameters, or a scalar (in which case the bound is\r\n        taken to be the same for all parameters.) Use ``np.inf`` with an\r\n        appropriate sign to disable bounds on all or some parameters.\r\n\r\n        .. versionadded:: 0.17\r\n    method : {'lm', 'trf', 'dogbox'}, optional\r\n        Method to use for optimization.  See `least_squares` for more details.\r\n        Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\r\n        provided. The method 'lm' won't work when the number of observations\r\n        is less than the number of variables, use 'trf' or 'dogbox' in this\r\n        case.\r\n\r\n        .. versionadded:: 0.17\r\n    jac : callable, string or None, optional\r\n        Function with signature ``jac(x, ...)`` which computes the Jacobian\r\n        matrix of the model function with respect to parameters as a dense\r\n        array_like structure. It will be scaled according to provided `sigma`.\r\n        If None (default), the Jacobian will be estimated numerically.\r\n        String keywords for 'trf' and 'dogbox' methods can be used to select\r\n        a finite difference scheme, see `least_squares`.\r\n\r\n        .. versionadded:: 0.18\r\n    kwargs\r\n        Keyword arguments passed to `leastsq` for ``method='lm'`` or\r\n        `least_squares` otherwise.\r\n\r\n    Returns\r\n    -------\r\n    popt : array\r\n        Optimal values for the parameters so that the sum of the squared\r\n        residuals of ``f(xdata, *popt) - ydata`` is minimized\r\n    pcov : 2d array\r\n        The estimated covariance of popt. The diagonals provide the variance\r\n        of the parameter estimate. To compute one standard deviation errors\r\n        on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\r\n\r\n        How the `sigma` parameter affects the estimated covariance\r\n        depends on `absolute_sigma` argument, as described above.\r\n\r\n        If the Jacobian matrix at the solution doesn't have a full rank, then\r\n        'lm' method returns a matrix filled with ``np.inf``, on the other hand\r\n        'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\r\n        the covariance matrix.\r\n\r\n    Raises\r\n    ------\r\n    ValueError\r\n        if either `ydata` or `xdata` contain NaNs, or if incompatible options\r\n        are used.\r\n\r\n    RuntimeError\r\n        if the least-squares minimization fails.\r\n\r\n    OptimizeWarning\r\n        if covariance of the parameters can not be estimated.\r\n\r\n    See Also\r\n    --------\r\n    least_squares : Minimize the sum of squares of nonlinear functions.\r\n    scipy.stats.linregress : Calculate a linear least squares regression for\r\n                             two sets of measurements.\r\n\r\n    Notes\r\n    -----\r\n    With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\r\n    through `leastsq`. Note that this algorithm can only deal with\r\n    unconstrained problems.\r\n\r\n    Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\r\n    the docstring of `least_squares` for more information.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> import matplotlib.pyplot as plt\r\n    >>> from scipy.optimize import curve_fit\r\n\r\n    >>> def func(x, a, b, c):\r\n    ...     return a * np.exp(-b * x) + c\r\n\r\n    Define the data to be fit with some noise:\r\n\r\n    >>> xdata = np.linspace(0, 4, 50)\r\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\r\n    >>> np.random.seed(1729)\r\n    >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\r\n    >>> ydata = y + y_noise\r\n    >>> plt.plot(xdata, ydata, 'b-', label='data')\r\n\r\n    Fit for the parameters a, b, c of the function `func`:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata)\r\n    >>> popt\r\n    array([ 2.55423706,  1.35190947,  0.47450618])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'r-',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    Constrain the optimization to the region of ``0 <= a <= 3``,\r\n    ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\r\n\r\n    >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\r\n    >>> popt\r\n    array([ 2.43708906,  1.        ,  0.35015434])\r\n    >>> plt.plot(xdata, func(xdata, *popt), 'g--',\r\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\r\n\r\n    >>> plt.xlabel('x')\r\n    >>> plt.ylabel('y')\r\n    >>> plt.legend()\r\n    >>> plt.show()\r\n\r\n    \"\"\"\r\n    if p0 is None:\r\n        # determine number of parameters by inspecting the function\r\n        from scipy._lib._util import getargspec_no_self as _getargspec\r\n        args, varargs, varkw, defaults = _getargspec(f)\r\n        if len(args) < 2:\r\n            raise ValueError(\"Unable to determine number of fit parameters.\")\r\n        n = len(args) - 1\r\n    else:\r\n        p0 = np.atleast_1d(p0)\r\n        n = p0.size\r\n\r\n    lb, ub = prepare_bounds(bounds, n)\r\n    if p0 is None:\r\n        p0 = _initialize_feasible(lb, ub)\r\n\r\n    bounded_problem = np.any((lb > -np.inf) | (ub < np.inf))\r\n    if method is None:\r\n        if bounded_problem:\r\n            method = 'trf'\r\n        else:\r\n            method = 'lm'\r\n\r\n    if method == 'lm' and bounded_problem:\r\n        raise ValueError(\"Method 'lm' only works for unconstrained problems. \"\r\n                         \"Use 'trf' or 'dogbox' instead.\")\r\n\r\n    # NaNs can not be handled\r\n    if check_finite:\r\n        ydata = np.asarray_chkfinite(ydata)\r\n    else:\r\n        ydata = np.asarray(ydata)\r\n\r\n    if isinstance(xdata, (list, tuple, np.ndarray)):\r\n        # `xdata` is passed straight to the user-defined `f`, so allow\r\n        # non-array_like `xdata`.\r\n        if check_finite:\r\n            xdata = np.asarray_chkfinite(xdata)\r\n        else:\r\n            xdata = np.asarray(xdata)\r\n\r\n    # Determine type of sigma\r\n    if sigma is not None:\r\n        sigma = np.asarray(sigma)\r\n\r\n        # if 1-d, sigma are errors, define transform = 1/sigma\r\n        if sigma.shape == (ydata.size, ):\r\n            transform = 1.0 / sigma\r\n        # if 2-d, sigma is the covariance matrix,\r\n        # define transform = L such that L L^T = C\r\n        elif sigma.shape == (ydata.size, ydata.size):\r\n            try:\r\n                # scipy.linalg.cholesky requires lower=True to return L L^T = A\r\n                transform = cholesky(sigma, lower=True)\r\n            except LinAlgError:\r\n                raise ValueError(\"`sigma` must be positive definite.\")\r\n        else:\r\n            raise ValueError(\"`sigma` has incorrect shape.\")\r\n    else:\r\n        transform = None\r\n\r\n    func = _wrap_func(f, xdata, ydata, transform)\r\n    if callable(jac):\r\n        jac = _wrap_jac(jac, xdata, transform)\r\n    elif jac is None and method != 'lm':\r\n        jac = '2-point'\r\n\r\n    if method == 'lm':\r\n        # Remove full_output from kwargs, otherwise we're passing it in twice.\r\n        return_full = kwargs.pop('full_output', False)\r\n        res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\r\n        popt, pcov, infodict, errmsg, ier = res\r\n        cost = np.sum(infodict['fvec'] ** 2)\r\n        if ier not in [1, 2, 3, 4]:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\r\n    else:\r\n        # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.\r\n        if 'max_nfev' not in kwargs:\r\n            kwargs['max_nfev'] = kwargs.pop('maxfev', None)\r\n\r\n        res = least_squares(func, p0, jac=jac, bounds=bounds, method=method,\r\n                            **kwargs)\r\n\r\n        if not res.success:\r\n            raise RuntimeError(\"Optimal parameters not found: \" + res.message)\r\n\r\n        cost = 2 * res.cost  # res.cost is half sum of squares!\r\n        popt = res.x\r\n\r\n        # Do Moore-Penrose inverse discarding zero singular values.\r\n        _, s, VT = svd(res.jac, full_matrices=False)\r\n        threshold = np.finfo(float).eps * max(res.jac.shape) * s[0]\r\n        s = s[s > threshold]\r\n        VT = VT[:s.size]\r\n        pcov = np.dot(VT.T / s**2, VT)\r\n        return_full = False\r\n\r\n    warn_cov = False\r\n    if pcov is None:\r\n        # indeterminate covariance\r\n        pcov = zeros((len(popt), len(popt)), dtype=float)\r\n        pcov.fill(inf)\r\n        warn_cov = True\r\n    elif not absolute_sigma:\r\n        if ydata.size > p0.size:\r\n            s_sq = cost / (ydata.size - p0.size)\r\n            pcov = pcov * s_sq\r\n        else:\r\n            pcov.fill(inf)\r\n            warn_cov = True\r\n\r\n    if warn_cov:\r\n        warnings.warn('Covariance of the parameters could not be estimated',\r\n                      category=OptimizeWarning)\r\n\r\n    if return_full:\r\n        return popt, pcov, infodict, errmsg, ier\r\n    else:\r\n        return popt, pcov",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def bench(args):\r\n    results_dir = Path(args.bench_results)\r\n    branch = args.branch\r\n    random_state = 1\r\n\r\n    results = defaultdict(list)\r\n\r\n    # Loop over all datasets for fitting and scoring the estimator:\r\n    n_samples_train = 1000\r\n    for n_samples_test in [\r\n        1000,\r\n        10000,\r\n        50000,\r\n    ]:\r\n        for n_features in [10, 100, 1000]:\r\n            for contamination in [0.01, 0.1, 0.5]:\r\n                for n_jobs in [1, 2, 3, 4]:\r\n                    X_train, X_test = get_data(\r\n                        n_samples_train,\r\n                        n_samples_test,\r\n                        n_features,\r\n                        contamination,\r\n                        random_state,\r\n                    )\r\n\r\n                    print(\"--- Fitting the IsolationForest estimator...\")\r\n                    model = IsolationForest(n_jobs=-1, random_state=random_state)\r\n                    tstart = time()\r\n                    model.fit(X_train)\r\n                    fit_time = time() - tstart\r\n\r\n                    # clearcache\r\n                    for _ in range(1000):\r\n                        1 + 1\r\n                    with parallel_config(\"threading\", n_jobs=n_jobs):\r\n                        tstart = time()\r\n                        model.decision_function(X_test)  # the lower, the more abnormal\r\n                        predict_time = time() - tstart\r\n\r\n                    results[\"predict_time\"].append(predict_time)\r\n                    results[\"fit_time\"].append(fit_time)\r\n                    results[\"n_samples_train\"].append(n_samples_train)\r\n                    results[\"n_samples_test\"].append(n_samples_test)\r\n                    results[\"n_features\"].append(n_features)\r\n                    results[\"contamination\"].append(contamination)\r\n                    results[\"n_jobs\"].append(n_jobs)\r\n\r\n    df = pd.DataFrame(results)\r\n    df.to_csv(results_dir / f\"{branch}.csv\", index=False)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def bench_scikit_tree_classifier(X, Y):\r\n    \"\"\"Benchmark with scikit-learn decision tree classifier\"\"\"\r\n\r\n    from sklearn.tree import DecisionTreeClassifier\r\n\r\n    gc.collect()\r\n\r\n    # start time\r\n    tstart = datetime.now()\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X, Y).predict(X)\r\n    delta = datetime.now() - tstart\r\n    # stop time\r\n\r\n    scikit_classifier_results.append(delta.seconds + delta.microseconds / mu_second)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_affinity_propagation_predict_error():\r\n    # Test exception in AffinityPropagation.predict\r\n    # Not fitted.\r\n    af = AffinityPropagation(affinity=\"euclidean\")\r\n    with pytest.raises(NotFittedError):\r\n        af.predict(X)\r\n\r\n    # Predict not supported when affinity=\"precomputed\".\r\n    S = np.dot(X, X.T)\r\n    af = AffinityPropagation(affinity=\"precomputed\", random_state=57)\r\n    af.fit(S)\r\n    with pytest.raises(ValueError, match=\"expecting 60 features as input\"):\r\n        af.predict(X)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_transform_target_regressor_error():\r\n    X, y = friedman\r\n    # provide a transformer and functions at the same time\r\n    regr = TransformedTargetRegressor(\r\n        regressor=LinearRegression(),\r\n        transformer=StandardScaler(),\r\n        func=np.exp,\r\n        inverse_func=np.log,\r\n    )\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=\"'transformer' and functions 'func'/'inverse_func' cannot both be set.\",\r\n    ):\r\n        regr.fit(X, y)\r\n    # fit with sample_weight with a regressor which does not support it\r\n    sample_weight = np.ones((y.shape[0],))\r\n    regr = TransformedTargetRegressor(\r\n        regressor=OrthogonalMatchingPursuit(), transformer=StandardScaler()\r\n    )\r\n    with pytest.raises(\r\n        TypeError,\r\n        match=r\"fit\\(\\) got an unexpected \" \"keyword argument 'sample_weight'\",\r\n    ):\r\n        regr.fit(X, y, sample_weight=sample_weight)\r\n\r\n    # one of (func, inverse_func) is given but the other one is not\r\n    regr = TransformedTargetRegressor(func=np.exp)\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=\"When 'func' is provided, 'inverse_func' must also be provided\",\r\n    ):\r\n        regr.fit(X, y)\r\n\r\n    regr = TransformedTargetRegressor(inverse_func=np.log)\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=\"When 'inverse_func' is provided, 'func' must also be provided\",\r\n    ):\r\n        regr.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_target_regressor_invertible():\r\n    X, y = friedman\r\n    regr = TransformedTargetRegressor(\r\n        regressor=LinearRegression(),\r\n        func=np.sqrt,\r\n        inverse_func=np.log,\r\n        check_inverse=True,\r\n    )\r\n    with pytest.warns(\r\n        UserWarning,\r\n        match=(r\"The provided functions.* are not strictly inverse of each other\"),\r\n    ):\r\n        regr.fit(X, y)\r\n    regr = TransformedTargetRegressor(\r\n        regressor=LinearRegression(), func=np.sqrt, inverse_func=np.log\r\n    )\r\n    regr.set_params(check_inverse=False)\r\n\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"error\", UserWarning)\r\n        regr.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_transform_target_regressor_not_warns_with_global_output_set(output_format):\r\n    \"\"\"Test that TransformedTargetRegressor will not raise warnings if\r\n    set_config(transform_output=\"pandas\"/\"polars\") is set globally; regression test for\r\n    issue #29361.\"\"\"\r\n    X, y = datasets.make_regression()\r\n    y = np.abs(y) + 1\r\n    with config_context(transform_output=output_format):\r\n        with warnings.catch_warnings():\r\n            warnings.simplefilter(\"error\")\r\n            TransformedTargetRegressor(\r\n                regressor=LinearRegression(), func=np.log, inverse_func=np.exp\r\n            ).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_factor_analysis(global_random_seed):\r\n    # Test FactorAnalysis ability to recover the data covariance structure\r\n    rng = np.random.RandomState(global_random_seed)\r\n    n_samples, n_features, n_components = 20, 5, 3\r\n\r\n    # Some random settings for the generative model\r\n    W = rng.randn(n_components, n_features)\r\n    # latent variable of dim 3, 20 of it\r\n    h = rng.randn(n_samples, n_components)\r\n    # using gamma to model different noise variance\r\n    # per component\r\n    noise = rng.gamma(1, size=n_features) * rng.randn(n_samples, n_features)\r\n\r\n    # generate observations\r\n    # wlog, mean is 0\r\n    X = np.dot(h, W) + noise\r\n\r\n    fas = []\r\n    for method in [\"randomized\", \"lapack\"]:\r\n        fa = FactorAnalysis(n_components=n_components, svd_method=method)\r\n        fa.fit(X)\r\n        fas.append(fa)\r\n\r\n        X_t = fa.transform(X)\r\n        assert X_t.shape == (n_samples, n_components)\r\n\r\n        assert_almost_equal(fa.loglike_[-1], fa.score_samples(X).sum())\r\n        assert_almost_equal(fa.score_samples(X).mean(), fa.score(X))\r\n\r\n        diff = np.all(np.diff(fa.loglike_))\r\n        assert diff > 0.0, \"Log likelihood dif not increase\"\r\n\r\n        # Sample Covariance\r\n        scov = np.cov(X, rowvar=0.0, bias=1.0)\r\n\r\n        # Model Covariance\r\n        mcov = fa.get_covariance()\r\n        diff = np.sum(np.abs(scov - mcov)) / W.size\r\n        assert diff < 0.2, \"Mean absolute difference is %f\" % diff\r\n        fa = FactorAnalysis(\r\n            n_components=n_components, noise_variance_init=np.ones(n_features)\r\n        )\r\n        with pytest.raises(ValueError):\r\n            fa.fit(X[:, :2])\r\n\r\n    def f(x, y):\r\n        return np.abs(getattr(x, y))  # sign will not be equal\r\n\r\n    fa1, fa2 = fas\r\n    for attr in [\"loglike_\", \"components_\", \"noise_variance_\"]:\r\n        assert_almost_equal(f(fa1, attr), f(fa2, attr))\r\n\r\n    fa1.max_iter = 1\r\n    fa1.verbose = True\r\n    with pytest.warns(ConvergenceWarning):\r\n        fa1.fit(X)\r\n\r\n    # Test get_covariance and get_precision with n_components == n_features\r\n    # with n_components < n_features and with n_components == 0\r\n    for n_components in [0, 2, X.shape[1]]:\r\n        fa.n_components = n_components\r\n        fa.fit(X)\r\n        cov = fa.get_covariance()\r\n        precision = fa.get_precision()\r\n        assert_array_almost_equal(np.dot(cov, precision), np.eye(X.shape[1]), 12)\r\n\r\n    # test rotation\r\n    n_components = 2\r\n\r\n    results, projections = {}, {}\r\n    for method in (None, \"varimax\", \"quartimax\"):\r\n        fa_var = FactorAnalysis(n_components=n_components, rotation=method)\r\n        results[method] = fa_var.fit_transform(X)\r\n        projections[method] = fa_var.get_covariance()\r\n    for rot1, rot2 in combinations([None, \"varimax\", \"quartimax\"], 2):\r\n        assert not np.allclose(results[rot1], results[rot2])\r\n        assert np.allclose(projections[rot1], projections[rot2], atol=3)\r\n\r\n    # test against R's psych::principal with rotate=\"varimax\"\r\n    # (i.e., the values below stem from rotating the components in R)\r\n    # R's factor analysis returns quite different values; therefore, we only\r\n    # test the rotation itself\r\n    factors = np.array(\r\n        [\r\n            [0.89421016, -0.35854928, -0.27770122, 0.03773647],\r\n            [-0.45081822, -0.89132754, 0.0932195, -0.01787973],\r\n            [0.99500666, -0.02031465, 0.05426497, -0.11539407],\r\n            [0.96822861, -0.06299656, 0.24411001, 0.07540887],\r\n        ]\r\n    )\r\n    r_solution = np.array(\r\n        [[0.962, 0.052], [-0.141, 0.989], [0.949, -0.300], [0.937, -0.251]]\r\n    )\r\n    rotated = _ortho_rotation(factors[:, :n_components], method=\"varimax\").T\r\n    assert_array_almost_equal(np.abs(rotated), np.abs(r_solution), decimal=3)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_fastica_convergence_fail():\r\n    # Test the FastICA algorithm on very simple data\r\n    # (see test_non_square_fastica).\r\n    # Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.\r\n    rng = np.random.RandomState(0)\r\n\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    t = np.linspace(0, 100, n_samples)\r\n    s1 = np.sin(t)\r\n    s2 = np.ceil(np.sin(np.pi * t))\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n\r\n    # Mixing matrix\r\n    mixing = rng.randn(6, 2)\r\n    m = np.dot(mixing, s)\r\n\r\n    # Do fastICA with tolerance 0. to ensure failing convergence\r\n    warn_msg = (\r\n        \"FastICA did not converge. Consider increasing tolerance \"\r\n        \"or the maximum number of iterations.\"\r\n    )\r\n    with pytest.warns(ConvergenceWarning, match=warn_msg):\r\n        ica = FastICA(\r\n            algorithm=\"parallel\", n_components=2, random_state=rng, max_iter=2, tol=0.0\r\n        )\r\n        ica.fit(m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_incremental_pca_sparse(sparse_container):\r\n    # Incremental PCA on sparse arrays.\r\n    X = iris.data\r\n    pca = PCA(n_components=2)\r\n    pca.fit_transform(X)\r\n    X_sparse = sparse_container(X)\r\n    batch_size = X_sparse.shape[0] // 3\r\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\r\n\r\n    X_transformed = ipca.fit_transform(X_sparse)\r\n\r\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\r\n    np.testing.assert_allclose(\r\n        ipca.explained_variance_ratio_.sum(),\r\n        pca.explained_variance_ratio_.sum(),\r\n        rtol=1e-3,\r\n    )\r\n\r\n    for n_components in [1, 2, X.shape[1]]:\r\n        ipca = IncrementalPCA(n_components, batch_size=batch_size)\r\n        ipca.fit(X_sparse)\r\n        cov = ipca.get_covariance()\r\n        precision = ipca.get_precision()\r\n        np.testing.assert_allclose(\r\n            np.dot(cov, precision), np.eye(X_sparse.shape[1]), atol=1e-13\r\n        )\r\n\r\n    with pytest.raises(\r\n        TypeError,\r\n        match=(\r\n            \"IncrementalPCA.partial_fit does not support \"\r\n            \"sparse input. Either convert data to dense \"\r\n            \"or use IncrementalPCA.fit to do so in batches.\"\r\n        ),\r\n    ):\r\n        ipca.partial_fit(X_sparse)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_fastica_simple(add_noise, global_random_seed, global_dtype):\r\n    if (\r\n        global_random_seed == 20\r\n        and global_dtype == np.float32\r\n        and not add_noise\r\n        and os.getenv(\"DISTRIB\") == \"ubuntu\"\r\n    ):\r\n        pytest.xfail(\r\n            \"FastICA instability with Ubuntu Atlas build with float32 \"\r\n            \"global_dtype. For more details, see \"\r\n            \"https://github.com/scikit-learn/scikit-learn/issues/24131#issuecomment-1208091119\"  # noqa\r\n        )\r\n\r\n    # Test the FastICA algorithm on very simple data.\r\n    rng = np.random.RandomState(global_random_seed)\r\n    n_samples = 1000\r\n    # Generate two sources:\r\n    s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\n    s2 = stats.t.rvs(1, size=n_samples, random_state=global_random_seed)\r\n    s = np.c_[s1, s2].T\r\n    center_and_norm(s)\r\n    s = s.astype(global_dtype)\r\n    s1, s2 = s\r\n\r\n    # Mixing angle\r\n    phi = 0.6\r\n    mixing = np.array([[np.cos(phi), np.sin(phi)], [np.sin(phi), -np.cos(phi)]])\r\n    mixing = mixing.astype(global_dtype)\r\n    m = np.dot(mixing, s)\r\n\r\n    if add_noise:\r\n        m += 0.1 * rng.randn(2, 1000)\r\n\r\n    center_and_norm(m)\r\n\r\n    # function as fun arg\r\n    def g_test(x):\r\n        return x**3, (3 * x**2).mean(axis=-1)\r\n\r\n    algos = [\"parallel\", \"deflation\"]\r\n    nls = [\"logcosh\", \"exp\", \"cube\", g_test]\r\n    whitening = [\"arbitrary-variance\", \"unit-variance\", False]\r\n    for algo, nl, whiten in itertools.product(algos, nls, whitening):\r\n        if whiten:\r\n            k_, mixing_, s_ = fastica(\r\n                m.T, fun=nl, whiten=whiten, algorithm=algo, random_state=rng\r\n            )\r\n            with pytest.raises(ValueError):\r\n                fastica(m.T, fun=np.tanh, whiten=whiten, algorithm=algo)\r\n        else:\r\n            pca = PCA(n_components=2, whiten=True, random_state=rng)\r\n            X = pca.fit_transform(m.T)\r\n            k_, mixing_, s_ = fastica(\r\n                X, fun=nl, algorithm=algo, whiten=False, random_state=rng\r\n            )\r\n            with pytest.raises(ValueError):\r\n                fastica(X, fun=np.tanh, algorithm=algo)\r\n        s_ = s_.T\r\n        # Check that the mixing model described in the docstring holds:\r\n        if whiten:\r\n            # XXX: exact reconstruction to standard relative tolerance is not\r\n            # possible. This is probably expected when add_noise is True but we\r\n            # also need a non-trivial atol in float32 when add_noise is False.\r\n            #\r\n            # Note that the 2 sources are non-Gaussian in this test.\r\n            atol = 1e-5 if global_dtype == np.float32 else 0\r\n            assert_allclose(np.dot(np.dot(mixing_, k_), m), s_, atol=atol)\r\n\r\n        center_and_norm(s_)\r\n        s1_, s2_ = s_\r\n        # Check to see if the sources have been estimated\r\n        # in the wrong order\r\n        if abs(np.dot(s1_, s2)) > abs(np.dot(s1_, s1)):\r\n            s2_, s1_ = s_\r\n        s1_ *= np.sign(np.dot(s1_, s1))\r\n        s2_ *= np.sign(np.dot(s2_, s2))\r\n\r\n        # Check that we have estimated the original sources\r\n        if not add_noise:\r\n            assert_allclose(np.dot(s1_, s1) / n_samples, 1, atol=1e-2)\r\n            assert_allclose(np.dot(s2_, s2) / n_samples, 1, atol=1e-2)\r\n        else:\r\n            assert_allclose(np.dot(s1_, s1) / n_samples, 1, atol=1e-1)\r\n            assert_allclose(np.dot(s2_, s2) / n_samples, 1, atol=1e-1)\r\n\r\n    # Test FastICA class\r\n    _, _, sources_fun = fastica(\r\n        m.T, fun=nl, algorithm=algo, random_state=global_random_seed\r\n    )\r\n    ica = FastICA(fun=nl, algorithm=algo, random_state=global_random_seed)\r\n    sources = ica.fit_transform(m.T)\r\n    assert ica.components_.shape == (2, 2)\r\n    assert sources.shape == (1000, 2)\r\n\r\n    assert_allclose(sources_fun, sources)\r\n    # Set atol to account for the different magnitudes of the elements in sources\r\n    # (from 1e-4 to 1e1).\r\n    atol = np.max(np.abs(sources)) * (1e-5 if global_dtype == np.float32 else 1e-7)\r\n    assert_allclose(sources, ica.transform(m.T), atol=atol)\r\n\r\n    assert ica.mixing_.shape == (2, 2)\r\n\r\n    ica = FastICA(fun=np.tanh, algorithm=algo)\r\n    with pytest.raises(ValueError):\r\n        ica.fit(m.T)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_dict_learning_reconstruction():\r\n    n_components = 12\r\n    dico = DictionaryLearning(\r\n        n_components, transform_algorithm=\"omp\", transform_alpha=0.001, random_state=0\r\n    )\r\n    code = dico.fit(X).transform(X)\r\n    assert_array_almost_equal(np.dot(code, dico.components_), X)\r\n    assert_array_almost_equal(dico.inverse_transform(code), X)\r\n\r\n    dico.set_params(transform_algorithm=\"lasso_lars\")\r\n    code = dico.transform(X)\r\n    assert_array_almost_equal(np.dot(code, dico.components_), X, decimal=2)\r\n    assert_array_almost_equal(dico.inverse_transform(code), X, decimal=2)\r\n\r\n    # test error raised for wrong code size\r\n    with pytest.raises(ValueError, match=\"Expected 12, got 11.\"):\r\n        dico.inverse_transform(code[:, :-1])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_name_validation(X, y, Ensemble):\r\n    # raise an error when the name contains dunder\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [(\"lr__\", LogisticRegression())]\r\n    else:\r\n        estimators = [(\"lr__\", LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Estimator names must not contain __: got \\['lr__'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name is not unique\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [(\"lr\", LogisticRegression()), (\"lr\", LogisticRegression())]\r\n    else:\r\n        estimators = [(\"lr\", LinearRegression()), (\"lr\", LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = r\"Names provided are not unique: \\['lr', 'lr'\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)\r\n\r\n    # raise an error when the name conflicts with the parameters\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        estimators = [(\"estimators\", LogisticRegression())]\r\n    else:\r\n        estimators = [(\"estimators\", LinearRegression())]\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"Estimator names conflict with constructor arguments\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_all_dropped(X, y, estimator):\r\n    # check that we raise a consistent error when all estimators are\r\n    # dropped\r\n    estimator.set_params(lr=\"drop\")\r\n    with pytest.raises(ValueError, match=\"All estimators are dropped.\"):\r\n        estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    rng = check_random_state(0)\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        iris.data, iris.target, random_state=rng\r\n    )\r\n\r\n    for estimator in [DecisionTreeClassifier(), SVC()]:\r\n        clf = BaggingClassifier(\r\n            estimator=estimator,\r\n            n_estimators=100,\r\n            bootstrap=True,\r\n            oob_score=True,\r\n            random_state=rng,\r\n        ).fit(X_train, y_train)\r\n\r\n        test_score = clf.score(X_test, y_test)\r\n\r\n        assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n        # Test with few estimators\r\n        warn_msg = (\r\n            \"Some inputs do not have OOB scores. This probably means too few \"\r\n            \"estimators were used to compute any reliable oob estimates.\"\r\n        )\r\n        with pytest.warns(UserWarning, match=warn_msg):\r\n            clf = BaggingClassifier(\r\n                estimator=estimator,\r\n                n_estimators=1,\r\n                bootstrap=True,\r\n                oob_score=True,\r\n                random_state=rng,\r\n            )\r\n            clf.fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ensemble_heterogeneous_estimators_type(Ensemble):\r\n    # check that ensemble will fail during validation if the underlying\r\n    # estimators are not of the same type (i.e. classifier or regressor)\r\n    # StackingClassifier can have an underlying regresor so it's not checked\r\n    if issubclass(Ensemble, ClassifierMixin):\r\n        X, y = make_classification(n_samples=10)\r\n        estimators = [(\"lr\", LinearRegression())]\r\n        ensemble_type = \"classifier\"\r\n    else:\r\n        X, y = make_regression(n_samples=10)\r\n        estimators = [(\"lr\", LogisticRegression())]\r\n        ensemble_type = \"regressor\"\r\n    ensemble = Ensemble(estimators=estimators)\r\n\r\n    err_msg = \"should be a {}\".format(ensemble_type)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bagging_classifier_with_missing_inputs():\r\n    # Check that BaggingClassifier can accept X with missing/infinite data\r\n    X = np.array(\r\n        [\r\n            [1, 3, 5],\r\n            [2, None, 6],\r\n            [2, np.nan, 6],\r\n            [2, np.inf, 6],\r\n            [2, -np.inf, 6],\r\n        ]\r\n    )\r\n    y = np.array([3, 6, 6, 6, 6])\r\n    classifier = DecisionTreeClassifier()\r\n    pipeline = make_pipeline(FunctionTransformer(replace), classifier)\r\n    pipeline.fit(X, y).predict(X)\r\n    bagging_classifier = BaggingClassifier(pipeline)\r\n    bagging_classifier.fit(X, y)\r\n    y_hat = bagging_classifier.predict(X)\r\n    assert y.shape == y_hat.shape\r\n    bagging_classifier.predict_log_proba(X)\r\n    bagging_classifier.predict_proba(X)\r\n\r\n    # Verify that exceptions can be raised by wrapper classifier\r\n    classifier = DecisionTreeClassifier()\r\n    pipeline = make_pipeline(classifier)\r\n    with pytest.raises(ValueError):\r\n        pipeline.fit(X, y)\r\n    bagging_classifier = BaggingClassifier(pipeline)\r\n    with pytest.raises(ValueError):\r\n        bagging_classifier.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_forest_y_sparse(csr_container):\r\n    X = [[1, 2, 3]]\r\n    y = csr_container([[4, 5, 6]])\r\n    est = RandomForestClassifier()\r\n    msg = \"sparse multilabel-indicator for y is not supported.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        est.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_gradient_boosting_with_init_pipeline():\r\n    # Check that the init estimator can be a pipeline (see issue #13466)\r\n\r\n    X, y = make_regression(random_state=0)\r\n    init = make_pipeline(LinearRegression())\r\n    gb = GradientBoostingRegressor(init=init)\r\n    gb.fit(X, y)  # pipeline without sample_weight works fine\r\n\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=\"The initial estimator Pipeline does not support sample weights\",\r\n    ):\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))\r\n\r\n    # Passing sample_weight to a pipeline raises a ValueError. This test makes\r\n    # sure we make the distinction between ValueError raised by a pipeline that\r\n    # was passed sample_weight, and a InvalidParameterError raised by a regular\r\n    # estimator whose input checking failed.\r\n    invalid_nu = 1.5\r\n    err_msg = (\r\n        \"The 'nu' parameter of NuSVR must be a float in the\"\r\n        f\" range (0.0, 1.0]. Got {invalid_nu} instead.\"\r\n    )\r\n    with pytest.raises(InvalidParameterError, match=re.escape(err_msg)):\r\n        # Note that NuSVR properly supports sample_weight\r\n        init = NuSVR(gamma=\"auto\", nu=invalid_nu)\r\n        gb = GradientBoostingRegressor(init=init)\r\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_regressor_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        reg = StackingRegressor(**params, cv=3)\r\n        reg.fit(scale(X_diabetes), y, sample_weight=np.ones(X_diabetes.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_prefit_error(stacker, X, y):\r\n    # check that NotFittedError is raised\r\n    # if base estimators are not fitted when cv=\"prefit\"\r\n    with pytest.raises(NotFittedError):\r\n        stacker.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_classifier_estimator_init(params, err_msg):\r\n    ensemble = VotingClassifier(**params)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ensemble.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_classifier_error(y, params, type_err, msg_err):\r\n    with pytest.raises(type_err, match=msg_err):\r\n        clf = StackingClassifier(**params, cv=3)\r\n        clf.fit(scale(X_iris), y, sample_weight=np.ones(X_iris.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_final_estimator_attribute_error():\r\n    \"\"\"Check that we raise the proper AttributeError when the final estimator\r\n    does not implement the `decision_function` method, which is decorated with\r\n    `available_if`.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/28108\r\n    \"\"\"\r\n    X, y = make_classification(random_state=42)\r\n\r\n    estimators = [\r\n        (\"lr\", LogisticRegression()),\r\n        (\"rf\", RandomForestClassifier(n_estimators=2, random_state=42)),\r\n    ]\r\n    # RandomForestClassifier does not implement 'decision_function' and should raise\r\n    # an AttributeError\r\n    final_estimator = RandomForestClassifier(n_estimators=2, random_state=42)\r\n    clf = StackingClassifier(\r\n        estimators=estimators, final_estimator=final_estimator, cv=3\r\n    )\r\n\r\n    outer_msg = \"This 'StackingClassifier' has no attribute 'decision_function'\"\r\n    inner_msg = \"'RandomForestClassifier' object has no attribute 'decision_function'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        clf.fit(X, y).decision_function(X)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_predictproba_hardvoting():\r\n    eclf = VotingClassifier(\r\n        estimators=[(\"lr1\", LogisticRegression()), (\"lr2\", LogisticRegression())],\r\n        voting=\"hard\",\r\n    )\r\n\r\n    inner_msg = \"predict_proba is not available when voting='hard'\"\r\n    outer_msg = \"'VotingClassifier' has no attribute 'predict_proba'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        eclf.predict_proba\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)\r\n\r\n    assert not hasattr(eclf, \"predict_proba\")\r\n    eclf.fit(X_scaled, y)\r\n    assert not hasattr(eclf, \"predict_proba\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_notfitted():\r\n    eclf = VotingClassifier(\r\n        estimators=[(\"lr1\", LogisticRegression()), (\"lr2\", LogisticRegression())],\r\n        voting=\"soft\",\r\n    )\r\n    ereg = VotingRegressor([(\"dr\", DummyRegressor())])\r\n    msg = (\r\n        \"This %s instance is not fitted yet. Call 'fit'\"\r\n        \" with appropriate arguments before using this estimator.\"\r\n    )\r\n    with pytest.raises(NotFittedError, match=msg % \"VotingClassifier\"):\r\n        eclf.predict(X)\r\n    with pytest.raises(NotFittedError, match=msg % \"VotingClassifier\"):\r\n        eclf.predict_proba(X)\r\n    with pytest.raises(NotFittedError, match=msg % \"VotingClassifier\"):\r\n        eclf.transform(X)\r\n    with pytest.raises(NotFittedError, match=msg % \"VotingRegressor\"):\r\n        ereg.predict(X_r)\r\n    with pytest.raises(NotFittedError, match=msg % \"VotingRegressor\"):\r\n        ereg.transform(X_r)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_stacking_cv_influence(stacker, X, y):\r\n    # check that the stacking affects the fit of the final estimator but not\r\n    # the fit of the base estimators\r\n    # note: ConvergenceWarning are catch since we are not worrying about the\r\n    # convergence here\r\n    stacker_cv_3 = clone(stacker)\r\n    stacker_cv_5 = clone(stacker)\r\n\r\n    stacker_cv_3.set_params(cv=3)\r\n    stacker_cv_5.set_params(cv=5)\r\n\r\n    stacker_cv_3.fit(X, y)\r\n    stacker_cv_5.fit(X, y)\r\n\r\n    # the base estimators should be identical\r\n    for est_cv_3, est_cv_5 in zip(stacker_cv_3.estimators_, stacker_cv_5.estimators_):\r\n        assert_allclose(est_cv_3.coef_, est_cv_5.coef_)\r\n\r\n    # the final estimator should be different\r\n    with pytest.raises(AssertionError, match=\"Not equal\"):\r\n        assert_allclose(\r\n            stacker_cv_3.final_estimator_.coef_, stacker_cv_5.final_estimator_.coef_\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sample_weight(global_random_seed):\r\n    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"\r\n    clf1 = LogisticRegression(random_state=global_random_seed)\r\n    clf2 = RandomForestClassifier(n_estimators=10, random_state=global_random_seed)\r\n    clf3 = SVC(probability=True, random_state=global_random_seed)\r\n    eclf1 = VotingClassifier(\r\n        estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"svc\", clf3)], voting=\"soft\"\r\n    ).fit(X_scaled, y, sample_weight=np.ones((len(y),)))\r\n    eclf2 = VotingClassifier(\r\n        estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"svc\", clf3)], voting=\"soft\"\r\n    ).fit(X_scaled, y)\r\n    assert_array_equal(eclf1.predict(X_scaled), eclf2.predict(X_scaled))\r\n    assert_array_almost_equal(\r\n        eclf1.predict_proba(X_scaled), eclf2.predict_proba(X_scaled)\r\n    )\r\n    sample_weight = np.random.RandomState(global_random_seed).uniform(size=(len(y),))\r\n    eclf3 = VotingClassifier(estimators=[(\"lr\", clf1)], voting=\"soft\")\r\n    eclf3.fit(X_scaled, y, sample_weight)\r\n    clf1.fit(X_scaled, y, sample_weight)\r\n    assert_array_equal(eclf3.predict(X_scaled), clf1.predict(X_scaled))\r\n    assert_array_almost_equal(\r\n        eclf3.predict_proba(X_scaled), clf1.predict_proba(X_scaled)\r\n    )\r\n\r\n    # check that an error is raised and indicative if sample_weight is not\r\n    # supported.\r\n    clf4 = KNeighborsClassifier()\r\n    eclf3 = VotingClassifier(\r\n        estimators=[(\"lr\", clf1), (\"svc\", clf3), (\"knn\", clf4)], voting=\"soft\"\r\n    )\r\n    msg = \"Underlying estimator KNeighborsClassifier does not support sample weights.\"\r\n    with pytest.raises(TypeError, match=msg):\r\n        eclf3.fit(X_scaled, y, sample_weight)\r\n\r\n    # check that _fit_single_estimator will raise the right error\r\n    # it should raise the original error if this is not linked to sample_weight\r\n    class ClassifierErrorFit(ClassifierMixin, BaseEstimator):\r\n        def fit(self, X_scaled, y, sample_weight):\r\n            raise TypeError(\"Error unrelated to sample_weight.\")\r\n\r\n    clf = ClassifierErrorFit()\r\n    with pytest.raises(TypeError, match=\"Error unrelated to sample_weight\"):\r\n        clf.fit(X_scaled, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_voting_verbose(estimator, capsys):\r\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\r\n    y = np.array([1, 1, 2, 2])\r\n\r\n    pattern = (\r\n        r\"\\[Voting\\].*\\(1 of 2\\) Processing lr, total=.*\\n\"\r\n        r\"\\[Voting\\].*\\(2 of 2\\) Processing rf, total=.*\\n$\"\r\n    )\r\n    clone(estimator).fit(X, y)\r\n    assert re.match(pattern, capsys.readouterr()[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboost_classifier_sample_weight_error():\r\n    # Test that it gives proper exception on incorrect sample weight.\r\n    clf = AdaBoostClassifier()\r\n    msg = re.escape(\"sample_weight.shape == (1,), expected (6,)\")\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.fit(X, y_class, sample_weight=np.asarray([-1]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_estimator():\r\n    # Test different estimators.\r\n    from sklearn.ensemble import RandomForestClassifier\r\n\r\n    # XXX doesn't work with y_class because RF doesn't support classes_\r\n    # Shouldn't AdaBoost run a LabelBinarizer?\r\n    clf = AdaBoostClassifier(RandomForestClassifier())\r\n    clf.fit(X, y_regr)\r\n\r\n    clf = AdaBoostClassifier(SVC())\r\n    clf.fit(X, y_class)\r\n\r\n    from sklearn.ensemble import RandomForestRegressor\r\n\r\n    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)\r\n    clf.fit(X, y_regr)\r\n\r\n    clf = AdaBoostRegressor(SVR(), random_state=0)\r\n    clf.fit(X, y_regr)\r\n\r\n    # Check that an empty discrete ensemble fails in fit, not predict.\r\n    X_fail = [[1, 1], [1, 1], [1, 1], [1, 1]]\r\n    y_fail = [\"foo\", \"bar\", 1, 2]\r\n    clf = AdaBoostClassifier(SVC())\r\n    with pytest.raises(ValueError, match=\"worse than random\"):\r\n        clf.fit(X_fail, y_fail)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboost_negative_weight_error(model, X, y):\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = -10\r\n\r\n    err_msg = \"Negative values in data passed to `sample_weight`\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        model.fit(X, y, sample_weight=sample_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_adaboostregressor_sample_weight():\r\n    # check that giving weight will have an influence on the error computed\r\n    # for a weak learner\r\n    rng = np.random.RandomState(42)\r\n    X = np.linspace(0, 100, num=1000)\r\n    y = (0.8 * X + 0.2) + (rng.rand(X.shape[0]) * 0.0001)\r\n    X = X.reshape(-1, 1)\r\n\r\n    # add an arbitrary outlier\r\n    X[-1] *= 10\r\n    y[-1] = 10000\r\n\r\n    # random_state=0 ensure that the underlying bootstrap will use the outlier\r\n    regr_no_outlier = AdaBoostRegressor(\r\n        estimator=LinearRegression(), n_estimators=1, random_state=0\r\n    )\r\n    regr_with_weight = clone(regr_no_outlier)\r\n    regr_with_outlier = clone(regr_no_outlier)\r\n\r\n    # fit 3 models:\r\n    # - a model containing the outlier\r\n    # - a model without the outlier\r\n    # - a model containing the outlier but with a null sample-weight\r\n    regr_with_outlier.fit(X, y)\r\n    regr_no_outlier.fit(X[:-1], y[:-1])\r\n    sample_weight = np.ones_like(y)\r\n    sample_weight[-1] = 0\r\n    regr_with_weight.fit(X, y, sample_weight=sample_weight)\r\n\r\n    score_with_outlier = regr_with_outlier.score(X[:-1], y[:-1])\r\n    score_no_outlier = regr_no_outlier.score(X[:-1], y[:-1])\r\n    score_with_weight = regr_with_weight.score(X[:-1], y[:-1])\r\n\r\n    assert score_with_outlier < score_no_outlier\r\n    assert score_with_outlier < score_with_weight\r\n    assert score_no_outlier == pytest.approx(score_with_weight)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pandas_nullable_dtype():\r\n    # Non regression test for https://github.com/scikit-learn/scikit-learn/issues/28317\r\n    pd = pytest.importorskip(\"pandas\")\r\n\r\n    rng = np.random.default_rng(0)\r\n    X = pd.DataFrame({\"a\": rng.integers(10, size=100)}).astype(pd.Int64Dtype())\r\n    y = rng.integers(2, size=100)\r\n\r\n    clf = HistGradientBoostingClassifier()\r\n    clf.fit(X, y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_categorical_bad_encoding_errors(Est, use_pandas, feature_name):\r\n    # Test errors when categories are encoded incorrectly\r\n\r\n    gb = Est(categorical_features=[True], max_bins=2)\r\n\r\n    if use_pandas:\r\n        pd = pytest.importorskip(\"pandas\")\r\n        X = pd.DataFrame({\"f0\": [0, 1, 2]})\r\n    else:\r\n        X = np.array([[0, 1, 2]]).T\r\n    y = np.arange(3)\r\n    msg = (\r\n        f\"Categorical feature {feature_name} is expected to have a \"\r\n        \"cardinality <= 2 but actually has a cardinality of 3.\"\r\n    )\r\n    with pytest.raises(ValueError, match=msg):\r\n        gb.fit(X, y)\r\n\r\n    # nans are ignored in the counts\r\n    X = np.array([[0, 1, np.nan]]).T\r\n    y = np.arange(3)\r\n    gb.fit(X, y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_categorical_spec_errors_with_feature_names(Est):\r\n    pd = pytest.importorskip(\"pandas\")\r\n    n_samples = 10\r\n    X = pd.DataFrame(\r\n        {\r\n            \"f0\": range(n_samples),\r\n            \"f1\": range(n_samples),\r\n            \"f2\": [1.0] * n_samples,\r\n        }\r\n    )\r\n    y = [0, 1] * (n_samples // 2)\r\n\r\n    est = Est(categorical_features=[\"f0\", \"f1\", \"f3\"])\r\n    expected_msg = re.escape(\r\n        \"categorical_features has a item value 'f3' which is not a valid \"\r\n        \"feature name of the training data.\"\r\n    )\r\n    with pytest.raises(ValueError, match=expected_msg):\r\n        est.fit(X, y)\r\n\r\n    est = Est(categorical_features=[\"f0\", \"f1\"])\r\n    expected_msg = re.escape(\r\n        \"categorical_features should be passed as an array of integers or \"\r\n        \"as a boolean mask when the model is fitted on data without feature \"\r\n        \"names.\"\r\n    )\r\n    with pytest.raises(ValueError, match=expected_msg):\r\n        est.fit(X.to_numpy(), y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_no_user_warning_with_scoring():\r\n    \"\"\"Check that no UserWarning is raised when scoring is set.\r\n\r\n    Non-regression test for #22907.\r\n    \"\"\"\r\n    pd = pytest.importorskip(\"pandas\")\r\n    X, y = make_regression(n_samples=50, random_state=0)\r\n    X_df = pd.DataFrame(X, columns=[f\"col{i}\" for i in range(X.shape[1])])\r\n\r\n    est = HistGradientBoostingRegressor(\r\n        random_state=0, scoring=\"neg_mean_absolute_error\", early_stopping=True\r\n    )\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"error\", UserWarning)\r\n        est.fit(X_df, y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_nan_support():\r\n    # Make sure nans are OK if the underlying estimator supports nans\r\n\r\n    rng = np.random.RandomState(0)\r\n    n_samples, n_features = 40, 4\r\n    X, y = make_regression(n_samples, n_features, random_state=0)\r\n    nan_mask = rng.randint(0, 2, size=(n_samples, n_features), dtype=bool)\r\n    X[nan_mask] = np.nan\r\n    sfs = SequentialFeatureSelector(\r\n        HistGradientBoostingRegressor(), n_features_to_select=\"auto\", cv=2\r\n    )\r\n    sfs.fit(X, y)\r\n    sfs.transform(X)\r\n\r\n    with pytest.raises(ValueError, match=\"Input X contains NaN\"):\r\n        # LinearRegression does not support nans\r\n        SequentialFeatureSelector(\r\n            LinearRegression(), n_features_to_select=\"auto\", cv=2\r\n        ).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_n_features_to_select():\r\n    n_features = 5\r\n    X, y = make_regression(n_features=n_features)\r\n    sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=n_features)\r\n    with pytest.raises(ValueError, match=\"n_features_to_select must be < n_features\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_forward_neg_tol_error():\r\n    \"\"\"Check that we raise an error when tol<0 and direction='forward'\"\"\"\r\n    X, y = make_regression(n_features=10, random_state=0)\r\n    sfs = SequentialFeatureSelector(\r\n        LinearRegression(),\r\n        n_features_to_select=\"auto\",\r\n        direction=\"forward\",\r\n        tol=-1e-3,\r\n    )\r\n\r\n    with pytest.raises(ValueError, match=\"tol must be strictly positive\"):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_from_model_estimator_attribute_error():\r\n    \"\"\"Check that we raise the proper AttributeError when the estimator\r\n    does not implement the `partial_fit` method, which is decorated with\r\n    `available_if`.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/28108\r\n    \"\"\"\r\n    # `LinearRegression` does not implement 'partial_fit' and should raise an\r\n    # AttributeError\r\n    from_model = SelectFromModel(estimator=LinearRegression())\r\n\r\n    outer_msg = \"This 'SelectFromModel' has no attribute 'partial_fit'\"\r\n    inner_msg = \"'LinearRegression' object has no attribute 'partial_fit'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        from_model.fit(data, y).partial_fit(data)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fit_rejects_params_with_no_routing_enabled():\r\n    X, y = make_classification(random_state=42)\r\n    est = LinearRegression()\r\n    sfs = SequentialFeatureSelector(estimator=est)\r\n\r\n    with pytest.raises(ValueError, match=\"is only supported if\"):\r\n        sfs.fit(X, y, sample_weight=np.ones_like(y))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_rfe_estimator_attribute_error():\r\n    \"\"\"Check that we raise the proper AttributeError when the estimator\r\n    does not implement the `decision_function` method, which is decorated with\r\n    `available_if`.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/28108\r\n    \"\"\"\r\n    iris = load_iris()\r\n\r\n    # `LinearRegression` does not implement 'decision_function' and should raise an\r\n    # AttributeError\r\n    rfe = RFE(estimator=LinearRegression())\r\n\r\n    outer_msg = \"This 'RFE' has no attribute 'decision_function'\"\r\n    inner_msg = \"'LinearRegression' object has no attribute 'decision_function'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        rfe.fit(iris.data, iris.target).decision_function(iris.data)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_is_fitted(regression_dataset):\r\n    \"\"\"Test that check_is_fitted works on frozen estimators.\"\"\"\r\n    X, y = regression_dataset\r\n\r\n    estimator = LinearRegression()\r\n    frozen = FrozenEstimator(estimator)\r\n    with pytest.raises(NotFittedError):\r\n        check_is_fitted(frozen)\r\n\r\n    estimator = LinearRegression().fit(X, y)\r\n    frozen = FrozenEstimator(estimator)\r\n    check_is_fitted(frozen)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_rfe_n_features_to_select_warning(ClsRFE, param):\r\n    \"\"\"Check if the correct warning is raised when trying to initialize a RFE\r\n    object with a n_features_to_select attribute larger than the number of\r\n    features present in the X variable that is passed to the fit method\r\n    \"\"\"\r\n    X, y = make_classification(n_features=20, random_state=0)\r\n\r\n    with pytest.warns(UserWarning, match=f\"{param}=21 > n_features=20\"):\r\n        # Create RFE/RFECV with n_features_to_select/min_features_to_select\r\n        # larger than the number of features present in the X variable\r\n        clsrfe = ClsRFE(estimator=LogisticRegression(), **{param: 21})\r\n        clsrfe.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_no_y_validation_model_fit(y):\r\n    # Make sure that other non-conventional y labels are not accepted\r\n\r\n    X, clusters = make_blobs(n_features=6)\r\n    sfs = SequentialFeatureSelector(\r\n        KMeans(),\r\n        n_features_to_select=3,\r\n    )\r\n\r\n    with pytest.raises((TypeError, ValueError)):\r\n        sfs.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_sample_weight_size_error():\r\n    \"\"\"Check that we raise an error when the size of `sample_weight` is not\r\n    consistent with `X` and `y`.\r\n    \"\"\"\r\n    est = LogisticRegression()\r\n    (X, y), n_targets = binary_classification_data\r\n    sample_weight = np.ones_like(y)\r\n    est.fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=\"sample_weight.shape ==\"):\r\n        partial_dependence(\r\n            est, X, features=[0], sample_weight=sample_weight[1:], grid_resolution=10\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_mixed_type_categorical():\r\n    \"\"\"Check that we raise a proper error when a column has mixed types and\r\n    the sorting of `np.unique` will fail.\"\"\"\r\n    X = np.array([\"A\", \"B\", \"C\", np.nan], dtype=object).reshape(-1, 1)\r\n    y = np.array([0, 1, 0, 1])\r\n\r\n    from sklearn.preprocessing import OrdinalEncoder\r\n\r\n    clf = make_pipeline(\r\n        OrdinalEncoder(encoded_missing_value=-1),\r\n        LogisticRegression(),\r\n    ).fit(X, y)\r\n    with pytest.raises(ValueError, match=\"The column #0 contains mixed data types\"):\r\n        partial_dependence(clf, X, features=[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_indices(estimator, features):\r\n    X, y = make_classification(random_state=0)\r\n    estimator = clone(estimator).fit(X, y)\r\n\r\n    err_msg = \"all features must be in\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, [features])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_error(estimator, params, err_msg):\r\n    X, y = make_classification(random_state=0)\r\n    estimator = clone(estimator).fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_permutation_importance_no_weights_scoring_function():\r\n    # Creating a scorer function that does not takes sample_weight\r\n    def my_scorer(estimator, X, y):\r\n        return 1\r\n\r\n    # Creating some data and estimator for the permutation test\r\n    x = np.array([[1, 2], [3, 4]])\r\n    y = np.array([1, 2])\r\n    w = np.array([1, 1])\r\n    lr = LinearRegression()\r\n    lr.fit(x, y)\r\n\r\n    # test that permutation_importance does not return error when\r\n    # sample_weight is None\r\n    try:\r\n        permutation_importance(lr, x, y, random_state=1, scoring=my_scorer, n_repeats=1)\r\n    except TypeError:\r\n        pytest.fail(\r\n            \"permutation_test raised an error when using a scorer \"\r\n            \"function that does not accept sample_weight even though \"\r\n            \"sample_weight was None\"\r\n        )\r\n\r\n    # test that permutation_importance raise exception when sample_weight is\r\n    # not None\r\n    with pytest.raises(TypeError):\r\n        permutation_importance(\r\n            lr, x, y, random_state=1, scoring=my_scorer, n_repeats=1, sample_weight=w\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_permutation_importance_max_samples_error():\r\n    \"\"\"Check that a proper error message is raised when `max_samples` is not\r\n    set to a valid input value.\r\n    \"\"\"\r\n    X = np.array([(1.0, 2.0, 3.0, 4.0)]).T\r\n    y = np.array([0, 1, 0, 1])\r\n\r\n    clf = LogisticRegression()\r\n    clf.fit(X, y)\r\n\r\n    err_msg = r\"max_samples must be <= n_samples\"\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        permutation_importance(clf, X, y, max_samples=5)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_partial_dependence_unknown_feature_string(estimator):\r\n    pd = pytest.importorskip(\"pandas\")\r\n    X, y = make_classification(random_state=0)\r\n    df = pd.DataFrame(X)\r\n    estimator = clone(estimator).fit(df, y)\r\n\r\n    features = [\"random\"]\r\n    err_msg = \"A given column is not a column of the dataframe\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        partial_dependence(estimator, df, features)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_input_data_dimension(pyplot):\r\n    \"\"\"Check that we raise an error when `X` does not have exactly 2 features.\"\"\"\r\n    X, y = make_classification(n_samples=10, n_features=4, random_state=0)\r\n\r\n    clf = LogisticRegression().fit(X, y)\r\n    msg = \"n_features must be equal to 2. Got 4 instead.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        DecisionBoundaryDisplay.from_estimator(estimator=clf, X=X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multi_output_multi_class_classifier_error(pyplot, response_method):\r\n    \"\"\"Check that multi-output multi-class classifier raises correct error.\"\"\"\r\n    X = np.asarray([[0, 1], [1, 2]])\r\n    y = np.asarray([[\"tree\", \"cat\"], [\"cat\", \"tree\"]])\r\n    tree = DecisionTreeClassifier().fit(X, y)\r\n\r\n    msg = \"Multi-label and multi-output multi-class classifiers are not supported\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        DecisionBoundaryDisplay.from_estimator(\r\n            tree,\r\n            X,\r\n            response_method=response_method,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_dataframe_support(pyplot, constructor_name):\r\n    \"\"\"Check that passing a dataframe at fit and to the Display does not\r\n    raise warnings.\r\n\r\n    Non-regression test for:\r\n    * https://github.com/scikit-learn/scikit-learn/issues/23311\r\n    * https://github.com/scikit-learn/scikit-learn/issues/28717\r\n    \"\"\"\r\n    df = _convert_container(\r\n        X, constructor_name=constructor_name, columns_name=[\"col_x\", \"col_y\"]\r\n    )\r\n    estimator = LogisticRegression().fit(df, y)\r\n\r\n    with warnings.catch_warnings():\r\n        # no warnings linked to feature names validation should be raised\r\n        warnings.simplefilter(\"error\", UserWarning)\r\n        DecisionBoundaryDisplay.from_estimator(estimator, df, response_method=\"predict\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_class_of_interest_multiclass(pyplot, response_method):\r\n    \"\"\"Check the behaviour of passing `class_of_interest` for plotting the output of\r\n    `predict_proba` and `decision_function` in the multiclass case.\r\n    \"\"\"\r\n    iris = load_iris()\r\n    X = iris.data[:, :2]\r\n    y = iris.target  # the target are numerical labels\r\n    class_of_interest_idx = 2\r\n\r\n    estimator = LogisticRegression().fit(X, y)\r\n    disp = DecisionBoundaryDisplay.from_estimator(\r\n        estimator,\r\n        X,\r\n        response_method=response_method,\r\n        class_of_interest=class_of_interest_idx,\r\n    )\r\n\r\n    # we will check that we plot the expected values as response\r\n    grid = np.concatenate([disp.xx0.reshape(-1, 1), disp.xx1.reshape(-1, 1)], axis=1)\r\n    response = getattr(estimator, response_method)(grid)[:, class_of_interest_idx]\r\n    assert_allclose(response.reshape(*disp.response.shape), disp.response)\r\n\r\n    # make the same test but this time using target as strings\r\n    y = iris.target_names[iris.target]\r\n    estimator = LogisticRegression().fit(X, y)\r\n\r\n    disp = DecisionBoundaryDisplay.from_estimator(\r\n        estimator,\r\n        X,\r\n        response_method=response_method,\r\n        class_of_interest=iris.target_names[class_of_interest_idx],\r\n    )\r\n\r\n    grid = np.concatenate([disp.xx0.reshape(-1, 1), disp.xx1.reshape(-1, 1)], axis=1)\r\n    response = getattr(estimator, response_method)(grid)[:, class_of_interest_idx]\r\n    assert_allclose(response.reshape(*disp.response.shape), disp.response)\r\n\r\n    # check that we raise an error for unknown labels\r\n    # this test should already be handled in `_get_response_values` but we can have this\r\n    # test here as well\r\n    err_msg = \"class_of_interest=2 is not a valid label: It should be one of\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        DecisionBoundaryDisplay.from_estimator(\r\n            estimator,\r\n            X,\r\n            response_method=response_method,\r\n            class_of_interest=class_of_interest_idx,\r\n        )\r\n\r\n    # TODO: remove this test when we handle multiclass with class_of_interest=None\r\n    # by showing the max of the decision function or the max of the predicted\r\n    # probabilities.\r\n    err_msg = \"Multiclass classifiers are only supported\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        DecisionBoundaryDisplay.from_estimator(\r\n            estimator,\r\n            X,\r\n            response_method=response_method,\r\n            class_of_interest=None,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multiclass_error(pyplot, response_method):\r\n    \"\"\"Check multiclass errors.\"\"\"\r\n    X, y = make_classification(n_classes=3, n_informative=3, random_state=0)\r\n    X = X[:, [0, 1]]\r\n    lr = LogisticRegression().fit(X, y)\r\n\r\n    msg = (\r\n        \"Multiclass classifiers are only supported when `response_method` is 'predict'\"\r\n        \" or 'auto'\"\r\n    )\r\n    with pytest.raises(ValueError, match=msg):\r\n        DecisionBoundaryDisplay.from_estimator(lr, X, response_method=response_method)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multilabel_classifier_error(pyplot, response_method):\r\n    \"\"\"Check that multilabel classifier raises correct error.\"\"\"\r\n    X, y = make_multilabel_classification(random_state=0)\r\n    X = X[:, :2]\r\n    tree = DecisionTreeClassifier().fit(X, y)\r\n\r\n    msg = \"Multi-label and multi-output multi-class classifiers are not supported\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        DecisionBoundaryDisplay.from_estimator(\r\n            tree,\r\n            X,\r\n            response_method=response_method,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_partial_dependence_legend(pyplot):\r\n    pd = pytest.importorskip(\"pandas\")\r\n    X = pd.DataFrame(\r\n        {\r\n            \"col_A\": [\"A\", \"B\", \"C\"],\r\n            \"col_B\": [1.0, 0.0, 2.0],\r\n            \"col_C\": [\"C\", \"B\", \"A\"],\r\n        }\r\n    )\r\n    y = np.array([1.2, 0.5, 0.45]).T\r\n\r\n    categorical_features = [\"col_A\", \"col_C\"]\r\n    preprocessor = make_column_transformer((OneHotEncoder(), categorical_features))\r\n    model = make_pipeline(preprocessor, LinearRegression())\r\n    model.fit(X, y)\r\n\r\n    disp = PartialDependenceDisplay.from_estimator(\r\n        model,\r\n        X,\r\n        features=[\"col_B\", \"col_C\"],\r\n        categorical_features=categorical_features,\r\n        kind=[\"both\", \"average\"],\r\n    )\r\n\r\n    legend_text = disp.axes_[0, 0].get_legend().get_texts()\r\n    assert len(legend_text) == 1\r\n    assert legend_text[0].get_text() == \"average\"\r\n    assert disp.axes_[0, 1].get_legend() is None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_partial_dependence_error(pyplot, data, params, err_msg):\r\n    X, y = data\r\n    estimator = LinearRegression().fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        PartialDependenceDisplay.from_estimator(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def fit(self, X, y, **params):\r\n        \"\"\"Fit the model using X, y as training data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,)\r\n            Target values.\r\n\r\n        **params : dict, default=None\r\n            Parameters to be passed to the CV splitter.\r\n\r\n            .. versionadded:: 1.4\r\n                Only available if `enable_metadata_routing=True`,\r\n                which can be set by using\r\n                ``sklearn.set_config(enable_metadata_routing=True)``.\r\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\r\n                more details.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns an instance of self.\r\n        \"\"\"\r\n        _raise_for_params(params, self, \"fit\")\r\n\r\n        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)\r\n        X = as_float_array(X, copy=self.copy_X)\r\n        y = as_float_array(y, copy=self.copy_X)\r\n\r\n        # init cross-validation generator\r\n        cv = check_cv(self.cv, classifier=False)\r\n\r\n        if _routing_enabled():\r\n            routed_params = process_routing(self, \"fit\", **params)\r\n        else:\r\n            routed_params = Bunch(splitter=Bunch(split={}))\r\n\r\n        # As we use cross-validation, the Gram matrix is not precomputed here\r\n        Gram = self.precompute\r\n        if hasattr(Gram, \"__array__\"):\r\n            warnings.warn(\r\n                'Parameter \"precompute\" cannot be an array in '\r\n                '%s. Automatically switch to \"auto\" instead.' % self.__class__.__name__\r\n            )\r\n            Gram = \"auto\"\r\n\r\n        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\r\n            delayed(_lars_path_residues)(\r\n                X[train],\r\n                y[train],\r\n                X[test],\r\n                y[test],\r\n                Gram=Gram,\r\n                copy=False,\r\n                method=self.method,\r\n                verbose=max(0, self.verbose - 1),\r\n                fit_intercept=self.fit_intercept,\r\n                max_iter=self.max_iter,\r\n                eps=self.eps,\r\n                positive=self.positive,\r\n            )\r\n            for train, test in cv.split(X, y, **routed_params.splitter.split)\r\n        )\r\n        all_alphas = np.concatenate(list(zip(*cv_paths))[0])\r\n        # Unique also sorts\r\n        all_alphas = np.unique(all_alphas)\r\n        # Take at most max_n_alphas values\r\n        stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))\r\n        all_alphas = all_alphas[::stride]\r\n\r\n        mse_path = np.empty((len(all_alphas), len(cv_paths)))\r\n        for index, (alphas, _, _, residues) in enumerate(cv_paths):\r\n            alphas = alphas[::-1]\r\n            residues = residues[::-1]\r\n            if alphas[0] != 0:\r\n                alphas = np.r_[0, alphas]\r\n                residues = np.r_[residues[0, np.newaxis], residues]\r\n            if alphas[-1] != all_alphas[-1]:\r\n                alphas = np.r_[alphas, all_alphas[-1]]\r\n                residues = np.r_[residues, residues[-1, np.newaxis]]\r\n            this_residues = interpolate.interp1d(alphas, residues, axis=0)(all_alphas)\r\n            this_residues **= 2\r\n            mse_path[:, index] = np.mean(this_residues, axis=-1)\r\n\r\n        mask = np.all(np.isfinite(mse_path), axis=-1)\r\n        all_alphas = all_alphas[mask]\r\n        mse_path = mse_path[mask]\r\n        # Select the alpha that minimizes left-out error\r\n        i_best_alpha = np.argmin(mse_path.mean(axis=-1))\r\n        best_alpha = all_alphas[i_best_alpha]\r\n\r\n        # Store our parameters\r\n        self.alpha_ = best_alpha\r\n        self.cv_alphas_ = all_alphas\r\n        self.mse_path_ = mse_path\r\n\r\n        # Now compute the full model using best_alpha\r\n        # it will call a lasso internally when self if LassoLarsCV\r\n        # as self.method == 'lasso'\r\n        self._fit(\r\n            X,\r\n            y,\r\n            max_iter=self.max_iter,\r\n            alpha=best_alpha,\r\n            Xy=None,\r\n            fit_path=True,\r\n        )\r\n        return self",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def fit(self, X, y, *, sample_weight=None, **fit_params):\r\n        \"\"\"Fit estimator using RANSAC algorithm.\r\n\r\n        Parameters\r\n        ----------\r\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\r\n            Training data.\r\n\r\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\r\n            Target values.\r\n\r\n        sample_weight : array-like of shape (n_samples,), default=None\r\n            Individual weights for each sample\r\n            raises error if sample_weight is passed and estimator\r\n            fit method does not support it.\r\n\r\n            .. versionadded:: 0.18\r\n\r\n        **fit_params : dict\r\n            Parameters routed to the `fit` method of the sub-estimator via the\r\n            metadata routing API.\r\n\r\n            .. versionadded:: 1.5\r\n\r\n                Only available if\r\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\r\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\r\n                details.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Fitted `RANSACRegressor` estimator.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If no valid consensus set could be found. This occurs if\r\n            `is_data_valid` and `is_model_valid` return False for all\r\n            `max_trials` randomly chosen sub-samples.\r\n        \"\"\"\r\n        # Need to validate separately here. We can't pass multi_output=True\r\n        # because that would allow y to be csr. Delay expensive finiteness\r\n        # check to the estimator's own input validation.\r\n        _raise_for_params(fit_params, self, \"fit\")\r\n        check_X_params = dict(accept_sparse=\"csr\", ensure_all_finite=False)\r\n        check_y_params = dict(ensure_2d=False)\r\n        X, y = validate_data(\r\n            self, X, y, validate_separately=(check_X_params, check_y_params)\r\n        )\r\n        check_consistent_length(X, y)\r\n\r\n        if self.estimator is not None:\r\n            estimator = clone(self.estimator)\r\n        else:\r\n            estimator = LinearRegression()\r\n\r\n        if self.min_samples is None:\r\n            if not isinstance(estimator, LinearRegression):\r\n                raise ValueError(\r\n                    \"`min_samples` needs to be explicitly set when estimator \"\r\n                    \"is not a LinearRegression.\"\r\n                )\r\n            min_samples = X.shape[1] + 1\r\n        elif 0 < self.min_samples < 1:\r\n            min_samples = np.ceil(self.min_samples * X.shape[0])\r\n        elif self.min_samples >= 1:\r\n            min_samples = self.min_samples\r\n        if min_samples > X.shape[0]:\r\n            raise ValueError(\r\n                \"`min_samples` may not be larger than number \"\r\n                \"of samples: n_samples = %d.\" % (X.shape[0])\r\n            )\r\n\r\n        if self.residual_threshold is None:\r\n            # MAD (median absolute deviation)\r\n            residual_threshold = np.median(np.abs(y - np.median(y)))\r\n        else:\r\n            residual_threshold = self.residual_threshold\r\n\r\n        if self.loss == \"absolute_error\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)\r\n            else:\r\n                loss_function = lambda y_true, y_pred: np.sum(\r\n                    np.abs(y_true - y_pred), axis=1\r\n                )\r\n        elif self.loss == \"squared_error\":\r\n            if y.ndim == 1:\r\n                loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2\r\n            else:\r\n                loss_function = lambda y_true, y_pred: np.sum(\r\n                    (y_true - y_pred) ** 2, axis=1\r\n                )\r\n\r\n        elif callable(self.loss):\r\n            loss_function = self.loss\r\n\r\n        random_state = check_random_state(self.random_state)\r\n\r\n        try:  # Not all estimator accept a random_state\r\n            estimator.set_params(random_state=random_state)\r\n        except ValueError:\r\n            pass\r\n\r\n        estimator_fit_has_sample_weight = has_fit_parameter(estimator, \"sample_weight\")\r\n        estimator_name = type(estimator).__name__\r\n        if sample_weight is not None and not estimator_fit_has_sample_weight:\r\n            raise ValueError(\r\n                \"%s does not support sample_weight. Sample\"\r\n                \" weights are only used for the calibration\"\r\n                \" itself.\" % estimator_name\r\n            )\r\n\r\n        if sample_weight is not None:\r\n            fit_params[\"sample_weight\"] = sample_weight\r\n\r\n        if _routing_enabled():\r\n            routed_params = process_routing(self, \"fit\", **fit_params)\r\n        else:\r\n            routed_params = Bunch()\r\n            routed_params.estimator = Bunch(fit={}, predict={}, score={})\r\n            if sample_weight is not None:\r\n                sample_weight = _check_sample_weight(sample_weight, X)\r\n                routed_params.estimator.fit = {\"sample_weight\": sample_weight}\r\n\r\n        n_inliers_best = 1\r\n        score_best = -np.inf\r\n        inlier_mask_best = None\r\n        X_inlier_best = None\r\n        y_inlier_best = None\r\n        inlier_best_idxs_subset = None\r\n        self.n_skips_no_inliers_ = 0\r\n        self.n_skips_invalid_data_ = 0\r\n        self.n_skips_invalid_model_ = 0\r\n\r\n        # number of data samples\r\n        n_samples = X.shape[0]\r\n        sample_idxs = np.arange(n_samples)\r\n\r\n        self.n_trials_ = 0\r\n        max_trials = self.max_trials\r\n        while self.n_trials_ < max_trials:\r\n            self.n_trials_ += 1\r\n\r\n            if (\r\n                self.n_skips_no_inliers_\r\n                + self.n_skips_invalid_data_\r\n                + self.n_skips_invalid_model_\r\n            ) > self.max_skips:\r\n                break\r\n\r\n            # choose random sample set\r\n            subset_idxs = sample_without_replacement(\r\n                n_samples, min_samples, random_state=random_state\r\n            )\r\n            X_subset = X[subset_idxs]\r\n            y_subset = y[subset_idxs]\r\n\r\n            # check if random sample set is valid\r\n            if self.is_data_valid is not None and not self.is_data_valid(\r\n                X_subset, y_subset\r\n            ):\r\n                self.n_skips_invalid_data_ += 1\r\n                continue\r\n\r\n            # cut `fit_params` down to `subset_idxs`\r\n            fit_params_subset = _check_method_params(\r\n                X, params=routed_params.estimator.fit, indices=subset_idxs\r\n            )\r\n\r\n            # fit model for current random sample set\r\n            estimator.fit(X_subset, y_subset, **fit_params_subset)\r\n\r\n            # check if estimated model is valid\r\n            if self.is_model_valid is not None and not self.is_model_valid(\r\n                estimator, X_subset, y_subset\r\n            ):\r\n                self.n_skips_invalid_model_ += 1\r\n                continue\r\n\r\n            # residuals of all data for current random sample model\r\n            y_pred = estimator.predict(X)\r\n            residuals_subset = loss_function(y, y_pred)\r\n\r\n            # classify data into inliers and outliers\r\n            inlier_mask_subset = residuals_subset <= residual_threshold\r\n            n_inliers_subset = np.sum(inlier_mask_subset)\r\n\r\n            # less inliers -> skip current random sample\r\n            if n_inliers_subset < n_inliers_best:\r\n                self.n_skips_no_inliers_ += 1\r\n                continue\r\n\r\n            # extract inlier data set\r\n            inlier_idxs_subset = sample_idxs[inlier_mask_subset]\r\n            X_inlier_subset = X[inlier_idxs_subset]\r\n            y_inlier_subset = y[inlier_idxs_subset]\r\n\r\n            # cut `fit_params` down to `inlier_idxs_subset`\r\n            score_params_inlier_subset = _check_method_params(\r\n                X, params=routed_params.estimator.score, indices=inlier_idxs_subset\r\n            )\r\n\r\n            # score of inlier data set\r\n            score_subset = estimator.score(\r\n                X_inlier_subset,\r\n                y_inlier_subset,\r\n                **score_params_inlier_subset,\r\n            )\r\n\r\n            # same number of inliers but worse score -> skip current random\r\n            # sample\r\n            if n_inliers_subset == n_inliers_best and score_subset < score_best:\r\n                continue\r\n\r\n            # save current random sample as best sample\r\n            n_inliers_best = n_inliers_subset\r\n            score_best = score_subset\r\n            inlier_mask_best = inlier_mask_subset\r\n            X_inlier_best = X_inlier_subset\r\n            y_inlier_best = y_inlier_subset\r\n            inlier_best_idxs_subset = inlier_idxs_subset\r\n\r\n            max_trials = min(\r\n                max_trials,\r\n                _dynamic_max_trials(\r\n                    n_inliers_best, n_samples, min_samples, self.stop_probability\r\n                ),\r\n            )\r\n\r\n            # break if sufficient number of inliers or score is reached\r\n            if n_inliers_best >= self.stop_n_inliers or score_best >= self.stop_score:\r\n                break\r\n\r\n        # if none of the iterations met the required criteria\r\n        if inlier_mask_best is None:\r\n            if (\r\n                self.n_skips_no_inliers_\r\n                + self.n_skips_invalid_data_\r\n                + self.n_skips_invalid_model_\r\n            ) > self.max_skips:\r\n                raise ValueError(\r\n                    \"RANSAC skipped more iterations than `max_skips` without\"\r\n                    \" finding a valid consensus set. Iterations were skipped\"\r\n                    \" because each randomly chosen sub-sample failed the\"\r\n                    \" passing criteria. See estimator attributes for\"\r\n                    \" diagnostics (n_skips*).\"\r\n                )\r\n            else:\r\n                raise ValueError(\r\n                    \"RANSAC could not find a valid consensus set. All\"\r\n                    \" `max_trials` iterations were skipped because each\"\r\n                    \" randomly chosen sub-sample failed the passing criteria.\"\r\n                    \" See estimator attributes for diagnostics (n_skips*).\"\r\n                )\r\n        else:\r\n            if (\r\n                self.n_skips_no_inliers_\r\n                + self.n_skips_invalid_data_\r\n                + self.n_skips_invalid_model_\r\n            ) > self.max_skips:\r\n                warnings.warn(\r\n                    (\r\n                        \"RANSAC found a valid consensus set but exited\"\r\n                        \" early due to skipping more iterations than\"\r\n                        \" `max_skips`. See estimator attributes for\"\r\n                        \" diagnostics (n_skips*).\"\r\n                    ),\r\n                    ConvergenceWarning,\r\n                )\r\n\r\n        # estimate final model using all inliers\r\n        fit_params_best_idxs_subset = _check_method_params(\r\n            X, params=routed_params.estimator.fit, indices=inlier_best_idxs_subset\r\n        )\r\n\r\n        estimator.fit(X_inlier_best, y_inlier_best, **fit_params_best_idxs_subset)\r\n\r\n        self.estimator_ = estimator\r\n        self.inlier_mask_ = inlier_mask_best\r\n        return self",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grid_resolution_with_categorical(pyplot, categorical_features, array_type):\r\n    \"\"\"Check that we raise a ValueError when the grid_resolution is too small\r\n    respect to the number of categories in the categorical features targeted.\r\n    \"\"\"\r\n    X = [[\"A\", 1, \"A\"], [\"B\", 0, \"C\"], [\"C\", 2, \"B\"]]\r\n    column_name = [\"col_A\", \"col_B\", \"col_C\"]\r\n    X = _convert_container(X, array_type, columns_name=column_name)\r\n    y = np.array([1.2, 0.5, 0.45]).T\r\n\r\n    preprocessor = make_column_transformer((OneHotEncoder(), categorical_features))\r\n    model = make_pipeline(preprocessor, LinearRegression())\r\n    model.fit(X, y)\r\n\r\n    err_msg = (\r\n        \"resolution of the computed grid is less than the minimum number of categories\"\r\n    )\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        PartialDependenceDisplay.from_estimator(\r\n            model,\r\n            X,\r\n            features=[\"col_C\"],\r\n            feature_names=column_name,\r\n            categorical_features=categorical_features,\r\n            grid_resolution=2,\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_balance_property(model, with_sample_weight, global_random_seed):\r\n    # Test that sum(y_predicted) == sum(y_observed) on the training set.\r\n    # This must hold for all linear models with deviance of an exponential disperson\r\n    # family as loss and the corresponding canonical link if fit_intercept=True.\r\n    # Examples:\r\n    #     - squared error and identity link (most linear models)\r\n    #     - Poisson deviance with log link\r\n    #     - log loss with logit link\r\n    # This is known as balance property or unconditional calibration/unbiasedness.\r\n    # For reference, see Corollary 3.18, 3.20 and Chapter 5.1.5 of\r\n    # M.V. Wuthrich and M. Merz, \"Statistical Foundations of Actuarial Learning and its\r\n    # Applications\" (June 3, 2022). http://doi.org/10.2139/ssrn.3822407\r\n\r\n    if (\r\n        with_sample_weight\r\n        and \"sample_weight\" not in inspect.signature(model.fit).parameters.keys()\r\n    ):\r\n        pytest.skip(\"Estimator does not support sample_weight.\")\r\n\r\n    rel = 2e-4  # test precision\r\n    if isinstance(model, SGDRegressor):\r\n        rel = 1e-1\r\n    elif hasattr(model, \"solver\") and model.solver == \"saga\":\r\n        rel = 1e-2\r\n\r\n    rng = np.random.RandomState(global_random_seed)\r\n    n_train, n_features, n_targets = 100, 10, None\r\n    if isinstance(\r\n        model,\r\n        (MultiTaskElasticNet, MultiTaskElasticNetCV, MultiTaskLasso, MultiTaskLassoCV),\r\n    ):\r\n        n_targets = 3\r\n    X = make_low_rank_matrix(n_samples=n_train, n_features=n_features, random_state=rng)\r\n    if n_targets:\r\n        coef = (\r\n            rng.uniform(low=-2, high=2, size=(n_features, n_targets))\r\n            / np.max(X, axis=0)[:, None]\r\n        )\r\n    else:\r\n        coef = rng.uniform(low=-2, high=2, size=n_features) / np.max(X, axis=0)\r\n\r\n    expectation = np.exp(X @ coef + 0.5)\r\n    y = rng.poisson(lam=expectation) + 1  # strict positive, i.e. y > 0\r\n    if is_classifier(model):\r\n        y = (y > expectation + 1).astype(np.float64)\r\n\r\n    if with_sample_weight:\r\n        sw = rng.uniform(low=1, high=10, size=y.shape[0])\r\n    else:\r\n        sw = None\r\n\r\n    model.set_params(fit_intercept=True)  # to be sure\r\n    if with_sample_weight:\r\n        model.fit(X, y, sample_weight=sw)\r\n    else:\r\n        model.fit(X, y)\r\n    # Assert balance property.\r\n    if is_classifier(model):\r\n        assert np.average(model.predict_proba(X)[:, 1], weights=sw) == pytest.approx(\r\n            np.average(y, weights=sw), rel=rel\r\n        )\r\n    else:\r\n        assert np.average(model.predict(X), weights=sw, axis=0) == pytest.approx(\r\n            np.average(y, weights=sw, axis=0), rel=rel\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_linear_regression_pd_sparse_dataframe_warning():\r\n    pd = pytest.importorskip(\"pandas\")\r\n\r\n    # Warning is raised only when some of the columns is sparse\r\n    df = pd.DataFrame({\"0\": np.random.randn(10)})\r\n    for col in range(1, 4):\r\n        arr = np.random.randn(10)\r\n        arr[:8] = 0\r\n        # all columns but the first column is sparse\r\n        if col != 0:\r\n            arr = pd.arrays.SparseArray(arr, fill_value=0)\r\n        df[str(col)] = arr\r\n\r\n    msg = \"pandas.DataFrame with sparse columns found.\"\r\n\r\n    reg = LinearRegression()\r\n    with pytest.warns(UserWarning, match=msg):\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])\r\n\r\n    # does not warn when the whole dataframe is sparse\r\n    df[\"0\"] = pd.arrays.SparseArray(df[\"0\"], fill_value=0)\r\n    assert hasattr(df, \"sparse\")\r\n\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"error\", UserWarning)\r\n        reg.fit(df.iloc[:, 0:2], df.iloc[:, 3])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_sparse_input_convergence_warning(csr_container):\r\n    X, y, _, _ = build_dataset(n_samples=1000, n_features=500)\r\n\r\n    with pytest.warns(ConvergenceWarning):\r\n        ElasticNet(max_iter=1, tol=0).fit(csr_container(X, dtype=np.float32), y)\r\n\r\n    # check that the model converges w/o convergence warnings\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"error\", ConvergenceWarning)\r\n        Lasso().fit(csr_container(X, dtype=np.float32), y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_lasso_lars_vs_lasso_cd_ill_conditioned2():\r\n    # Create an ill-conditioned situation in which the LARS has to go\r\n    # far in the path to converge, and check that LARS and coordinate\r\n    # descent give the same answers\r\n    # Note it used to be the case that Lars had to use the drop for good\r\n    # strategy for this but this is no longer the case with the\r\n    # equality_tolerance checks\r\n    X = [[1e20, 1e20, 0], [-1e-32, 0, 0], [1, 1, 1]]\r\n    y = [10, 10, 1]\r\n    alpha = 0.0001\r\n\r\n    def objective_function(coef):\r\n        return 1.0 / (2.0 * len(X)) * linalg.norm(\r\n            y - np.dot(X, coef)\r\n        ) ** 2 + alpha * linalg.norm(coef, 1)\r\n\r\n    lars = linear_model.LassoLars(alpha=alpha)\r\n    warning_message = \"Regressors in active set degenerate.\"\r\n    with pytest.warns(ConvergenceWarning, match=warning_message):\r\n        lars.fit(X, y)\r\n    lars_coef_ = lars.coef_\r\n    lars_obj = objective_function(lars_coef_)\r\n\r\n    coord_descent = linear_model.Lasso(alpha=alpha, tol=1e-4)\r\n    cd_coef_ = coord_descent.fit(X, y).coef_\r\n    cd_obj = objective_function(cd_coef_)\r\n\r\n    assert lars_obj < cd_obj * (1.0 + 1e-8)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_elasticnet_precompute_incorrect_gram():\r\n    # check that passing an invalid precomputed Gram matrix will raise an\r\n    # error.\r\n    X, y, _, _ = build_dataset()\r\n\r\n    rng = np.random.RandomState(0)\r\n\r\n    X_centered = X - np.average(X, axis=0)\r\n    garbage = rng.standard_normal(X.shape)\r\n    precompute = np.dot(garbage.T, garbage)\r\n\r\n    clf = ElasticNet(alpha=0.01, precompute=precompute)\r\n    msg = \"Gram matrix.*did not pass validation.*\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.fit(X_centered, y)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_enet_cv_grid_search(sample_weight):\r\n    \"\"\"Test that ElasticNetCV gives same result as GridSearchCV.\"\"\"\r\n    n_samples, n_features = 200, 10\r\n    cv = 5\r\n    X, y = make_regression(\r\n        n_samples=n_samples,\r\n        n_features=n_features,\r\n        effective_rank=10,\r\n        n_informative=n_features - 4,\r\n        noise=10,\r\n        random_state=0,\r\n    )\r\n    if sample_weight:\r\n        sample_weight = np.linspace(1, 5, num=n_samples)\r\n    else:\r\n        sample_weight = None\r\n\r\n    alphas = np.logspace(np.log10(1e-5), np.log10(1), num=10)\r\n    l1_ratios = [0.1, 0.5, 0.9]\r\n    reg = ElasticNetCV(cv=cv, alphas=alphas, l1_ratio=l1_ratios)\r\n    reg.fit(X, y, sample_weight=sample_weight)\r\n\r\n    param = {\"alpha\": alphas, \"l1_ratio\": l1_ratios}\r\n    gs = GridSearchCV(\r\n        estimator=ElasticNet(),\r\n        param_grid=param,\r\n        cv=cv,\r\n        scoring=\"neg_mean_squared_error\",\r\n    ).fit(X, y, sample_weight=sample_weight)\r\n\r\n    assert reg.l1_ratio_ == pytest.approx(gs.best_params_[\"l1_ratio\"])\r\n    assert reg.alpha_ == pytest.approx(gs.best_params_[\"alpha\"])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_is_model_valid():\r\n    def is_model_valid(estimator, X, y):\r\n        assert X.shape[0] == 2\r\n        assert y.shape[0] == 2\r\n        return False\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator,\r\n        min_samples=2,\r\n        residual_threshold=5,\r\n        is_model_valid=is_model_valid,\r\n        random_state=0,\r\n    )\r\n    with pytest.raises(ValueError):\r\n        ransac_estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_exceed_max_skips():\r\n    def is_data_valid(X, y):\r\n        return False\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator, is_data_valid=is_data_valid, max_trials=5, max_skips=3\r\n    )\r\n\r\n    msg = \"RANSAC skipped more iterations than `max_skips`\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ransac_estimator.fit(X, y)\r\n    assert ransac_estimator.n_skips_no_inliers_ == 0\r\n    assert ransac_estimator.n_skips_invalid_data_ == 4\r\n    assert ransac_estimator.n_skips_invalid_model_ == 0",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_max_trials():\r\n    estimator = LinearRegression()\r\n\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator,\r\n        min_samples=2,\r\n        residual_threshold=5,\r\n        max_trials=0,\r\n        random_state=0,\r\n    )\r\n    with pytest.raises(ValueError):\r\n        ransac_estimator.fit(X, y)\r\n\r\n    # there is a 1e-9 chance it will take these many trials. No good reason\r\n    # 1e-2 isn't enough, can still happen\r\n    # 2 is the what ransac defines  as min_samples = X.shape[1] + 1\r\n    max_trials = _dynamic_max_trials(len(X) - len(outliers), X.shape[0], 2, 1 - 1e-9)\r\n    ransac_estimator = RANSACRegressor(estimator, min_samples=2)\r\n    for i in range(50):\r\n        ransac_estimator.set_params(min_samples=2, random_state=i)\r\n        ransac_estimator.fit(X, y)\r\n        assert ransac_estimator.n_trials_ < max_trials + 1",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_is_data_valid():\r\n    def is_data_valid(X, y):\r\n        assert X.shape[0] == 2\r\n        assert y.shape[0] == 2\r\n        return False\r\n\r\n    rng = np.random.RandomState(0)\r\n    X = rng.rand(10, 2)\r\n    y = rng.rand(10, 1)\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator,\r\n        min_samples=2,\r\n        residual_threshold=5,\r\n        is_data_valid=is_data_valid,\r\n        random_state=0,\r\n    )\r\n    with pytest.raises(ValueError):\r\n        ransac_estimator.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_no_valid_data():\r\n    def is_data_valid(X, y):\r\n        return False\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator, is_data_valid=is_data_valid, max_trials=5\r\n    )\r\n\r\n    msg = \"RANSAC could not find a valid consensus set\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ransac_estimator.fit(X, y)\r\n    assert ransac_estimator.n_skips_no_inliers_ == 0\r\n    assert ransac_estimator.n_skips_invalid_data_ == 5\r\n    assert ransac_estimator.n_skips_invalid_model_ == 0",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_min_n_samples():\r\n    estimator = LinearRegression()\r\n    ransac_estimator1 = RANSACRegressor(\r\n        estimator, min_samples=2, residual_threshold=5, random_state=0\r\n    )\r\n    ransac_estimator2 = RANSACRegressor(\r\n        estimator,\r\n        min_samples=2.0 / X.shape[0],\r\n        residual_threshold=5,\r\n        random_state=0,\r\n    )\r\n    ransac_estimator5 = RANSACRegressor(\r\n        estimator, min_samples=2, residual_threshold=5, random_state=0\r\n    )\r\n    ransac_estimator6 = RANSACRegressor(estimator, residual_threshold=5, random_state=0)\r\n    ransac_estimator7 = RANSACRegressor(\r\n        estimator, min_samples=X.shape[0] + 1, residual_threshold=5, random_state=0\r\n    )\r\n    # GH #19390\r\n    ransac_estimator8 = RANSACRegressor(\r\n        Ridge(), min_samples=None, residual_threshold=5, random_state=0\r\n    )\r\n\r\n    ransac_estimator1.fit(X, y)\r\n    ransac_estimator2.fit(X, y)\r\n    ransac_estimator5.fit(X, y)\r\n    ransac_estimator6.fit(X, y)\r\n\r\n    assert_array_almost_equal(\r\n        ransac_estimator1.predict(X), ransac_estimator2.predict(X)\r\n    )\r\n    assert_array_almost_equal(\r\n        ransac_estimator1.predict(X), ransac_estimator5.predict(X)\r\n    )\r\n    assert_array_almost_equal(\r\n        ransac_estimator1.predict(X), ransac_estimator6.predict(X)\r\n    )\r\n\r\n    with pytest.raises(ValueError):\r\n        ransac_estimator7.fit(X, y)\r\n\r\n    err_msg = \"`min_samples` needs to be explicitly set\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ransac_estimator8.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_no_valid_model():\r\n    def is_model_valid(estimator, X, y):\r\n        return False\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator, is_model_valid=is_model_valid, max_trials=5\r\n    )\r\n\r\n    msg = \"RANSAC could not find a valid consensus set\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ransac_estimator.fit(X, y)\r\n    assert ransac_estimator.n_skips_no_inliers_ == 0\r\n    assert ransac_estimator.n_skips_invalid_data_ == 0\r\n    assert ransac_estimator.n_skips_invalid_model_ == 5",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ransac_warn_exceed_max_skips():\r\n    global cause_skip\r\n    cause_skip = False\r\n\r\n    def is_data_valid(X, y):\r\n        global cause_skip\r\n        if not cause_skip:\r\n            cause_skip = True\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    estimator = LinearRegression()\r\n    ransac_estimator = RANSACRegressor(\r\n        estimator, is_data_valid=is_data_valid, max_skips=3, max_trials=5\r\n    )\r\n    warning_message = (\r\n        \"RANSAC found a valid consensus set but exited \"\r\n        \"early due to skipping more iterations than \"\r\n        \"`max_skips`. See estimator attributes for \"\r\n        \"diagnostics.\"\r\n    )\r\n    with pytest.warns(ConvergenceWarning, match=warning_message):\r\n        ransac_estimator.fit(X, y)\r\n    assert ransac_estimator.n_skips_no_inliers_ == 0\r\n    assert ransac_estimator.n_skips_invalid_data_ == 4\r\n    assert ransac_estimator.n_skips_invalid_model_ == 0",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_array_api_error_and_warnings_for_solver_parameter(array_namespace):\r\n    xp = _array_api_for_tests(array_namespace, device=None)\r\n\r\n    X_iris_xp = xp.asarray(X_iris[:5])\r\n    y_iris_xp = xp.asarray(y_iris[:5])\r\n\r\n    available_solvers = Ridge._parameter_constraints[\"solver\"][0].options\r\n    for solver in available_solvers - {\"auto\", \"svd\"}:\r\n        ridge = Ridge(solver=solver, positive=solver == \"lbfgs\")\r\n        expected_msg = (\r\n            f\"Array API dispatch to namespace {xp.__name__} only supports \"\r\n            f\"solver 'svd'. Got '{solver}'.\"\r\n        )\r\n\r\n        with pytest.raises(ValueError, match=expected_msg):\r\n            with config_context(array_api_dispatch=True):\r\n                ridge.fit(X_iris_xp, y_iris_xp)\r\n\r\n    ridge = Ridge(solver=\"auto\", positive=True)\r\n    expected_msg = (\r\n        \"The solvers that support positive fitting do not support \"\r\n        f\"Array API dispatch to namespace {xp.__name__}. Please \"\r\n        \"either disable Array API dispatch, or use a numpy-like \"\r\n        \"namespace, or set `positive=False`.\"\r\n    )\r\n\r\n    with pytest.raises(ValueError, match=expected_msg):\r\n        with config_context(array_api_dispatch=True):\r\n            ridge.fit(X_iris_xp, y_iris_xp)\r\n\r\n    ridge = Ridge()\r\n    expected_msg = (\r\n        f\"Using Array API dispatch to namespace {xp.__name__} with `solver='auto'` \"\r\n        \"will result in using the solver 'svd'. The results may differ from those \"\r\n        \"when using a Numpy array, because in that case the preferred solver would \"\r\n        \"be cholesky. Set `solver='svd'` to suppress this warning.\"\r\n    )\r\n    with pytest.warns(UserWarning, match=expected_msg):\r\n        with config_context(array_api_dispatch=True):\r\n            ridge.fit(X_iris_xp, y_iris_xp)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_array_api_numpy_namespace_no_warning(array_namespace):\r\n    xp = _array_api_for_tests(array_namespace, device=None)\r\n\r\n    X_iris_xp = xp.asarray(X_iris[:5])\r\n    y_iris_xp = xp.asarray(y_iris[:5])\r\n\r\n    ridge = Ridge()\r\n    expected_msg = (\r\n        \"Results might be different than when Array API dispatch is \"\r\n        \"disabled, or when a numpy-like namespace is used\"\r\n    )\r\n\r\n    with warnings.catch_warnings():\r\n        warnings.filterwarnings(\"error\", message=expected_msg, category=UserWarning)\r\n        with config_context(array_api_dispatch=True):\r\n            ridge.fit(X_iris_xp, y_iris_xp)\r\n\r\n    # All numpy namespaces are compatible with all solver, in particular\r\n    # solvers that support `positive=True` (like 'lbfgs') should work.\r\n    with config_context(array_api_dispatch=True):\r\n        Ridge(solver=\"auto\", positive=True).fit(X_iris_xp, y_iris_xp)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_underflow_or_overlow():\r\n    with np.errstate(all=\"raise\"):\r\n        # Generate some weird data with hugely unscaled features\r\n        rng = np.random.RandomState(0)\r\n        n_samples = 100\r\n        n_features = 10\r\n\r\n        X = rng.normal(size=(n_samples, n_features))\r\n        X[:, :2] *= 1e300\r\n        assert np.isfinite(X).all()\r\n\r\n        # Use MinMaxScaler to scale the data without introducing a numerical\r\n        # instability (computing the standard deviation naively is not possible\r\n        # on this data)\r\n        X_scaled = MinMaxScaler().fit_transform(X)\r\n        assert np.isfinite(X_scaled).all()\r\n\r\n        # Define a ground truth on the scaled data\r\n        ground_truth = rng.normal(size=n_features)\r\n        y = (np.dot(X_scaled, ground_truth) > 0.0).astype(np.int32)\r\n        assert_array_equal(np.unique(y), [0, 1])\r\n\r\n        model = SGDClassifier(alpha=0.1, loss=\"squared_hinge\", max_iter=500)\r\n\r\n        # smoke test: model is stable on scaled data\r\n        model.fit(X_scaled, y)\r\n        assert np.isfinite(model.coef_).all()\r\n\r\n        # model is numerically unstable on unscaled data\r\n        msg_regxp = (\r\n            r\"Floating-point under-/overflow occurred at epoch #.*\"\r\n            \" Scaling input data with StandardScaler or MinMaxScaler\"\r\n            \" might help.\"\r\n        )\r\n        with pytest.raises(ValueError, match=msg_regxp):\r\n            model.fit(X, y)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_kwargs_without_metadata_routing_error():\r\n    # Test that kwargs are not supported in scorers if metadata routing is not\r\n    # enabled.\r\n    # TODO: remove when enable_metadata_routing is deprecated\r\n    def score(y_true, y_pred, param=None):\r\n        return 1  # pragma: no cover\r\n\r\n    X, y = make_classification(\r\n        n_samples=50, n_features=2, n_redundant=0, random_state=0\r\n    )\r\n\r\n    clf = DecisionTreeClassifier().fit(X, y)\r\n    scorer = make_scorer(score)\r\n    with config_context(enable_metadata_routing=False):\r\n        with pytest.raises(\r\n            ValueError, match=\"is only supported if enable_metadata_routing=True\"\r\n        ):\r\n            scorer(clf, X, y, param=\"blah\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_scoring_multimetric_raise_exc():\r\n    \"\"\"Test that check_scoring returns error code for a subset of scorers in\r\n    multimetric scoring if raise_exc=False and raises otherwise.\"\"\"\r\n\r\n    def raising_scorer(estimator, X, y):\r\n        raise ValueError(\"That doesn't work.\")\r\n\r\n    X, y = make_classification(n_samples=150, n_features=10, random_state=0)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = LogisticRegression().fit(X_train, y_train)\r\n\r\n    # \"raising_scorer\" is raising ValueError and should return an string representation\r\n    # of the error of the last scorer:\r\n    scoring = {\r\n        \"accuracy\": make_scorer(accuracy_score),\r\n        \"raising_scorer\": raising_scorer,\r\n    }\r\n    scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=False)\r\n    scores = scoring_call(clf, X_test, y_test)\r\n    assert \"That doesn't work.\" in scores[\"raising_scorer\"]\r\n\r\n    # should raise an error\r\n    scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=True)\r\n    err_msg = \"That doesn't work.\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        scores = scoring_call(clf, X_test, y_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_curve_scorer_pos_label(global_random_seed):\r\n    \"\"\"Check that we propagate properly the `pos_label` parameter to the scorer.\"\"\"\r\n    n_samples = 30\r\n    X, y = make_classification(\r\n        n_samples=n_samples, weights=[0.9, 0.1], random_state=global_random_seed\r\n    )\r\n    estimator = LogisticRegression().fit(X, y)\r\n\r\n    curve_scorer = _CurveScorer(\r\n        recall_score,\r\n        sign=1,\r\n        response_method=\"predict_proba\",\r\n        thresholds=10,\r\n        kwargs={\"pos_label\": 1},\r\n    )\r\n    scores_pos_label_1, thresholds_pos_label_1 = curve_scorer(estimator, X, y)\r\n\r\n    curve_scorer = _CurveScorer(\r\n        recall_score,\r\n        sign=1,\r\n        response_method=\"predict_proba\",\r\n        thresholds=10,\r\n        kwargs={\"pos_label\": 0},\r\n    )\r\n    scores_pos_label_0, thresholds_pos_label_0 = curve_scorer(estimator, X, y)\r\n\r\n    # Since `pos_label` is forwarded to the curve_scorer, the thresholds are not equal.\r\n    assert not (thresholds_pos_label_1 == thresholds_pos_label_0).all()\r\n    # The min-max range for the thresholds is defined by the probabilities of the\r\n    # `pos_label` class (the column of `predict_proba`).\r\n    y_pred = estimator.predict_proba(X)\r\n    assert thresholds_pos_label_0.min() == pytest.approx(y_pred.min(axis=0)[0])\r\n    assert thresholds_pos_label_0.max() == pytest.approx(y_pred.max(axis=0)[0])\r\n    assert thresholds_pos_label_1.min() == pytest.approx(y_pred.min(axis=0)[1])\r\n    assert thresholds_pos_label_1.max() == pytest.approx(y_pred.max(axis=0)[1])\r\n\r\n    # The recall cannot be negative and `pos_label=1` should have a higher recall\r\n    # since there is less samples to be considered.\r\n    assert 0.0 < scores_pos_label_0.min() < scores_pos_label_1.min()\r\n    assert scores_pos_label_0.max() == pytest.approx(1.0)\r\n    assert scores_pos_label_1.max() == pytest.approx(1.0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_display_curve_error_classifier(pyplot, data, data_binary, Display):\r\n    \"\"\"Check that a proper error is raised when only binary classification is\r\n    supported.\"\"\"\r\n    X, y = data\r\n    X_binary, y_binary = data_binary\r\n    clf = DecisionTreeClassifier().fit(X, y)\r\n\r\n    # Case 1: multiclass classifier with multiclass target\r\n    msg = \"Expected 'estimator' to be a binary classifier. Got 3 classes instead.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(clf, X, y)\r\n\r\n    # Case 2: multiclass classifier with binary target\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(clf, X_binary, y_binary)\r\n\r\n    # Case 3: binary classifier with multiclass target\r\n    clf = DecisionTreeClassifier().fit(X_binary, y_binary)\r\n    msg = \"The target y is not binary. Got multiclass type of target.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(clf, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_display_curve_error_regression(pyplot, data_binary, Display):\r\n    \"\"\"Check that we raise an error with regressor.\"\"\"\r\n\r\n    # Case 1: regressor\r\n    X, y = data_binary\r\n    regressor = DecisionTreeRegressor().fit(X, y)\r\n\r\n    msg = \"Expected 'estimator' to be a binary classifier. Got DecisionTreeRegressor\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(regressor, X, y)\r\n\r\n    # Case 2: regression target\r\n    classifier = DecisionTreeClassifier().fit(X, y)\r\n    # Force `y_true` to be seen as a regression problem\r\n    y = y + 0.5\r\n    msg = \"The target y is not binary. Got continuous type of target.\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(classifier, X, y)\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_predictions(y, regressor.fit(X, y).predict(X))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_display_curve_not_fitted_errors(pyplot, data_binary, clf, Display):\r\n    \"\"\"Check that a proper error is raised when the classifier is not\r\n    fitted.\"\"\"\r\n    X, y = data_binary\r\n    # clone since we parametrize the test and the classifier will be fitted\r\n    # when testing the second and subsequent plotting function\r\n    model = clone(clf)\r\n    with pytest.raises(NotFittedError):\r\n        Display.from_estimator(model, X, y)\r\n    model.fit(X, y)\r\n    disp = Display.from_estimator(model, X, y)\r\n    assert model.__class__.__name__ in disp.line_.get_label()\r\n    assert disp.estimator_name == model.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_display_curve_n_samples_consistency(pyplot, data_binary, Display):\r\n    \"\"\"Check the error raised when `y_pred` or `sample_weight` have inconsistent\r\n    length.\"\"\"\r\n    X, y = data_binary\r\n    classifier = DecisionTreeClassifier().fit(X, y)\r\n\r\n    msg = \"Found input variables with inconsistent numbers of samples\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(classifier, X[:-2], y)\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(classifier, X, y[:-2])\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_estimator(classifier, X, y, sample_weight=np.ones(X.shape[0] - 2))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_display_curve_error_pos_label(pyplot, data_binary, Display):\r\n    \"\"\"Check consistence of error message when `pos_label` should be specified.\"\"\"\r\n    X, y = data_binary\r\n    y = y + 10\r\n\r\n    classifier = DecisionTreeClassifier().fit(X, y)\r\n    y_pred = classifier.predict_proba(X)[:, -1]\r\n    msg = r\"y_true takes value in {10, 11} and pos_label is not specified\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        Display.from_predictions(y, y_pred)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_raises_on_score_list():\r\n    # Test that when a list of scores is returned, we raise proper errors.\r\n    X, y = make_blobs(random_state=0)\r\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\r\n    clf = DecisionTreeClassifier()\r\n    with pytest.raises(ValueError):\r\n        cross_val_score(clf, X, y, scoring=f1_scorer_no_average)\r\n    grid_search = GridSearchCV(\r\n        clf, scoring=f1_scorer_no_average, param_grid={\"max_depth\": [1, 2]}\r\n    )\r\n    with pytest.raises(ValueError):\r\n        grid_search.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_confusion_matrix_display_validation(pyplot):\r\n    \"\"\"Check that we raise the proper error when validating parameters.\"\"\"\r\n    X, y = make_classification(\r\n        n_samples=100, n_informative=5, n_classes=5, random_state=0\r\n    )\r\n\r\n    with pytest.raises(NotFittedError):\r\n        ConfusionMatrixDisplay.from_estimator(SVC(), X, y)\r\n\r\n    regressor = SVR().fit(X, y)\r\n    y_pred_regressor = regressor.predict(X)\r\n    y_pred_classifier = SVC().fit(X, y).predict(X)\r\n\r\n    err_msg = \"ConfusionMatrixDisplay.from_estimator only supports classifiers\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ConfusionMatrixDisplay.from_estimator(regressor, X, y)\r\n\r\n    err_msg = \"Mix type of y not allowed, got types\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        # Force `y_true` to be seen as a regression problem\r\n        ConfusionMatrixDisplay.from_predictions(y + 0.5, y_pred_classifier)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ConfusionMatrixDisplay.from_predictions(y, y_pred_regressor)\r\n\r\n    err_msg = \"Found input variables with inconsistent numbers of samples\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        ConfusionMatrixDisplay.from_predictions(y, y_pred_classifier[::2])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multiclass_roc_proba_scorer(scorer_name, metric):\r\n    scorer = get_scorer(scorer_name)\r\n    X, y = make_classification(\r\n        n_classes=3, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n    y_proba = lr.predict_proba(X)\r\n    expected_score = metric(y, y_proba)\r\n\r\n    assert scorer(lr, X, y) == pytest.approx(expected_score)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_display_plotting(\r\n    pyplot, constructor_name, response_method, drop_intermediate\r\n):\r\n    \"\"\"Check the overall plotting rendering.\"\"\"\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n    pos_label = 1\r\n\r\n    classifier = LogisticRegression().fit(X, y)\r\n    classifier.fit(X, y)\r\n\r\n    y_pred = getattr(classifier, response_method)(X)\r\n    y_pred = y_pred if y_pred.ndim == 1 else y_pred[:, pos_label]\r\n\r\n    # safe guard for the binary if/else construction\r\n    assert constructor_name in (\"from_estimator\", \"from_predictions\")\r\n\r\n    if constructor_name == \"from_estimator\":\r\n        display = PrecisionRecallDisplay.from_estimator(\r\n            classifier,\r\n            X,\r\n            y,\r\n            response_method=response_method,\r\n            drop_intermediate=drop_intermediate,\r\n        )\r\n    else:\r\n        display = PrecisionRecallDisplay.from_predictions(\r\n            y, y_pred, pos_label=pos_label, drop_intermediate=drop_intermediate\r\n        )\r\n\r\n    precision, recall, _ = precision_recall_curve(\r\n        y, y_pred, pos_label=pos_label, drop_intermediate=drop_intermediate\r\n    )\r\n    average_precision = average_precision_score(y, y_pred, pos_label=pos_label)\r\n\r\n    np.testing.assert_allclose(display.precision, precision)\r\n    np.testing.assert_allclose(display.recall, recall)\r\n    assert display.average_precision == pytest.approx(average_precision)\r\n\r\n    import matplotlib as mpl\r\n\r\n    assert isinstance(display.line_, mpl.lines.Line2D)\r\n    assert isinstance(display.ax_, mpl.axes.Axes)\r\n    assert isinstance(display.figure_, mpl.figure.Figure)\r\n\r\n    assert display.ax_.get_xlabel() == \"Recall (Positive label: 1)\"\r\n    assert display.ax_.get_ylabel() == \"Precision (Positive label: 1)\"\r\n    assert display.ax_.get_adjustable() == \"box\"\r\n    assert display.ax_.get_aspect() in (\"equal\", 1.0)\r\n    assert display.ax_.get_xlim() == display.ax_.get_ylim() == (-0.01, 1.01)\r\n\r\n    # plotting passing some new parameters\r\n    display.plot(alpha=0.8, name=\"MySpecialEstimator\")\r\n    expected_label = f\"MySpecialEstimator (AP = {average_precision:0.2f})\"\r\n    assert display.line_.get_label() == expected_label\r\n    assert display.line_.get_alpha() == pytest.approx(0.8)\r\n\r\n    # Check that the chance level line is not plotted by default\r\n    assert display.chance_level_ is None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_confusion_matrix_pipeline(pyplot, clf):\r\n    \"\"\"Check the behaviour of the plotting with more complex pipeline.\"\"\"\r\n    n_classes = 5\r\n    X, y = make_classification(\r\n        n_samples=100, n_informative=5, n_classes=n_classes, random_state=0\r\n    )\r\n    with pytest.raises(NotFittedError):\r\n        ConfusionMatrixDisplay.from_estimator(clf, X, y)\r\n    clf.fit(X, y)\r\n    y_pred = clf.predict(X)\r\n\r\n    disp = ConfusionMatrixDisplay.from_estimator(clf, X, y)\r\n    cm = confusion_matrix(y, y_pred)\r\n\r\n    assert_allclose(disp.confusion_matrix, cm)\r\n    assert disp.text_.shape == (n_classes, n_classes)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_display_string_labels(pyplot):\r\n    # regression test #15738\r\n    cancer = load_breast_cancer()\r\n    X, y = cancer.data, cancer.target_names[cancer.target]\r\n\r\n    lr = make_pipeline(StandardScaler(), LogisticRegression())\r\n    lr.fit(X, y)\r\n    for klass in cancer.target_names:\r\n        assert klass in lr.classes_\r\n    display = PrecisionRecallDisplay.from_estimator(lr, X, y)\r\n\r\n    y_pred = lr.predict_proba(X)[:, 1]\r\n    avg_prec = average_precision_score(y, y_pred, pos_label=lr.classes_[1])\r\n\r\n    assert display.average_precision == pytest.approx(avg_prec)\r\n    assert display.estimator_name == lr.__class__.__name__\r\n\r\n    err_msg = r\"y_true takes value in {'benign', 'malignant'}\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        PrecisionRecallDisplay.from_predictions(y, y_pred)\r\n\r\n    display = PrecisionRecallDisplay.from_predictions(\r\n        y, y_pred, pos_label=lr.classes_[1]\r\n    )\r\n    assert display.average_precision == pytest.approx(avg_prec)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_prediction_error_display_raise_error(\r\n    pyplot, class_method, regressor, params, err_type, err_msg\r\n):\r\n    \"\"\"Check that we raise the proper error when making the parameters\r\n    # validation.\"\"\"\r\n    with pytest.raises(err_type, match=err_msg):\r\n        if class_method == \"from_estimator\":\r\n            PredictionErrorDisplay.from_estimator(regressor, X, y, **params)\r\n        else:\r\n            y_pred = regressor.predict(X)\r\n            PredictionErrorDisplay.from_predictions(y_true=y, y_pred=y_pred, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_roc_curve_display_complex_pipeline(pyplot, data_binary, clf, constructor_name):\r\n    \"\"\"Check the behaviour with complex pipeline.\"\"\"\r\n    X, y = data_binary\r\n\r\n    clf = clone(clf)\r\n\r\n    if constructor_name == \"from_estimator\":\r\n        with pytest.raises(NotFittedError):\r\n            RocCurveDisplay.from_estimator(clf, X, y)\r\n\r\n    clf.fit(X, y)\r\n\r\n    if constructor_name == \"from_estimator\":\r\n        display = RocCurveDisplay.from_estimator(clf, X, y)\r\n        name = clf.__class__.__name__\r\n    else:\r\n        display = RocCurveDisplay.from_predictions(y, y)\r\n        name = \"Classifier\"\r\n\r\n    assert name in display.line_.get_label()\r\n    assert display.estimator_name == name",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precision_recall_display_pipeline(pyplot, clf):\r\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\r\n    with pytest.raises(NotFittedError):\r\n        PrecisionRecallDisplay.from_estimator(clf, X, y)\r\n    clf.fit(X, y)\r\n    display = PrecisionRecallDisplay.from_estimator(clf, X, y)\r\n    assert display.estimator_name == clf.__class__.__name__",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multimetric_scorer_exception_handling(raise_exc):\r\n    \"\"\"Check that the calling of the `_MultimetricScorer` returns\r\n    exception messages in the result dict for the failing scorers\r\n    in case of `raise_exc` is `False` and if `raise_exc` is `True`,\r\n    then the proper exception is raised.\r\n    \"\"\"\r\n    scorers = {\r\n        \"failing_1\": \"neg_mean_squared_log_error\",\r\n        \"non_failing\": \"neg_median_absolute_error\",\r\n        \"failing_2\": \"neg_mean_squared_log_error\",\r\n    }\r\n\r\n    X, y = make_classification(\r\n        n_samples=50, n_features=2, n_redundant=0, random_state=0\r\n    )\r\n    # neg_mean_squared_log_error fails if y contains values less than or equal to -1\r\n    y *= -1\r\n\r\n    clf = DecisionTreeClassifier().fit(X, y)\r\n\r\n    scorer_dict = _check_multimetric_scoring(clf, scorers)\r\n    multi_scorer = _MultimetricScorer(scorers=scorer_dict, raise_exc=raise_exc)\r\n\r\n    error_msg = (\r\n        \"Mean Squared Logarithmic Error cannot be used when \"\r\n        \"targets contain values less than or equal to -1.\"\r\n    )\r\n\r\n    if raise_exc:\r\n        with pytest.raises(ValueError, match=error_msg):\r\n            multi_scorer(clf, X, y)\r\n    else:\r\n        result = multi_scorer(clf, X, y)\r\n\r\n        exception_message_1 = result[\"failing_1\"]\r\n        score = result[\"non_failing\"]\r\n        exception_message_2 = result[\"failing_2\"]\r\n\r\n        assert isinstance(exception_message_1, str) and error_msg in exception_message_1\r\n        assert isinstance(score, float)\r\n        assert isinstance(exception_message_2, str) and error_msg in exception_message_2",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_thresholded_scorers():\r\n    # Test scorers that take thresholds.\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = LogisticRegression(random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer(\"roc_auc\")(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\r\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n    assert_almost_equal(score1, score3)\r\n\r\n    logscore = get_scorer(\"neg_log_loss\")(clf, X_test, y_test)\r\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\r\n    assert_almost_equal(-logscore, logloss)\r\n\r\n    # same for an estimator without decision_function\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, y_train)\r\n    score1 = get_scorer(\"roc_auc\")(clf, X_test, y_test)\r\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\r\n    assert_almost_equal(score1, score2)\r\n\r\n    # test with a regressor (no decision_function)\r\n    reg = DecisionTreeRegressor()\r\n    reg.fit(X_train, y_train)\r\n    err_msg = \"DecisionTreeRegressor has none of the following attributes\"\r\n    with pytest.raises(AttributeError, match=err_msg):\r\n        get_scorer(\"roc_auc\")(reg, X_test, y_test)\r\n\r\n    # Test that an exception is raised on more than two classes\r\n    X, y = make_blobs(random_state=0, centers=3)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf.fit(X_train, y_train)\r\n    with pytest.raises(ValueError, match=\"multi_class must be in \\\\('ovo', 'ovr'\\\\)\"):\r\n        get_scorer(\"roc_auc\")(clf, X_test, y_test)\r\n\r\n    # test error is raised with a single class present in model\r\n    # (predict_proba shape is not suitable for binary auc)\r\n    X, y = make_blobs(random_state=0, centers=2)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n    clf = DecisionTreeClassifier()\r\n    clf.fit(X_train, np.zeros_like(y_train))\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer(\"roc_auc\")(clf, X_test, y_test)\r\n\r\n    # for proba scorers\r\n    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\r\n        get_scorer(\"neg_log_loss\")(clf, X_test, y_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_plot_roc_curve_pos_label(pyplot, response_method, constructor_name):\r\n    # check that we can provide the positive label and display the proper\r\n    # statistics\r\n    X, y = load_breast_cancer(return_X_y=True)\r\n    # create an highly imbalanced\r\n    idx_positive = np.flatnonzero(y == 1)\r\n    idx_negative = np.flatnonzero(y == 0)\r\n    idx_selected = np.hstack([idx_negative, idx_positive[:25]])\r\n    X, y = X[idx_selected], y[idx_selected]\r\n    X, y = shuffle(X, y, random_state=42)\r\n    # only use 2 features to make the problem even harder\r\n    X = X[:, :2]\r\n    y = np.array([\"cancer\" if c == 1 else \"not cancer\" for c in y], dtype=object)\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X,\r\n        y,\r\n        stratify=y,\r\n        random_state=0,\r\n    )\r\n\r\n    classifier = LogisticRegression()\r\n    classifier.fit(X_train, y_train)\r\n\r\n    # sanity check to be sure the positive class is classes_[0] and that we\r\n    # are betrayed by the class imbalance\r\n    assert classifier.classes_.tolist() == [\"cancer\", \"not cancer\"]\r\n\r\n    y_pred = getattr(classifier, response_method)(X_test)\r\n    # we select the corresponding probability columns or reverse the decision\r\n    # function otherwise\r\n    y_pred_cancer = -1 * y_pred if y_pred.ndim == 1 else y_pred[:, 0]\r\n    y_pred_not_cancer = y_pred if y_pred.ndim == 1 else y_pred[:, 1]\r\n\r\n    if constructor_name == \"from_estimator\":\r\n        display = RocCurveDisplay.from_estimator(\r\n            classifier,\r\n            X_test,\r\n            y_test,\r\n            pos_label=\"cancer\",\r\n            response_method=response_method,\r\n        )\r\n    else:\r\n        display = RocCurveDisplay.from_predictions(\r\n            y_test,\r\n            y_pred_cancer,\r\n            pos_label=\"cancer\",\r\n        )\r\n\r\n    roc_auc_limit = 0.95679\r\n\r\n    assert display.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert trapezoid(display.tpr, display.fpr) == pytest.approx(roc_auc_limit)\r\n\r\n    if constructor_name == \"from_estimator\":\r\n        display = RocCurveDisplay.from_estimator(\r\n            classifier,\r\n            X_test,\r\n            y_test,\r\n            response_method=response_method,\r\n            pos_label=\"not cancer\",\r\n        )\r\n    else:\r\n        display = RocCurveDisplay.from_predictions(\r\n            y_test,\r\n            y_pred_not_cancer,\r\n            pos_label=\"not cancer\",\r\n        )\r\n\r\n    assert display.roc_auc == pytest.approx(roc_auc_limit)\r\n    assert trapezoid(display.tpr, display.fpr) == pytest.approx(roc_auc_limit)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multiclass_roc_proba_scorer_label():\r\n    scorer = make_scorer(\r\n        roc_auc_score,\r\n        multi_class=\"ovo\",\r\n        labels=[0, 1, 2],\r\n        response_method=\"predict_proba\",\r\n    )\r\n    X, y = make_classification(\r\n        n_classes=3, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n    y_proba = lr.predict_proba(X)\r\n\r\n    y_binary = y == 0\r\n    expected_score = roc_auc_score(\r\n        y_binary, y_proba, multi_class=\"ovo\", labels=[0, 1, 2]\r\n    )\r\n\r\n    assert scorer(lr, X, y_binary) == pytest.approx(expected_score)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_scorer_select_proba_error(scorer):\r\n    # check that we raise the proper error when passing an unknown\r\n    # pos_label\r\n    X, y = make_classification(\r\n        n_classes=2, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n    assert scorer._kwargs[\"pos_label\"] not in np.unique(y).tolist()\r\n\r\n    err_msg = \"is not a valid label\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        scorer(lr, X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multimetric_scoring_metadata_routing():\r\n    # Test that _MultimetricScorer properly routes metadata.\r\n    def score1(y_true, y_pred):\r\n        return 1\r\n\r\n    def score2(y_true, y_pred, sample_weight=\"test\"):\r\n        # make sure sample_weight is not passed\r\n        assert sample_weight == \"test\"\r\n        return 1\r\n\r\n    def score3(y_true, y_pred, sample_weight=None):\r\n        # make sure sample_weight is passed\r\n        assert sample_weight is not None\r\n        return 1\r\n\r\n    scorers = {\r\n        \"score1\": make_scorer(score1),\r\n        \"score2\": make_scorer(score2).set_score_request(sample_weight=False),\r\n        \"score3\": make_scorer(score3).set_score_request(sample_weight=True),\r\n    }\r\n\r\n    X, y = make_classification(\r\n        n_samples=50, n_features=2, n_redundant=0, random_state=0\r\n    )\r\n\r\n    clf = DecisionTreeClassifier().fit(X, y)\r\n\r\n    scorer_dict = _check_multimetric_scoring(clf, scorers)\r\n    multi_scorer = _MultimetricScorer(scorers=scorer_dict)\r\n    # this should fail, because metadata routing is not enabled and w/o it we\r\n    # don't support different metadata for different scorers.\r\n    # TODO: remove when enable_metadata_routing is deprecated\r\n    with config_context(enable_metadata_routing=False):\r\n        with pytest.raises(TypeError, match=\"got an unexpected keyword argument\"):\r\n            multi_scorer(clf, X, y, sample_weight=1)\r\n\r\n    # This passes since routing is done.\r\n    multi_scorer(clf, X, y, sample_weight=1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_metadata_kwarg_conflict():\r\n    \"\"\"This test makes sure the right warning is raised if the user passes\r\n    some metadata both as a constructor to make_scorer, and during __call__.\r\n    \"\"\"\r\n    X, y = make_classification(\r\n        n_classes=3, n_informative=3, n_samples=20, random_state=0\r\n    )\r\n    lr = LogisticRegression().fit(X, y)\r\n\r\n    scorer = make_scorer(\r\n        roc_auc_score,\r\n        response_method=\"predict_proba\",\r\n        multi_class=\"ovo\",\r\n        labels=lr.classes_,\r\n    )\r\n    with pytest.warns(UserWarning, match=\"already set as kwargs\"):\r\n        scorer.set_score_request(labels=True)\r\n\r\n    with pytest.warns(UserWarning, match=\"There is an overlap\"):\r\n        scorer(lr, X, y, labels=lr.classes_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_mixture_n_components_greater_than_n_samples_error(estimator):\r\n    \"\"\"Check error when n_components <= n_samples\"\"\"\r\n    rng = np.random.RandomState(0)\r\n    X = rng.rand(10, 5)\r\n    estimator.set_params(n_components=12)\r\n\r\n    msg = \"Expected n_samples >= n_components\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        estimator.fit(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_tuned_threshold_classifier_no_binary(data):\r\n    \"\"\"Check that we raise an informative error message for non-binary problem.\"\"\"\r\n    err_msg = \"Only binary classification is supported.\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        TunedThresholdClassifierCV(LogisticRegression()).fit(*data)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_tuned_threshold_classifier_conflict_cv_refit(params, err_type, err_msg):\r\n    \"\"\"Check that we raise an informative error message when `cv` and `refit`\r\n    cannot be used together.\r\n    \"\"\"\r\n    X, y = make_classification(n_samples=100, random_state=0)\r\n    with pytest.raises(err_type, match=err_msg):\r\n        TunedThresholdClassifierCV(LogisticRegression(), **params).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_fixed_threshold_classifier_classes_():\r\n    \"\"\"Check that the classes_ attribute is properly set.\"\"\"\r\n    X, y = make_classification(random_state=0)\r\n    with pytest.raises(\r\n        AttributeError, match=\"The underlying estimator is not fitted yet.\"\r\n    ):\r\n        FixedThresholdClassifier(estimator=LogisticRegression()).classes_\r\n\r\n    classifier = LogisticRegression().fit(X, y)\r\n    fixed_threshold_classifier = FixedThresholdClassifier(estimator=classifier)\r\n    assert_array_equal(fixed_threshold_classifier.classes_, classifier.classes_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_tuned_threshold_classifier_metric_with_parameter():\r\n    \"\"\"Check that we can pass a metric with a parameter in addition check that\r\n    `f_beta` with `beta=1` is equivalent to `f1` and different from `f_beta` with\r\n    `beta=2`.\r\n    \"\"\"\r\n    X, y = load_breast_cancer(return_X_y=True)\r\n    lr = make_pipeline(StandardScaler(), LogisticRegression()).fit(X, y)\r\n    model_fbeta_1 = TunedThresholdClassifierCV(\r\n        estimator=lr, scoring=make_scorer(fbeta_score, beta=1)\r\n    ).fit(X, y)\r\n    model_fbeta_2 = TunedThresholdClassifierCV(\r\n        estimator=lr, scoring=make_scorer(fbeta_score, beta=2)\r\n    ).fit(X, y)\r\n    model_f1 = TunedThresholdClassifierCV(\r\n        estimator=lr, scoring=make_scorer(f1_score)\r\n    ).fit(X, y)\r\n\r\n    assert model_fbeta_1.best_threshold_ == pytest.approx(model_f1.best_threshold_)\r\n    assert model_fbeta_1.best_threshold_ != pytest.approx(model_fbeta_2.best_threshold_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grid_search_bad_param_grid():\r\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\r\n    param_dict = {\"C\": 1}\r\n    clf = SVC(gamma=\"auto\")\r\n    error_msg = re.escape(\r\n        \"Parameter grid for parameter 'C' needs to be a list or \"\r\n        \"a numpy array, but got 1 (of type int) instead. Single \"\r\n        \"values need to be wrapped in a list with one element.\"\r\n    )\r\n    search = GridSearchCV(clf, param_dict)\r\n    with pytest.raises(TypeError, match=error_msg):\r\n        search.fit(X, y)\r\n\r\n    param_dict = {\"C\": []}\r\n    clf = SVC()\r\n    error_msg = re.escape(\r\n        \"Parameter grid for parameter 'C' need to be a non-empty sequence, got: []\"\r\n    )\r\n    search = GridSearchCV(clf, param_dict)\r\n    with pytest.raises(ValueError, match=error_msg):\r\n        search.fit(X, y)\r\n\r\n    param_dict = {\"C\": \"1,2,3\"}\r\n    clf = SVC(gamma=\"auto\")\r\n    error_msg = re.escape(\r\n        \"Parameter grid for parameter 'C' needs to be a list or a numpy array, \"\r\n        \"but got '1,2,3' (of type str) instead. Single values need to be \"\r\n        \"wrapped in a list with one element.\"\r\n    )\r\n    search = GridSearchCV(clf, param_dict)\r\n    with pytest.raises(TypeError, match=error_msg):\r\n        search.fit(X, y)\r\n\r\n    param_dict = {\"C\": np.ones((3, 2))}\r\n    clf = SVC()\r\n    search = GridSearchCV(clf, param_dict)\r\n    with pytest.raises(ValueError):\r\n        search.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test__custom_fit_no_run_search():\r\n    class NoRunSearchSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n        def fit(self, X, y=None, groups=None, **fit_params):\r\n            return self\r\n\r\n    # this should not raise any exceptions\r\n    NoRunSearchSearchCV(SVC()).fit(X, y)\r\n\r\n    class BadSearchCV(BaseSearchCV):\r\n        def __init__(self, estimator, **kwargs):\r\n            super().__init__(estimator, **kwargs)\r\n\r\n    with pytest.raises(NotImplementedError, match=\"_run_search not implemented.\"):\r\n        # this should raise a NotImplementedError\r\n        BadSearchCV(SVC()).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_empty_cv_iterator_error():\r\n    # Use global X, y\r\n\r\n    # create cv\r\n    cv = KFold(n_splits=3).split(X)\r\n\r\n    # pop all of it, this should cause the expected ValueError\r\n    [u for u in cv]\r\n    # cv is empty now\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {\"alpha\": [1e-3, 1e-2, 1e-1]}, cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=(\r\n            \"No fits were performed. \"\r\n            \"Was the CV iterator empty\\\\? \"\r\n            \"Were there no candidates\\\\?\"\r\n        ),\r\n    ):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_random_search_bad_cv():\r\n    # Use global X, y\r\n\r\n    class BrokenKFold(KFold):\r\n        def get_n_splits(self, *args, **kw):\r\n            return 1\r\n\r\n    # create bad cv\r\n    cv = BrokenKFold(n_splits=3)\r\n\r\n    train_size = 100\r\n    ridge = RandomizedSearchCV(Ridge(), {\"alpha\": [1e-3, 1e-2, 1e-1]}, cv=cv, n_jobs=4)\r\n\r\n    # assert that this raises an error\r\n    with pytest.raises(\r\n        ValueError,\r\n        match=(\r\n            \"cv.split and cv.get_n_splits returned \"\r\n            \"inconsistent results. Expected \\\\d+ \"\r\n            \"splits, got \\\\d+\"\r\n        ),\r\n    ):\r\n        ridge.fit(X[:train_size], y[:train_size])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_searchcv_raise_warning_with_non_finite_score(\r\n    SearchCV, specialized_params, return_train_score\r\n):\r\n    # Non-regression test for:\r\n    # https://github.com/scikit-learn/scikit-learn/issues/10529\r\n    # Check that we raise a UserWarning when a non-finite score is\r\n    # computed in the SearchCV\r\n    X, y = make_classification(n_classes=2, random_state=0)\r\n\r\n    class FailingScorer:\r\n        \"\"\"Scorer that will fail for some split but not all.\"\"\"\r\n\r\n        def __init__(self):\r\n            self.n_counts = 0\r\n\r\n        def __call__(self, estimator, X, y):\r\n            self.n_counts += 1\r\n            if self.n_counts % 5 == 0:\r\n                return np.nan\r\n            return 1\r\n\r\n    grid = SearchCV(\r\n        DecisionTreeClassifier(),\r\n        scoring=FailingScorer(),\r\n        cv=3,\r\n        return_train_score=return_train_score,\r\n        **specialized_params,\r\n    )\r\n\r\n    with pytest.warns(UserWarning) as warn_msg:\r\n        grid.fit(X, y)\r\n\r\n    set_with_warning = [\"test\", \"train\"] if return_train_score else [\"test\"]\r\n    assert len(warn_msg) == len(set_with_warning)\r\n    for msg, dataset in zip(warn_msg, set_with_warning):\r\n        assert f\"One or more of the {dataset} scores are non-finite\" in str(msg.message)\r\n\r\n    # all non-finite scores should be equally ranked last\r\n    last_rank = grid.cv_results_[\"rank_test_score\"].max()\r\n    non_finite_mask = np.isnan(grid.cv_results_[\"mean_test_score\"])\r\n    assert_array_equal(grid.cv_results_[\"rank_test_score\"][non_finite_mask], last_rank)\r\n    # all finite scores should be better ranked than the non-finite scores\r\n    assert np.all(grid.cv_results_[\"rank_test_score\"][~non_finite_mask] < last_rank)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_score_rejects_params_with_no_routing_enabled(SearchCV, param_search):\r\n    \"\"\"*SearchCV should reject **params when metadata routing is not enabled\r\n    since this is added only when routing is enabled.\"\"\"\r\n    X, y = make_classification(random_state=42)\r\n    est = LinearSVC()\r\n    param_grid_search = {param_search: {\"C\": [1]}}\r\n\r\n    gs = SearchCV(est, cv=2, **param_grid_search).fit(X, y)\r\n\r\n    with pytest.raises(ValueError, match=\"is only supported if\"):\r\n        gs.score(X, y, metadata=1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_with_estimators_issue_29157():\r\n    \"\"\"Check cv_results_ for estimators with a `dtype` parameter, e.g. OneHotEncoder.\"\"\"\r\n    pd = pytest.importorskip(\"pandas\")\r\n    df = pd.DataFrame(\r\n        {\r\n            \"numeric_1\": [1, 2, 3, 4, 5],\r\n            \"object_1\": [\"a\", \"a\", \"a\", \"a\", \"a\"],\r\n            \"target\": [1.0, 4.1, 2.0, 3.0, 1.0],\r\n        }\r\n    )\r\n    X = df.drop(\"target\", axis=1)\r\n    y = df[\"target\"]\r\n    enc = ColumnTransformer(\r\n        [(\"enc\", OneHotEncoder(sparse_output=False), [\"object_1\"])],\r\n        remainder=\"passthrough\",\r\n    )\r\n    pipe = Pipeline(\r\n        [\r\n            (\"enc\", enc),\r\n            (\"regressor\", LinearRegression()),\r\n        ]\r\n    )\r\n    grid_params = {\r\n        \"enc__enc\": [\r\n            OneHotEncoder(sparse_output=False),\r\n            OrdinalEncoder(),\r\n        ]\r\n    }\r\n    grid_search = GridSearchCV(pipe, grid_params, cv=2)\r\n    grid_search.fit(X, y)\r\n    assert grid_search.cv_results_[\"param_enc__enc\"].dtype == object",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_default_iid(SearchCV, specialized_params):\r\n    # Test the IID parameter  TODO: Clearly this test does something else???\r\n    # noise-free simple 2d-data\r\n    X, y = make_blobs(\r\n        centers=[[0, 0], [1, 0], [0, 1], [1, 1]],\r\n        random_state=0,\r\n        cluster_std=0.1,\r\n        shuffle=False,\r\n        n_samples=80,\r\n    )\r\n    # split dataset into two folds that are not iid\r\n    # first one contains data of all 4 blobs, second only from two.\r\n    mask = np.ones(X.shape[0], dtype=bool)\r\n    mask[np.where(y == 1)[0][::2]] = 0\r\n    mask[np.where(y == 2)[0][::2]] = 0\r\n    # this leads to perfect classification on one fold and a score of 1/3 on\r\n    # the other\r\n    # create \"cv\" for splits\r\n    cv = [[mask, ~mask], [~mask, mask]]\r\n\r\n    common_params = {\"estimator\": SVC(), \"cv\": cv, \"return_train_score\": True}\r\n    search = SearchCV(**common_params, **specialized_params)\r\n    search.fit(X, y)\r\n\r\n    test_cv_scores = np.array(\r\n        [\r\n            search.cv_results_[\"split%d_test_score\" % s][0]\r\n            for s in range(search.n_splits_)\r\n        ]\r\n    )\r\n    test_mean = search.cv_results_[\"mean_test_score\"][0]\r\n    test_std = search.cv_results_[\"std_test_score\"][0]\r\n\r\n    train_cv_scores = np.array(\r\n        [\r\n            search.cv_results_[\"split%d_train_score\" % s][0]\r\n            for s in range(search.n_splits_)\r\n        ]\r\n    )\r\n    train_mean = search.cv_results_[\"mean_train_score\"][0]\r\n    train_std = search.cv_results_[\"std_train_score\"][0]\r\n\r\n    assert search.cv_results_[\"param_C\"][0] == 1\r\n    # scores are the same as above\r\n    assert_allclose(test_cv_scores, [1, 1.0 / 3.0])\r\n    assert_allclose(train_cv_scores, [1, 1])\r\n    # Unweighted mean/std is used\r\n    assert test_mean == pytest.approx(np.mean(test_cv_scores))\r\n    assert test_std == pytest.approx(np.std(test_cv_scores))\r\n\r\n    # For the train scores, we do not take a weighted mean irrespective of\r\n    # i.i.d. or not\r\n    assert train_mean == pytest.approx(1)\r\n    assert train_std == pytest.approx(0)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_search_cv_score_samples_error(search_cv):\r\n    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)\r\n    search_cv.fit(X, y)\r\n\r\n    # Make sure to error out when underlying estimator does not implement\r\n    # the method `score_samples`\r\n    outer_msg = f\"'{search_cv.__class__.__name__}' has no attribute 'score_samples'\"\r\n    inner_msg = \"'DecisionTreeClassifier' object has no attribute 'score_samples'\"\r\n\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        search_cv.score_samples(X)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg == str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_grid_search_error():\r\n    # Test that grid search will capture errors on data with different length\r\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\r\n\r\n    clf = LinearSVC()\r\n    cv = GridSearchCV(clf, {\"C\": [0.1, 1.0]})\r\n    with pytest.raises(ValueError):\r\n        cv.fit(X_[:180], y_)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _loss_grad_lbfgs(self, transformation, X, same_class_mask, sign=1.0):\r\n        \"\"\"Compute the loss and the loss gradient w.r.t. `transformation`.\r\n\r\n        Parameters\r\n        ----------\r\n        transformation : ndarray of shape (n_components * n_features,)\r\n            The raveled linear transformation on which to compute loss and\r\n            evaluate gradient.\r\n\r\n        X : ndarray of shape (n_samples, n_features)\r\n            The training samples.\r\n\r\n        same_class_mask : ndarray of shape (n_samples, n_samples)\r\n            A mask where `mask[i, j] == 1` if `X[i]` and `X[j]` belong\r\n            to the same class, and `0` otherwise.\r\n\r\n        Returns\r\n        -------\r\n        loss : float\r\n            The loss computed for the given transformation.\r\n\r\n        gradient : ndarray of shape (n_components * n_features,)\r\n            The new (flattened) gradient of the loss.\r\n        \"\"\"\r\n\r\n        if self.n_iter_ == 0:\r\n            self.n_iter_ += 1\r\n            if self.verbose:\r\n                header_fields = [\"Iteration\", \"Objective Value\", \"Time(s)\"]\r\n                header_fmt = \"{:>10} {:>20} {:>10}\"\r\n                header = header_fmt.format(*header_fields)\r\n                cls_name = self.__class__.__name__\r\n                print(\"[{}]\".format(cls_name))\r\n                print(\r\n                    \"[{}] {}\\n[{}] {}\".format(\r\n                        cls_name, header, cls_name, \"-\" * len(header)\r\n                    )\r\n                )\r\n\r\n        t_funcall = time.time()\r\n\r\n        transformation = transformation.reshape(-1, X.shape[1])\r\n        X_embedded = np.dot(X, transformation.T)  # (n_samples, n_components)\r\n\r\n        # Compute softmax distances\r\n        p_ij = pairwise_distances(X_embedded, squared=True)\r\n        np.fill_diagonal(p_ij, np.inf)\r\n        p_ij = softmax(-p_ij)  # (n_samples, n_samples)\r\n\r\n        # Compute loss\r\n        masked_p_ij = p_ij * same_class_mask\r\n        p = np.sum(masked_p_ij, axis=1, keepdims=True)  # (n_samples, 1)\r\n        loss = np.sum(p)\r\n\r\n        # Compute gradient of loss w.r.t. `transform`\r\n        weighted_p_ij = masked_p_ij - p_ij * p\r\n        weighted_p_ij_sym = weighted_p_ij + weighted_p_ij.T\r\n        np.fill_diagonal(weighted_p_ij_sym, -weighted_p_ij.sum(axis=0))\r\n        gradient = 2 * X_embedded.T.dot(weighted_p_ij_sym).dot(X)\r\n        # time complexity of the gradient: O(n_components x n_samples x (\r\n        # n_samples + n_features))\r\n\r\n        if self.verbose:\r\n            t_funcall = time.time() - t_funcall\r\n            values_fmt = \"[{}] {:>10} {:>20.6e} {:>10.2f}\"\r\n            print(\r\n                values_fmt.format(\r\n                    self.__class__.__name__, self.n_iter_, loss, t_funcall\r\n                )\r\n            )\r\n            sys.stdout.flush()\r\n\r\n        return sign * loss, sign * gradient.ravel()",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_cross_val_score_precomputed():\r\n    # test for svm with precomputed kernel\r\n    svm = SVC(kernel=\"precomputed\")\r\n    iris = load_iris()\r\n    X, y = iris.data, iris.target\r\n    linear_kernel = np.dot(X, X.T)\r\n    score_precomputed = cross_val_score(svm, linear_kernel, y)\r\n    svm = SVC(kernel=\"linear\")\r\n    score_linear = cross_val_score(svm, X, y)\r\n    assert_array_almost_equal(score_precomputed, score_linear)\r\n\r\n    # test with callable\r\n    svm = SVC(kernel=lambda x, y: np.dot(x, y.T))\r\n    score_callable = cross_val_score(svm, X, y)\r\n    assert_array_almost_equal(score_precomputed, score_callable)\r\n\r\n    # Error raised for non-square X\r\n    svm = SVC(kernel=\"precomputed\")\r\n    with pytest.raises(ValueError):\r\n        cross_val_score(svm, X, y)\r\n\r\n    # test error is raised when the precomputed kernel is not array-like\r\n    # or sparse\r\n    with pytest.raises(ValueError):\r\n        cross_val_score(svm, linear_kernel.tolist(), y)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_cross_val_predict(coo_container):\r\n    X, y = load_diabetes(return_X_y=True)\r\n    cv = KFold()\r\n\r\n    est = Ridge()\r\n\r\n    # Naive loop (should be same as cross_val_predict):\r\n    preds2 = np.zeros_like(y)\r\n    for train, test in cv.split(X, y):\r\n        est.fit(X[train], y[train])\r\n        preds2[test] = est.predict(X[test])\r\n\r\n    preds = cross_val_predict(est, X, y, cv=cv)\r\n    assert_array_almost_equal(preds, preds2)\r\n\r\n    preds = cross_val_predict(est, X, y)\r\n    assert len(preds) == len(y)\r\n\r\n    cv = LeaveOneOut()\r\n    preds = cross_val_predict(est, X, y, cv=cv)\r\n    assert len(preds) == len(y)\r\n\r\n    Xsp = X.copy()\r\n    Xsp *= Xsp > np.median(Xsp)\r\n    Xsp = coo_container(Xsp)\r\n    preds = cross_val_predict(est, Xsp, y)\r\n    assert_array_almost_equal(len(preds), len(y))\r\n\r\n    preds = cross_val_predict(KMeans(n_init=\"auto\"), X)\r\n    assert len(preds) == len(y)\r\n\r\n    class BadCV:\r\n        def split(self, X, y=None, groups=None):\r\n            for i in range(4):\r\n                yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])\r\n\r\n    with pytest.raises(ValueError):\r\n        cross_val_predict(est, X, y, cv=BadCV())\r\n\r\n    X, y = load_iris(return_X_y=True)\r\n\r\n    warning_message = (\r\n        r\"Number of classes in training fold \\(2\\) does \"\r\n        r\"not match total number of classes \\(3\\). \"\r\n        \"Results may not be appropriate for your use case.\"\r\n    )\r\n    with pytest.warns(RuntimeWarning, match=warning_message):\r\n        cross_val_predict(\r\n            LogisticRegression(solver=\"liblinear\"),\r\n            X,\r\n            y,\r\n            method=\"predict_proba\",\r\n            cv=KFold(2),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_prefitted_throws_error():\r\n    # Test that passing a pre-fitted classifier and calling predict throws an\r\n    # error\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.raises(\r\n        NotFittedError,\r\n        match=\"This SelfTrainingClassifier instance is not fitted yet\",\r\n    ):\r\n        st.predict(X_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_no_unlabeled():\r\n    # Test that training on a fully labeled dataset produces the same results\r\n    # as training the classifier by itself.\r\n    knn = KNeighborsClassifier()\r\n    knn.fit(X_train, y_train)\r\n    st = SelfTrainingClassifier(knn)\r\n    with pytest.warns(UserWarning, match=\"y contains no unlabeled samples\"):\r\n        st.fit(X_train, y_train)\r\n    assert_array_equal(knn.predict(X_test), st.predict(X_test))\r\n    # Assert that all samples were labeled in iteration 0 (since there were no\r\n    # unlabeled samples).\r\n    assert np.all(st.labeled_iter_ == 0)\r\n    assert st.termination_condition_ == \"all_labeled\"",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_deprecation_warning_base_estimator():\r\n    warn_msg = \"`base_estimator` has been deprecated in 1.6 and will be removed\"\r\n    with pytest.warns(FutureWarning, match=warn_msg):\r\n        SelfTrainingClassifier(base_estimator=DecisionTreeClassifier()).fit(\r\n            X_train, y_train_missing_labels\r\n        )\r\n\r\n    error_msg = \"You must pass an estimator to SelfTrainingClassifier\"\r\n    with pytest.raises(ValueError, match=error_msg):\r\n        SelfTrainingClassifier().fit(X_train, y_train_missing_labels)\r\n\r\n    error_msg = \"You must pass only one estimator to SelfTrainingClassifier.\"\r\n    with pytest.raises(ValueError, match=error_msg):\r\n        SelfTrainingClassifier(\r\n            base_estimator=DecisionTreeClassifier(), estimator=DecisionTreeClassifier()\r\n        ).fit(X_train, y_train_missing_labels)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_strings_dtype():\r\n    clf = SelfTrainingClassifier(KNeighborsClassifier())\r\n    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)\r\n    labels_multiclass = [\"one\", \"two\", \"three\"]\r\n\r\n    y_strings = np.take(labels_multiclass, y)\r\n\r\n    with pytest.raises(ValueError, match=\"dtype\"):\r\n        clf.fit(X, y_strings)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_self_training_estimator_attribute_error():\r\n    \"\"\"Check that we raise the proper AttributeErrors when the `estimator`\r\n    does not implement the `predict_proba` method, which is called from within\r\n    `fit`, or `decision_function`, which is decorated with `available_if`.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/28108\r\n    \"\"\"\r\n    # `SVC` with `probability=False` does not implement 'predict_proba' that\r\n    # is required internally in `fit` of `SelfTrainingClassifier`. We expect\r\n    # an AttributeError to be raised.\r\n    estimator = SVC(probability=False, gamma=\"scale\")\r\n    self_training = SelfTrainingClassifier(estimator)\r\n\r\n    with pytest.raises(AttributeError, match=\"has no attribute 'predict_proba'\"):\r\n        self_training.fit(X_train, y_train_missing_labels)\r\n\r\n    # `DecisionTreeClassifier` does not implement 'decision_function' and\r\n    # should raise an AttributeError\r\n    self_training = SelfTrainingClassifier(estimator=DecisionTreeClassifier())\r\n\r\n    outer_msg = \"This 'SelfTrainingClassifier' has no attribute 'decision_function'\"\r\n    inner_msg = \"'DecisionTreeClassifier' object has no attribute 'decision_function'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        self_training.fit(X_train, y_train_missing_labels).decision_function(X_train)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error(lil_container):\r\n    # Test that it gives proper exception on deficient input\r\n    clf = svm.SVC()\r\n    X_sp = lil_container(X)\r\n\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X_sp, Y2)\r\n\r\n    clf.fit(X_sp, Y)\r\n    assert_array_equal(clf.predict(T), true_result)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_warns_k_best():\r\n    st = SelfTrainingClassifier(KNeighborsClassifier(), criterion=\"k_best\", k_best=1000)\r\n    with pytest.warns(UserWarning, match=\"k_best is larger than\"):\r\n        st.fit(X_train, y_train_missing_labels)\r\n\r\n    assert st.termination_condition_ == \"all_labeled\"",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_precomputed():\r\n    # SVC with a precomputed kernel.\r\n    # We test it with a toy dataset and with iris.\r\n    clf = svm.SVC(kernel=\"precomputed\")\r\n    # Gram matrix for train data (square matrix)\r\n    # (we use just a linear kernel)\r\n    K = np.dot(X, np.array(X).T)\r\n    clf.fit(K, Y)\r\n    # Gram matrix for test data (rectangular matrix)\r\n    KT = np.dot(T, np.array(X).T)\r\n    pred = clf.predict(KT)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(KT.T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\r\n    assert_array_equal(clf.support_, [1, 3])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    KT = np.zeros_like(KT)\r\n    for i in range(len(T)):\r\n        for j in clf.support_:\r\n            KT[i, j] = np.dot(T[i], X[j])\r\n\r\n    pred = clf.predict(KT)\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # same as before, but using a callable function instead of the kernel\r\n    # matrix. kernel is just a linear kernel\r\n\r\n    def kfunc(x, y):\r\n        return np.dot(x, y.T)\r\n\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(np.array(X), Y)\r\n    pred = clf.predict(T)\r\n\r\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\r\n    assert_array_equal(clf.intercept_, [0])\r\n    assert_array_almost_equal(clf.support_, [1, 3])\r\n    assert_array_equal(pred, true_result)\r\n\r\n    # test a precomputed kernel with the iris dataset\r\n    # and check parameters against a linear SVC\r\n    clf = svm.SVC(kernel=\"precomputed\")\r\n    clf2 = svm.SVC(kernel=\"linear\")\r\n    K = np.dot(iris.data, iris.data.T)\r\n    clf.fit(K, iris.target)\r\n    clf2.fit(iris.data, iris.target)\r\n    pred = clf.predict(K)\r\n    assert_array_almost_equal(clf.support_, clf2.support_)\r\n    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)\r\n    assert_array_almost_equal(clf.intercept_, clf2.intercept_)\r\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\r\n\r\n    # Gram matrix for test data but compute KT[i,j]\r\n    # for support vectors j only.\r\n    K = np.zeros_like(K)\r\n    for i in range(len(iris.data)):\r\n        for j in clf.support_:\r\n            K[i, j] = np.dot(iris.data[i], iris.data[j])\r\n\r\n    pred = clf.predict(K)\r\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\r\n\r\n    clf = svm.SVC(kernel=kfunc)\r\n    clf.fit(iris.data, iris.target)\r\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_pickle_version_warning_is_not_raised_with_matching_version():\r\n    iris = datasets.load_iris()\r\n    tree = DecisionTreeClassifier().fit(iris.data, iris.target)\r\n    tree_pickle = pickle.dumps(tree)\r\n    assert b\"_sklearn_version\" in tree_pickle\r\n\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"error\")\r\n        tree_restored = pickle.loads(tree_pickle)\r\n\r\n    # test that we can predict with the restored decision tree classifier\r\n    score_of_original = tree.score(iris.data, iris.target)\r\n    score_of_restored = tree_restored.score(iris.data, iris.target)\r\n    assert score_of_original == score_of_restored",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bad_input(lil_container):\r\n    # Test dimensions for labels\r\n    Y2 = Y[:-1]  # wrong dimensions for labels\r\n    with pytest.raises(ValueError):\r\n        svm.SVC().fit(X, Y2)\r\n\r\n    # Test with arrays that are non-contiguous.\r\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\r\n        Xf = np.asfortranarray(X)\r\n        assert not Xf.flags[\"C_CONTIGUOUS\"]\r\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\r\n        yf = yf[:, -1]\r\n        assert not yf.flags[\"F_CONTIGUOUS\"]\r\n        assert not yf.flags[\"C_CONTIGUOUS\"]\r\n        clf.fit(Xf, yf)\r\n        assert_array_equal(clf.predict(T), true_result)\r\n\r\n    # error for precomputed kernelsx\r\n    clf = svm.SVC(kernel=\"precomputed\")\r\n    with pytest.raises(ValueError):\r\n        clf.fit(X, Y)\r\n\r\n    # predict with sparse input when trained with dense\r\n    clf = svm.SVC().fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(lil_container(X))\r\n\r\n    Xt = np.array(X).T\r\n    clf.fit(np.dot(X, Xt), Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(X)\r\n\r\n    clf = svm.SVC()\r\n    clf.fit(X, Y)\r\n    with pytest.raises(ValueError):\r\n        clf.predict(Xt)",
        "labels": [
            "Matrix Multiplication API Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_unfitted():\r\n    X = \"foo!\"  # input validation not required when SVM not fitted\r\n\r\n    clf = svm.SVC()\r\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)\r\n\r\n    clf = svm.NuSVR()\r\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\r\n        clf.predict(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_libsvm_convergence_warnings():\r\n    a = svm.SVC(\r\n        kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, max_iter=2\r\n    )\r\n    warning_msg = (\r\n        r\"Solver terminated early \\(max_iter=2\\).  Consider pre-processing \"\r\n        r\"your data with StandardScaler or MinMaxScaler.\"\r\n    )\r\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\r\n        a.fit(np.array(X), Y)\r\n    assert np.all(a.n_iter_ == 2)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_svc_nonfinite_params():\r\n    # Check SVC throws ValueError when dealing with non-finite parameter values\r\n    rng = np.random.RandomState(0)\r\n    n_samples = 10\r\n    fmax = np.finfo(np.float64).max\r\n    X = fmax * rng.uniform(size=(n_samples, 2))\r\n    y = rng.randint(0, 2, size=n_samples)\r\n\r\n    clf = svm.SVC()\r\n    msg = \"The dual coefficients or intercepts are not finite\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        clf.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_lda_predict_proba(solver, n_classes):\r\n    def generate_dataset(n_samples, centers, covariances, random_state=None):\r\n        \"\"\"Generate a multivariate normal data given some centers and\r\n        covariances\"\"\"\r\n        rng = check_random_state(random_state)\r\n        X = np.vstack(\r\n            [\r\n                rng.multivariate_normal(mean, cov, size=n_samples // len(centers))\r\n                for mean, cov in zip(centers, covariances)\r\n            ]\r\n        )\r\n        y = np.hstack(\r\n            [[clazz] * (n_samples // len(centers)) for clazz in range(len(centers))]\r\n        )\r\n        return X, y\r\n\r\n    blob_centers = np.array([[0, 0], [-10, 40], [-30, 30]])[:n_classes]\r\n    blob_stds = np.array([[[10, 10], [10, 100]]] * len(blob_centers))\r\n    X, y = generate_dataset(\r\n        n_samples=90000, centers=blob_centers, covariances=blob_stds, random_state=42\r\n    )\r\n    lda = LinearDiscriminantAnalysis(\r\n        solver=solver, store_covariance=True, shrinkage=None\r\n    ).fit(X, y)\r\n    # check that the empirical means and covariances are close enough to the\r\n    # one used to generate the data\r\n    assert_allclose(lda.means_, blob_centers, atol=1e-1)\r\n    assert_allclose(lda.covariance_, blob_stds[0], atol=1)\r\n\r\n    # implement the method to compute the probability given in The Elements\r\n    # of Statistical Learning (cf. p.127, Sect. 4.4.5 \"Logistic Regression\r\n    # or LDA?\")\r\n    precision = linalg.inv(blob_stds[0])\r\n    alpha_k = []\r\n    alpha_k_0 = []\r\n    for clazz in range(len(blob_centers) - 1):\r\n        alpha_k.append(\r\n            np.dot(precision, (blob_centers[clazz] - blob_centers[-1])[:, np.newaxis])\r\n        )\r\n        alpha_k_0.append(\r\n            np.dot(\r\n                -0.5 * (blob_centers[clazz] + blob_centers[-1])[np.newaxis, :],\r\n                alpha_k[-1],\r\n            )\r\n        )\r\n\r\n    sample = np.array([[-22, 22]])\r\n\r\n    def discriminant_func(sample, coef, intercept, clazz):\r\n        return np.exp(intercept[clazz] + np.dot(sample, coef[clazz])).item()\r\n\r\n    prob = np.array(\r\n        [\r\n            float(\r\n                discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                / (\r\n                    1\r\n                    + sum(\r\n                        [\r\n                            discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                            for clazz in range(n_classes - 1)\r\n                        ]\r\n                    )\r\n                )\r\n            )\r\n            for clazz in range(n_classes - 1)\r\n        ]\r\n    )\r\n\r\n    prob_ref = 1 - np.sum(prob)\r\n\r\n    # check the consistency of the computed probability\r\n    # all probabilities should sum to one\r\n    prob_ref_2 = float(\r\n        1\r\n        / (\r\n            1\r\n            + sum(\r\n                [\r\n                    discriminant_func(sample, alpha_k, alpha_k_0, clazz)\r\n                    for clazz in range(n_classes - 1)\r\n                ]\r\n            )\r\n        )\r\n    )\r\n\r\n    assert prob_ref == pytest.approx(prob_ref_2)\r\n    # check that the probability of LDA are close to the theoretical\r\n    # probabilities\r\n    assert_allclose(\r\n        lda.predict_proba(sample), np.hstack([prob, prob_ref])[np.newaxis], atol=1e-2\r\n    )",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_fit_docstring_attributes(name, Estimator):\r\n    pytest.importorskip(\"numpydoc\")\r\n    from numpydoc import docscrape\r\n\r\n    doc = docscrape.ClassDoc(Estimator)\r\n    attributes = doc[\"Attributes\"]\r\n\r\n    if Estimator.__name__ in (\r\n        \"HalvingRandomSearchCV\",\r\n        \"RandomizedSearchCV\",\r\n        \"HalvingGridSearchCV\",\r\n        \"GridSearchCV\",\r\n    ):\r\n        est = _construct_searchcv_instance(Estimator)\r\n    elif Estimator.__name__ in (\r\n        \"ColumnTransformer\",\r\n        \"Pipeline\",\r\n        \"FeatureUnion\",\r\n    ):\r\n        est = _construct_compose_pipeline_instance(Estimator)\r\n    elif Estimator.__name__ == \"SparseCoder\":\r\n        est = _construct_sparse_coder(Estimator)\r\n    elif Estimator.__name__ == \"FrozenEstimator\":\r\n        X, y = make_classification(n_samples=20, n_features=5, random_state=0)\r\n        est = Estimator(LogisticRegression().fit(X, y))\r\n    else:\r\n        # TODO(devtools): use _tested_estimators instead of all_estimators in the\r\n        # decorator\r\n        est = next(_construct_instances(Estimator))\r\n\r\n    if Estimator.__name__ == \"SelectKBest\":\r\n        est.set_params(k=2)\r\n    elif Estimator.__name__ == \"DummyClassifier\":\r\n        est.set_params(strategy=\"stratified\")\r\n    elif Estimator.__name__ == \"CCA\" or Estimator.__name__.startswith(\"PLS\"):\r\n        # default = 2 is invalid for single target\r\n        est.set_params(n_components=1)\r\n    elif Estimator.__name__ in (\r\n        \"GaussianRandomProjection\",\r\n        \"SparseRandomProjection\",\r\n    ):\r\n        # default=\"auto\" raises an error with the shape of `X`\r\n        est.set_params(n_components=2)\r\n    elif Estimator.__name__ == \"TSNE\":\r\n        # default raises an error, perplexity must be less than n_samples\r\n        est.set_params(perplexity=2)\r\n\r\n    # Low max iter to speed up tests: we are only interested in checking the existence\r\n    # of fitted attributes. This should be invariant to whether it has converged or not.\r\n    if \"max_iter\" in est.get_params():\r\n        est.set_params(max_iter=2)\r\n        # min value for `TSNE` is 250\r\n        if Estimator.__name__ == \"TSNE\":\r\n            est.set_params(max_iter=250)\r\n\r\n    if \"random_state\" in est.get_params():\r\n        est.set_params(random_state=0)\r\n\r\n    # In case we want to deprecate some attributes in the future\r\n    skipped_attributes = {}\r\n\r\n    if Estimator.__name__.endswith(\"Vectorizer\"):\r\n        # Vectorizer require some specific input data\r\n        if Estimator.__name__ in (\r\n            \"CountVectorizer\",\r\n            \"HashingVectorizer\",\r\n            \"TfidfVectorizer\",\r\n        ):\r\n            X = [\r\n                \"This is the first document.\",\r\n                \"This document is the second document.\",\r\n                \"And this is the third one.\",\r\n                \"Is this the first document?\",\r\n            ]\r\n        elif Estimator.__name__ == \"DictVectorizer\":\r\n            X = [{\"foo\": 1, \"bar\": 2}, {\"foo\": 3, \"baz\": 1}]\r\n        y = None\r\n    else:\r\n        X, y = make_classification(\r\n            n_samples=20,\r\n            n_features=3,\r\n            n_redundant=0,\r\n            n_classes=2,\r\n            random_state=2,\r\n        )\r\n\r\n        y = _enforce_estimator_tags_y(est, y)\r\n        X = _enforce_estimator_tags_X(est, X)\r\n\r\n    if est.__sklearn_tags__().target_tags.one_d_labels:\r\n        est.fit(y)\r\n    elif est.__sklearn_tags__().target_tags.two_d_labels:\r\n        est.fit(np.c_[y, y])\r\n    elif est.__sklearn_tags__().input_tags.three_d_array:\r\n        est.fit(X[np.newaxis, ...], y)\r\n    else:\r\n        est.fit(X, y)\r\n\r\n    for attr in attributes:\r\n        if attr.name in skipped_attributes:\r\n            continue\r\n        desc = \" \".join(attr.desc).lower()\r\n        # As certain attributes are present \"only\" if a certain parameter is\r\n        # provided, this checks if the word \"only\" is present in the attribute\r\n        # description, and if not the attribute is required to be present.\r\n        if \"only \" in desc:\r\n            continue\r\n        # ignore deprecation warnings\r\n        with ignore_warnings(category=FutureWarning):\r\n            assert hasattr(est, attr.name)\r\n\r\n    fit_attr = _get_all_fitted_attributes(est)\r\n    fit_attr_names = [attr.name for attr in attributes]\r\n    undocumented_attrs = set(fit_attr).difference(fit_attr_names)\r\n    undocumented_attrs = set(undocumented_attrs).difference(skipped_attributes)\r\n    if undocumented_attrs:\r\n        raise AssertionError(\r\n            f\"Undocumented attributes for {Estimator.__name__}: {undocumented_attrs}\"\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_additive_chi2_sampler(csr_container):\r\n    # test that AdditiveChi2Sampler approximates kernel on random data\r\n\r\n    # compute exact kernel\r\n    # abbreviations for easier formula\r\n    X_ = X[:, np.newaxis, :].copy()\r\n    Y_ = Y[np.newaxis, :, :].copy()\r\n\r\n    large_kernel = 2 * X_ * Y_ / (X_ + Y_)\r\n\r\n    # reduce to n_samples_x x n_samples_y by summing over features\r\n    kernel = large_kernel.sum(axis=2)\r\n\r\n    # approximate kernel mapping\r\n    transform = AdditiveChi2Sampler(sample_steps=3)\r\n    X_trans = transform.fit_transform(X)\r\n    Y_trans = transform.transform(Y)\r\n\r\n    kernel_approx = np.dot(X_trans, Y_trans.T)\r\n\r\n    assert_array_almost_equal(kernel, kernel_approx, 1)\r\n\r\n    X_sp_trans = transform.fit_transform(csr_container(X))\r\n    Y_sp_trans = transform.transform(csr_container(Y))\r\n\r\n    assert_array_equal(X_trans, X_sp_trans.toarray())\r\n    assert_array_equal(Y_trans, Y_sp_trans.toarray())\r\n\r\n    # test error is raised on negative input\r\n    Y_neg = Y.copy()\r\n    Y_neg[0, 0] = -1\r\n    msg = \"Negative values in data passed to\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        transform.fit(Y_neg)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_skewed_chi2_sampler():\r\n    # test that RBFSampler approximates kernel on random data\r\n\r\n    # compute exact kernel\r\n    c = 0.03\r\n    # set on negative component but greater than c to ensure that the kernel\r\n    # approximation is valid on the group (-c; +\\infty) endowed with the skewed\r\n    # multiplication.\r\n    Y_ = Y.copy()\r\n    Y_[0, 0] = -c / 2.0\r\n\r\n    # abbreviations for easier formula\r\n    X_c = (X + c)[:, np.newaxis, :]\r\n    Y_c = (Y_ + c)[np.newaxis, :, :]\r\n\r\n    # we do it in log-space in the hope that it's more stable\r\n    # this array is n_samples_x x n_samples_y big x n_features\r\n    log_kernel = (\r\n        (np.log(X_c) / 2.0) + (np.log(Y_c) / 2.0) + np.log(2.0) - np.log(X_c + Y_c)\r\n    )\r\n    # reduce to n_samples_x x n_samples_y by summing over features in log-space\r\n    kernel = np.exp(log_kernel.sum(axis=2))\r\n\r\n    # approximate kernel mapping\r\n    transform = SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42)\r\n    X_trans = transform.fit_transform(X)\r\n    Y_trans = transform.transform(Y_)\r\n\r\n    kernel_approx = np.dot(X_trans, Y_trans.T)\r\n    assert_array_almost_equal(kernel, kernel_approx, 1)\r\n    assert np.isfinite(kernel).all(), \"NaNs found in the Gram matrix\"\r\n    assert np.isfinite(kernel_approx).all(), \"NaNs found in the approximate Gram matrix\"\r\n\r\n    # test error is raised on when inputs contains values smaller than -c\r\n    Y_neg = Y_.copy()\r\n    Y_neg[0, 0] = -c * 2.0\r\n    msg = \"X may not contain entries smaller than -skewedness\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        transform.transform(Y_neg)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_nystroem_precomputed_kernel():\r\n    # Non-regression: test Nystroem on precomputed kernel.\r\n    # PR - 14706\r\n    rnd = np.random.RandomState(12)\r\n    X = rnd.uniform(size=(10, 4))\r\n\r\n    K = polynomial_kernel(X, degree=2, coef0=0.1)\r\n    nystroem = Nystroem(kernel=\"precomputed\", n_components=X.shape[0])\r\n    X_transformed = nystroem.fit_transform(K)\r\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\r\n\r\n    # if degree, gamma or coef0 is passed, we raise a ValueError\r\n    msg = \"Don't pass gamma, coef0 or degree to Nystroem\"\r\n    params = ({\"gamma\": 1}, {\"coef0\": 1}, {\"degree\": 2})\r\n    for param in params:\r\n        ny = Nystroem(kernel=\"precomputed\", n_components=X.shape[0], **param)\r\n        with pytest.raises(ValueError, match=msg):\r\n            ny.fit(K)",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_metaestimator_warnings():\r\n    class WeightedMetaRegressorWarn(WeightedMetaRegressor):\r\n        __metadata_request__fit = {\"sample_weight\": metadata_routing.WARN}\r\n\r\n    with pytest.warns(\r\n        UserWarning, match=\"Support for .* has recently been added to this class\"\r\n    ):\r\n        WeightedMetaRegressorWarn(\r\n            estimator=LinearRegression().set_fit_request(sample_weight=False)\r\n        ).fit(X, y, sample_weight=my_weights)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ovo_partial_fit_predict():\r\n    temp = datasets.load_iris()\r\n    X, y = temp.data, temp.target\r\n    ovo1 = OneVsOneClassifier(MultinomialNB())\r\n    ovo1.partial_fit(X[:100], y[:100], np.unique(y))\r\n    ovo1.partial_fit(X[100:], y[100:])\r\n    pred1 = ovo1.predict(X)\r\n\r\n    ovo2 = OneVsOneClassifier(MultinomialNB())\r\n    ovo2.fit(X, y)\r\n    pred2 = ovo2.predict(X)\r\n    assert len(ovo1.estimators_) == n_classes * (n_classes - 1) / 2\r\n    assert np.mean(y == pred1) > 0.65\r\n    assert_almost_equal(pred1, pred2)\r\n\r\n    # Test when mini-batches have binary target classes\r\n    ovo1 = OneVsOneClassifier(MultinomialNB())\r\n    ovo1.partial_fit(X[:60], y[:60], np.unique(y))\r\n    ovo1.partial_fit(X[60:], y[60:])\r\n    pred1 = ovo1.predict(X)\r\n    ovo2 = OneVsOneClassifier(MultinomialNB())\r\n    pred2 = ovo2.fit(X, y).predict(X)\r\n\r\n    assert_almost_equal(pred1, pred2)\r\n    assert len(ovo1.estimators_) == len(np.unique(y))\r\n    assert np.mean(y == pred1) > 0.65\r\n\r\n    ovo = OneVsOneClassifier(MultinomialNB())\r\n    X = np.random.rand(14, 2)\r\n    y = [1, 1, 2, 3, 3, 0, 0, 4, 4, 4, 4, 4, 2, 2]\r\n    ovo.partial_fit(X[:7], y[:7], [0, 1, 2, 3, 4])\r\n    ovo.partial_fit(X[7:], y[7:])\r\n    pred = ovo.predict(X)\r\n    ovo2 = OneVsOneClassifier(MultinomialNB())\r\n    pred2 = ovo2.fit(X, y).predict(X)\r\n    assert_almost_equal(pred, pred2)\r\n\r\n    # raises error when mini-batch does not have classes from all_classes\r\n    ovo = OneVsOneClassifier(MultinomialNB())\r\n    error_y = [0, 1, 2, 3, 4, 5, 2]\r\n    message_re = escape(\r\n        \"Mini-batch contains {0} while it must be subset of {1}\".format(\r\n            np.unique(error_y), np.unique(y)\r\n        )\r\n    )\r\n    with pytest.raises(ValueError, match=message_re):\r\n        ovo.partial_fit(X[:7], error_y, np.unique(y))\r\n\r\n    # test partial_fit only exists if estimator has it:\r\n    ovr = OneVsOneClassifier(SVC())\r\n    assert not hasattr(ovr, \"partial_fit\")",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ovo_one_class():\r\n    # Test error for OvO with one class\r\n    X = np.eye(4)\r\n    y = np.array([\"a\"] * 4)\r\n\r\n    ovo = OneVsOneClassifier(LinearSVC())\r\n    msg = \"when only one class\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ovo.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ovo_float_y():\r\n    # Test that the OvO errors on float targets\r\n    X = iris.data\r\n    y = iris.data[:, 0]\r\n\r\n    ovo = OneVsOneClassifier(LinearSVC())\r\n    msg = \"Unknown label type\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ovo.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ovr_always_present():\r\n    # Test that ovr works with classes that are always present or absent.\r\n    # Note: tests is the case where _ConstantPredictor is utilised\r\n    X = np.ones((10, 2))\r\n    X[:5, :] = 0\r\n\r\n    # Build an indicator matrix where two features are always on.\r\n    # As list of lists, it would be: [[int(i >= 5), 2, 3] for i in range(10)]\r\n    y = np.zeros((10, 3))\r\n    y[5:, 0] = 1\r\n    y[:, 1] = 1\r\n    y[:, 2] = 1\r\n\r\n    ovr = OneVsRestClassifier(LogisticRegression())\r\n    msg = r\"Label .+ is present in all training examples\"\r\n    with pytest.warns(UserWarning, match=msg):\r\n        ovr.fit(X, y)\r\n    y_pred = ovr.predict(X)\r\n    assert_array_equal(np.array(y_pred), np.array(y))\r\n    y_pred = ovr.decision_function(X)\r\n    assert np.unique(y_pred[:, -2:]) == 1\r\n    y_pred = ovr.predict_proba(X)\r\n    assert_array_equal(y_pred[:, -1], np.ones(X.shape[0]))\r\n\r\n    # y has a constantly absent label\r\n    y = np.zeros((10, 2))\r\n    y[5:, 0] = 1  # variable label\r\n    ovr = OneVsRestClassifier(LogisticRegression())\r\n\r\n    msg = r\"Label not 1 is present in all training examples\"\r\n    with pytest.warns(UserWarning, match=msg):\r\n        ovr.fit(X, y)\r\n    y_pred = ovr.predict_proba(X)\r\n    assert_array_equal(y_pred[:, -1], np.zeros(X.shape[0]))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_multi_output_delegate_predict_proba():\r\n    \"\"\"Check the behavior for the delegation of predict_proba to the underlying\r\n    estimator\"\"\"\r\n\r\n    # A base estimator with `predict_proba`should expose the method even before fit\r\n    moc = MultiOutputClassifier(LogisticRegression())\r\n    assert hasattr(moc, \"predict_proba\")\r\n    moc.fit(X, y)\r\n    assert hasattr(moc, \"predict_proba\")\r\n\r\n    # A base estimator without `predict_proba` should raise an AttributeError\r\n    moc = MultiOutputClassifier(LinearSVC())\r\n    assert not hasattr(moc, \"predict_proba\")\r\n\r\n    outer_msg = \"'MultiOutputClassifier' has no attribute 'predict_proba'\"\r\n    inner_msg = \"'LinearSVC' object has no attribute 'predict_proba'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        moc.predict_proba(X)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg == str(exec_info.value.__cause__)\r\n\r\n    moc.fit(X, y)\r\n    assert not hasattr(moc, \"predict_proba\")\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        moc.predict_proba(X)\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg == str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_classifier_chain_tuple_invalid_order():\r\n    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\r\n    y = [[3, 2], [2, 3], [3, 2]]\r\n    order = tuple([1, 2])\r\n\r\n    chain = ClassifierChain(RandomForestClassifier(), order=order)\r\n\r\n    with pytest.raises(ValueError, match=\"invalid order\"):\r\n        chain.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_base_estimator_deprecation(Estimator):\r\n    \"\"\"Check that we warn about the deprecation of `base_estimator`.\"\"\"\r\n    X = np.array([[1, 2], [3, 4]])\r\n    y = np.array([[1, 0], [0, 1]])\r\n\r\n    estimator = LogisticRegression()\r\n\r\n    with pytest.warns(FutureWarning):\r\n        Estimator(base_estimator=estimator).fit(X, y)\r\n\r\n    with pytest.raises(ValueError):\r\n        Estimator(base_estimator=estimator, estimator=estimator).fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_ecoc_float_y():\r\n    # Test that the OCC errors on float targets\r\n    X = iris.data\r\n    y = iris.data[:, 0]\r\n\r\n    ovo = OutputCodeClassifier(LinearSVC())\r\n    msg = \"Unknown label type\"\r\n    with pytest.raises(ValueError, match=msg):\r\n        ovo.fit(X, y)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_classifier_chain_verbose(capsys):\r\n    X, y = make_multilabel_classification(\r\n        n_samples=100, n_features=5, n_classes=3, n_labels=3, random_state=0\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    pattern = (\r\n        r\"\\[Chain\\].*\\(1 of 3\\) Processing order 0, total=.*\\n\"\r\n        r\"\\[Chain\\].*\\(2 of 3\\) Processing order 1, total=.*\\n\"\r\n        r\"\\[Chain\\].*\\(3 of 3\\) Processing order 2, total=.*\\n$\"\r\n    )\r\n\r\n    classifier = ClassifierChain(\r\n        DecisionTreeClassifier(),\r\n        order=[0, 1, 2],\r\n        random_state=0,\r\n        verbose=True,\r\n    )\r\n    classifier.fit(X_train, y_train)\r\n    assert re.match(pattern, capsys.readouterr()[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_regressor_chain_verbose(capsys):\r\n    X, y = make_regression(n_samples=125, n_targets=3, random_state=0)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    pattern = (\r\n        r\"\\[Chain\\].*\\(1 of 3\\) Processing order 1, total=.*\\n\"\r\n        r\"\\[Chain\\].*\\(2 of 3\\) Processing order 0, total=.*\\n\"\r\n        r\"\\[Chain\\].*\\(3 of 3\\) Processing order 2, total=.*\\n$\"\r\n    )\r\n    regressor = RegressorChain(\r\n        LinearRegression(),\r\n        order=[1, 0, 2],\r\n        random_state=0,\r\n        verbose=True,\r\n    )\r\n    regressor.fit(X_train, y_train)\r\n    assert re.match(pattern, capsys.readouterr()[0])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pipeline_set_output_integration():\r\n    \"\"\"Test pipeline's set_output with feature names.\"\"\"\r\n    pytest.importorskip(\"pandas\")\r\n\r\n    X, y = load_iris(as_frame=True, return_X_y=True)\r\n\r\n    pipe = make_pipeline(StandardScaler(), LogisticRegression())\r\n    pipe.set_output(transform=\"pandas\")\r\n    pipe.fit(X, y)\r\n\r\n    feature_names_in_ = pipe[:-1].get_feature_names_out()\r\n    log_reg_feature_names = pipe[-1].feature_names_in_\r\n\r\n    assert_array_equal(feature_names_in_, log_reg_feature_names)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_feature_union_set_output():\r\n    \"\"\"Test feature union with set_output API.\"\"\"\r\n    pd = pytest.importorskip(\"pandas\")\r\n\r\n    X, _ = load_iris(as_frame=True, return_X_y=True)\r\n    X_train, X_test = train_test_split(X, random_state=0)\r\n    union = FeatureUnion([(\"scalar\", StandardScaler()), (\"pca\", PCA())])\r\n    union.set_output(transform=\"pandas\")\r\n    union.fit(X_train)\r\n\r\n    X_trans = union.transform(X_test)\r\n    assert isinstance(X_trans, pd.DataFrame)\r\n    assert_array_equal(X_trans.columns, union.get_feature_names_out())\r\n    assert_array_equal(X_trans.index, X_test.index)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_score_samples_on_pipeline_without_score_samples():\r\n    X = np.array([[1], [2]])\r\n    y = np.array([1, 2])\r\n    # Test that a pipeline does not have score_samples method when the final\r\n    # step of the pipeline does not have score_samples defined.\r\n    pipe = make_pipeline(LogisticRegression())\r\n    pipe.fit(X, y)\r\n\r\n    inner_msg = \"'LogisticRegression' object has no attribute 'score_samples'\"\r\n    outer_msg = \"'Pipeline' has no attribute 'score_samples'\"\r\n    with pytest.raises(AttributeError, match=outer_msg) as exec_info:\r\n        pipe.score_samples(X)\r\n\r\n    assert isinstance(exec_info.value.__cause__, AttributeError)\r\n    assert inner_msg in str(exec_info.value.__cause__)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pipeline_invalid_parameters():\r\n    # Test the various init parameters of the pipeline in fit\r\n    # method\r\n    pipeline = Pipeline([(1, 1)])\r\n    with pytest.raises(TypeError):\r\n        pipeline.fit([[1]], [1])\r\n\r\n    # Check that we can't fit pipelines with objects without fit\r\n    # method\r\n    msg = (\r\n        \"Last step of Pipeline should implement fit \"\r\n        \"or be the string 'passthrough'\"\r\n        \".*NoFit.*\"\r\n    )\r\n    pipeline = Pipeline([(\"clf\", NoFit())])\r\n    with pytest.raises(TypeError, match=msg):\r\n        pipeline.fit([[1]], [1])\r\n\r\n    # Smoke test with only an estimator\r\n    clf = NoTrans()\r\n    pipe = Pipeline([(\"svc\", clf)])\r\n    assert pipe.get_params(deep=True) == dict(\r\n        svc__a=None, svc__b=None, svc=clf, **pipe.get_params(deep=False)\r\n    )\r\n\r\n    # Check that params are set\r\n    pipe.set_params(svc__a=0.1)\r\n    assert clf.a == 0.1\r\n    assert clf.b is None\r\n    # Smoke test the repr:\r\n    repr(pipe)\r\n\r\n    # Test with two objects\r\n    clf = SVC()\r\n    filter1 = SelectKBest(f_classif)\r\n    pipe = Pipeline([(\"anova\", filter1), (\"svc\", clf)])\r\n\r\n    # Check that estimators are not cloned on pipeline construction\r\n    assert pipe.named_steps[\"anova\"] is filter1\r\n    assert pipe.named_steps[\"svc\"] is clf\r\n\r\n    # Check that we can't fit with non-transformers on the way\r\n    # Note that NoTrans implements fit, but not transform\r\n    msg = \"All intermediate steps should be transformers.*\\\\bNoTrans\\\\b.*\"\r\n    pipeline = Pipeline([(\"t\", NoTrans()), (\"svc\", clf)])\r\n    with pytest.raises(TypeError, match=msg):\r\n        pipeline.fit([[1]], [1])\r\n\r\n    # Check that params are set\r\n    pipe.set_params(svc__C=0.1)\r\n    assert clf.C == 0.1\r\n    # Smoke test the repr:\r\n    repr(pipe)\r\n\r\n    # Check that params are not set when naming them wrong\r\n    msg = re.escape(\r\n        \"Invalid parameter 'C' for estimator SelectKBest(). Valid parameters are: ['k',\"\r\n        \" 'score_func'].\"\r\n    )\r\n    with pytest.raises(ValueError, match=msg):\r\n        pipe.set_params(anova__C=0.1)\r\n\r\n    # Test clone\r\n    pipe2 = clone(pipe)\r\n    assert pipe.named_steps[\"svc\"] is not pipe2.named_steps[\"svc\"]\r\n\r\n    # Check that apart from estimators, the parameters are the same\r\n    params = pipe.get_params(deep=True)\r\n    params2 = pipe2.get_params(deep=True)\r\n\r\n    for x in pipe.get_params(deep=False):\r\n        params.pop(x)\r\n\r\n    for x in pipe2.get_params(deep=False):\r\n        params2.pop(x)\r\n\r\n    # Remove estimators that where copied\r\n    params.pop(\"svc\")\r\n    params.pop(\"anova\")\r\n    params2.pop(\"svc\")\r\n    params2.pop(\"anova\")\r\n    assert params == params2",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_pipeline_param_error():\r\n    clf = make_pipeline(LogisticRegression())\r\n    with pytest.raises(\r\n        ValueError, match=\"Pipeline.fit does not accept the sample_weight parameter\"\r\n    ):\r\n        clf.fit([[0], [0]], [0, 1], sample_weight=[1, 1])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_classes_property():\r\n    X = iris.data\r\n    y = iris.target\r\n\r\n    reg = make_pipeline(SelectKBest(k=1), LinearRegression())\r\n    reg.fit(X, y)\r\n    with pytest.raises(AttributeError):\r\n        getattr(reg, \"classes_\")\r\n\r\n    clf = make_pipeline(SelectKBest(k=1), LogisticRegression(random_state=0))\r\n    with pytest.raises(AttributeError):\r\n        getattr(clf, \"classes_\")\r\n    clf.fit(X, y)\r\n    assert_array_equal(clf.classes_, np.unique(y))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error():\r\n    # Test that it gives proper exception on deficient input.\r\n    for name, TreeEstimator in CLF_TREES.items():\r\n        # predict before fit\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict_proba(X)\r\n\r\n        est.fit(X, y)\r\n        X2 = [[-2, -1, 1]]  # wrong feature shape for sample\r\n        with pytest.raises(ValueError):\r\n            est.predict_proba(X2)\r\n\r\n        # Wrong dimensions\r\n        est = TreeEstimator()\r\n        y2 = y[:-1]\r\n        with pytest.raises(ValueError):\r\n            est.fit(X, y2)\r\n\r\n        # Test with arrays that are non-contiguous.\r\n        Xf = np.asfortranarray(X)\r\n        est = TreeEstimator()\r\n        est.fit(Xf, y)\r\n        assert_almost_equal(est.predict(T), true_result)\r\n\r\n        # predict before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.predict(T)\r\n\r\n        # predict on vector with different dims\r\n        est.fit(X, y)\r\n        t = np.asarray(T)\r\n        with pytest.raises(ValueError):\r\n            est.predict(t[:, 1:])\r\n\r\n        # wrong sample shape\r\n        Xt = np.array(X).T\r\n\r\n        est = TreeEstimator()\r\n        est.fit(np.dot(X, Xt), y)\r\n        with pytest.raises(ValueError):\r\n            est.predict(X)\r\n        with pytest.raises(ValueError):\r\n            est.apply(X)\r\n\r\n        clf = TreeEstimator()\r\n        clf.fit(X, y)\r\n        with pytest.raises(ValueError):\r\n            clf.predict(Xt)\r\n        with pytest.raises(ValueError):\r\n            clf.apply(Xt)\r\n\r\n        # apply before fitting\r\n        est = TreeEstimator()\r\n        with pytest.raises(NotFittedError):\r\n            est.apply(T)\r\n\r\n    # non positive target for Poisson splitting Criterion\r\n    est = DecisionTreeRegressor(criterion=\"poisson\")\r\n    with pytest.raises(ValueError, match=\"y is not positive.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [0, 0, 0])\r\n    with pytest.raises(ValueError, match=\"Some.*y are negative.*Poisson\"):\r\n        est.fit([[0, 1, 2]], [5, -0.1, 2])",
        "labels": [
            "Matrix Multiplication API Misused"
        ]
    },
    {
        "code": "def test_big_input():\r\n    # Test if the warning for too large inputs is appropriate.\r\n    X = np.repeat(10**40.0, 4).astype(np.float64).reshape(-1, 1)\r\n    clf = DecisionTreeClassifier()\r\n    with pytest.raises(ValueError, match=\"float32\"):\r\n        clf.fit(X, [0, 1, 0, 1])",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_get_response_error(estimator, X, y, err_msg, params):\r\n    \"\"\"Check that we raise the proper error messages in _get_response_values_binary.\"\"\"\r\n\r\n    estimator.fit(X, y)\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        _get_response_values_binary(estimator, X, **params)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_get_response_values_classifier_inconsistent_y_pred_for_binary_proba(\r\n    response_method,\r\n):\r\n    \"\"\"Check that `_get_response_values` will raise an error when `y_pred` has a\r\n    single class with `predict_proba`.\"\"\"\r\n    X, y_two_class = make_classification(n_samples=10, n_classes=2, random_state=0)\r\n    y_single_class = np.zeros_like(y_two_class)\r\n    classifier = DecisionTreeClassifier().fit(X, y_single_class)\r\n\r\n    err_msg = (\r\n        r\"Got predict_proba of shape \\(10, 1\\), but need classifier with \"\r\n        r\"two classes\"\r\n    )\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        _get_response_values(classifier, X, response_method=response_method)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_set_output_pandas_keep_index():\r\n    \"\"\"Check that set_output does not override index.\r\n\r\n    Non-regression test for gh-25730.\r\n    \"\"\"\r\n    pd = pytest.importorskip(\"pandas\")\r\n\r\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=[0, 1])\r\n    est = EstimatorWithSetOutputIndex().set_output(transform=\"pandas\")\r\n    est.fit(X)\r\n\r\n    X_trans = est.transform(X)\r\n    assert_array_equal(X_trans.index, [\"s0\", \"s1\"])",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_get_response_values_classifier_unknown_pos_label(response_method):\r\n    \"\"\"Check that `_get_response_values` raises the proper error message with\r\n    classifier.\"\"\"\r\n    X, y = make_classification(n_samples=10, n_classes=2, random_state=0)\r\n    classifier = LogisticRegression().fit(X, y)\r\n\r\n    # provide a `pos_label` which is not in `y`\r\n    err_msg = r\"pos_label=whatever is not a valid label: It should be one of \\[0 1\\]\"\r\n    with pytest.raises(ValueError, match=err_msg):\r\n        _get_response_values(\r\n            classifier,\r\n            X,\r\n            response_method=response_method,\r\n            pos_label=\"whatever\",\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_is_fitted():\r\n    # Check is TypeError raised when non estimator instance passed\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(ARDRegression)\r\n    with pytest.raises(TypeError):\r\n        check_is_fitted(\"SVR\")\r\n\r\n    ard = ARDRegression()\r\n    svr = SVR()\r\n\r\n    try:\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(ard)\r\n        with pytest.raises(NotFittedError):\r\n            check_is_fitted(svr)\r\n    except ValueError:\r\n        assert False, \"check_is_fitted failed with ValueError\"\r\n\r\n    # NotFittedError is a subclass of both ValueError and AttributeError\r\n    msg = \"Random message %(name)s, %(name)s\"\r\n    match = \"Random message ARDRegression, ARDRegression\"\r\n    with pytest.raises(ValueError, match=match):\r\n        check_is_fitted(ard, msg=msg)\r\n\r\n    msg = \"Another message %(name)s, %(name)s\"\r\n    match = \"Another message SVR, SVR\"\r\n    with pytest.raises(AttributeError, match=match):\r\n        check_is_fitted(svr, msg=msg)\r\n\r\n    ard.fit(*make_blobs())\r\n    svr.fit(*make_blobs())\r\n\r\n    assert check_is_fitted(ard) is None\r\n    assert check_is_fitted(svr) is None",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_array_links_to_imputer_doc_only_for_X(input_name, retype):\r\n    data = retype(np.arange(4).reshape(2, 2).astype(np.float64))\r\n    data[0, 0] = np.nan\r\n    estimator = SVR()\r\n    extended_msg = (\r\n        f\"\\n{estimator.__class__.__name__} does not accept missing values\"\r\n        \" encoded as NaN natively. For supervised learning, you might want\"\r\n        \" to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor\"\r\n        \" which accept missing values encoded as NaNs natively.\"\r\n        \" Alternatively, it is possible to preprocess the\"\r\n        \" data, for instance by using an imputer transformer in a pipeline\"\r\n        \" or drop samples with missing values. See\"\r\n        \" https://scikit-learn.org/stable/modules/impute.html\"\r\n        \" You can find a list of all estimators that handle NaN values\"\r\n        \" at the following page:\"\r\n        \" https://scikit-learn.org/stable/modules/impute.html\"\r\n        \"#estimators-that-handle-nan-values\"\r\n    )\r\n\r\n    with pytest.raises(ValueError, match=f\"Input {input_name} contains NaN\") as ctx:\r\n        check_array(\r\n            data,\r\n            estimator=estimator,\r\n            input_name=input_name,\r\n            accept_sparse=True,\r\n        )\r\n\r\n    if input_name == \"X\":\r\n        assert extended_msg in ctx.value.args[0]\r\n    else:\r\n        assert extended_msg not in ctx.value.args[0]\r\n\r\n    if input_name == \"X\":\r\n        # Veriy that _validate_data is automatically called with the right argument\r\n        # to generate the same exception:\r\n        with pytest.raises(ValueError, match=f\"Input {input_name} contains NaN\") as ctx:\r\n            SVR().fit(data, np.ones(data.shape[0]))\r\n        assert extended_msg in ctx.value.args[0]",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_openai_callback(self, mocker):\r\n        os.environ[\"PANDASAI_API_URL\"] = \"\"\r\n        os.environ[\"PANDASAI_API_KEY\"] = \"\"\r\n\r\n        df = pd.DataFrame([1, 2, 3])\r\n        llm = OpenAI(api_token=\"test\")\r\n        llm_response = OpenAIObject(\r\n            {\r\n                \"choices\": [\r\n                    {\r\n                        \"text\": \"```df.sum()```\",\r\n                        \"index\": 0,\r\n                        \"logprobs\": None,\r\n                        \"finish_reason\": \"stop\",\r\n                        \"start_text\": \"\",\r\n                    }\r\n                ],\r\n                \"model\": llm.model,\r\n                \"usage\": OpenAIObject(\r\n                    {\r\n                        \"prompt_tokens\": 2,\r\n                        \"completion_tokens\": 1,\r\n                        \"total_tokens\": 3,\r\n                    }\r\n                ),\r\n            }\r\n        )\r\n        mocker.patch.object(llm.client, \"create\", return_value=llm_response)\r\n\r\n        agent = Agent([df], config={\"llm\": llm, \"enable_cache\": False})\r\n        with get_openai_callback() as cb:\r\n            agent.chat(\"some question 1\")\r\n            assert cb.total_tokens == 3\r\n            assert cb.prompt_tokens == 2\r\n            assert cb.completion_tokens == 1\r\n            assert cb.total_cost > 0\r\n\r\n        total_tokens = cb.total_tokens\r\n\r\n        with get_openai_callback() as cb:\r\n            agent.chat(\"some question 2\")\r\n            agent.chat(\"some question 3\")\r\n\r\n        assert cb.total_tokens == total_tokens * 2\r\n\r\n        with get_openai_callback() as cb:\r\n            agent.chat(\"some question 4\")\r\n            agent.chat(\"some question 5\")\r\n            agent.chat(\"some question 6\")\r\n\r\n        assert cb.total_tokens == total_tokens * 3",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_str_with_train_docs_and_qa(\r\n        self, chromadb_mock, output_type, output_type_template\r\n    ):\r\n        \"\"\"Test casting of prompt to string and interpolation of context.\r\n\r\n        Args:\r\n            output_type (str): output type\r\n            output_type_template (str): output type template\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n\r\n        os.environ[\"PANDASAI_API_URL\"] = \"SERVER_URL\"\r\n        os.environ[\"PANDASAI_API_KEY\"] = \"API_KEY\"\r\n\r\n        chromadb_instance = chromadb_mock.return_value\r\n        chromadb_instance.get_relevant_docs_documents.return_value = [[\"documents1\"]]\r\n        chromadb_instance.get_relevant_qa_documents.return_value = [[\"query1\"]]\r\n        llm = FakeLLM()\r\n        agent = Agent(\r\n            PandasConnector({\"original_df\": pd.DataFrame({\"a\": [1], \"b\": [4]})}),\r\n            config={\"llm\": llm},\r\n        )\r\n        agent.train(queries=[\"query1\"], codes=[\"code1\"], docs=[\"document1\"])\r\n        prompt = GeneratePythonCodePrompt(\r\n            context=agent.context,\r\n            output_type=output_type,\r\n        )\r\n\r\n        expected_prompt_content = f\"\"\"<dataframe>\r\ndfs[0]:1x2\r\na,b\r\n1,4\r\n</dataframe>\r\n\r\n\r\n\r\n\r\nUpdate this initial code:\r\n```python\r\n# TODO: import the required dependencies\r\nimport pandas as pd\r\n\r\n# Write code here\r\n\r\n# Declare result var: \r\n{output_type_template}\r\n\r\n```\r\n\r\n\r\nYou can utilize these examples as a reference for generating code.\r\n\r\n['query1']\r\n\r\nHere are additional documents for reference. Feel free to use them to answer.\r\n['documents1']\r\n\r\n\r\n\r\nVariable `dfs: list[pd.DataFrame]` is already declared.\r\n\r\nAt the end, declare \"result\" variable as a dictionary of type and value.\r\n\r\n\r\nGenerate python code and return full updated code:\"\"\"  # noqa E501\r\n        actual_prompt_content = prompt.to_string()\r\n        if sys.platform.startswith(\"win\"):\r\n            actual_prompt_content = actual_prompt_content.replace(\"\\r\\n\", \"\\n\")\r\n\r\n        assert actual_prompt_content == expected_prompt_content",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_str_with_train_docs(\r\n        self, chromadb_mock, output_type, output_type_template\r\n    ):\r\n        \"\"\"Test casting of prompt to string and interpolation of context.\r\n\r\n        Args:\r\n            output_type (str): output type\r\n            output_type_template (str): output type template\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n\r\n        chromadb_instance = chromadb_mock.return_value\r\n        chromadb_instance.get_relevant_docs_documents.return_value = [[\"query1\"]]\r\n        llm = FakeLLM()\r\n        agent = Agent(\r\n            PandasConnector({\"original_df\": pd.DataFrame({\"a\": [1], \"b\": [4]})}),\r\n            config={\"llm\": llm, \"dataframe_serializer\": DataframeSerializerType.CSV},\r\n        )\r\n        agent.train(docs=[\"document1\"])\r\n        prompt = GeneratePythonCodePrompt(\r\n            context=agent.context,\r\n            output_type=output_type,\r\n        )\r\n\r\n        expected_prompt_content = f\"\"\"<dataframe>\r\ndfs[0]:1x2\r\na,b\r\n1,4\r\n</dataframe>\r\n\r\n\r\n\r\n\r\nUpdate this initial code:\r\n```python\r\n# TODO: import the required dependencies\r\nimport pandas as pd\r\n\r\n# Write code here\r\n\r\n# Declare result var: \r\n{output_type_template}\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\nHere are additional documents for reference. Feel free to use them to answer.\r\n['query1']\r\n\r\n\r\n\r\nVariable `dfs: list[pd.DataFrame]` is already declared.\r\n\r\nAt the end, declare \"result\" variable as a dictionary of type and value.\r\n\r\n\r\nGenerate python code and return full updated code:\"\"\"  # noqa E501\r\n        actual_prompt_content = prompt.to_string()\r\n        if sys.platform.startswith(\"win\"):\r\n            actual_prompt_content = actual_prompt_content.replace(\"\\r\\n\", \"\\n\")\r\n\r\n        assert actual_prompt_content == expected_prompt_content",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_str_geenerate_code_prompt_to_json(self, chromadb_mock):\r\n        \"\"\"Test casting of prompt to string and interpolation of context.\r\n\r\n        Args:\r\n            output_type (str): output type\r\n            output_type_template (str): output type template\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n\r\n        chromadb_instance = chromadb_mock.return_value\r\n        chromadb_instance.get_relevant_docs_documents.return_value = [[\"documents1\"]]\r\n        chromadb_instance.get_relevant_qa_documents.return_value = [[\"query1\"]]\r\n        llm = FakeLLM()\r\n        agent = Agent(\r\n            PandasConnector({\"original_df\": pd.DataFrame({\"a\": [1], \"b\": [4]})}),\r\n            config={\"llm\": llm},\r\n        )\r\n        agent.train(queries=[\"query1\"], codes=[\"code1\"], docs=[\"document1\"])\r\n        prompt = GeneratePythonCodePrompt(\r\n            context=agent.context, viz_lib=\"\", output_type=None\r\n        )\r\n        prompt_json = prompt.to_json()\r\n        if sys.platform.startswith(\"win\"):\r\n            prompt_json[\"prompt\"] = prompt_json[\"prompt\"].replace(\"\\r\\n\", \"\\n\")\r\n\r\n        assert prompt_json == {\r\n            \"datasets\": [\r\n                {\"name\": None, \"description\": None, \"head\": [{\"a\": 1, \"b\": 4}]}\r\n            ],\r\n            \"conversation\": [],\r\n            \"system_prompt\": None,\r\n            \"prompt\": '<dataframe>\\ndfs[0]:1x2\\na,b\\n1,4\\n</dataframe>\\n\\n\\n\\n\\nUpdate this initial code:\\n```python\\n# TODO: import the required dependencies\\nimport pandas as pd\\n\\n# Write code here\\n\\n# Declare result var: \\ntype (possible values \"string\", \"number\", \"dataframe\", \"plot\"). Examples: { \"type\": \"string\", \"value\": f\"The highest salary is {highest_salary}.\" } or { \"type\": \"number\", \"value\": 125 } or { \"type\": \"dataframe\", \"value\": pd.DataFrame({...}) } or { \"type\": \"plot\", \"value\": \"temp_chart.png\" }\\n\\n```\\n\\n\\nYou can utilize these examples as a reference for generating code.\\n\\n[\\'query1\\']\\n\\nHere are additional documents for reference. Feel free to use them to answer.\\n[\\'documents1\\']\\n\\n\\n\\nVariable `dfs: list[pd.DataFrame]` is already declared.\\n\\nAt the end, declare \"result\" variable as a dictionary of type and value.\\n\\n\\nGenerate python code and return full updated code:',\r\n            \"config\": {\"direct_sql\": False, \"viz_lib\": \"\", \"output_type\": None},\r\n        }",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_str_relations(self, chromadb_mock, output_type, output_type_template):\r\n        \"\"\"Test casting of prompt to string and interpolation of context.\r\n\r\n        Args:\r\n            output_type (str): output type\r\n            output_type_template (str): output type template\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n\r\n        os.environ[\"PANDASAI_API_URL\"] = \"SERVER_URL\"\r\n        os.environ[\"PANDASAI_API_KEY\"] = \"API_KEY\"\r\n\r\n        chromadb_instance = chromadb_mock.return_value\r\n        chromadb_instance.get_relevant_qa_documents.return_value = [[\"query1\"]]\r\n        llm = FakeLLM()\r\n        agent = Agent(\r\n            PandasConnector(\r\n                {\"original_df\": pd.DataFrame({\"a\": [1], \"b\": [4]})},\r\n                connector_relations=[PrimaryKey(\"a\")],\r\n            ),\r\n            config={\"llm\": llm, \"dataframe_serializer\": DataframeSerializerType.CSV},\r\n        )\r\n        agent.train([\"query1\"], [\"code1\"])\r\n        prompt = GeneratePythonCodePrompt(\r\n            context=agent.context,\r\n            output_type=output_type,\r\n        )\r\n\r\n        expected_prompt_content = f\"\"\"dfs[0]:\r\n  name: null\r\n  description: null\r\n  type: pd.DataFrame\r\n  rows: 1\r\n  columns: 2\r\n  schema:\r\n    fields:\r\n    - name: a\r\n      type: int64\r\n      samples:\r\n      - 1\r\n      constraints: PRIMARY KEY (a)\r\n    - name: b\r\n      type: int64\r\n      samples:\r\n      - 4\r\n\r\n\r\n\r\n\r\nUpdate this initial code:\r\n```python\r\n# TODO: import the required dependencies\r\nimport pandas as pd\r\n\r\n# Write code here\r\n\r\n# Declare result var: \r\n{output_type_template}\r\n\r\n```\r\n\r\n\r\nYou can utilize these examples as a reference for generating code.\r\n\r\n['query1']\r\n\r\n\r\n\r\n\r\n\r\nVariable `dfs: list[pd.DataFrame]` is already declared.\r\n\r\nAt the end, declare \"result\" variable as a dictionary of type and value.\r\n\r\n\r\nGenerate python code and return full updated code:\"\"\"  # noqa E501\r\n        actual_prompt_content = prompt.to_string()\r\n        if sys.platform.startswith(\"win\"):\r\n            actual_prompt_content = actual_prompt_content.replace(\"\\r\\n\", \"\\n\")\r\n\r\n        print(actual_prompt_content)\r\n\r\n        assert actual_prompt_content == expected_prompt_content",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _evaluate_fold(x, meta):\r\n    # unpack args\r\n    X_train = x[\"X_train\"]\r\n    X_test = x[\"X_test\"]\r\n    y_train = x[\"y_train\"]\r\n    y_test = x[\"y_test\"]\r\n    C_train = x.get(\"C_train\", None)\r\n    C_test = x.get(\"C_test\", None)\r\n\r\n    estimator = meta[\"estimator\"]\r\n    scoring = meta[\"scoring\"]\r\n    return_data = meta[\"return_data\"]\r\n    error_score = meta[\"error_score\"]\r\n\r\n    # set default result values in case estimator fitting fails\r\n    score = error_score\r\n    fit_time = np.nan\r\n    pred_time = np.nan\r\n    y_pred = pd.NA\r\n\r\n    # results and cache dictionaries\r\n    temp_result = dict()\r\n    y_preds_cache = dict()\r\n\r\n    try:\r\n        # fit/update\r\n        start_fit = time.perf_counter()\r\n\r\n        estimator = estimator.clone()\r\n        estimator.fit(X_train, y_train, C=C_train)\r\n\r\n        fit_time = time.perf_counter() - start_fit\r\n\r\n        pred_type = {\r\n            \"pred_quantiles\": \"predict_quantiles\",\r\n            \"pred_interval\": \"predict_interval\",\r\n            \"pred_proba\": \"predict_proba\",\r\n            None: \"predict\",\r\n        }\r\n        # predict\r\n        start_pred = time.perf_counter()\r\n        # cache prediction from the first scitype and reuse it to compute other metrics\r\n        for scitype in scoring:\r\n            method = getattr(estimator, pred_type[scitype])\r\n            for metric in scoring.get(scitype):\r\n                pred_args = _get_pred_args_from_metric(scitype, metric)\r\n                if pred_args == {}:\r\n                    time_key = f\"{scitype}_time\"\r\n                    result_key = f\"test_{metric.name}\"\r\n                    y_pred_key = f\"y_{scitype}\"\r\n                else:\r\n                    argval = list(pred_args.values())[0]\r\n                    time_key = f\"{scitype}_{argval}_time\"\r\n                    result_key = f\"test_{metric.name}_{argval}\"\r\n                    y_pred_key = f\"y_{scitype}_{argval}\"\r\n\r\n                # make prediction\r\n                if y_pred_key not in y_preds_cache.keys():\r\n                    start_pred = time.perf_counter()\r\n                    y_pred = method(X_test, **pred_args)\r\n                    pred_time = time.perf_counter() - start_pred\r\n                    temp_result[time_key] = [pred_time]\r\n                    y_preds_cache[y_pred_key] = [y_pred]\r\n                else:\r\n                    y_pred = y_preds_cache[y_pred_key][0]\r\n\r\n                # score prediction and store score\r\n                score = metric(y_test, y_pred, y_train=y_train, C_true=C_test)\r\n                temp_result[result_key] = [score]\r\n\r\n    except Exception as e:\r\n        if error_score == \"raise\":\r\n            raise e\r\n        else:\r\n            warnings.warn(\r\n                f\"\"\"\r\n                In evaluate, fitting of estimator {type(estimator).__name__} failed,\r\n                you can set error_score='raise' in evaluate to see\r\n                the exception message. Fit failed for len(y_train)={len(y_train)}.\r\n                The score will be set to {error_score}.\r\n                Failed estimator with parameters: {estimator}.\r\n                \"\"\",\r\n                stacklevel=2,\r\n            )\r\n\r\n    # format results data frame and return\r\n    temp_result[\"fit_time\"] = [fit_time]\r\n    temp_result[\"pred_time\"] = [pred_time]\r\n    temp_result[\"len_y_train\"] = [len(y_train)]\r\n    if return_data:\r\n        temp_result[\"y_train\"] = [y_train]\r\n        temp_result[\"y_test\"] = [y_test]\r\n        temp_result.update(y_preds_cache)\r\n    result = pd.DataFrame(temp_result)\r\n    result = result.astype({\"len_y_train\": int})\r\n\r\n    column_order = _get_column_order_and_datatype(scoring, return_data)\r\n    result = result.reindex(columns=column_order.keys())\r\n\r\n    return result",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_str_with_train_qa(self, chromadb_mock, output_type, output_type_template):\r\n        \"\"\"Test casting of prompt to string and interpolation of context.\r\n\r\n        Args:\r\n            output_type (str): output type\r\n            output_type_template (str): output type template\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n\r\n        os.environ[\"PANDASAI_API_URL\"] = \"SERVER_URL\"\r\n        os.environ[\"PANDASAI_API_KEY\"] = \"API_KEY\"\r\n\r\n        chromadb_instance = chromadb_mock.return_value\r\n        chromadb_instance.get_relevant_qa_documents.return_value = [[\"query1\"]]\r\n        llm = FakeLLM()\r\n        agent = Agent(\r\n            PandasConnector({\"original_df\": pd.DataFrame({\"a\": [1], \"b\": [4]})}),\r\n            config={\"llm\": llm, \"dataframe_serializer\": DataframeSerializerType.CSV},\r\n        )\r\n        agent.train([\"query1\"], [\"code1\"])\r\n        prompt = GeneratePythonCodePrompt(\r\n            context=agent.context,\r\n            output_type=output_type,\r\n        )\r\n\r\n        expected_prompt_content = f\"\"\"<dataframe>\r\ndfs[0]:1x2\r\na,b\r\n1,4\r\n</dataframe>\r\n\r\n\r\n\r\n\r\nUpdate this initial code:\r\n```python\r\n# TODO: import the required dependencies\r\nimport pandas as pd\r\n\r\n# Write code here\r\n\r\n# Declare result var: \r\n{output_type_template}\r\n\r\n```\r\n\r\n\r\nYou can utilize these examples as a reference for generating code.\r\n\r\n['query1']\r\n\r\n\r\n\r\n\r\n\r\nVariable `dfs: list[pd.DataFrame]` is already declared.\r\n\r\nAt the end, declare \"result\" variable as a dictionary of type and value.\r\n\r\n\r\nGenerate python code and return full updated code:\"\"\"  # noqa E501\r\n        actual_prompt_content = prompt.to_string()\r\n        if sys.platform.startswith(\"win\"):\r\n            actual_prompt_content = actual_prompt_content.replace(\"\\r\\n\", \"\\n\")\r\n\r\n        assert actual_prompt_content == expected_prompt_content",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def _predict_quantiles(self, X, alpha):\r\n        \"\"\"Compute/return quantile predictions.\r\n\r\n        private _predict_quantiles containing the core logic,\r\n            called from predict_quantiles and default _predict_interval\r\n\r\n        Parameters\r\n        ----------\r\n        X : pandas DataFrame, must have same columns as X in `fit`\r\n            data to predict labels for\r\n        alpha : guaranteed list of float\r\n            A list of probabilities at which quantile predictions are computed.\r\n\r\n        Returns\r\n        -------\r\n        quantiles : pd.DataFrame\r\n            Column has multi-index: first level is variable name from ``y`` in fit,\r\n                second level being the values of alpha passed to the function.\r\n            Row index is equal to row index of ``X``.\r\n            Entries are quantile predictions, for var in col index,\r\n                at quantile probability in second col index, for the row index.\r\n        \"\"\"\r\n        quantiles = alpha\r\n\r\n        if self.feature_groups is not None:\r\n            feature_names = list()\r\n            for feature in self.feature_groups:\r\n                if isinstance(feature, tuple):\r\n                    for f in feature:\r\n                        feature_names.append(f)\r\n                else:\r\n                    feature_names.append(feature)\r\n            if not set(feature_names).issubset(set(X.columns)):\r\n                raise ValueError(f\"{feature} is not in X\")\r\n\r\n        is_given_proba = False\r\n        warning = (\r\n            \"{} percentile doesn't trained, return QPD's quantile value, \"\r\n            \"which is given by predict_proba(), \"\r\n            \"if you need more plausible quantile value, \"\r\n            \"please train regressor again for specified quantile estimation\"\r\n        )\r\n        if isinstance(quantiles, list):\r\n            for q in quantiles:\r\n                if not (q in self.quantiles):\r\n                    warnings.warn(warning.format(q), stacklevel=2)\r\n                    is_given_proba = True\r\n        elif isinstance(quantiles, float):\r\n            if not (quantiles in self.quantiles):\r\n                warnings.warn(warning.format(quantiles), stacklevel=2)\r\n                is_given_proba = True\r\n        else:\r\n            raise ValueError(\"quantile needs to be float or list of floats\")\r\n\r\n        index = X.index\r\n        y_cols = self._y_cols\r\n\r\n        columns = pd.MultiIndex.from_product(\r\n            [y_cols, quantiles],\r\n        )\r\n\r\n        # predict quantiles\r\n        self.quantile_values = list()\r\n        if is_given_proba:\r\n            qpd = self.predict_proba(X.copy())\r\n            pred = np.asarray([np.squeeze(qpd.ppf(q)) for q in quantiles]).T\r\n            quantiles = pd.DataFrame(pred, index=X.index, columns=columns)\r\n\r\n        else:\r\n            for est in self.quantile_est:\r\n                yhat = est.predict(X.copy())\r\n                self.quantile_values.append(yhat)\r\n\r\n            quantiles = pd.DataFrame(\r\n                np.transpose(self.quantile_values), index=index, columns=columns\r\n            )\r\n\r\n        return quantiles",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def create_aishell1mix_metadata(\r\n    aishell1_dir, aishell1_md_dir, wham_dir, wham_md_dir, md_dir, n_src\r\n):\r\n    \"\"\"Generate aishell1mix metadata according to aishell1 metadata\"\"\"\r\n\r\n    # Dataset name\r\n    dataset = f\"aishell1mix{n_src}\"\r\n    # List metadata files in aishell1\r\n    aishell1_md_files = os.listdir(aishell1_md_dir)\r\n    # List metadata files in wham_noise\r\n    wham_md_files = os.listdir(wham_md_dir)\r\n    # If you wish to ignore some metadata files add their name here\r\n    # Example : to_be_ignored = ['dev-other.csv']\r\n    to_be_ignored = []\r\n\r\n    check_already_generated(md_dir, dataset, to_be_ignored, aishell1_md_files)\r\n    # Go through each metadata file and create metadata accordingly\r\n    for aishell1_md_file in aishell1_md_files:\r\n        if not aishell1_md_file.endswith(\".csv\"):\r\n            print(f\"{aishell1_md_file} is not a csv file, continue.\")\r\n            continue\r\n        # Get the name of the corresponding noise md file\r\n        try:\r\n            wham_md_file = [\r\n                f for f in wham_md_files if f.startswith(aishell1_md_file)\r\n            ][0]\r\n        except IndexError:\r\n            print(\r\n                \"Wham metadata are missing you can either generate the \"\r\n                \"missing wham files or add the aishell1 metadata to \"\r\n                \"to_be_ignored list\"\r\n            )\r\n            break\r\n\r\n        # Open .csv files from aishell1\r\n        aishell1_md = pd.read_csv(\r\n            os.path.join(aishell1_md_dir, aishell1_md_file), engine=\"python\"\r\n        )\r\n        # Open .csv files from wham_noise\r\n        wham_md = pd.read_csv(\r\n            os.path.join(wham_md_dir, wham_md_file), engine=\"python\"\r\n        )\r\n        # Filenames\r\n        save_path = os.path.join(md_dir, \"_\".join([dataset, aishell1_md_file]))\r\n        info_name = (\r\n            \"_\".join([dataset, aishell1_md_file.split(\".\")[0], \"info\"]) + \".csv\"\r\n        )\r\n        info_save_path = os.path.join(md_dir, info_name)\r\n        print(f\"Creating {os.path.basename(save_path)} file in {md_dir}\")\r\n        # Create dataframe\r\n        mixtures_md, mixtures_info = create_aishell1mix_df(\r\n            aishell1_md, aishell1_dir, wham_md, wham_dir, n_src\r\n        )\r\n        # Round number of files\r\n        mixtures_md = mixtures_md[: len(mixtures_md) // 100 * 100]\r\n        mixtures_info = mixtures_info[: len(mixtures_info) // 100 * 100]\r\n\r\n        # Save csv files\r\n        mixtures_md.to_csv(save_path, index=False)\r\n        mixtures_info.to_csv(info_save_path, index=False)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def check_already_generated(md_dir, dataset, to_be_ignored, aishell1_md_files):\r\n    # Check if the metadata files in aishell1 already have been used\r\n    already_generated = os.listdir(md_dir)\r\n    for generated in already_generated:\r\n        if generated.startswith(f\"{dataset}\") and \"info\" not in generated:\r\n            if \"train\" in generated:\r\n                to_be_ignored.append(\"train.csv\")\r\n            elif \"dev\" in generated:\r\n                to_be_ignored.append(\"dev.csv\")\r\n            elif \"test\" in generated:\r\n                to_be_ignored.append(\"test.csv\")\r\n            print(\r\n                f\"{generated} already exists in \"\r\n                f\"{md_dir} it won't be overwritten\"\r\n            )\r\n    for element in to_be_ignored:\r\n        aishell1_md_files.remove(element)",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def dataio_prep(hparams):\r\n    \"Creates the datasets and their data processing pipelines.\"\r\n\r\n    data_audio_folder = hparams[\"audio_data_folder\"]\r\n    config_sample_rate = hparams[\"sample_rate\"]\r\n    label_encoder = sb.dataio.encoder.CategoricalEncoder()\r\n    hparams[\"resampler\"] = torchaudio.transforms.Resample(\r\n        new_freq=config_sample_rate\r\n    )\r\n\r\n    # 2. Define audio pipeline:\r\n    @sb.utils.data_pipeline.takes(\"wav\")\r\n    @sb.utils.data_pipeline.provides(\"sig\")\r\n    def audio_pipeline(wav):\r\n        \"\"\"Load the signal, and pass it and its length to the corruption class.\r\n        This is done on the CPU in the `collate_fn`.\"\"\"\r\n\r\n        wave_file = data_audio_folder + \"/{:}\".format(wav)\r\n\r\n        sig, read_sr = torchaudio.load(wave_file)\r\n\r\n        # If multi-channels, downmix it to a mono channel\r\n        sig = torch.squeeze(sig)\r\n        if len(sig.shape) > 1:\r\n            sig = torch.mean(sig, dim=0)\r\n\r\n        # Convert sample rate to required config_sample_rate\r\n        if read_sr != config_sample_rate:\r\n            # Re-initialize sampler if source file sample rate changed compared to last file\r\n            if read_sr != hparams[\"resampler\"].orig_freq:\r\n                hparams[\"resampler\"] = torchaudio.transforms.Resample(\r\n                    orig_freq=read_sr, new_freq=config_sample_rate\r\n                )\r\n            # Resample audio\r\n            sig = hparams[\"resampler\"].forward(sig)\r\n\r\n        sig = sig.float()\r\n        sig = sig / sig.max()\r\n        return sig\r\n\r\n    # 3. Define label pipeline:\r\n    @sb.utils.data_pipeline.takes(\"class_string\")\r\n    @sb.utils.data_pipeline.provides(\"class_string\", \"class_string_encoded\")\r\n    def label_pipeline(class_string):\r\n        yield class_string\r\n        class_string_encoded = label_encoder.encode_label_torch(class_string)\r\n        yield class_string_encoded\r\n\r\n    # Define datasets. We also connect the dataset with the data processing\r\n    # functions defined above.\r\n    datasets = {}\r\n    data_info = {\r\n        \"train\": hparams[\"train_annotation\"],\r\n        \"valid\": hparams[\"valid_annotation\"],\r\n        \"test\": hparams[\"test_annotation\"],\r\n    }\r\n    for dataset in data_info:\r\n        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\r\n            json_path=data_info[dataset],\r\n            replacements={\"data_root\": hparams[\"data_folder\"]},\r\n            dynamic_items=[audio_pipeline, label_pipeline],\r\n            output_keys=[\"id\", \"sig\", \"class_string_encoded\"],\r\n        )\r\n\r\n    # Load or compute the label encoder (with multi-GPU DDP support)\r\n    # Please, take a look into the lab_enc_file to see the label to index\r\n    # mappinng.\r\n    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\r\n    label_encoder.load_or_create(\r\n        path=lab_enc_file,\r\n        from_didatasets=[datasets[\"train\"]],\r\n        output_key=\"class_string\",\r\n    )\r\n\r\n    return datasets, label_encoder",
        "labels": [
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def prepare_dvoice(\r\n    data_folder,\r\n    save_folder,\r\n    train_csv_file=None,\r\n    dev_csv_file=None,\r\n    test_csv_file=None,\r\n    accented_letters=False,\r\n    language=\"fongbe\",\r\n    skip_prep=False,\r\n):\r\n    if skip_prep:\r\n        return\r\n\r\n    # If not specified point toward standard location w.r.t DVoice tree\r\n    if train_csv_file is None:\r\n        train_csv_file = data_folder + \"texts/train.csv\"\r\n    else:\r\n        train_csv_file = train_csv_file\r\n\r\n    if dev_csv_file is None:\r\n        dev_csv_file = data_folder + \"texts/dev.csv\"\r\n    else:\r\n        dev_csv_file = dev_csv_file\r\n\r\n    if test_csv_file is None:\r\n        test_csv_file = data_folder + \"texts/test.csv\"\r\n    else:\r\n        test_csv_file = test_csv_file\r\n\r\n    # Setting the save folder\r\n    if not os.path.exists(save_folder):\r\n        os.makedirs(save_folder)\r\n\r\n    # Setting the ALFFA-Dataset csv files\r\n    ALFFA_LANGUAGES = [\"amharic\", \"fongbe\", \"wolof\"]\r\n    if language in ALFFA_LANGUAGES:\r\n        df = alffa_public_prepare(language, data_folder)\r\n        train, dev, test = train_validate_test_split(df)\r\n        train.to_csv(f\"{data_folder}/train.csv\", index=False, sep=\"\\t\")\r\n        dev.to_csv(f\"{data_folder}/dev.csv\", index=False, sep=\"\\t\")\r\n        test.to_csv(f\"{data_folder}/test.csv\", index=False, sep=\"\\t\")\r\n\r\n    if language == \"swahili\":\r\n        df = swahili_prepare(data_folder)\r\n        train, dev, test = train_validate_test_split(df)\r\n        train.to_csv(f\"{data_folder}/train.csv\", index=False, sep=\"\\t\")\r\n        dev.to_csv(f\"{data_folder}/dev.csv\", index=False, sep=\"\\t\")\r\n        test.to_csv(f\"{data_folder}/test.csv\", index=False, sep=\"\\t\")\r\n\r\n    if language == \"multilingual\":\r\n        ALFFA_LANGUAGES = [\"amharic\", \"wolof\"]\r\n        df_alffa = pd.DataFrame()\r\n        for lang in ALFFA_LANGUAGES:\r\n            data_folder2 = (\r\n                data_folder + f\"/ALFFA_PUBLIC/ASR/{lang.upper()}/data\"\r\n            )\r\n            df_l = alffa_public_prepare(lang, data_folder2)\r\n            df_l[\"wav\"] = df_l[\"wav\"].map(\r\n                lambda x: f\"ALFFA_PUBLIC/ASR/{lang.upper()}/data/\"\r\n                + x.replace(f\"{data_folder}/\", \"\")\r\n            )\r\n            df_alffa = pd.concat([df_alffa, df_l], ignore_index=True)\r\n        df_sw = swahili_prepare(data_folder)\r\n\r\n        train_darija = pd.read_csv(\r\n            f\"{data_folder}/DVOICE/darija/texts/train.csv\", sep=\"\\t\"\r\n        )\r\n        dev_darija = pd.read_csv(\r\n            f\"{data_folder}/DVOICE/darija/texts/dev.csv\", sep=\"\\t\"\r\n        )\r\n        test_darija = pd.read_csv(\r\n            f\"{data_folder}/DVOICE/darija/texts/test.csv\", sep=\"\\t\"\r\n        )\r\n        df_dar = pd.concat(\r\n            [train_darija, dev_darija, test_darija], ignore_index=True\r\n        )\r\n        df_dar[\"wav\"] = df_dar[\"wav\"].map(lambda x: \"DVOICE/darija/wavs/\" + x)\r\n        df = pd.concat([df_alffa, df_sw, df_dar], ignore_index=True)\r\n        train, dev, test = train_validate_test_split(df)\r\n        train.to_csv(f\"{data_folder}/train.csv\", index=False, sep=\"\\t\")\r\n        dev.to_csv(f\"{data_folder}/dev.csv\", index=False, sep=\"\\t\")\r\n        test.to_csv(f\"{data_folder}/test.csv\", index=False, sep=\"\\t\")\r\n\r\n    # Setting output files\r\n    save_csv_train = save_folder + \"/train.csv\"\r\n    save_csv_dev = save_folder + \"/dev.csv\"\r\n    save_csv_test = save_folder + \"/test.csv\"\r\n\r\n    # If csv already exists, we skip the data preparation\r\n    if skip(save_csv_train, save_csv_dev, save_csv_test):\r\n        msg = \"%s already exists, skipping data preparation!\" % (save_csv_train)\r\n        logger.info(msg)\r\n\r\n        msg = \"%s already exists, skipping data preparation!\" % (save_csv_dev)\r\n        logger.info(msg)\r\n\r\n        msg = \"%s already exists, skipping data preparation!\" % (save_csv_test)\r\n        logger.info(msg)\r\n\r\n        return\r\n\r\n    # Additional checks to make sure the folder contains the data\r\n    check_dvoice_folders(data_folder, language)\r\n\r\n    # Creating csv file for training data\r\n    if train_csv_file is not None:\r\n        create_csv(\r\n            train_csv_file,\r\n            save_csv_train,\r\n            data_folder,\r\n            accented_letters,\r\n            language,\r\n        )\r\n\r\n    # Creating csv file for dev data\r\n    if dev_csv_file is not None:\r\n        create_csv(\r\n            dev_csv_file,\r\n            save_csv_dev,\r\n            data_folder,\r\n            accented_letters,\r\n            language,\r\n        )\r\n\r\n    # Creating csv file for test data\r\n    if test_csv_file is not None:\r\n        create_csv(\r\n            test_csv_file,\r\n            save_csv_test,\r\n            data_folder,\r\n            accented_letters,\r\n            language,\r\n        )",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_check_feature_names_in_pandas():\r\n    \"\"\"Check behavior of check_feature_names_in for pandas dataframes.\"\"\"\r\n    pd = pytest.importorskip(\"pandas\")\r\n    names = [\"a\", \"b\", \"c\"]\r\n    df = pd.DataFrame([[0.0, 1.0, 2.0]], columns=names)\r\n    est = PassthroughTransformer().fit(df)\r\n\r\n    names = est.get_feature_names_out()\r\n    assert_array_equal(names, [\"a\", \"b\", \"c\"])\r\n\r\n    with pytest.raises(ValueError, match=\"input_features is not equal to\"):\r\n        est.get_feature_names_out([\"x1\", \"x2\", \"x3\"])",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def check_dataframe_column_names_consistency(name, estimator_orig):\r\n    try:\r\n        import pandas as pd\r\n    except ImportError:\r\n        raise SkipTest(\r\n            \"pandas is not installed: not checking column name consistency for pandas\"\r\n        )\r\n\r\n    tags = get_tags(estimator_orig)\r\n    is_supported_X_types = tags.input_tags.two_d_array or tags.input_tags.categorical\r\n\r\n    if not is_supported_X_types or tags.no_validation:\r\n        return\r\n\r\n    rng = np.random.RandomState(0)\r\n\r\n    estimator = clone(estimator_orig)\r\n    set_random_state(estimator)\r\n\r\n    X_orig = rng.normal(size=(150, 8))\r\n\r\n    X_orig = _enforce_estimator_tags_X(estimator, X_orig)\r\n    n_samples, n_features = X_orig.shape\r\n\r\n    names = np.array([f\"col_{i}\" for i in range(n_features)])\r\n    X = pd.DataFrame(X_orig, columns=names, copy=False)\r\n\r\n    if is_regressor(estimator):\r\n        y = rng.normal(size=n_samples)\r\n    else:\r\n        y = rng.randint(low=0, high=2, size=n_samples)\r\n    y = _enforce_estimator_tags_y(estimator, y)\r\n\r\n    # Check that calling `fit` does not raise any warnings about feature names.\r\n    with warnings.catch_warnings():\r\n        warnings.filterwarnings(\r\n            \"error\",\r\n            message=\"X does not have valid feature names\",\r\n            category=UserWarning,\r\n            module=\"sklearn\",\r\n        )\r\n        estimator.fit(X, y)\r\n\r\n    if not hasattr(estimator, \"feature_names_in_\"):\r\n        raise ValueError(\r\n            \"Estimator does not have a feature_names_in_ \"\r\n            \"attribute after fitting with a dataframe\"\r\n        )\r\n    assert isinstance(estimator.feature_names_in_, np.ndarray)\r\n    assert estimator.feature_names_in_.dtype == object\r\n    assert_array_equal(estimator.feature_names_in_, names)\r\n\r\n    # Only check sklearn estimators for feature_names_in_ in docstring\r\n    module_name = estimator_orig.__module__\r\n    if (\r\n        module_name.startswith(\"sklearn.\")\r\n        and not (\"test_\" in module_name or module_name.endswith(\"_testing\"))\r\n        and (\"feature_names_in_\" not in (estimator_orig.__doc__))\r\n    ):\r\n        raise ValueError(\r\n            f\"Estimator {name} does not document its feature_names_in_ attribute\"\r\n        )\r\n\r\n    check_methods = []\r\n    for method in (\r\n        \"predict\",\r\n        \"transform\",\r\n        \"decision_function\",\r\n        \"predict_proba\",\r\n        \"score\",\r\n        \"score_samples\",\r\n        \"predict_log_proba\",\r\n    ):\r\n        if not hasattr(estimator, method):\r\n            continue\r\n\r\n        callable_method = getattr(estimator, method)\r\n        if method == \"score\":\r\n            callable_method = partial(callable_method, y=y)\r\n        check_methods.append((method, callable_method))\r\n\r\n    for _, method in check_methods:\r\n        with warnings.catch_warnings():\r\n            warnings.filterwarnings(\r\n                \"error\",\r\n                message=\"X does not have valid feature names\",\r\n                category=UserWarning,\r\n                module=\"sklearn\",\r\n            )\r\n            method(X)  # works without UserWarning for valid features\r\n\r\n    invalid_names = [\r\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\r\n        (\r\n            [f\"another_prefix_{i}\" for i in range(n_features)],\r\n            (\r\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\r\n                \" another_prefix_1\\n\"\r\n            ),\r\n        ),\r\n        (\r\n            names[:3],\r\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\r\n        ),\r\n    ]\r\n    params = {\r\n        key: value\r\n        for key, value in estimator.get_params().items()\r\n        if \"early_stopping\" in key\r\n    }\r\n    early_stopping_enabled = any(value is True for value in params.values())\r\n\r\n    for invalid_name, additional_message in invalid_names:\r\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\r\n\r\n        expected_msg = re.escape(\r\n            \"The feature names should match those that were passed during fit.\\n\"\r\n            f\"{additional_message}\"\r\n        )\r\n        for name, method in check_methods:\r\n            with raises(\r\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\r\n            ):\r\n                method(X_bad)\r\n\r\n        # partial_fit checks on second call\r\n        # Do not call partial fit if early_stopping is on\r\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\r\n            continue\r\n\r\n        estimator = clone(estimator_orig)\r\n        if is_classifier(estimator):\r\n            classes = np.unique(y)\r\n            estimator.partial_fit(X, y, classes=classes)\r\n        else:\r\n            estimator.partial_fit(X, y)\r\n\r\n        with raises(ValueError, match=expected_msg):\r\n            estimator.partial_fit(X_bad, y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def dataio_prepare(hparams):\r\n    \"\"\"This function prepares the datasets to be used in the brain class.\r\n    It also defines the data processing pipeline through user-defined functions.\r\n    \"\"\"\r\n\r\n    data_folder = hparams[\"data_folder\"]\r\n\r\n    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_train\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n\r\n    if hparams[\"sorting\"] == \"ascending\":\r\n        # we sort training data to speed up training and get better results.\r\n        train_data = train_data.filtered_sorted(sort_key=\"duration\")\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"descending\":\r\n        train_data = train_data.filtered_sorted(\r\n            sort_key=\"duration\", reverse=True\r\n        )\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"random\":\r\n        pass\r\n\r\n    else:\r\n        raise NotImplementedError(\r\n            \"sorting must be random, ascending or descending\"\r\n        )\r\n\r\n    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_valid\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_test\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    test_data = test_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    datasets = [train_data, valid_data, test_data]\r\n\r\n    tokenizer = hparams[\"tokenizer\"]\r\n\r\n    # 2. Define audio pipeline:\r\n    @sb.utils.data_pipeline.takes(\"wav\")\r\n    @sb.utils.data_pipeline.provides(\"sig\")\r\n    def audio_pipeline(wav):\r\n        sig = sb.dataio.dataio.read_audio(wav)\r\n        return sig\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\r\n\r\n    # 3. Define text pipeline:\r\n    @sb.utils.data_pipeline.takes(\"semantics\")\r\n    @sb.utils.data_pipeline.provides(\r\n        \"semantics\", \"token_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\r\n    )\r\n    def text_pipeline(semantics):\r\n        yield semantics\r\n        tokens_list = tokenizer.encode_as_ids(semantics)\r\n        yield tokens_list\r\n        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\r\n        yield tokens_bos\r\n        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\r\n        yield tokens_eos\r\n        tokens = torch.LongTensor(tokens_list)\r\n        yield tokens\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\r\n\r\n    # 4. Set output:\r\n    sb.dataio.dataset.set_output_keys(\r\n        datasets,\r\n        [\"id\", \"sig\", \"semantics\", \"tokens_bos\", \"tokens_eos\", \"tokens\"],\r\n    )\r\n    return train_data, valid_data, test_data, tokenizer",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def dataio_prepare(hparams):\r\n    \"\"\"This function prepares the datasets to be used in the brain class.\r\n    It also defines the data processing pipeline through user-defined functions.\r\n    \"\"\"\r\n\r\n    data_folder = hparams[\"data_folder\"]\r\n\r\n    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_train\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n\r\n    if hparams[\"sorting\"] == \"ascending\":\r\n        # we sort training data to speed up training and get better results.\r\n        train_data = train_data.filtered_sorted(sort_key=\"duration\")\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"descending\":\r\n        train_data = train_data.filtered_sorted(\r\n            sort_key=\"duration\", reverse=True\r\n        )\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"random\":\r\n        pass\r\n\r\n    else:\r\n        raise NotImplementedError(\r\n            \"sorting must be random, ascending or descending\"\r\n        )\r\n\r\n    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_valid\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_test\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    test_data = test_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    datasets = [train_data, valid_data, test_data]\r\n\r\n    tokenizer = hparams[\"tokenizer\"]\r\n\r\n    # 2. Define audio pipeline:\r\n    @sb.utils.data_pipeline.takes(\"wav\")\r\n    @sb.utils.data_pipeline.provides(\"sig\")\r\n    def audio_pipeline(wav):\r\n        sig = sb.dataio.dataio.read_audio(wav)\r\n        return sig\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\r\n\r\n    # 3. Define text pipeline:\r\n    @sb.utils.data_pipeline.takes(\"semantics\")\r\n    @sb.utils.data_pipeline.provides(\r\n        \"semantics\", \"token_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\r\n    )\r\n    def text_pipeline(semantics):\r\n        yield semantics\r\n        tokens_list = tokenizer.encode_as_ids(semantics)\r\n        yield tokens_list\r\n        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\r\n        yield tokens_bos\r\n        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\r\n        yield tokens_eos\r\n        tokens = torch.LongTensor(tokens_list)\r\n        yield tokens\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\r\n\r\n    # 4. Set output:\r\n    sb.dataio.dataset.set_output_keys(\r\n        datasets,\r\n        [\"id\", \"sig\", \"semantics\", \"tokens_bos\", \"tokens_eos\", \"tokens\"],\r\n    )\r\n    return train_data, valid_data, test_data, tokenizer",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def dataio_prepare(hparams):\r\n    \"\"\"This function prepares the datasets to be used in the brain class.\r\n    It also defines the data processing pipeline through user-defined functions.\r\n    \"\"\"\r\n\r\n    data_folder = hparams[\"data_folder\"]\r\n\r\n    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_train\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n\r\n    if hparams[\"sorting\"] == \"ascending\":\r\n        # we sort training data to speed up training and get better results.\r\n        train_data = train_data.filtered_sorted(sort_key=\"duration\")\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"descending\":\r\n        train_data = train_data.filtered_sorted(\r\n            sort_key=\"duration\", reverse=True\r\n        )\r\n        # when sorting do not shuffle in dataloader ! otherwise is pointless\r\n        hparams[\"dataloader_opts\"][\"shuffle\"] = False\r\n\r\n    elif hparams[\"sorting\"] == \"random\":\r\n        pass\r\n\r\n    else:\r\n        raise NotImplementedError(\r\n            \"sorting must be random, ascending or descending\"\r\n        )\r\n\r\n    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_valid\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\r\n        csv_path=hparams[\"csv_test\"],\r\n        replacements={\"data_root\": data_folder},\r\n    )\r\n    test_data = test_data.filtered_sorted(sort_key=\"duration\")\r\n\r\n    datasets = [train_data, valid_data, test_data]\r\n\r\n    asr_tokenizer = hparams[\"asr_tokenizer\"]\r\n    slu_tokenizer = hparams[\"slu_tokenizer\"]\r\n\r\n    # 2. Define input pipeline:\r\n    @sb.utils.data_pipeline.takes(\"transcript\")\r\n    @sb.utils.data_pipeline.provides(\"transcript\", \"transcript_tokens\")\r\n    def transcript_pipeline(transcript):\r\n        yield transcript\r\n        transcript_tokens_list = asr_tokenizer.encode_as_ids(transcript)\r\n        transcript_tokens = torch.LongTensor(transcript_tokens_list)\r\n        yield transcript_tokens\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, transcript_pipeline)\r\n\r\n    # 3. Define output pipeline:\r\n    @sb.utils.data_pipeline.takes(\"semantics\")\r\n    @sb.utils.data_pipeline.provides(\r\n        \"semantics\",\r\n        \"semantics_token_list\",\r\n        \"semantics_tokens_bos\",\r\n        \"semantics_tokens_eos\",\r\n        \"semantics_tokens\",\r\n    )\r\n    def semantics_pipeline(semantics):\r\n        yield semantics\r\n        semantics_tokens_list = slu_tokenizer.encode_as_ids(semantics)\r\n        yield semantics_tokens_list\r\n        semantics_tokens_bos = torch.LongTensor(\r\n            [hparams[\"bos_index\"]] + (semantics_tokens_list)\r\n        )\r\n        yield semantics_tokens_bos\r\n        semantics_tokens_eos = torch.LongTensor(\r\n            semantics_tokens_list + [hparams[\"eos_index\"]]\r\n        )\r\n        yield semantics_tokens_eos\r\n        semantics_tokens = torch.LongTensor(semantics_tokens_list)\r\n        yield semantics_tokens\r\n\r\n    sb.dataio.dataset.add_dynamic_item(datasets, semantics_pipeline)\r\n\r\n    # 4. Set output:\r\n    sb.dataio.dataset.set_output_keys(\r\n        datasets,\r\n        [\r\n            \"id\",\r\n            \"transcript\",\r\n            \"transcript_tokens\",\r\n            \"semantics\",\r\n            \"semantics_tokens_bos\",\r\n            \"semantics_tokens_eos\",\r\n            \"semantics_tokens\",\r\n        ],\r\n    )\r\n    return train_data, valid_data, test_data, asr_tokenizer, slu_tokenizer",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def fit_batch(self, batch):\r\n        \"Compute gradients and update either D or G based on sub-stage.\"\r\n        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\r\n        loss_tracker = 0\r\n        if self.sub_stage == SubStage.CURRENT:\r\n            for mode in [\"enh\", \"noisy\"]:\r\n                loss = self.compute_objectives(\r\n                    predictions, batch, sb.Stage.TRAIN, f\"D_{mode}\"\r\n                )\r\n                self.d_optimizer.zero_grad()\r\n                loss.backward()\r\n                torch.nn.utils.clip_grad_norm_(\r\n                    self.modules.parameters(), self.max_grad_norm\r\n                )\r\n                self.d_optimizer.step()\r\n                loss_tracker += loss.detach() / 3\r\n        elif self.sub_stage == SubStage.HISTORICAL:\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"D_enh\"\r\n            )\r\n            self.d_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.d_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n        elif self.sub_stage == SubStage.GENERATOR:\r\n            for name, param in self.modules.generator.named_parameters():\r\n                if \"Learnable_sigmoid\" in name:\r\n                    param.data = torch.clamp(\r\n                        param, max=3.5\r\n                    )  # to prevent gradient goes to infinity\r\n\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"generator\"\r\n            )\r\n            self.g_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.g_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n\r\n        return loss_tracker",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fit_batch(self, batch):\r\n        \"Compute gradients and update either D or G based on sub-stage.\"\r\n        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\r\n        loss_tracker = 0\r\n        if self.sub_stage == SubStage.CURRENT:\r\n            for mode in [\"clean\", \"enh\", \"noisy\"]:\r\n                loss = self.compute_objectives(\r\n                    predictions, batch, sb.Stage.TRAIN, f\"D_{mode}\"\r\n                )\r\n                self.d_optimizer.zero_grad()\r\n                loss.backward()\r\n                torch.nn.utils.clip_grad_norm_(\r\n                    self.modules.parameters(), self.max_grad_norm\r\n                )\r\n                self.d_optimizer.step()\r\n                loss_tracker += loss.detach() / 3\r\n        elif self.sub_stage == SubStage.HISTORICAL:\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"D_enh\"\r\n            )\r\n            self.d_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.d_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n        elif self.sub_stage == SubStage.GENERATOR:\r\n            for name, param in self.modules.generator.named_parameters():\r\n                if \"Learnable_sigmoid\" in name:\r\n                    param.data = torch.clamp(\r\n                        param, max=3.5\r\n                    )  # to prevent gradient goes to infinity\r\n                    param.data[param != param] = 3.5  # set 'nan' to 3.5\r\n\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"generator\"\r\n            )\r\n\r\n            self.g_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.g_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n\r\n        return loss_tracker",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def fit_batch(self, batch):\r\n        \"Compute gradients and update either D or G based on sub-stage.\"\r\n        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\r\n        loss_tracker = 0\r\n        if self.sub_stage == SubStage.CURRENT:\r\n            for mode in [\"enh\", \"noisy\"]:\r\n                loss = self.compute_objectives(\r\n                    predictions, batch, sb.Stage.TRAIN, f\"D_{mode}\"\r\n                )\r\n                self.d_optimizer.zero_grad()\r\n                loss.backward()\r\n                torch.nn.utils.clip_grad_norm_(\r\n                    self.modules.parameters(), self.max_grad_norm\r\n                )\r\n                self.d_optimizer.step()\r\n                loss_tracker += loss.detach() / 3\r\n        elif self.sub_stage == SubStage.HISTORICAL:\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"D_enh\"\r\n            )\r\n            self.d_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.d_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n        elif self.sub_stage == SubStage.GENERATOR:\r\n            for name, param in self.modules.generator.named_parameters():\r\n                if \"Learnable_sigmoid\" in name:\r\n                    param.data = torch.clamp(\r\n                        param, max=3.5\r\n                    )  # to prevent gradient goes to infinity\r\n\r\n            loss = self.compute_objectives(\r\n                predictions, batch, sb.Stage.TRAIN, \"generator\"\r\n            )\r\n            self.g_optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(\r\n                self.modules.parameters(), self.max_grad_norm\r\n            )\r\n            self.g_optimizer.step()\r\n            loss_tracker += loss.detach()\r\n\r\n        return loss_tracker",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        token_type_ids: torch.Tensor,\r\n        attention_mask: torch.Tensor,\r\n    ):\r\n        \"\"\"Takes an input a history of conversation and returns its corresponding reply.\r\n\r\n        Arguments\r\n        ---------\r\n        input_ids : torch.Tensor\r\n            A batch of input-id to transform to features.\r\n        token_type_ids : torch.Tensor\r\n            Token Type(Speaker) for each token in input_ids.\r\n        attention_mask : torch.Tensor\r\n            A batch of attention_mask.\r\n\r\n        Returns\r\n        -------\r\n        output : torch.Tensor\r\n            Reply to conversation\r\n        \"\"\"\r\n        with torch.set_grad_enabled(not self.freeze):\r\n            output = self.model.forward(\r\n                input_ids,\r\n                token_type_ids=token_type_ids,\r\n                attention_mask=attention_mask,\r\n            )\r\n        return output",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\r\n        \"\"\"Takes an input a history of conversation and returns its corresponding reply.\r\n\r\n        Arguments\r\n        ---------\r\n        input_ids : torch.Tensor\r\n            A batch of input-id to transform to features.\r\n        attention_mask : torch.Tensor\r\n            A batch of attention_mask.\r\n\r\n        Returns\r\n        -------\r\n        output : torch.Tensor\r\n            Reply to conversation.\r\n        \"\"\"\r\n        with torch.set_grad_enabled(not self.freeze):\r\n            output = self.model.forward(\r\n                input_ids, attention_mask=attention_mask\r\n            )\r\n        return output",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_subsequence_loss():\r\n    from speechbrain.lobes.models.g2p.homograph import SubsequenceLoss\r\n    from speechbrain.nnet.losses import nll_loss\r\n\r\n    phn_dim = 4\r\n    phns = torch.tensor(\r\n        [\r\n            [1, 2, 3, 0, 3, 1, 2, 1, 0, 3, 1, 2, 0, 0],\r\n            [1, 2, 3, 1, 0, 3, 2, 1, 0, 1, 3, 2, 0, 0],\r\n            [1, 2, 3, 1, 2, 3, 0, 1, 3, 1, 3, 2, 0, 1],\r\n        ]\r\n    )\r\n    phn_lens = torch.tensor([12, 12, 14])\r\n\r\n    preds = torch.tensor(\r\n        [\r\n            [1, 3, 3, 0, 3, 1, 2, 1, 0, 3, 1, 2],\r\n            [1, 1, 2, 1, 0, 3, 2, 1, 0, 1, 3, 2],\r\n            [3, 2, 1, 1, 2, 3, 0, 1, 3, 2, 3, 3],\r\n        ]\r\n    )\r\n\r\n    p_seq = _batch_fake_probs(preds, phn_dim)\r\n\r\n    start = torch.tensor([0, 5, 7])\r\n    end = torch.tensor([3, 8, 12])\r\n\r\n    word_phns_pred = torch.tensor(\r\n        [[1, 3, 3, 0, 0], [3, 2, 1, 0, 0], [1, 3, 2, 3, 3]]\r\n    )\r\n    word_phns_ref = torch.tensor(\r\n        [[1, 2, 3, 0, 0], [3, 2, 1, 0, 0], [1, 3, 1, 3, 2]]\r\n    )\r\n    word_p_seq = _batch_fake_probs(word_phns_pred, phn_dim)\r\n    word_lengths = torch.tensor([3, 3, 5]) / 5\r\n\r\n    loss = SubsequenceLoss(seq_cost=nll_loss, word_separator=0)\r\n    loss_value = loss.forward(phns, phn_lens, p_seq.log(), start, end)\r\n    loss_value_ref = nll_loss(word_p_seq.log(), word_phns_ref, word_lengths)\r\n    assert loss_value == loss_value_ref",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(\r\n        self,\r\n        img_size: int = 1024,\r\n        patch_size: int = 16,\r\n        in_chans: int = 3,\r\n        embed_dim: int = 768,\r\n        depth: int = 12,\r\n        num_heads: int = 12,\r\n        mlp_ratio: float = 4.0,\r\n        out_chans: int = 256,\r\n        qkv_bias: bool = True,\r\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\r\n        act_layer: Type[nn.Module] = nn.GELU,\r\n        use_abs_pos: bool = True,\r\n        use_rel_pos: bool = False,\r\n        rel_pos_zero_init: bool = True,\r\n        window_size: int = 0,\r\n        global_attn_indexes: Tuple[int, ...] = (),\r\n    ) -> None:\r\n        \"\"\"\r\n        Initializes an ImageEncoderViT instance for encoding images using Vision Transformer architecture.\r\n\r\n        Args:\r\n            img_size (int): Input image size, assumed to be square.\r\n            patch_size (int): Size of image patches.\r\n            in_chans (int): Number of input image channels.\r\n            embed_dim (int): Dimension of patch embeddings.\r\n            depth (int): Number of transformer blocks.\r\n            num_heads (int): Number of attention heads in each block.\r\n            mlp_ratio (float): Ratio of MLP hidden dimension to embedding dimension.\r\n            out_chans (int): Number of output channels from the neck module.\r\n            qkv_bias (bool): If True, adds learnable bias to query, key, value projections.\r\n            norm_layer (Type[nn.Module]): Type of normalization layer to use.\r\n            act_layer (Type[nn.Module]): Type of activation layer to use.\r\n            use_abs_pos (bool): If True, uses absolute positional embeddings.\r\n            use_rel_pos (bool): If True, adds relative positional embeddings to attention maps.\r\n            rel_pos_zero_init (bool): If True, initializes relative positional parameters to zero.\r\n            window_size (int): Size of attention window for windowed attention blocks.\r\n            global_attn_indexes (Tuple[int, ...]): Indices of blocks that use global attention.\r\n\r\n        Attributes:\r\n            img_size (int): Dimension of input images.\r\n            patch_embed (PatchEmbed): Module for patch embedding.\r\n            pos_embed (nn.Parameter | None): Absolute positional embedding for patches.\r\n            blocks (nn.ModuleList): List of transformer blocks.\r\n            neck (nn.Sequential): Neck module for final processing.\r\n\r\n        Examples:\r\n            >>> encoder = ImageEncoderViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12)\r\n            >>> input_image = torch.randn(1, 3, 224, 224)\r\n            >>> output = encoder(input_image)\r\n            >>> print(output.shape)\r\n        \"\"\"\r\n        super().__init__()\r\n        self.img_size = img_size\r\n\r\n        self.patch_embed = PatchEmbed(\r\n            kernel_size=(patch_size, patch_size),\r\n            stride=(patch_size, patch_size),\r\n            in_chans=in_chans,\r\n            embed_dim=embed_dim,\r\n        )\r\n\r\n        self.pos_embed: Optional[nn.Parameter] = None\r\n        if use_abs_pos:\r\n            # Initialize absolute positional embedding with pretrain image size.\r\n            self.pos_embed = nn.Parameter(torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim))\r\n\r\n        self.blocks = nn.ModuleList()\r\n        for i in range(depth):\r\n            block = Block(\r\n                dim=embed_dim,\r\n                num_heads=num_heads,\r\n                mlp_ratio=mlp_ratio,\r\n                qkv_bias=qkv_bias,\r\n                norm_layer=norm_layer,\r\n                act_layer=act_layer,\r\n                use_rel_pos=use_rel_pos,\r\n                rel_pos_zero_init=rel_pos_zero_init,\r\n                window_size=window_size if i not in global_attn_indexes else 0,\r\n                input_size=(img_size // patch_size, img_size // patch_size),\r\n            )\r\n            self.blocks.append(block)\r\n\r\n        self.neck = nn.Sequential(\r\n            nn.Conv2d(\r\n                embed_dim,\r\n                out_chans,\r\n                kernel_size=1,\r\n                bias=False,\r\n            ),\r\n            LayerNorm2d(out_chans),\r\n            nn.Conv2d(\r\n                out_chans,\r\n                out_chans,\r\n                kernel_size=3,\r\n                padding=1,\r\n                bias=False,\r\n            ),\r\n            LayerNorm2d(out_chans),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(\r\n        self,\r\n        embed_dim=256,\r\n        kernel_size=4,\r\n        stride=4,\r\n        padding=0,\r\n        total_stride=16,\r\n        activation=nn.GELU,\r\n    ):\r\n        \"\"\"Initializes a mask downsampler module for progressive downsampling and channel expansion.\"\"\"\r\n        super().__init__()\r\n        num_layers = int(math.log2(total_stride) // math.log2(stride))\r\n        assert stride**num_layers == total_stride\r\n        self.encoder = nn.Sequential()\r\n        mask_in_chans, mask_out_chans = 1, 1\r\n        for _ in range(num_layers):\r\n            mask_out_chans = mask_in_chans * (stride**2)\r\n            self.encoder.append(\r\n                nn.Conv2d(\r\n                    mask_in_chans,\r\n                    mask_out_chans,\r\n                    kernel_size=kernel_size,\r\n                    stride=stride,\r\n                    padding=padding,\r\n                )\r\n            )\r\n            self.encoder.append(LayerNorm2d(mask_out_chans))\r\n            self.encoder.append(activation())\r\n            mask_in_chans = mask_out_chans\r\n\r\n        self.encoder.append(nn.Conv2d(mask_out_chans, embed_dim, kernel_size=1))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def save_metrics(self, metrics):\r\n        \"\"\"Saves training metrics to a CSV file.\"\"\"\r\n        keys, vals = list(metrics.keys()), list(metrics.values())\r\n        n = len(metrics) + 2  # number of cols\r\n        s = \"\" if self.csv.exists() else ((\"%s,\" * n % tuple([\"epoch\", \"time\"] + keys)).rstrip(\",\") + \"\\n\")  # header\r\n        t = time.time() - self.train_time_start\r\n        with open(self.csv, \"a\") as f:\r\n            f.write(s + (\"%.6g,\" * n % tuple([self.epoch + 1, t] + vals)).rstrip(\",\") + \"\\n\")",
        "labels": [
            "Dataframe Conversion API Misused"
        ]
    },
    {
        "code": "def forward(self, im, augment=False, visualize=False, embed=None):\r\n        \"\"\"\r\n        Runs inference on the YOLOv8 MultiBackend model.\r\n\r\n        Args:\r\n            im (torch.Tensor): The image tensor to perform inference on.\r\n            augment (bool): whether to perform data augmentation during inference, defaults to False\r\n            visualize (bool): whether to visualize the output predictions, defaults to False\r\n            embed (list, optional): A list of feature vectors/embeddings to return.\r\n\r\n        Returns:\r\n            (tuple): Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)\r\n        \"\"\"\r\n        b, ch, h, w = im.shape  # batch, channel, height, width\r\n        if self.fp16 and im.dtype != torch.float16:\r\n            im = im.half()  # to FP16\r\n        if self.nhwc:\r\n            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\r\n\r\n        # PyTorch\r\n        if self.pt or self.nn_module:\r\n            y = self.model(im, augment=augment, visualize=visualize, embed=embed)\r\n\r\n        # TorchScript\r\n        elif self.jit:\r\n            y = self.model(im)\r\n\r\n        # ONNX OpenCV DNN\r\n        elif self.dnn:\r\n            im = im.cpu().numpy()  # torch to numpy\r\n            self.net.setInput(im)\r\n            y = self.net.forward()\r\n\r\n        # ONNX Runtime\r\n        elif self.onnx or self.imx:\r\n            if self.dynamic:\r\n                im = im.cpu().numpy()  # torch to numpy\r\n                y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\r\n            else:\r\n                if not self.cuda:\r\n                    im = im.cpu()\r\n                self.io.bind_input(\r\n                    name=\"images\",\r\n                    device_type=im.device.type,\r\n                    device_id=im.device.index if im.device.type == \"cuda\" else 0,\r\n                    element_type=np.float16 if self.fp16 else np.float32,\r\n                    shape=tuple(im.shape),\r\n                    buffer_ptr=im.data_ptr(),\r\n                )\r\n                self.session.run_with_iobinding(self.io)\r\n                y = self.bindings\r\n            if self.imx:\r\n                # boxes, conf, cls\r\n                y = np.concatenate([y[0], y[1][:, :, None], y[2][:, :, None]], axis=-1)\r\n\r\n        # OpenVINO\r\n        elif self.xml:\r\n            im = im.cpu().numpy()  # FP32\r\n\r\n            if self.inference_mode in {\"THROUGHPUT\", \"CUMULATIVE_THROUGHPUT\"}:  # optimized for larger batch-sizes\r\n                n = im.shape[0]  # number of images in batch\r\n                results = [None] * n  # preallocate list with None to match the number of images\r\n\r\n                def callback(request, userdata):\r\n                    \"\"\"Places result in preallocated list using userdata index.\"\"\"\r\n                    results[userdata] = request.results\r\n\r\n                # Create AsyncInferQueue, set the callback and start asynchronous inference for each input image\r\n                async_queue = self.ov.runtime.AsyncInferQueue(self.ov_compiled_model)\r\n                async_queue.set_callback(callback)\r\n                for i in range(n):\r\n                    # Start async inference with userdata=i to specify the position in results list\r\n                    async_queue.start_async(inputs={self.input_name: im[i : i + 1]}, userdata=i)  # keep image as BCHW\r\n                async_queue.wait_all()  # wait for all inference requests to complete\r\n                y = np.concatenate([list(r.values())[0] for r in results])\r\n\r\n            else:  # inference_mode = \"LATENCY\", optimized for fastest first result at batch-size 1\r\n                y = list(self.ov_compiled_model(im).values())\r\n\r\n        # TensorRT\r\n        elif self.engine:\r\n            if self.dynamic and im.shape != self.bindings[\"images\"].shape:\r\n                if self.is_trt10:\r\n                    self.context.set_input_shape(\"images\", im.shape)\r\n                    self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=im.shape)\r\n                    for name in self.output_names:\r\n                        self.bindings[name].data.resize_(tuple(self.context.get_tensor_shape(name)))\r\n                else:\r\n                    i = self.model.get_binding_index(\"images\")\r\n                    self.context.set_binding_shape(i, im.shape)\r\n                    self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=im.shape)\r\n                    for name in self.output_names:\r\n                        i = self.model.get_binding_index(name)\r\n                        self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\r\n\r\n            s = self.bindings[\"images\"].shape\r\n            assert im.shape == s, f\"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}\"\r\n            self.binding_addrs[\"images\"] = int(im.data_ptr())\r\n            self.context.execute_v2(list(self.binding_addrs.values()))\r\n            y = [self.bindings[x].data for x in sorted(self.output_names)]\r\n\r\n        # CoreML\r\n        elif self.coreml:\r\n            im = im[0].cpu().numpy()\r\n            im_pil = Image.fromarray((im * 255).astype(\"uint8\"))\r\n            # im = im.resize((192, 320), Image.BILINEAR)\r\n            y = self.model.predict({\"image\": im_pil})  # coordinates are xywh normalized\r\n            if \"confidence\" in y:\r\n                raise TypeError(\r\n                    \"Ultralytics only supports inference of non-pipelined CoreML models exported with \"\r\n                    f\"'nms=False', but 'model={w}' has an NMS pipeline created by an 'nms=True' export.\"\r\n                )\r\n                # TODO: CoreML NMS inference handling\r\n                # from ultralytics.utils.ops import xywh2xyxy\r\n                # box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\r\n                # conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float32)\r\n                # y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\r\n            y = list(y.values())\r\n            if len(y) == 2 and len(y[1].shape) != 4:  # segmentation model\r\n                y = list(reversed(y))  # reversed for segmentation models (pred, proto)\r\n\r\n        # PaddlePaddle\r\n        elif self.paddle:\r\n            im = im.cpu().numpy().astype(np.float32)\r\n            self.input_handle.copy_from_cpu(im)\r\n            self.predictor.run()\r\n            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\r\n\r\n        # MNN\r\n        elif self.mnn:\r\n            input_var = self.torch_to_mnn(im)\r\n            output_var = self.net.onForward([input_var])\r\n            y = [x.read() for x in output_var]\r\n\r\n        # NCNN\r\n        elif self.ncnn:\r\n            mat_in = self.pyncnn.Mat(im[0].cpu().numpy())\r\n            with self.net.create_extractor() as ex:\r\n                ex.input(self.net.input_names()[0], mat_in)\r\n                # WARNING: 'output_names' sorted as a temporary fix for https://github.com/pnnx/pnnx/issues/130\r\n                y = [np.array(ex.extract(x)[1])[None] for x in sorted(self.net.output_names())]\r\n\r\n        # NVIDIA Triton Inference Server\r\n        elif self.triton:\r\n            im = im.cpu().numpy()  # torch to numpy\r\n            y = self.model(im)\r\n\r\n        # RKNN\r\n        elif self.rknn:\r\n            im = (im.cpu().numpy() * 255).astype(\"uint8\")\r\n            im = im if isinstance(im, (list, tuple)) else [im]\r\n            y = self.rknn_model.inference(inputs=im)\r\n\r\n        # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\r\n        else:\r\n            im = im.cpu().numpy()\r\n            if self.saved_model:  # SavedModel\r\n                y = self.model(im, training=False) if self.keras else self.model(im)\r\n                if not isinstance(y, list):\r\n                    y = [y]\r\n            elif self.pb:  # GraphDef\r\n                y = self.frozen_func(x=self.tf.constant(im))\r\n            else:  # Lite or Edge TPU\r\n                details = self.input_details[0]\r\n                is_int = details[\"dtype\"] in {np.int8, np.int16}  # is TFLite quantized int8 or int16 model\r\n                if is_int:\r\n                    scale, zero_point = details[\"quantization\"]\r\n                    im = (im / scale + zero_point).astype(details[\"dtype\"])  # de-scale\r\n                self.interpreter.set_tensor(details[\"index\"], im)\r\n                self.interpreter.invoke()\r\n                y = []\r\n                for output in self.output_details:\r\n                    x = self.interpreter.get_tensor(output[\"index\"])\r\n                    if is_int:\r\n                        scale, zero_point = output[\"quantization\"]\r\n                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale\r\n                    if x.ndim == 3:  # if task is not classification, excluding masks (ndim=4) as well\r\n                        # Denormalize xywh by image size. See https://github.com/ultralytics/ultralytics/pull/1695\r\n                        # xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models\r\n                        if x.shape[-1] == 6 or self.end2end:  # end-to-end model\r\n                            x[:, :, [0, 2]] *= w\r\n                            x[:, :, [1, 3]] *= h\r\n                            if self.task == \"pose\":\r\n                                x[:, :, 6::3] *= w\r\n                                x[:, :, 7::3] *= h\r\n                        else:\r\n                            x[:, [0, 2]] *= w\r\n                            x[:, [1, 3]] *= h\r\n                            if self.task == \"pose\":\r\n                                x[:, 5::3] *= w\r\n                                x[:, 6::3] *= h\r\n                    y.append(x)\r\n            # TF segment fixes: export is reversed vs ONNX export and protos are transposed\r\n            if len(y) == 2:  # segment with (det, proto) output order reversed\r\n                if len(y[1].shape) != 4:\r\n                    y = list(reversed(y))  # should be y = (1, 116, 8400), (1, 160, 160, 32)\r\n                if y[1].shape[-1] == 6:  # end-to-end model\r\n                    y = [y[1]]\r\n                else:\r\n                    y[1] = np.transpose(y[1], (0, 3, 1, 2))  # should be y = (1, 116, 8400), (1, 32, 160, 160)\r\n            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\r\n\r\n        # for x in y:\r\n        #     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes\r\n        if isinstance(y, (list, tuple)):\r\n            if len(self.names) == 999 and (self.task == \"segment\" or len(y) == 2):  # segments and names not defined\r\n                nc = y[0].shape[1] - y[1].shape[1] - 4  # y = (1, 32, 160, 160), (1, 116, 8400)\r\n                self.names = {i: f\"class{i}\" for i in range(nc)}\r\n            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\r\n        else:\r\n            return self.from_numpy(y)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, layer, num_layers, dim=None, input_projection=False):\r\n        \"\"\"\r\n        Initializes the Fuser module for feature fusion through multiple layers.\r\n\r\n        This module creates a sequence of identical layers and optionally applies an input projection.\r\n\r\n        Args:\r\n            layer (nn.Module): The layer to be replicated in the fuser.\r\n            num_layers (int): The number of times to replicate the layer.\r\n            dim (int | None): The dimension for input projection, if used.\r\n            input_projection (bool): Whether to use input projection.\r\n\r\n        Examples:\r\n            >>> layer = nn.Linear(64, 64)\r\n            >>> fuser = Fuser(layer, num_layers=3, dim=64, input_projection=True)\r\n            >>> input_tensor = torch.randn(1, 64)\r\n            >>> output = fuser(input_tensor)\r\n        \"\"\"\r\n        super().__init__()\r\n        self.proj = nn.Identity()\r\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\r\n\r\n        if input_projection:\r\n            assert dim is not None\r\n            self.proj = nn.Conv2d(dim, dim, kernel_size=1)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def loss(self, batch, preds=None):\r\n        \"\"\"\r\n        Compute the loss for the given batch of data.\r\n\r\n        Args:\r\n            batch (dict): Dictionary containing image and label data.\r\n            preds (torch.Tensor, optional): Precomputed model predictions. Defaults to None.\r\n\r\n        Returns:\r\n            (tuple): A tuple containing the total loss and main three losses in a tensor.\r\n        \"\"\"\r\n        if not hasattr(self, \"criterion\"):\r\n            self.criterion = self.init_criterion()\r\n\r\n        img = batch[\"img\"]\r\n        # NOTE: preprocess gt_bbox and gt_labels to list.\r\n        bs = len(img)\r\n        batch_idx = batch[\"batch_idx\"]\r\n        gt_groups = [(batch_idx == i).sum().item() for i in range(bs)]\r\n        targets = {\r\n            \"cls\": batch[\"cls\"].to(img.device, dtype=torch.long).view(-1),\r\n            \"bboxes\": batch[\"bboxes\"].to(device=img.device),\r\n            \"batch_idx\": batch_idx.to(img.device, dtype=torch.long).view(-1),\r\n            \"gt_groups\": gt_groups,\r\n        }\r\n\r\n        preds = self.predict(img, batch=targets) if preds is None else preds\r\n        dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta = preds if self.training else preds[1]\r\n        if dn_meta is None:\r\n            dn_bboxes, dn_scores = None, None\r\n        else:\r\n            dn_bboxes, dec_bboxes = torch.split(dec_bboxes, dn_meta[\"dn_num_split\"], dim=2)\r\n            dn_scores, dec_scores = torch.split(dec_scores, dn_meta[\"dn_num_split\"], dim=2)\r\n\r\n        dec_bboxes = torch.cat([enc_bboxes.unsqueeze(0), dec_bboxes])  # (7, bs, 300, 4)\r\n        dec_scores = torch.cat([enc_scores.unsqueeze(0), dec_scores])\r\n\r\n        loss = self.criterion(\r\n            (dec_bboxes, dec_scores), targets, dn_bboxes=dn_bboxes, dn_scores=dn_scores, dn_meta=dn_meta\r\n        )\r\n        # NOTE: There are like 12 losses in RTDETR, backward with all losses but only show the main three losses.\r\n        return sum(loss.values()), torch.as_tensor(\r\n            [loss[k].detach() for k in [\"loss_giou\", \"loss_class\", \"loss_bbox\"]], device=img.device\r\n        )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, cfg=\"yolo11n.yaml\", ch=3, nc=None, verbose=True):  # model, input channels, number of classes\r\n        \"\"\"Initialize the YOLO detection model with the given config and parameters.\"\"\"\r\n        super().__init__()\r\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict\r\n        if self.yaml[\"backbone\"][0][2] == \"Silence\":\r\n            LOGGER.warning(\r\n                \"WARNING ⚠️ YOLOv9 `Silence` module is deprecated in favor of nn.Identity. \"\r\n                \"Please delete local *.pt file and re-download the latest model checkpoint.\"\r\n            )\r\n            self.yaml[\"backbone\"][0][2] = \"nn.Identity\"\r\n\r\n        # Define model\r\n        ch = self.yaml[\"ch\"] = self.yaml.get(\"ch\", ch)  # input channels\r\n        if nc and nc != self.yaml[\"nc\"]:\r\n            LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\r\n            self.yaml[\"nc\"] = nc  # override YAML value\r\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\r\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}  # default names dict\r\n        self.inplace = self.yaml.get(\"inplace\", True)\r\n        self.end2end = getattr(self.model[-1], \"end2end\", False)\r\n\r\n        # Build strides\r\n        m = self.model[-1]  # Detect()\r\n        if isinstance(m, Detect):  # includes all Detect subclasses like Segment, Pose, OBB, WorldDetect\r\n            s = 256  # 2x min stride\r\n            m.inplace = self.inplace\r\n\r\n            def _forward(x):\r\n                \"\"\"Performs a forward pass through the model, handling different Detect subclass types accordingly.\"\"\"\r\n                if self.end2end:\r\n                    return self.forward(x)[\"one2many\"]\r\n                return self.forward(x)[0] if isinstance(m, (Segment, Pose, OBB)) else self.forward(x)\r\n\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forward\r\n            self.stride = m.stride\r\n            m.bias_init()  # only run once\r\n        else:\r\n            self.stride = torch.Tensor([32])  # default stride for i.e. RTDETR\r\n\r\n        # Init weights, biases\r\n        initialize_weights(self)\r\n        if verbose:\r\n            self.info()\r\n            LOGGER.info(\"\")",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def benchmark(\r\n    model=WEIGHTS_DIR / \"yolo11n.pt\",\r\n    data=None,\r\n    imgsz=160,\r\n    half=False,\r\n    int8=False,\r\n    device=\"cpu\",\r\n    verbose=False,\r\n    eps=1e-3,\r\n):\r\n    \"\"\"\r\n    Benchmark a YOLO model across different formats for speed and accuracy.\r\n\r\n    Args:\r\n        model (str | Path): Path to the model file or directory.\r\n        data (str | None): Dataset to evaluate on, inherited from TASK2DATA if not passed.\r\n        imgsz (int): Image size for the benchmark.\r\n        half (bool): Use half-precision for the model if True.\r\n        int8 (bool): Use int8-precision for the model if True.\r\n        device (str): Device to run the benchmark on, either 'cpu' or 'cuda'.\r\n        verbose (bool | float): If True or a float, assert benchmarks pass with given metric.\r\n        eps (float): Epsilon value for divide by zero prevention.\r\n\r\n    Returns:\r\n        (pandas.DataFrame): A pandas DataFrame with benchmark results for each format, including file size, metric,\r\n            and inference time.\r\n\r\n    Examples:\r\n        Benchmark a YOLO model with default settings:\r\n        >>> from ultralytics.utils.benchmarks import benchmark\r\n        >>> benchmark(model=\"yolo11n.pt\", imgsz=640)\r\n    \"\"\"\r\n    import pandas as pd  # scope for faster 'import ultralytics'\r\n\r\n    pd.options.display.max_columns = 10\r\n    pd.options.display.width = 120\r\n    device = select_device(device, verbose=False)\r\n    if isinstance(model, (str, Path)):\r\n        model = YOLO(model)\r\n    is_end2end = getattr(model.model.model[-1], \"end2end\", False)\r\n\r\n    y = []\r\n    t0 = time.time()\r\n    for i, (name, format, suffix, cpu, gpu, _) in enumerate(zip(*export_formats().values())):\r\n        emoji, filename = \"❌\", None  # export defaults\r\n        try:\r\n            # Checks\r\n            if i == 7:  # TF GraphDef\r\n                assert model.task != \"obb\", \"TensorFlow GraphDef not supported for OBB task\"\r\n            elif i == 9:  # Edge TPU\r\n                assert LINUX and not ARM64, \"Edge TPU export only supported on non-aarch64 Linux\"\r\n            elif i in {5, 10}:  # CoreML and TF.js\r\n                assert MACOS or (LINUX and not ARM64), (\r\n                    \"CoreML and TF.js export only supported on macOS and non-aarch64 Linux\"\r\n                )\r\n            if i in {5}:  # CoreML\r\n                assert not IS_PYTHON_3_12, \"CoreML not supported on Python 3.12\"\r\n            if i in {6, 7, 8}:  # TF SavedModel, TF GraphDef, and TFLite\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 TensorFlow exports not supported by onnx2tf yet\"\r\n            if i in {9, 10}:  # TF EdgeTPU and TF.js\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 TensorFlow exports not supported by onnx2tf yet\"\r\n            if i == 11:  # Paddle\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 Paddle exports not supported yet\"\r\n                assert not is_end2end, \"End-to-end models not supported by PaddlePaddle yet\"\r\n                assert LINUX or MACOS, \"Windows Paddle exports not supported yet\"\r\n            if i == 12:  # MNN\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 MNN exports not supported yet\"\r\n            if i == 13:  # NCNN\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 NCNN exports not supported yet\"\r\n            if i == 14:  # IMX\r\n                assert not is_end2end\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 IMX exports not supported\"\r\n                assert model.task == \"detect\", \"IMX only supported for detection task\"\r\n                assert \"C2f\" in model.__str__(), \"IMX only supported for YOLOv8\"\r\n            if i == 15:  # RKNN\r\n                assert not isinstance(model, YOLOWorld), \"YOLOWorldv2 RKNN exports not supported yet\"\r\n                assert not is_end2end, \"End-to-end models not supported by RKNN yet\"\r\n                assert LINUX, \"RKNN only supported on Linux\"\r\n                assert not is_rockchip(), \"RKNN Inference only supported on Rockchip devices\"\r\n            if \"cpu\" in device.type:\r\n                assert cpu, \"inference not supported on CPU\"\r\n            if \"cuda\" in device.type:\r\n                assert gpu, \"inference not supported on GPU\"\r\n\r\n            # Export\r\n            if format == \"-\":\r\n                filename = model.pt_path or model.ckpt_path or model.model_name\r\n                exported_model = model  # PyTorch format\r\n            else:\r\n                filename = model.export(imgsz=imgsz, format=format, half=half, int8=int8, device=device, verbose=False)\r\n                exported_model = YOLO(filename, task=model.task)\r\n                assert suffix in str(filename), \"export failed\"\r\n            emoji = \"❎\"  # indicates export succeeded\r\n\r\n            # Predict\r\n            assert model.task != \"pose\" or i != 7, \"GraphDef Pose inference is not supported\"\r\n            assert i not in {9, 10}, \"inference not supported\"  # Edge TPU and TF.js are unsupported\r\n            assert i != 5 or platform.system() == \"Darwin\", \"inference only supported on macOS>=10.13\"  # CoreML\r\n            if i in {13}:\r\n                assert not is_end2end, \"End-to-end torch.topk operation is not supported for NCNN prediction yet\"\r\n            exported_model.predict(ASSETS / \"bus.jpg\", imgsz=imgsz, device=device, half=half)\r\n\r\n            # Validate\r\n            data = data or TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect\r\n            key = TASK2METRIC[model.task]  # task to metric, i.e. metrics/mAP50-95(B) for task=detect\r\n            results = exported_model.val(\r\n                data=data, batch=1, imgsz=imgsz, plots=False, device=device, half=half, int8=int8, verbose=False\r\n            )\r\n            metric, speed = results.results_dict[key], results.speed[\"inference\"]\r\n            fps = round(1000 / (speed + eps), 2)  # frames per second\r\n            y.append([name, \"✅\", round(file_size(filename), 1), round(metric, 4), round(speed, 2), fps])\r\n        except Exception as e:\r\n            if verbose:\r\n                assert type(e) is AssertionError, f\"Benchmark failure for {name}: {e}\"\r\n            LOGGER.warning(f\"ERROR ❌️ Benchmark failure for {name}: {e}\")\r\n            y.append([name, emoji, round(file_size(filename), 1), None, None, None])  # mAP, t_inference\r\n\r\n    # Print results\r\n    check_yolo(device=device)  # print system info\r\n    df = pd.DataFrame(y, columns=[\"Format\", \"Status❔\", \"Size (MB)\", key, \"Inference time (ms/im)\", \"FPS\"])\r\n\r\n    name = model.model_name\r\n    s = f\"\\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\\n{df}\\n\"\r\n    LOGGER.info(s)\r\n    with open(\"benchmarks.log\", \"a\", errors=\"ignore\", encoding=\"utf-8\") as f:\r\n        f.write(s)\r\n\r\n    if verbose and isinstance(verbose, float):\r\n        metrics = df[key].array  # values to compare to floor\r\n        floor = verbose  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n\r\n        assert all(x > floor for x in metrics if pd.notna(x)), f\"Benchmark failure: metric(s) < floor {floor}\"\r\n\r\n    return df",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def run(\r\n    weights=ROOT / \"yolov5s.pt\",  # weights path\r\n    imgsz=640,  # inference size (pixels)\r\n    batch_size=1,  # batch size\r\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\r\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\r\n    half=False,  # use FP16 half-precision inference\r\n    test=False,  # test exports only\r\n    pt_only=False,  # test PyTorch only\r\n    hard_fail=False,  # throw error on benchmark failure\r\n):\r\n    \"\"\"\r\n    Run YOLOv5 benchmarks on multiple export formats and log results for model performance evaluation.\r\n\r\n    Args:\r\n        weights (Path | str): Path to the model weights file (default: ROOT / \"yolov5s.pt\").\r\n        imgsz (int): Inference size in pixels (default: 640).\r\n        batch_size (int): Batch size for inference (default: 1).\r\n        data (Path | str): Path to the dataset.yaml file (default: ROOT / \"data/coco128.yaml\").\r\n        device (str): CUDA device, e.g., '0' or '0,1,2,3' or 'cpu' (default: \"\").\r\n        half (bool): Use FP16 half-precision inference (default: False).\r\n        test (bool): Test export formats only (default: False).\r\n        pt_only (bool): Test PyTorch format only (default: False).\r\n        hard_fail (bool): Throw an error on benchmark failure if True (default: False).\r\n\r\n    Returns:\r\n        None. Logs information about the benchmark results, including the format, size, mAP50-95, and inference time.\r\n\r\n    Notes:\r\n        Supported export formats and models include PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, CoreML,\r\n            TensorFlow SavedModel, TensorFlow GraphDef, TensorFlow Lite, and TensorFlow Edge TPU. Edge TPU and TF.js\r\n            are unsupported.\r\n\r\n    Example:\r\n        ```python\r\n        $ python benchmarks.py --weights yolov5s.pt --img 640\r\n        ```\r\n\r\n    Usage:\r\n        Install required packages:\r\n          $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU support\r\n          $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow   # GPU support\r\n          $ pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # TensorRT\r\n\r\n        Run benchmarks:\r\n          $ python benchmarks.py --weights yolov5s.pt --img 640\r\n    \"\"\"\r\n    y, t = [], time.time()\r\n    device = select_device(device)\r\n    model_type = type(attempt_load(weights, fuse=False))  # DetectionModel, SegmentationModel, etc.\r\n    for i, (name, f, suffix, cpu, gpu) in export.export_formats().iterrows():  # index, (name, file, suffix, CPU, GPU)\r\n        try:\r\n            assert i not in (9, 10), \"inference not supported\"  # Edge TPU and TF.js are unsupported\r\n            assert i != 5 or platform.system() == \"Darwin\", \"inference only supported on macOS>=10.13\"  # CoreML\r\n            if \"cpu\" in device.type:\r\n                assert cpu, \"inference not supported on CPU\"\r\n            if \"cuda\" in device.type:\r\n                assert gpu, \"inference not supported on GPU\"\r\n\r\n            # Export\r\n            if f == \"-\":\r\n                w = weights  # PyTorch format\r\n            else:\r\n                w = export.run(\r\n                    weights=weights, imgsz=[imgsz], include=[f], batch_size=batch_size, device=device, half=half\r\n                )[-1]  # all others\r\n            assert suffix in str(w), \"export failed\"\r\n\r\n            # Validate\r\n            if model_type == SegmentationModel:\r\n                result = val_seg(data, w, batch_size, imgsz, plots=False, device=device, task=\"speed\", half=half)\r\n                metric = result[0][7]  # (box(p, r, map50, map), mask(p, r, map50, map), *loss(box, obj, cls))\r\n            else:  # DetectionModel:\r\n                result = val_det(data, w, batch_size, imgsz, plots=False, device=device, task=\"speed\", half=half)\r\n                metric = result[0][3]  # (p, r, map50, map, *loss(box, obj, cls))\r\n            speed = result[2][1]  # times (preprocess, inference, postprocess)\r\n            y.append([name, round(file_size(w), 1), round(metric, 4), round(speed, 2)])  # MB, mAP, t_inference\r\n        except Exception as e:\r\n            if hard_fail:\r\n                assert type(e) is AssertionError, f\"Benchmark --hard-fail for {name}: {e}\"\r\n            LOGGER.warning(f\"WARNING ⚠️ Benchmark failure for {name}: {e}\")\r\n            y.append([name, None, None, None])  # mAP, t_inference\r\n        if pt_only and i == 0:\r\n            break  # break after PyTorch\r\n\r\n    # Print results\r\n    LOGGER.info(\"\\n\")\r\n    parse_opt()\r\n    notebook_init()  # print system info\r\n    c = [\"Format\", \"Size (MB)\", \"mAP50-95\", \"Inference time (ms)\"] if map else [\"Format\", \"Export\", \"\", \"\"]\r\n    py = pd.DataFrame(y, columns=c)\r\n    LOGGER.info(f\"\\nBenchmarks complete ({time.time() - t:.2f}s)\")\r\n    LOGGER.info(str(py if map else py.iloc[:, :2]))\r\n    if hard_fail and isinstance(hard_fail, str):\r\n        metrics = py[\"mAP50-95\"].array  # values to compare to floor\r\n        floor = eval(hard_fail)  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n\r\n        assert all(x > floor for x in metrics if pd.notna(x)), f\"HARD FAIL: mAP50-95 < floor {floor}\"\r\n    return py",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def pipeline_coreml(model, im, file, names, y, mlmodel, prefix=colorstr(\"CoreML Pipeline:\")):\r\n    \"\"\"\r\n    Convert a PyTorch YOLOv5 model to CoreML format with Non-Maximum Suppression (NMS), handling different input/output\r\n    shapes, and saving the model.\r\n\r\n    Args:\r\n        model (torch.nn.Module): The YOLOv5 PyTorch model to be converted.\r\n        im (torch.Tensor): Example input tensor with shape (N, C, H, W), where N is the batch size, C is the number of channels,\r\n            H is the height, and W is the width.\r\n        file (Path): Path to save the converted CoreML model.\r\n        names (dict[int, str]): Dictionary mapping class indices to class names.\r\n        y (torch.Tensor): Output tensor from the PyTorch model's forward pass.\r\n        mlmodel (bool): Flag indicating whether to export as older *.mlmodel format (default is False).\r\n        prefix (str): Custom prefix for logging messages.\r\n\r\n    Returns:\r\n        (Path): Path to the saved CoreML model (.mlmodel).\r\n\r\n    Raises:\r\n        AssertionError: If the number of class names does not match the number of classes in the model.\r\n\r\n    Notes:\r\n        - This function requires `coremltools` to be installed.\r\n        - Running this function on a non-macOS environment might not support some features.\r\n        - Flexible input shapes and additional NMS options can be customized within the function.\r\n\r\n    Examples:\r\n        ```python\r\n        from pathlib import Path\r\n        import torch\r\n\r\n        model = torch.load('yolov5s.pt')  # Load YOLOv5 model\r\n        im = torch.zeros((1, 3, 640, 640))  # Example input tensor\r\n\r\n        names = {0: \"person\", 1: \"bicycle\", 2: \"car\", ...}  # Define class names\r\n\r\n        y = model(im)  # Perform forward pass to get model output\r\n\r\n        output_file = Path('yolov5s.mlmodel')  # Convert to CoreML\r\n        pipeline_coreml(model, im, output_file, names, y)\r\n        ```\r\n    \"\"\"\r\n    import coremltools as ct\r\n    from PIL import Image\r\n\r\n    f = file.with_suffix(\".mlmodel\") if mlmodel else file.with_suffix(\".mlpackage\")\r\n    print(f\"{prefix} starting pipeline with coremltools {ct.__version__}...\")\r\n    batch_size, ch, h, w = list(im.shape)  # BCHW\r\n    t = time.time()\r\n\r\n    # YOLOv5 Output shapes\r\n    spec = model.get_spec()\r\n    out0, out1 = iter(spec.description.output)\r\n    if platform.system() == \"Darwin\":\r\n        img = Image.new(\"RGB\", (w, h))  # img(192 width, 320 height)\r\n        # img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection\r\n        out = model.predict({\"image\": img})\r\n        out0_shape, out1_shape = out[out0.name].shape, out[out1.name].shape\r\n    else:  # linux and windows can not run model.predict(), get sizes from pytorch output y\r\n        s = tuple(y[0].shape)\r\n        out0_shape, out1_shape = (s[1], s[2] - 5), (s[1], 4)  # (3780, 80), (3780, 4)\r\n\r\n    # Checks\r\n    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\r\n    na, nc = out0_shape\r\n    # na, nc = out0.type.multiArrayType.shape  # number anchors, classes\r\n    assert len(names) == nc, f\"{len(names)} names found for nc={nc}\"  # check\r\n\r\n    # Define output shapes (missing)\r\n    out0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\r\n    out1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\r\n    # spec.neuralNetwork.preprocessing[0].featureName = '0'\r\n\r\n    # Flexible input shapes\r\n    # from coremltools.models.neural_network import flexible_shape_utils\r\n    # s = [] # shapes\r\n    # s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\r\n    # s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\r\n    # flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\r\n    # r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\r\n    # r.add_height_range((192, 640))\r\n    # r.add_width_range((192, 640))\r\n    # flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\r\n\r\n    # Print\r\n    print(spec.description)\r\n\r\n    # Model from spec\r\n    weights_dir = None\r\n    weights_dir = None if mlmodel else str(f / \"Data/com.apple.CoreML/weights\")\r\n    model = ct.models.MLModel(spec, weights_dir=weights_dir)\r\n\r\n    # 3. Create NMS protobuf\r\n    nms_spec = ct.proto.Model_pb2.Model()\r\n    nms_spec.specificationVersion = 5\r\n    for i in range(2):\r\n        decoder_output = model._spec.description.output[i].SerializeToString()\r\n        nms_spec.description.input.add()\r\n        nms_spec.description.input[i].ParseFromString(decoder_output)\r\n        nms_spec.description.output.add()\r\n        nms_spec.description.output[i].ParseFromString(decoder_output)\r\n\r\n    nms_spec.description.output[0].name = \"confidence\"\r\n    nms_spec.description.output[1].name = \"coordinates\"\r\n\r\n    output_sizes = [nc, 4]\r\n    for i in range(2):\r\n        ma_type = nms_spec.description.output[i].type.multiArrayType\r\n        ma_type.shapeRange.sizeRanges.add()\r\n        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\r\n        ma_type.shapeRange.sizeRanges[0].upperBound = -1\r\n        ma_type.shapeRange.sizeRanges.add()\r\n        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\r\n        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\r\n        del ma_type.shape[:]\r\n\r\n    nms = nms_spec.nonMaximumSuppression\r\n    nms.confidenceInputFeatureName = out0.name  # 1x507x80\r\n    nms.coordinatesInputFeatureName = out1.name  # 1x507x4\r\n    nms.confidenceOutputFeatureName = \"confidence\"\r\n    nms.coordinatesOutputFeatureName = \"coordinates\"\r\n    nms.iouThresholdInputFeatureName = \"iouThreshold\"\r\n    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\r\n    nms.iouThreshold = 0.45\r\n    nms.confidenceThreshold = 0.25\r\n    nms.pickTop.perClass = True\r\n    nms.stringClassLabels.vector.extend(names.values())\r\n    nms_model = ct.models.MLModel(nms_spec)\r\n\r\n    # 4. Pipeline models together\r\n    pipeline = ct.models.pipeline.Pipeline(\r\n        input_features=[\r\n            (\"image\", ct.models.datatypes.Array(3, ny, nx)),\r\n            (\"iouThreshold\", ct.models.datatypes.Double()),\r\n            (\"confidenceThreshold\", ct.models.datatypes.Double()),\r\n        ],\r\n        output_features=[\"confidence\", \"coordinates\"],\r\n    )\r\n    pipeline.add_model(model)\r\n    pipeline.add_model(nms_model)\r\n\r\n    # Correct datatypes\r\n    pipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\r\n    pipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\r\n    pipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\r\n\r\n    # Update metadata\r\n    pipeline.spec.specificationVersion = 5\r\n    pipeline.spec.description.metadata.versionString = \"https://github.com/ultralytics/yolov5\"\r\n    pipeline.spec.description.metadata.shortDescription = \"https://github.com/ultralytics/yolov5\"\r\n    pipeline.spec.description.metadata.author = \"glenn.jocher@ultralytics.com\"\r\n    pipeline.spec.description.metadata.license = \"https://github.com/ultralytics/yolov5/blob/master/LICENSE\"\r\n    pipeline.spec.description.metadata.userDefined.update(\r\n        {\r\n            \"classes\": \",\".join(names.values()),\r\n            \"iou_threshold\": str(nms.iouThreshold),\r\n            \"confidence_threshold\": str(nms.confidenceThreshold),\r\n        }\r\n    )\r\n\r\n    # Save the model\r\n    model = ct.models.MLModel(pipeline.spec, weights_dir=weights_dir)\r\n    model.input_description[\"image\"] = \"Input image\"\r\n    model.input_description[\"iouThreshold\"] = f\"(optional) IOU Threshold override (default: {nms.iouThreshold})\"\r\n    model.input_description[\"confidenceThreshold\"] = (\r\n        f\"(optional) Confidence Threshold override (default: {nms.confidenceThreshold})\"\r\n    )\r\n    model.output_description[\"confidence\"] = 'Boxes × Class confidence (see user-defined metadata \"classes\")'\r\n    model.output_description[\"coordinates\"] = \"Boxes × [x, y, width, height] (relative to image size)\"\r\n    model.save(f)  # pipelined\r\n    print(f\"{prefix} pipeline success ({time.time() - t:.2f}s), saved as {f} ({file_size(f):.1f} MB)\")",
        "labels": [
            "Dataframe Conversion API Misused",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, cfg=\"yolov5s.yaml\", ch=3, nc=None, anchors=None):\r\n        \"\"\"Initializes YOLOv5 model with configuration file, input channels, number of classes, and custom anchors.\"\"\"\r\n        super().__init__()\r\n        if isinstance(cfg, dict):\r\n            self.yaml = cfg  # model dict\r\n        else:  # is *.yaml\r\n            import yaml  # for torch hub\r\n\r\n            self.yaml_file = Path(cfg).name\r\n            with open(cfg, encoding=\"ascii\", errors=\"ignore\") as f:\r\n                self.yaml = yaml.safe_load(f)  # model dict\r\n\r\n        # Define model\r\n        ch = self.yaml[\"ch\"] = self.yaml.get(\"ch\", ch)  # input channels\r\n        if nc and nc != self.yaml[\"nc\"]:\r\n            LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\r\n            self.yaml[\"nc\"] = nc  # override yaml value\r\n        if anchors:\r\n            LOGGER.info(f\"Overriding model.yaml anchors with anchors={anchors}\")\r\n            self.yaml[\"anchors\"] = round(anchors)  # override yaml value\r\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\r\n        self.names = [str(i) for i in range(self.yaml[\"nc\"])]  # default names\r\n        self.inplace = self.yaml.get(\"inplace\", True)\r\n\r\n        # Build strides, anchors\r\n        m = self.model[-1]  # Detect()\r\n        if isinstance(m, (Detect, Segment)):\r\n\r\n            def _forward(x):\r\n                \"\"\"Passes the input 'x' through the model and returns the processed output.\"\"\"\r\n                return self.forward(x)[0] if isinstance(m, Segment) else self.forward(x)\r\n\r\n            s = 256  # 2x min stride\r\n            m.inplace = self.inplace\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases()  # only run once\r\n\r\n        # Init weights, biases\r\n        initialize_weights(self)\r\n        self.info()\r\n        LOGGER.info(\"\")",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def forward(self, ims, size=640, augment=False, profile=False):\r\n        \"\"\"\r\n        Performs inference on inputs with optional augment & profiling.\r\n\r\n        Supports various formats including file, URI, OpenCV, PIL, numpy, torch.\r\n        \"\"\"\r\n        # For size(height=640, width=1280), RGB images example inputs are:\r\n        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath\r\n        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'\r\n        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\r\n        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)\r\n        #   numpy:           = np.zeros((640,1280,3))  # HWC\r\n        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\r\n        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\r\n\r\n        dt = (Profile(), Profile(), Profile())\r\n        with dt[0]:\r\n            if isinstance(size, int):  # expand\r\n                size = (size, size)\r\n            p = next(self.model.parameters()) if self.pt else torch.empty(1, device=self.model.device)  # param\r\n            autocast = self.amp and (p.device.type != \"cpu\")  # Automatic Mixed Precision (AMP) inference\r\n            if isinstance(ims, torch.Tensor):  # torch\r\n                with amp.autocast(autocast):\r\n                    return self.model(ims.to(p.device).type_as(p), augment=augment)  # inference\r\n\r\n            # Pre-process\r\n            n, ims = (len(ims), list(ims)) if isinstance(ims, (list, tuple)) else (1, [ims])  # number, list of images\r\n            shape0, shape1, files = [], [], []  # image and inference shapes, filenames\r\n            for i, im in enumerate(ims):\r\n                f = f\"image{i}\"  # filename\r\n                if isinstance(im, (str, Path)):  # filename or uri\r\n                    im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith(\"http\") else im), im\r\n                    im = np.asarray(exif_transpose(im))\r\n                elif isinstance(im, Image.Image):  # PIL Image\r\n                    im, f = np.asarray(exif_transpose(im)), getattr(im, \"filename\", f) or f\r\n                files.append(Path(f).with_suffix(\".jpg\").name)\r\n                if im.shape[0] < 5:  # image in CHW\r\n                    im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\r\n                im = im[..., :3] if im.ndim == 3 else cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)  # enforce 3ch input\r\n                s = im.shape[:2]  # HWC\r\n                shape0.append(s)  # image shape\r\n                g = max(size) / max(s)  # gain\r\n                shape1.append([int(y * g) for y in s])\r\n                ims[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update\r\n            shape1 = [make_divisible(x, self.stride) for x in np.array(shape1).max(0)]  # inf shape\r\n            x = [letterbox(im, shape1, auto=False)[0] for im in ims]  # pad\r\n            x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW\r\n            x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32\r\n\r\n        with amp.autocast(autocast):\r\n            # Inference\r\n            with dt[1]:\r\n                y = self.model(x, augment=augment)  # forward\r\n\r\n            # Post-process\r\n            with dt[2]:\r\n                y = non_max_suppression(\r\n                    y if self.dmb else y[0],\r\n                    self.conf,\r\n                    self.iou,\r\n                    self.classes,\r\n                    self.agnostic,\r\n                    self.multi_label,\r\n                    max_det=self.max_det,\r\n                )  # NMS\r\n                for i in range(n):\r\n                    scale_boxes(shape1, y[i][:, :4], shape0[i])\r\n\r\n            return Detections(ims, y, files, dt, self.names, x.shape)",
        "labels": [
            "PyTorch Call Method Misused",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def forward(self, im, augment=False, visualize=False):\r\n        \"\"\"Performs YOLOv5 inference on input images with options for augmentation and visualization.\"\"\"\r\n        b, ch, h, w = im.shape  # batch, channel, height, width\r\n        if self.fp16 and im.dtype != torch.float16:\r\n            im = im.half()  # to FP16\r\n        if self.nhwc:\r\n            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\r\n\r\n        if self.pt:  # PyTorch\r\n            y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\r\n        elif self.jit:  # TorchScript\r\n            y = self.model(im)\r\n        elif self.dnn:  # ONNX OpenCV DNN\r\n            im = im.cpu().numpy()  # torch to numpy\r\n            self.net.setInput(im)\r\n            y = self.net.forward()\r\n        elif self.onnx:  # ONNX Runtime\r\n            im = im.cpu().numpy()  # torch to numpy\r\n            y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\r\n        elif self.xml:  # OpenVINO\r\n            im = im.cpu().numpy()  # FP32\r\n            y = list(self.ov_compiled_model(im).values())\r\n        elif self.engine:  # TensorRT\r\n            if self.dynamic and im.shape != self.bindings[\"images\"].shape:\r\n                i = self.model.get_binding_index(\"images\")\r\n                self.context.set_binding_shape(i, im.shape)  # reshape if dynamic\r\n                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=im.shape)\r\n                for name in self.output_names:\r\n                    i = self.model.get_binding_index(name)\r\n                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\r\n            s = self.bindings[\"images\"].shape\r\n            assert im.shape == s, f\"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}\"\r\n            self.binding_addrs[\"images\"] = int(im.data_ptr())\r\n            self.context.execute_v2(list(self.binding_addrs.values()))\r\n            y = [self.bindings[x].data for x in sorted(self.output_names)]\r\n        elif self.coreml:  # CoreML\r\n            im = im.cpu().numpy()\r\n            im = Image.fromarray((im[0] * 255).astype(\"uint8\"))\r\n            # im = im.resize((192, 320), Image.BILINEAR)\r\n            y = self.model.predict({\"image\": im})  # coordinates are xywh normalized\r\n            if \"confidence\" in y:\r\n                box = xywh2xyxy(y[\"coordinates\"] * [[w, h, w, h]])  # xyxy pixels\r\n                conf, cls = y[\"confidence\"].max(1), y[\"confidence\"].argmax(1).astype(np.float)\r\n                y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\r\n            else:\r\n                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\r\n        elif self.paddle:  # PaddlePaddle\r\n            im = im.cpu().numpy().astype(np.float32)\r\n            self.input_handle.copy_from_cpu(im)\r\n            self.predictor.run()\r\n            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\r\n        elif self.triton:  # NVIDIA Triton Inference Server\r\n            y = self.model(im)\r\n        else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\r\n            im = im.cpu().numpy()\r\n            if self.saved_model:  # SavedModel\r\n                y = self.model(im, training=False) if self.keras else self.model(im)\r\n            elif self.pb:  # GraphDef\r\n                y = self.frozen_func(x=self.tf.constant(im))\r\n            else:  # Lite or Edge TPU\r\n                input = self.input_details[0]\r\n                int8 = input[\"dtype\"] == np.uint8  # is TFLite quantized uint8 model\r\n                if int8:\r\n                    scale, zero_point = input[\"quantization\"]\r\n                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale\r\n                self.interpreter.set_tensor(input[\"index\"], im)\r\n                self.interpreter.invoke()\r\n                y = []\r\n                for output in self.output_details:\r\n                    x = self.interpreter.get_tensor(output[\"index\"])\r\n                    if int8:\r\n                        scale, zero_point = output[\"quantization\"]\r\n                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale\r\n                    y.append(x)\r\n            if len(y) == 2 and len(y[1].shape) != 4:\r\n                y = list(reversed(y))\r\n            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\r\n            y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\r\n\r\n        if isinstance(y, (list, tuple)):\r\n            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\r\n        else:\r\n            return self.from_numpy(y)",
        "labels": [
            "PyTorch Call Method Misused",
            "In-Place APIs Misused"
        ]
    },
    {
        "code": "def check_dataset(data, autodownload=True):\r\n    \"\"\"Validates and/or auto-downloads a dataset, returning its configuration as a dictionary.\"\"\"\r\n    # Download (optional)\r\n    extract_dir = \"\"\r\n    if isinstance(data, (str, Path)) and (is_zipfile(data) or is_tarfile(data)):\r\n        download(data, dir=f\"{DATASETS_DIR}/{Path(data).stem}\", unzip=True, delete=False, curl=False, threads=1)\r\n        data = next((DATASETS_DIR / Path(data).stem).rglob(\"*.yaml\"))\r\n        extract_dir, autodownload = data.parent, False\r\n\r\n    # Read yaml (optional)\r\n    if isinstance(data, (str, Path)):\r\n        data = yaml_load(data)  # dictionary\r\n\r\n    # Checks\r\n    for k in \"train\", \"val\", \"names\":\r\n        assert k in data, emojis(f\"data.yaml '{k}:' field missing ❌\")\r\n    if isinstance(data[\"names\"], (list, tuple)):  # old array format\r\n        data[\"names\"] = dict(enumerate(data[\"names\"]))  # convert to dict\r\n    assert all(isinstance(k, int) for k in data[\"names\"].keys()), \"data.yaml names keys must be integers, i.e. 2: car\"\r\n    data[\"nc\"] = len(data[\"names\"])\r\n\r\n    # Resolve paths\r\n    path = Path(extract_dir or data.get(\"path\") or \"\")  # optional 'path' default to '.'\r\n    if not path.is_absolute():\r\n        path = (ROOT / path).resolve()\r\n        data[\"path\"] = path  # download scripts\r\n    for k in \"train\", \"val\", \"test\":\r\n        if data.get(k):  # prepend path\r\n            if isinstance(data[k], str):\r\n                x = (path / data[k]).resolve()\r\n                if not x.exists() and data[k].startswith(\"../\"):\r\n                    x = (path / data[k][3:]).resolve()\r\n                data[k] = str(x)\r\n            else:\r\n                data[k] = [str((path / x).resolve()) for x in data[k]]\r\n\r\n    # Parse yaml\r\n    train, val, test, s = (data.get(x) for x in (\"train\", \"val\", \"test\", \"download\"))\r\n    if val:\r\n        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\r\n        if not all(x.exists() for x in val):\r\n            LOGGER.info(\"\\nDataset not found ⚠️, missing paths %s\" % [str(x) for x in val if not x.exists()])\r\n            if not s or not autodownload:\r\n                raise Exception(\"Dataset not found ❌\")\r\n            t = time.time()\r\n            if s.startswith(\"http\") and s.endswith(\".zip\"):  # URL\r\n                f = Path(s).name  # filename\r\n                LOGGER.info(f\"Downloading {s} to {f}...\")\r\n                torch.hub.download_url_to_file(s, f)\r\n                Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)  # create root\r\n                unzip_file(f, path=DATASETS_DIR)  # unzip\r\n                Path(f).unlink()  # remove zip\r\n                r = None  # success\r\n            elif s.startswith(\"bash \"):  # bash script\r\n                LOGGER.info(f\"Running {s} ...\")\r\n                r = subprocess.run(s, shell=True)\r\n            else:  # python script\r\n                r = exec(s, {\"yaml\": data})  # return None\r\n            dt = f\"({round(time.time() - t, 1)}s)\"\r\n            s = f\"success ✅ {dt}, saved to {colorstr('bold', DATASETS_DIR)}\" if r in (0, None) else f\"failure {dt} ❌\"\r\n            LOGGER.info(f\"Dataset download {s}\")\r\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\", progress=True)  # download fonts\r\n    return data",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def apply_classifier(x, model, img, im0):\r\n    \"\"\"Applies second-stage classifier to YOLO outputs, filtering detections by class match.\"\"\"\r\n    # Example model = torchvision.models.__dict__['efficientnet_b0'](pretrained=True).to(device).eval()\r\n    im0 = [im0] if isinstance(im0, np.ndarray) else im0\r\n    for i, d in enumerate(x):  # per image\r\n        if d is not None and len(d):\r\n            d = d.clone()\r\n\r\n            # Reshape and pad cutouts\r\n            b = xyxy2xywh(d[:, :4])  # boxes\r\n            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\r\n            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\r\n            d[:, :4] = xywh2xyxy(b).long()\r\n\r\n            # Rescale boxes from img_size to im0 size\r\n            scale_boxes(img.shape[2:], d[:, :4], im0[i].shape)\r\n\r\n            # Classes\r\n            pred_cls1 = d[:, 5].long()\r\n            ims = []\r\n            for a in d:\r\n                cutout = im0[i][int(a[1]) : int(a[3]), int(a[0]) : int(a[2])]\r\n                im = cv2.resize(cutout, (224, 224))  # BGR\r\n\r\n                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\r\n                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\r\n                im /= 255  # 0 - 255 to 0.0 - 1.0\r\n                ims.append(im)\r\n\r\n            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\r\n            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\r\n\r\n    return x",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def train_one_epoch(\r\n    cnn_model: Any,\r\n    train_loader: DataLoader,\r\n    optimizer: torch.optim.Optimizer,\r\n    loss: nn.Module,\r\n):\r\n    \"\"\"Train the model for a single epoch.\r\n\r\n    Args:\r\n        cnn_model (Any): The instantiated model to train.\r\n        train_loader (DataLoader): The batched train data to consider.\r\n        optimizer (torch.optim.Optimizer): The optimizer to use for computing the gradients.\r\n        loss (nn.Module): The loss function to use.\r\n    \"\"\"\r\n\r\n    # Set the network to training mode (enable gradient computation on normalization layers)\r\n    cnn_model.train()\r\n\r\n    accumulated_loss = 0\r\n    for data, target in train_loader:\r\n        # Clear the gradients\r\n        optimizer.zero_grad()\r\n\r\n        # Run the forward pass\r\n        output = cnn_model(data)\r\n\r\n        # Compute the loss\r\n        loss_value = loss(output, target.long())\r\n\r\n        # Compute the gradients with backpropagation\r\n        loss_value.backward()\r\n\r\n        # Update the weights using the gradients\r\n        optimizer.step()\r\n\r\n        # Store the computed loss\r\n        accumulated_loss += loss_value.item()\r\n\r\n    print(f\"Loss: {accumulated_loss / len(train_loader):.2f}\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def generate_figure(data: Dict[str, Any], path_to_csv: Path, path_to_png: Path):\r\n    \"\"\"Generate the figure of timing evolution on last 4 weeks using GitHub API data.\r\n\r\n    Arguments:\r\n        data (Dict[str, Any]): a dict with the results of the Github API\r\n        path_to_csv (Path): path to csv where to dump filtered data\r\n        path_to_png (Path): path to png of the figure\r\n    \"\"\"\r\n    number_of_runs = data[\"total_count\"]\r\n    runs = data[\"workflow_runs\"]\r\n\r\n    # Should be equal\r\n    print(number_of_runs, len(runs))\r\n\r\n    # Cast to DataFrame\r\n    dataframe = pd.DataFrame(runs)\r\n    for col in [\"updated_at\", \"created_at\", \"run_started_at\"]:\r\n        values = dataframe[col]\r\n        assert isinstance(values, pd.Series)\r\n        dataframe[col] = pd.to_datetime(values, unit=\"ns\")\r\n\r\n    # Copy\r\n    sub_df = dataframe.copy()\r\n\r\n    # Filtering\r\n    sub_df = sub_df[sub_df[\"conclusion\"] == \"success\"]\r\n    assert isinstance(sub_df, pd.DataFrame)\r\n    sub_df = sub_df[sub_df[\"status\"] == \"completed\"]\r\n    assert isinstance(sub_df, pd.DataFrame)\r\n    mask = sub_df[\"name\"].apply(\r\n        lambda elt: elt\r\n        in {\"CML builds (weekly or not)\", \"CML weekly builds\", \"concrete-ml CI Pipeline\"}\r\n    )\r\n    assert isinstance(mask, pd.Series)\r\n    sub_df = sub_df[mask]\r\n    assert isinstance(sub_df, pd.DataFrame)\r\n\r\n    # 4 week history\r\n    now = datetime.datetime.now()\r\n    start_date = pd.Timestamp(now, tz=\"UTC\") - pd.Timedelta(weeks=4)\r\n\r\n    # Filter based on starting date\r\n    mask = sub_df[\"run_started_at\"] >= start_date\r\n    assert isinstance(mask, pd.Series)\r\n    sub_df = sub_df[mask]\r\n    # Handle dates\r\n    sub_df[\"time_diff\"] = (sub_df[\"updated_at\"] - sub_df[\"run_started_at\"]).astype(int)\r\n    # Nanoseconds to seconds\r\n    sub_df[\"time_diff\"] = sub_df[\"time_diff\"] / 1e9\r\n    # seconds to minutes\r\n    sub_df[\"time_diff\"] = sub_df[\"time_diff\"] / 60\r\n    # Compute relative time (for regression)\r\n    sub_df[\"relative_time\"] = sub_df[\"run_started_at\"] - sub_df[\"run_started_at\"].min()\r\n    assert isinstance(sub_df, pd.DataFrame)\r\n\r\n    sub_df.to_csv(str(path_to_csv.resolve()))\r\n\r\n    # Just to generate the regression line\r\n    regressor = LinearRegression()\r\n    x_train_df = sub_df[[\"relative_time\"]]\r\n    assert isinstance(x_train_df, pd.DataFrame)\r\n    x_train = x_train_df.astype(int)\r\n    regressor.fit(X=x_train, y=sub_df[\"time_diff\"])\r\n    y_pred = regressor.predict(x_train)\r\n\r\n    # Generate figure\r\n    fig, axis = plt.subplots(figsize=(8, 4), dpi=800)\r\n    assert isinstance(axis, Axes)\r\n\r\n    fig.suptitle(\"Successful Concrete ML-CI time execution in minutes over the 4 last weeks\")\r\n    axis.set_xlabel(\"Date\")\r\n    axis.set_ylabel(\"Minutes\")\r\n    # ax.set_yscale(\"log\")\r\n    axis.scatter(sub_df[\"run_started_at\"], sub_df[\"time_diff\"], marker=\"x\")\r\n    axis.plot(sub_df[\"run_started_at\"], y_pred, color=\"red\")\r\n    plt.xticks(rotation=\"vertical\")\r\n    plt.tight_layout()\r\n\r\n    # Save figure\r\n    fig.savefig(str(path_to_png.resolve()))",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def compile_and_simulated_fhe_inference(\r\n    estimator: torch.nn.Module,\r\n    calibration_data: numpy.ndarray,\r\n    ground_truth: numpy.ndarray,\r\n    p_error: float,\r\n    n_bits: int,\r\n    is_qat: bool,\r\n    metric: Callable,\r\n    predict: str,\r\n    **kwargs: Dict,\r\n) -> Tuple[numpy.ndarray, float]:\r\n    \"\"\"Get the quantized module of a given model in FHE, simulated or not.\r\n\r\n    Supported models are:\r\n    - Built-in models, including trees and QNN,\r\n    - Quantized aware trained model are supported using Brevitas framework,\r\n    - Torch models can be converted into post-trained quantized models.\r\n\r\n    Args:\r\n        estimator (torch.nn.Module): Torch model or a built-in model\r\n        calibration_data (numpy.ndarray): Calibration data required for compilation\r\n        ground_truth (numpy.ndarray): The ground truth\r\n        p_error (float): Concrete ML uses table lookup (TLU) to represent any non-linear\r\n        n_bits (int): Quantization bits\r\n        is_qat (bool): True, if the NN has been trained through QAT.\r\n            If `False` it is converted into post-trained quantized model.\r\n        metric (Callable): Classification or regression evaluation metric.\r\n        predict (str): The predict method to use.\r\n        kwargs (Dict): Hyper-parameters to use for the metric.\r\n\r\n    Returns:\r\n        Tuple[numpy.ndarray, float]: De-quantized or quantized output model depending on\r\n        `is_benchmark_test` and the score.\r\n\r\n    Raises:\r\n        ValueError: If the model is neither a built-in model nor a torch neural network.\r\n    \"\"\"\r\n\r\n    compile_params: Dict = {}\r\n    compile_function: Callable[..., Any]\r\n    dequantized_output: numpy.ndarray\r\n\r\n    # Custom neural networks with QAT\r\n    if isinstance(estimator, torch.nn.Module):\r\n        if is_qat and is_brevitas_model(estimator):\r\n            compile_function = compile_brevitas_qat_model\r\n        else:\r\n            # Custom neural networks with PTQ\r\n            compile_function = compile_torch_model\r\n            compile_params = {\"import_qat\": is_qat, \"n_bits\": n_bits}\r\n\r\n        quantized_module = compile_function(\r\n            torch_model=estimator,\r\n            torch_inputset=calibration_data,\r\n            p_error=p_error,\r\n            **compile_params,\r\n        )\r\n\r\n        dequantized_output = quantized_module.forward(calibration_data, fhe=\"simulate\")\r\n\r\n    elif is_model_class_in_a_list(\r\n        estimator, _get_sklearn_all_models()\r\n    ) and not is_model_class_in_a_list(estimator, _get_sklearn_linear_models()):\r\n        if not estimator.is_fitted:\r\n            estimator.fit(calibration_data, ground_truth)\r\n\r\n        estimator.compile(calibration_data, p_error=p_error)\r\n        predict_method = getattr(estimator, predict)\r\n        dequantized_output = predict_method(calibration_data, fhe=\"simulate\")\r\n\r\n    else:\r\n        raise ValueError(\r\n            f\"`{type(estimator)}` is not supported. \"\r\n            \"Supported types are: custom Torch, Brevitas NNs and built-in models (trees and QNNs).\"\r\n        )\r\n\r\n    score = metric(ground_truth, dequantized_output, **kwargs)\r\n\r\n    return dequantized_output, score",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def __init__(self, use_conv, use_qat, inp_size, n_bits):\r\n        super().__init__()\r\n\r\n        if use_conv:\r\n            # Initialize a blur filter float weights\r\n            np_weights = numpy.asarray([[[[1, 1, 1], [1, 4, 1], [1, 1, 1]]]])\r\n\r\n            if use_qat:\r\n                self.mixing_layer = nn.Sequential(\r\n                    qnn.QuantIdentity(bit_width=n_bits),\r\n                    qnn.QuantConv2d(1, 1, 3, stride=1, bias=True, weight_bit_width=n_bits),\r\n                )\r\n                layer_obj = self.mixing_layer[1]\r\n            else:\r\n                self.mixing_layer = nn.Conv2d(1, 1, 3, stride=1, bias=True)\r\n                layer_obj = self.mixing_layer\r\n        else:\r\n            # Initialize a linear layer with 1s\r\n            np_weights = numpy.asarray([[1] * inp_size])\r\n            if use_qat:\r\n                self.mixing_layer = nn.Sequential(\r\n                    qnn.QuantIdentity(bit_width=n_bits),\r\n                    qnn.QuantLinear(inp_size, inp_size, bias=True, weight_bit_width=n_bits),\r\n                )\r\n                layer_obj = self.mixing_layer[1]\r\n            else:\r\n                self.mixing_layer = nn.Linear(inp_size, inp_size, bias=True)\r\n                layer_obj = self.mixing_layer\r\n\r\n        layer_obj.weight.data = torch.from_numpy(np_weights).float()\r\n        layer_obj.bias.data = torch.rand(size=(1,))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, input_output, act):\r\n        super().__init__()\r\n        self.feat = nn.Sequential()\r\n        in_features = input_output\r\n        self.n_layers = 2\r\n        self.biases = [torch.Tensor(size=(1,)) for _ in range(self.n_layers)]\r\n        for b in self.biases:\r\n            nn.init.uniform_(b)\r\n\r\n        for idx in range(self.n_layers):\r\n            out_features = in_features if idx == self.n_layers - 1 else in_features\r\n            layer_name = f\"fc{idx}\"\r\n            layer = nn.Linear(in_features=in_features, out_features=out_features, bias=False)\r\n            self.feat.add_module(layer_name, layer)\r\n            in_features = out_features\r\n\r\n        self.act = act()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _set_onnx_model(self, test_input: numpy.ndarray) -> None:\r\n        \"\"\"Retrieve the model's ONNX graph using Hummingbird conversion.\r\n\r\n        Args:\r\n            test_input (numpy.ndarray): An input data used to trace the model execution.\r\n        \"\"\"\r\n        # Check that the underlying sklearn model has been set and fit\r\n        assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()\r\n\r\n        self.onnx_model_ = hb_convert(\r\n            self.sklearn_model,\r\n            backend=\"onnx\",\r\n            test_input=test_input,\r\n            extra_config={\"onnx_target_opset\": OPSET_VERSION_FOR_ONNX_EXPORT},\r\n        ).model\r\n\r\n        self._clean_graph()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def __init__(self, input_output, act):\r\n        super().__init__()\r\n        self.feat = nn.Sequential()\r\n        in_features = input_output\r\n        self.n_layers = 2\r\n        self.biases = [torch.Tensor(size=(input_output,)) for _ in range(self.n_layers)]\r\n        for b in self.biases:\r\n            nn.init.uniform_(b)\r\n\r\n        for idx in range(self.n_layers):\r\n            out_features = in_features if idx == self.n_layers - 1 else in_features\r\n            layer_name = f\"fc{idx}\"\r\n            layer = nn.Linear(in_features=in_features, out_features=out_features, bias=False)\r\n            self.feat.add_module(layer_name, layer)\r\n            in_features = out_features\r\n\r\n        self.act = act()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _get_equivalent_float_module(self) -> torch.nn.Sequential:\r\n        \"\"\"Build a topologically equivalent Torch module that can be used on floating points.\r\n\r\n        Returns:\r\n            float_module (torch.nn.Sequential): The equivalent float module.\r\n        \"\"\"\r\n        # Instantiate a new sequential module\r\n        float_module = torch.nn.Sequential()\r\n\r\n        layer_index = -1\r\n\r\n        # Iterate over the model's sub-modules\r\n        for module in self.base_module.features:\r\n\r\n            # If the module is not a QuantIdentity, it is either a QuantLinear or an activation\r\n            if not isinstance(module, qnn.QuantIdentity):\r\n                # If the module is a QuantLinear, replace it with a Linear module\r\n                if isinstance(module, qnn.QuantLinear):\r\n                    layer_index += 1\r\n\r\n                    linear_name = f\"fc{layer_index}\"\r\n                    linear_layer = torch.nn.Linear(\r\n                        module.in_features,\r\n                        module.out_features,\r\n                        module.bias is not None,\r\n                    )\r\n\r\n                    float_module.add_module(linear_name, linear_layer)\r\n\r\n                # Else, it is a module representing the activation function, which needs to be\r\n                # added as well\r\n                else:\r\n                    activation_name = f\"act{layer_index}\"\r\n                    float_module.add_module(activation_name, module)\r\n\r\n        return float_module",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _set_onnx_model(self, test_input: numpy.ndarray) -> None:\r\n        \"\"\"Retrieve the model's ONNX graph using Hummingbird conversion.\r\n\r\n        Args:\r\n            test_input (numpy.ndarray): An input data used to trace the model execution.\r\n        \"\"\"\r\n        # Check that the underlying sklearn model has been set and fit\r\n        assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()\r\n\r\n        model_for_onnx = LinearRegression()\r\n        model_for_onnx.coef_ = self.sklearn_model.coef_\r\n        model_for_onnx.intercept_ = self.sklearn_model.intercept_\r\n\r\n        self.onnx_model_ = hb_convert(\r\n            model_for_onnx,\r\n            backend=\"onnx\",\r\n            test_input=test_input,\r\n            extra_config={\"onnx_target_opset\": OPSET_VERSION_FOR_ONNX_EXPORT},\r\n        ).model\r\n\r\n        self._clean_graph()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def _set_onnx_model(self, test_input: numpy.ndarray) -> None:\r\n        \"\"\"Retrieve the model's ONNX graph using Hummingbird conversion.\r\n\r\n        Args:\r\n            test_input (numpy.ndarray): An input data used to trace the model execution.\r\n        \"\"\"\r\n        # Check that the underlying sklearn model has been set and fit\r\n        assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()\r\n\r\n        model_for_onnx = LogisticRegression()\r\n        model_for_onnx.coef_ = self.sklearn_model.coef_\r\n        model_for_onnx.intercept_ = self.sklearn_model.intercept_\r\n\r\n        assert_true(\r\n            hasattr(self.sklearn_model, \"classes_\"),\r\n            \"The fit method should have been called on this model\",\r\n        )\r\n        model_for_onnx.classes_ = getattr(self.sklearn_model, \"classes_\", None)\r\n\r\n        self.onnx_model_ = hb_convert(\r\n            model_for_onnx,\r\n            backend=\"onnx\",\r\n            test_input=test_input,\r\n            extra_config={\"onnx_target_opset\": OPSET_VERSION_FOR_ONNX_EXPORT},\r\n        ).model\r\n\r\n        self._clean_graph()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def forward(self, x: torch.Tensor, fhe: str = \"disable\") -> torch.Tensor:\r\n        \"\"\"Forward pass of the hybrid model.\r\n\r\n        Args:\r\n            x (torch.Tensor): The input tensor.\r\n            fhe (str): The Fully Homomorphic Encryption (FHE) mode (default is \"disable\").\r\n\r\n        Returns:\r\n            torch.Tensor: The output tensor.\r\n\r\n        Raises:\r\n            AssertionError: if the execution mode is not supported\r\n        \"\"\"\r\n        self.set_fhe_mode(fhe)\r\n\r\n        # Validate the FHE mode\r\n        fhe_mode = HybridFHEMode(fhe)\r\n\r\n        if has_glwe_backend() and self._has_only_large_linear_layers:\r\n            if fhe_mode == HybridFHEMode.SIMULATE:\r\n                raise AssertionError(\r\n                    \"When the HybridFHEModel is instantiated with only \"\r\n                    \"linear remote layers, fhe=simulate is not supported for now.\",\r\n                )\r\n\r\n            if fhe_mode in (HybridFHEMode.EXECUTE, HybridFHEMode.REMOTE, HybridFHEMode.DISABLE):\r\n                # Initialize executor only if not already done\r\n                self.executor = self.executor or GLWELinearLayerExecutor()\r\n\r\n                # Generate keys only if needed and not already done\r\n                if fhe_mode != HybridFHEMode.DISABLE and self.executor.private_key is None:\r\n                    self.executor.keygen()\r\n\r\n        # Update executor for all remote modules\r\n        for module in self.remote_modules.values():\r\n            module.executor = self.executor\r\n\r\n        result = self.model(x)\r\n\r\n        return result",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def _set_onnx_model(self, test_input: numpy.ndarray) -> None:\r\n        \"\"\"Retrieve the model's ONNX graph using Hummingbird conversion.\r\n\r\n        Args:\r\n            test_input (numpy.ndarray): An input data used to trace the model execution.\r\n        \"\"\"\r\n        # Check that the underlying sklearn model has been set and fit\r\n        assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()\r\n\r\n        self.onnx_model_ = hb_convert(\r\n            self.sklearn_model,\r\n            backend=\"onnx\",\r\n            test_input=test_input,\r\n            extra_config={\r\n                \"onnx_target_opset\": OPSET_VERSION_FOR_ONNX_EXPORT,\r\n                # pylint: disable-next=protected-access, no-member\r\n                constants.BATCH_SIZE: self.sklearn_model._fit_X.shape[0],\r\n            },\r\n        ).model\r\n\r\n        self._clean_graph()",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_failure_bad_data_types(model_classes, container, bad_types, expected_error, load_data):\r\n    \"\"\"Check that training using data with unsupported dtypes raises an expected error.\"\"\"\r\n    for model_class in model_classes:\r\n        # Generate the data\r\n        x, y = load_data(model_class)\r\n\r\n        x, y = x.astype(bad_types[0]), y.astype(bad_types[1])\r\n\r\n        if container == \"torch\":\r\n            # Convert input and target to Torch tensors if the values are not string, as torch\r\n            # Tensors only handles numerical values\r\n            if \"str\" not in bad_types:\r\n                x, y = torch.tensor(x), torch.tensor(y)\r\n\r\n        elif container == \"pandas\":\r\n            # Convert input to a Pandas DataFrame or Pandas Series\r\n            x, y = pandas.DataFrame(x), pandas.DataFrame(y)\r\n\r\n        # Instantiate the model\r\n        model = model_class()\r\n\r\n        # Train the model, which should raise the expected error\r\n        with pytest.raises(ValueError, match=expected_error):\r\n            model.fit(x, y)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_error_raise_unsupported_pandas_values(model_class, bad_value, expected_error):\r\n    \"\"\"Test that using Pandas data-frame with unsupported values as input raises correct errors.\"\"\"\r\n\r\n    dic = {\r\n        \"Col One\": [1, 2, bad_value, 3],\r\n        \"Col Two\": [4, 5, 6, bad_value],\r\n        \"Col Three\": [bad_value, 7, 8, 9],\r\n    }\r\n\r\n    # Creating a dataframe using dictionary\r\n    x_train = pandas.DataFrame(dic)\r\n    y_train = x_train[\"Col Three\"]\r\n\r\n    model = model_class(n_bits=2)\r\n\r\n    # The error message changed in one of our dependencies\r\n    assert sys.version_info.major == 3\r\n    if sys.version_info.minor <= 7:\r\n        if expected_error == \"Input X contains NaN.\":\r\n            expected_error = \"Input contains NaN*\"\r\n\r\n    with pytest.raises(ValueError, match=expected_error):\r\n        model.fit(x_train, y_train)",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_brevitas_intermediary_values(\r\n    n_layers,\r\n    n_bits_w_a,\r\n    n_accum_bits,\r\n    activation_function,\r\n    n_outputs,\r\n    input_dim,\r\n    model_class,\r\n    load_data,\r\n    signed,\r\n    narrow,\r\n):  # pylint: disable=too-many-statements, too-many-locals\r\n    \"\"\"Test the correctness of the results of quantized NN classifiers through the sklearn\r\n    wrapper.\r\n\r\n    First, we train a Torch classifier, with various quantization options (narrow/signed/bits).\r\n    Then, we wrap the trained model in a debug module. This module will capture the quantized\r\n    integer values that are input to conv/linear layers. We also capture the quantized integer\r\n    weights. For both weights and quantized inputs, we also capture the corresponding floating\r\n    point value that produced the integer value.\r\n\r\n    Next, we convert the Torch model to a QuantizedModule. We use the debug feature to capture\r\n    the integer and floating point values that are inputs to all the conv/linear layers.\r\n\r\n    Finally, we compare the integer values from the Torch/brevitas execution with those captured\r\n    by the QuantizedModule execution and find differences. When a difference in integers is found\r\n    we print the offending raw floating point values, and, when available, quantization options.\r\n    \"\"\"\r\n\r\n    # Get the data-set. The data generation is seeded in load_data.\r\n    if is_classifier_or_partial_classifier(model_class):\r\n        x, y = load_data(\r\n            model_class,\r\n            n_samples=1000,\r\n            n_features=input_dim,\r\n            n_redundant=0,\r\n            n_repeated=0,\r\n            n_informative=input_dim,\r\n            n_classes=n_outputs,\r\n            class_sep=2,\r\n        )\r\n\r\n    # Get the data-set. The data generation is seeded in load_data.\r\n    elif is_regressor_or_partial_regressor(model_class):\r\n        x, y, _ = load_data(\r\n            model_class,\r\n            n_samples=1000,\r\n            n_features=input_dim,\r\n            n_informative=input_dim,\r\n            n_targets=n_outputs,\r\n            noise=2,\r\n            coef=True,\r\n        )\r\n        if y.ndim == 1:\r\n            y = numpy.expand_dims(y, 1)\r\n    else:\r\n        raise ValueError(f\"Data generator not implemented for {str(model_class)}\")\r\n\r\n    # Perform a classic test-train split (deterministic by fixing the seed)\r\n    x_train, x_test, y_train, _ = train_test_split(\r\n        x,\r\n        y,\r\n        test_size=0.25,\r\n        random_state=numpy.random.randint(0, 2**15),\r\n    )\r\n\r\n    params = {\r\n        \"module__n_layers\": n_layers,\r\n        \"module__n_w_bits\": n_bits_w_a,\r\n        \"module__n_a_bits\": n_bits_w_a,\r\n        \"module__n_accum_bits\": n_accum_bits,\r\n        \"module__activation_function\": activation_function,\r\n        \"module__quant_signed\": signed,\r\n        \"module__quant_narrow\": narrow,\r\n        \"max_epochs\": 10,\r\n        \"verbose\": 0,\r\n    }\r\n\r\n    concrete_model = model_class(**params)\r\n\r\n    # Compute mean/stdev on training set and normalize both train and test sets with them\r\n    normalizer = StandardScaler()\r\n    x_train = normalizer.fit_transform(x_train)\r\n    x_test = normalizer.transform(x_test)\r\n\r\n    concrete_model.fit(x_train, y_train)\r\n\r\n    # Wrap the original torch module with a debug module that captures intermediary values\r\n    class DebugQNNModel(SparseQuantNeuralNetwork):\r\n        \"\"\"Wrapper class that extracts intermediary values from a Brevitas QAT net.\"\"\"\r\n\r\n        intermediary_values = []\r\n        intermediary_inp_values_float = []\r\n        quant_weights = []\r\n        raw_weights = []\r\n        narrow_range_inp = []\r\n        narrow_range_weight = []\r\n\r\n        def forward(self, x):\r\n            for mod in self.features:\r\n                if isinstance(mod, qnn.QuantLinear):\r\n                    self.intermediary_inp_values_float.append(x.value.detach().numpy())\r\n                x = mod(x)\r\n                if isinstance(mod, qnn.QuantIdentity):\r\n                    self.intermediary_values.append(x.int().detach().numpy())\r\n                    self.narrow_range_inp.append(mod.act_quant.is_narrow_range)\r\n                elif isinstance(mod, qnn.QuantLinear):\r\n                    self.narrow_range_weight.append(mod.weight_quant.is_narrow_range)\r\n                    self.quant_weights.append(mod.int_weight().detach().numpy())\r\n                    self.raw_weights.append(mod.quant_weight().value.detach().numpy())\r\n            return x\r\n\r\n    params_module = {\r\n        param.replace(\"module__\", \"\"): value\r\n        for param, value in params.items()\r\n        if \"module__\" in param\r\n    }\r\n\r\n    # Concrete ML and Concrete Python use float64, so we need to force pytorch to use the same, as\r\n    # it defaults to float32. Note that this change is global and may interfere with\r\n    # threading or multiprocessing. Thus this test can not be launched in parallel with others.\r\n    torch.set_default_dtype(torch.float64)\r\n\r\n    # Wrap the original model, and copy its weights\r\n    dbg_model = DebugQNNModel(**params_module, input_dim=input_dim, n_outputs=n_outputs)\r\n    dbg_model.load_state_dict(concrete_model.base_module.state_dict())\r\n\r\n    # Execute on the test set and capture debug values\r\n    dbg_model(torch.tensor(x_test.astype(numpy.float64)))\r\n\r\n    # Execute the Concrete ML model on the test set and capture debug values\r\n    _, cml_debug_values = concrete_model.quantized_module_.forward(\r\n        x_test, debug=True, fhe=\"disable\"\r\n    )\r\n\r\n    cml_intermediary_values = [\r\n        q_arr[0].qvalues for name, q_arr in cml_debug_values.items() if \"Gemm\" in name\r\n    ]\r\n    cml_input_values = [\r\n        q_arr[0].values for name, q_arr in cml_debug_values.items() if \"Gemm\" in name\r\n    ]\r\n    cml_quantizers = [\r\n        q_arr[0].quantizer for name, q_arr in cml_debug_values.items() if \"Gemm\" in name\r\n    ]\r\n    cml_quant_weights = [\r\n        q_arr[1].qvalues for name, q_arr in cml_debug_values.items() if \"Gemm\" in name\r\n    ]\r\n    cml_raw_weights = [\r\n        q_arr[1].values for name, q_arr in cml_debug_values.items() if \"Gemm\" in name\r\n    ]\r\n\r\n    # Make sure the quantization options were well set by brevitas\r\n    assert len(set(dbg_model.narrow_range_inp)) > 0 and dbg_model.narrow_range_inp[0] == narrow\r\n    assert (\r\n        len(set(dbg_model.narrow_range_weight)) > 0 and dbg_model.narrow_range_weight[0] == narrow\r\n    )\r\n\r\n    # Iterate across conv/linear layers\r\n\r\n    # pylint: disable-next=consider-using-enumerate\r\n    for idx in range(len(cml_intermediary_values)):\r\n        # Check if any activations are different between Brevitas and Concrete ML\r\n        diff_inp = numpy.abs(cml_intermediary_values[idx] - dbg_model.intermediary_values[idx])\r\n        error = \"\"\r\n        if numpy.any(diff_inp) > 0:\r\n            # If any mismatches, then extract them and print them\r\n            indices = numpy.nonzero(diff_inp)\r\n            error = (\r\n                f\"Mismatched values in layer {idx} at input indices: {numpy.transpose(indices)}\\n\"\r\n                f\"Concrete ML Inputs were: {cml_input_values[idx][indices]} \\n\"\r\n                f\"Concrete ML quantized to {cml_intermediary_values[idx][indices]}\\n\"\r\n                f\"Brevitas inputs were {dbg_model.intermediary_inp_values_float[idx][indices]}\\n\"\r\n                f\"Brevitas quantized to {dbg_model.intermediary_values[idx][indices]}\\n \"\r\n                f\"Quant params were {str(cml_quantizers[idx].__dict__)}\\n \"\r\n            )\r\n\r\n        # Assert if there were any mismatches\r\n        assert numpy.all(diff_inp == 0), error\r\n\r\n        # Check if any weights are different between Brevitas and Concrete ML\r\n        diff_weights = numpy.abs(cml_quant_weights[idx] - dbg_model.quant_weights[idx])\r\n        weights_ok = True\r\n\r\n        if numpy.any(diff_weights) > 0:\r\n            indices = numpy.nonzero(diff_weights)\r\n            diff_raw_weights = numpy.abs(\r\n                dbg_model.raw_weights[idx][indices] - cml_raw_weights[idx][indices]\r\n            )\r\n\r\n            # Here, numpy.all returns Numpy's `bool_` type, which is a different type than `bool`.\r\n            # Since `weights_ok` is initialized using a `bool`, mypy complains and we therefore\r\n            # need to force the type\r\n            weights_ok = bool(numpy.all(diff_raw_weights > 0.0001))\r\n\r\n            error = (\r\n                f\"Mismatched weights in layer {idx} at input indices: {numpy.transpose(indices)}\\n\"\r\n                f\"Concrete ML raw weights were: {cml_raw_weights[idx][indices]} \\n\"\r\n                f\"Concrete ML quantized to {cml_quant_weights[idx][indices]}\\n\"\r\n                f\"Brevitas weights were {dbg_model.raw_weights[idx][indices]}\\n\"\r\n                f\"Brevitas quantized to {dbg_model.quant_weights[idx][indices]}\\n \"\r\n            )\r\n\r\n        assert weights_ok, error\r\n\r\n    torch.set_default_dtype(torch.float32)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def check_input_support(model_class, n_bits, default_configuration, x, y, input_type):\r\n    \"\"\"Test all models with Pandas, List or Torch inputs.\"\"\"\r\n\r\n    def cast_input(x, y, input_type):\r\n        \"Convert x and y either in Pandas, List, Numpy or Torch type.\"\r\n\r\n        assert input_type in [\r\n            \"pandas\",\r\n            \"torch\",\r\n            \"list\",\r\n            \"numpy\",\r\n        ], \"Not a valid type casting\"\r\n\r\n        if input_type.lower() == \"pandas\":\r\n            # Turn into Pandas\r\n            x = pandas.DataFrame(x)\r\n            y = pandas.Series(y) if y.ndim == 1 else pandas.DataFrame(y)\r\n        elif input_type.lower() == \"torch\":\r\n            # Turn into Torch\r\n            x = torch.tensor(x)\r\n            y = torch.tensor(y)\r\n        elif input_type.lower() == \"list\":\r\n            # Turn into List\r\n            x = x.tolist()\r\n            y = y.tolist()\r\n        elif input_type.lower() == \"numpy\":\r\n            assert isinstance(x, numpy.ndarray), f\"Wrong type {type(x)}\"\r\n            assert isinstance(y, numpy.ndarray), f\"Wrong type {type(y)}\"\r\n        return x, y\r\n\r\n    model = instantiate_model_generic(model_class, n_bits=n_bits)\r\n    x, y = cast_input(x, y, input_type=input_type)\r\n\r\n    model.fit(x, y)\r\n\r\n    # Make sure `predict` is working when FHE is disabled\r\n    model.predict(x)\r\n\r\n    # Similarly, we test `predict_proba` for classifiers\r\n    # KNeighborsClassifier does not provide a predict_proba method for now\r\n    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3962\r\n    if (\r\n        is_classifier_or_partial_classifier(model)\r\n        and get_model_name(model_class) != \"KNeighborsClassifier\"\r\n    ):\r\n        model.predict_proba(x)\r\n\r\n    model.compile(x, default_configuration)\r\n\r\n    # Make sure `predict` is working when FHE is disabled\r\n    model.predict(x, fhe=\"simulate\")\r\n\r\n    # Similarly, we test `predict_proba` for classifiers\r\n    # KNeighborsClassifier does not provide a predict_proba method for now\r\n    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3962\r\n    if (\r\n        is_classifier_or_partial_classifier(model)\r\n        and get_model_name(model_class) != \"KNeighborsClassifier\"\r\n    ):\r\n        model.predict_proba(x, fhe=\"simulate\")",
        "labels": [
            "Columns and DataType Not Explicitly Set"
        ]
    },
    {
        "code": "def test_compile_embedding_model(default_configuration):\r\n    \"\"\"Test compiling the EmbeddingModel using compile_torch_model.\"\"\"\r\n\r\n    # Set up the EmbeddingModel parameters\r\n    num_embeddings = 10\r\n    embedding_dim = 5\r\n\r\n    # Create the model\r\n    model = EmbeddingModel(num_embeddings, embedding_dim)\r\n\r\n    # Create integer input data\r\n    n_samples = 100\r\n    inputset = torch.randint(0, num_embeddings - 1, size=(n_samples, 1)).long()\r\n\r\n    # Compile the model\r\n    compiled_model = compile_torch_model(\r\n        model,\r\n        inputset,\r\n        n_bits=8,\r\n        configuration=default_configuration,\r\n        rounding_threshold_bits=8,\r\n        p_error=0.01,\r\n    )\r\n\r\n    # Test the compiled model\r\n    test_input = torch.tensor([[3], [6], [1]]).long()\r\n    torch_output = model(test_input).detach().numpy()\r\n    test_input_numpy = test_input.numpy()\r\n    compiled_output = compiled_model.forward(test_input_numpy)\r\n\r\n    # Check if the outputs are close\r\n    numpy.testing.assert_allclose(torch_output, compiled_output, atol=1e-1)\r\n\r\n    # Test FHE simulation\r\n    fhe_output = compiled_model.forward(test_input_numpy, fhe=\"simulate\")\r\n\r\n    numpy.testing.assert_allclose(torch_output, fhe_output, atol=1e-1)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_multi_output(model_object, default_configuration):\r\n    \"\"\"Test torch compilation with multi-output models.\"\"\"\r\n    # Create model and random dataset\r\n    model = model_object()\r\n    x = numpy.random.randint(0, 2, size=(100, 3, 10)).astype(numpy.float64)\r\n    y = numpy.random.randint(0, 2, size=(100, 3, 10)).astype(numpy.float64)\r\n\r\n    # Pytorch baseline\r\n    torch_result = model(x[[0]], y[[0]])\r\n\r\n    # Compile with low bit width\r\n    quantized_module = compile_torch_model(\r\n        model, (x, y), n_bits=4, configuration=default_configuration\r\n    )\r\n    qm_result = quantized_module.forward(x[[0]], y[[0]])\r\n    simulation_result = quantized_module.forward(x[[0]], y[[0]], fhe=\"simulate\")\r\n\r\n    # Assert that we have the expected number of outputs\r\n    assert isinstance(qm_result, tuple) and len(qm_result) == 2\r\n    assert isinstance(simulation_result, tuple) and len(simulation_result) == 2\r\n    assert isinstance(torch_result, tuple) and len(torch_result) == 2\r\n\r\n    # Assert that we are exact between simulation and clear quantized\r\n    for qm_res, sim_res in zip(qm_result, simulation_result):\r\n        assert isinstance(qm_res, numpy.ndarray)\r\n        assert isinstance(sim_res, numpy.ndarray)\r\n        assert array_allclose_and_same_shape(qm_res, sim_res, atol=1e-30)\r\n\r\n    # Assert that we aren't too far away from torch with low bit width\r\n    for qm_res, trch_res in zip(qm_result, torch_result):\r\n        assert isinstance(qm_res, numpy.ndarray)\r\n        assert isinstance(trch_res, numpy.ndarray)\r\n\r\n        # Very high tolerance because we use low bit width\r\n        assert array_allclose_and_same_shape(qm_res, trch_res, atol=1e-1)\r\n\r\n    # Create quantized module with high bit width\r\n    quantized_module = build_quantized_module(\r\n        model,\r\n        (x, y),\r\n        n_bits=24,\r\n    )\r\n    qm_result = quantized_module.forward(x[[0]], y[[0]])\r\n\r\n    # Assert that we the correct number of outputs again\r\n    assert isinstance(qm_result, tuple) and len(qm_result) == 2\r\n\r\n    # Assert that we have the same results as torch with high bit width quantization\r\n    for qm_res, trch_res in zip(qm_result, torch_result):\r\n        assert isinstance(qm_res, numpy.ndarray)\r\n        assert isinstance(trch_res, numpy.ndarray)\r\n\r\n        # Very low tolerance because we use high bit width\r\n        assert array_allclose_and_same_shape(qm_res, trch_res, atol=1e-10)",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_shape_operations_net(\r\n    model_class,\r\n    simulate,\r\n    n_channels,\r\n    is_qat,\r\n    default_configuration,\r\n    check_graph_output_has_no_tlu,\r\n    check_float_array_equal,\r\n):\r\n    \"\"\"Test a pattern of reshaping, concatenation, chunk extraction.\"\"\"\r\n    model = model_class(is_qat)\r\n\r\n    # Shape transformation do not support >1 example in the inputset\r\n    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3871\r\n    inputset = numpy.random.uniform(size=(1, n_channels, 2, 2))\r\n\r\n    if is_qat:\r\n        quantized_module = compile_brevitas_qat_model(\r\n            model,\r\n            inputset,\r\n            configuration=default_configuration,\r\n            p_error=0.01,\r\n        )\r\n    else:\r\n        quantized_module = compile_torch_model(\r\n            model,\r\n            inputset,\r\n            configuration=default_configuration,\r\n            n_bits=3,\r\n            p_error=0.01,\r\n        )\r\n\r\n    # In QAT quantization options are consistent across all the layers\r\n    # which allows for the elimination of TLUs\r\n    # In PTQ there are TLUs in the graph because Shape/Concat/Transpose\r\n    # must quantize inputs with some default quantization options\r\n\r\n    # In QAT testing in FHE is fast since there are no TLUs\r\n    # For PTQ we only test that the model can be compiled and that it can be executed\r\n    if is_qat or simulate:\r\n        fhe_mode = \"simulate\" if simulate else \"execute\"\r\n\r\n        predictions = quantized_module.forward(inputset, fhe=fhe_mode)\r\n\r\n        torch_output = model(torch.tensor(inputset)).detach().numpy()\r\n\r\n        assert predictions.shape == torch_output.shape, \"Output shape must be the same.\"\r\n\r\n        # In PTQ the results do not match because of a-priori set quantization options\r\n        # Currently no solution for concat/reshape/transpose correctness in PTQ is proposed.\r\n        if is_qat:\r\n            check_float_array_equal(torch_output, predictions, atol=0.05, rtol=0)\r\n\r\n            # In QAT, since the quantization is defined a-priori, all TLUs will be removed\r\n            # and the input quantizer is moved to the clear. We can thus check there are no TLUs\r\n            # in the graph\r\n            check_graph_output_has_no_tlu(quantized_module.fhe_circuit.graph)\r\n            assert \"lookup_table\" not in quantized_module.fhe_circuit.mlir",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_qat_import_bits_check(default_configuration):\r\n    \"\"\"Test that compile_brevitas_qat_model does not need an n_bits config.\"\"\"\r\n\r\n    input_features = 10\r\n\r\n    model = SingleMixNet(False, True, 10, 2)\r\n\r\n    n_examples = 50\r\n\r\n    # All these n_bits configurations should be valid\r\n    # and produce the same result, as the input/output bit-widths for this network\r\n    # are ignored due to the input/output TLU elimination\r\n    n_bits_valid = [\r\n        4,\r\n        2,\r\n        {\"model_inputs\": 4, \"model_outputs\": 4},\r\n        {\"model_inputs\": 2, \"model_outputs\": 2},\r\n    ]\r\n\r\n    # Create random input\r\n    inputset = numpy.random.uniform(-100, 100, size=(n_examples, input_features))\r\n\r\n    # Compile with no quantization bit-width, defaults are used\r\n    quantized_numpy_module = compile_brevitas_qat_model(\r\n        model,\r\n        inputset,\r\n        configuration=default_configuration,\r\n    )\r\n\r\n    n_percent_inputset_examples_test = 0.1\r\n    # Using the input-set allows to remove any chance of overflow.\r\n    x_test = create_test_inputset(inputset, n_percent_inputset_examples_test)\r\n\r\n    # The result of compiling without any n_bits (default)\r\n    predictions = quantized_numpy_module.forward(*x_test, fhe=\"disable\")\r\n\r\n    # Compare the results of running with n_bits=None to the results running with\r\n    # all the other n_bits configs. The results should be the same as bit-widths\r\n    # are ignored for this network (they are overridden with Brevitas values stored in ONNX).\r\n    for n_bits in n_bits_valid:\r\n        quantized_numpy_module = compile_brevitas_qat_model(\r\n            model,\r\n            inputset,\r\n            n_bits=n_bits,\r\n            configuration=default_configuration,\r\n        )\r\n\r\n        new_predictions = quantized_numpy_module.forward(*x_test, fhe=\"disable\")\r\n\r\n        assert numpy.all(predictions == new_predictions)\r\n\r\n    n_bits_invalid = [\r\n        {\"XYZ\": 8, \"model_inputs\": 8},\r\n        {\"XYZ\": 8},\r\n    ]\r\n\r\n    # Test that giving a dictionary with invalid keys does not work\r\n    for n_bits in n_bits_invalid:\r\n        with pytest.raises(\r\n            AssertionError, match=\".*n_bits should only contain the following keys.*\"\r\n        ):\r\n            quantized_numpy_module = compile_brevitas_qat_model(\r\n                model,\r\n                inputset,\r\n                n_bits=n_bits,\r\n                configuration=default_configuration,\r\n            )",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def test_lora_move_optimizer():\r\n    \"\"\"Test LoraTrainer.train with batches of different types.\"\"\"\r\n    model = DummyLoRAModel()\r\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\r\n\r\n    class ListDataset(Dataset):\r\n        \"\"\"Dataset with list items.\"\"\"\r\n\r\n        def __init__(self, data):\r\n            self.data = data\r\n\r\n        def __len__(self):\r\n            return len(self.data)\r\n\r\n        def __getitem__(self, idx):\r\n            return self.data[idx]\r\n\r\n    dataset_list = [(torch.randn(5, 10), torch.randn(5, 10)) for _ in range(200)]\r\n    train_loader_list: DataLoader = DataLoader(ListDataset(dataset_list), batch_size=1)\r\n\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    for _, batch in enumerate(train_loader_list):\r\n        res = model(batch[0])[\"logits\"]\r\n        loss = torch.nn.functional.mse_loss(res, batch[1])\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n    optimizer_to(optimizer, \"cpu\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train_and_evaluate_model(\r\n    x_train, y_train, x_test, y_test, batch_size, iteration, model, model_type=\"torch\"\r\n):\r\n    \"\"\"\r\n    Train and evaluate the given model, supporting both torch and quantized models.\r\n    \"\"\"\r\n    x_train_batches, y_train_batches = create_batches(x_train, y_train, batch_size, iteration)\r\n    x_test_batches, _ = create_batches(x_test, y_test, batch_size, iteration)\r\n\r\n    n_features = x_train_batches.shape[2]\r\n    weights, bias = initialize_parameters(1, n_features, 1)\r\n\r\n    if model_type == \"torch\":\r\n        trained_weights = weights\r\n        for i in range(iteration):\r\n            trained_weights = model.forward(\r\n                x_train_batches[[i]], y_train_batches[[i]], trained_weights, bias\r\n            )\r\n        trained_weights = trained_weights.detach().numpy()\r\n    elif model_type == \"quantized\":\r\n        n_bits = 24\r\n\r\n        # Build a compile set for weights and biases\r\n        weights_compile, bias_compile = initialize_parameters(iteration, n_features, 1)\r\n        q_module = build_quantized_module(\r\n            model,\r\n            torch_inputset=(x_train_batches, y_train_batches, weights_compile, bias_compile),\r\n            n_bits=n_bits,\r\n        )\r\n        trained_weights = weights.detach().numpy()\r\n        for i in range(iteration):\r\n            trained_weights = q_module.forward(\r\n                x_train_batches.detach().numpy()[[i]],\r\n                y_train_batches.detach().numpy()[[i]],\r\n                trained_weights,\r\n                bias.detach().numpy(),\r\n            )\r\n\r\n    predictions = []\r\n    for i in range(x_test_batches.shape[0]):\r\n        batch_predictions = model.predict(\r\n            x_test_batches[[i]], torch.tensor(trained_weights, dtype=torch.float32), bias\r\n        ).round()\r\n        predictions.append(batch_predictions)\r\n    predictions = torch.cat(predictions).numpy().flatten()\r\n\r\n    min_length = min(len(predictions), len(y_test))\r\n    return accuracy_score(y_test[:min_length], predictions[:min_length])",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def train_model(self):\r\n\r\n        # training starts\r\n        if self.args.detect_nan:\r\n            torch.autograd.set_detect_anomaly(True)\r\n\r\n        for epoch in range(self.starting_epoch, self.args.epochs):\r\n\r\n            # Set to training mode\r\n            self.model.train()\r\n            self.criterion.train()\r\n\r\n            # Init metrics\r\n            epoch_meters = TrainingEpochMeters()\r\n            start_data_loading = time.time()\r\n\r\n            for i, data in enumerate(self.train_loader):\r\n                (input, target) = data\r\n                input = input.to(self.device, non_blocking=True)\r\n                target = target.to(self.device, non_blocking=True)\r\n\r\n                target_var = target\r\n\r\n                # measure data loading time\r\n                epoch_meters.data_time.update(time.time() - start_data_loading)\r\n\r\n                # Training batch starts\r\n                start_batch = time.time()\r\n                output = self.model(input)\r\n                loss = self.criterion(output, target_var)\r\n\r\n                # compute gradient and do SGD step\r\n                self.optimizer.zero_grad()\r\n                loss.backward()\r\n                self.optimizer.step()\r\n\r\n                self.model.clip_weights(-1, 1)\r\n\r\n                # measure elapsed time\r\n                epoch_meters.batch_time.update(time.time() - start_batch)\r\n\r\n                if i % int(self.args.log_freq) == 0 or i == len(self.train_loader) - 1:\r\n                    prec1, prec5 = accuracy(output.detach(), target, topk=(1, 5))\r\n                    epoch_meters.losses.update(loss.item(), input.size(0))\r\n                    epoch_meters.top1.update(prec1.item(), input.size(0))\r\n                    epoch_meters.top5.update(prec5.item(), input.size(0))\r\n                    self.logger.training_batch_cli_log(\r\n                        epoch_meters, epoch, i, len(self.train_loader)\r\n                    )\r\n\r\n                # training batch ends\r\n                start_data_loading = time.time()\r\n\r\n            # Set the learning rate\r\n            if self.scheduler is not None:\r\n                self.scheduler.step(epoch)\r\n            else:\r\n                # Set the learning rate\r\n                if epoch % 40 == 0:\r\n                    self.optimizer.param_groups[0][\"lr\"] *= 0.5\r\n\r\n            # Perform eval\r\n            with torch.no_grad():\r\n                top1avg = self.eval_model(epoch)\r\n\r\n            # checkpoint\r\n            if top1avg >= self.best_val_acc and not self.args.dry_run:\r\n                self.best_val_acc = top1avg\r\n                self.checkpoint_best(epoch, \"best.tar\")\r\n            elif not self.args.dry_run:\r\n                self.checkpoint_best(epoch, \"checkpoint.tar\")\r\n\r\n        # training ends\r\n        if not self.args.dry_run:\r\n            return os.path.join(self.checkpoints_dir_path, \"best.tar\")",
        "labels": [
            "Gradients Not Cleared Before Backward Propagation"
        ]
    },
    {
        "code": "def train(dev_folder=\"./dev\"):\r\n    # Download the data-sets\r\n    if not os.path.isfile(\"Tweets.csv\"):\r\n        raise ValueError(\r\n            \"Please launch the `download_data.sh` script in order to get the data-sets.\"\r\n        )\r\n\r\n    train = pd.read_csv(\"Tweets.csv\", index_col=0)\r\n\r\n    text_X = train[\"text\"]\r\n    y = train[\"airline_sentiment\"]\r\n    y = y.replace([\"negative\", \"neutral\", \"positive\"], [0, 1, 2])\r\n\r\n    pos_ratio = y.value_counts()[2] / y.value_counts().sum()\r\n    neg_ratio = y.value_counts()[0] / y.value_counts().sum()\r\n    neutral_ratio = y.value_counts()[1] / y.value_counts().sum()\r\n\r\n    print(f\"Proportion of positive examples: {round(pos_ratio * 100, 2)}%\")\r\n    print(f\"Proportion of negative examples: {round(neg_ratio * 100, 2)}%\")\r\n    print(f\"Proportion of neutral examples: {round(neutral_ratio * 100, 2)}%\")\r\n\r\n    # Split in train test\r\n    text_X_train, text_X_test, y_train, y_test = train_test_split(\r\n        text_X, y, test_size=0.1, random_state=42\r\n    )\r\n\r\n    # # ### 2. A transformer approach to text representation\r\n    # #\r\n    # # [**Transformers**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are neural networks that are often trained to predict the next words to appear in a text (this is commonly called self-supervised learning).\r\n    # #\r\n    # # They are powerful tools for all kind of Natural Language Processing tasks but supporting a transformer model in FHE might not always be ideal as they are quite big models. However, we can still leverage their hidden representation for any text and feed it to a more FHE-friendly machine learning model (in this notebook we will use XGBoost) for classification.\r\n    # #\r\n    # # Here we will use the transformer model from the amazing [**Huggingface**](https://huggingface.co/) repository.\r\n\r\n    # Add MPS (for macOS with Apple Silicon or AMD GPUs) support when error is fixed. For now, we\r\n    # observe a decrease in torch's top1 accuracy when using MPS devices\r\n    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3953\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n    # # Load the tokenizer (converts text to tokens)\r\n    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\r\n\r\n    # # Load the pre-trained model\r\n    transformer_model = AutoModelForSequenceClassification.from_pretrained(\r\n        \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\r\n    )\r\n\r\n    # Vectorize the text using the transformer\r\n    list_text_X_train = text_X_train.tolist()\r\n    list_text_X_test = text_X_test.tolist()\r\n\r\n    X_train_transformer = text_to_tensor(list_text_X_train, transformer_model, tokenizer, device)\r\n    X_test_transformer = text_to_tensor(list_text_X_test, transformer_model, tokenizer, device)\r\n    # Now we have a representation for each tweet, we can train a model on these.\r\n\r\n    # Build our model\r\n    model = XGBClassifier()\r\n\r\n    # A gridsearch to find the best parameters\r\n    parameters = {\r\n        \"n_bits\": [2, 3],\r\n        \"max_depth\": [1],\r\n        \"n_estimators\": [10, 30, 50],\r\n        \"n_jobs\": [-1],\r\n    }\r\n\r\n    grid_search = GridSearchCV(model, parameters, cv=3, n_jobs=1, scoring=\"accuracy\")\r\n    grid_search.fit(X_train_transformer, y_train)\r\n\r\n    # Check the accuracy of the best model\r\n    print(f\"Best score: {grid_search.best_score_}\")\r\n\r\n    # Check best hyper-parameters\r\n    print(f\"Best parameters: {grid_search.best_params_}\")\r\n\r\n    # Extract best model\r\n    best_model = grid_search.best_estimator_\r\n    assert isinstance(best_model, XGBClassifier)\r\n\r\n    # Compute the metrics for each class\r\n\r\n    y_proba = best_model.predict_proba(X_test_transformer)\r\n\r\n    # Compute the accuracy\r\n    y_pred = numpy.argmax(y_proba, axis=1)\r\n    accuracy_transformer_xgboost = numpy.mean(y_pred == y_test)\r\n    print(f\"Accuracy: {accuracy_transformer_xgboost:.4f}\")\r\n\r\n    y_pred_positive = y_proba[:, 2]\r\n    y_pred_negative = y_proba[:, 0]\r\n    y_pred_neutral = y_proba[:, 1]\r\n\r\n    ap_positive_transformer_xgboost = average_precision_score((y_test == 2), y_pred_positive)\r\n    ap_negative_transformer_xgboost = average_precision_score((y_test == 0), y_pred_negative)\r\n    ap_neutral_transformer_xgboost = average_precision_score((y_test == 1), y_pred_neutral)\r\n\r\n    print(f\"Average precision score for positive class: \" f\"{ap_positive_transformer_xgboost:.4f}\")\r\n    print(f\"Average precision score for negative class: \" f\"{ap_negative_transformer_xgboost:.4f}\")\r\n    print(f\"Average precision score for neutral class: \" f\"{ap_neutral_transformer_xgboost:.4f}\")\r\n\r\n    # Our FHE-friendly XGBoost model does 38% better than the XGBoost model built over TF-IDF representation of the text. Note that here we are still not using FHE and only evaluating the model.\r\n    # Interestingly, using XGBoost over the transformer representation of the text matches the performance of the transformer model alone.\r\n\r\n    # Get probabilities predictions in clear\r\n    y_pred_test = best_model.predict_proba(X_test_transformer)\r\n\r\n    # See what are the top predictions based on the probabilities in y_pred_test\r\n    print(\"5 most positive tweets (class 2):\")\r\n    for i in range(5):\r\n        print(text_X_test.iloc[y_pred_test[:, 2].argsort()[-1 - i]])\r\n\r\n    print(\"-\" * 100)\r\n\r\n    print(\"5 most negative tweets (class 0):\")\r\n    for i in range(5):\r\n        print(text_X_test.iloc[y_pred_test[:, 0].argsort()[-1 - i]])\r\n\r\n    # Now we can see where the model is wrong\r\n    y_pred_test_0 = y_pred_test[y_test == 0]\r\n    text_X_test_0 = text_X_test[y_test == 0]\r\n\r\n    print(\"5 most positive (predicted) tweets that are actually negative (ground truth class 0):\")\r\n    for i in range(5):\r\n        print(text_X_test_0.iloc[y_pred_test_0[:, 2].argsort()[-1 - i]])\r\n\r\n    print(\"-\" * 100)\r\n\r\n    y_pred_test_2 = y_pred_test[y_test == 2]\r\n    text_X_test_2 = text_X_test[y_test == 2]\r\n    print(\"5 most negative (predicted) tweets that are actually positive (ground truth class 2):\")\r\n    for i in range(5):\r\n        print(text_X_test_2.iloc[y_pred_test_2[:, 0].argsort()[-1 - i]])\r\n\r\n    # Interestingly, these misclassifications are not obvious and some actually look rather like mislabeled. Also, it seems that the model is having a hard time to find ironic tweets.\r\n    #\r\n    # Now we have our model trained which has some great accuracy. We can have it predict over the encrypted representation.\r\n\r\n    # ### Sentiment Analysis of the Tweet with Fully Homomorphic Encryption\r\n    #\r\n    # Now that we have our model ready for FHE inference and our data ready for encryption we can use the model in a privacy preserving manner with FHE.\r\n\r\n    # Compile the model to get the FHE inference engine\r\n    # (this may take a few minutes depending on the selected model)\r\n    start = time.perf_counter()\r\n    best_model.compile(X_train_transformer)\r\n    end = time.perf_counter()\r\n    print(f\"Compilation time: {end - start:.4f} seconds\")\r\n\r\n    # Write a custom example and predict in FHE\r\n    tested_tweet = [\"AirFrance is awesome, almost as much as Zama!\"]\r\n    X_tested_tweet = text_to_tensor(tested_tweet, transformer_model, tokenizer, device)\r\n    clear_proba = best_model.predict_proba(X_tested_tweet)\r\n\r\n    # Now we predict with FHE over a single tweet and print the time it takes\r\n    start = time.perf_counter()\r\n    decrypted_proba = best_model.predict_proba(X_tested_tweet, fhe=\"execute\")\r\n    end = time.perf_counter()\r\n    fhe_exec_time = end - start\r\n    print(f\"FHE inference time: {fhe_exec_time:.4f} seconds\")\r\n\r\n    print(f\"Probabilities from the FHE inference: {decrypted_proba}\")\r\n    print(f\"Probabilities from the clear model: {clear_proba}\")\r\n\r\n    # Export the final model such that we can reuse it in a client/server environment\r\n\r\n    # Export the model to ONNX\r\n    onnx.save(best_model.onnx_model_, \"server_model.onnx\")  # pylint: disable=protected-access\r\n\r\n    # Export some data to be used for compilation\r\n    X_train_numpy = X_train_transformer[:100]\r\n\r\n    # Merge the two arrays in a pandas dataframe\r\n    X_test_numpy_df = pd.DataFrame(X_train_numpy)\r\n\r\n    # to csv\r\n    X_test_numpy_df.to_csv(\"samples_for_compilation.csv\")\r\n\r\n    # Save the model to be pushed to a server later\r\n    from concrete.ml.deployment import FHEModelDev\r\n\r\n    fhe_api = FHEModelDev(dev_folder, best_model)\r\n    fhe_api.save()",
        "labels": [
            "Columns and DataType Not Explicitly Set",
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def run_experiment(resnet18, calib_images, processor, fhe_mode=\"disable\"):\r\n\r\n    # Define ranges for n_bits and rounding_threshold_bits\r\n    n_bits_range = range(2, 16)\r\n    rounding_threshold_bits_range = list(range(2, 9)) + [None]  # 2 to 8 and None\r\n\r\n    # Initialize a dictionary to store accuracies for each combination\r\n    accuracies = {}\r\n\r\n    total_combinations = len(n_bits_range) * len(rounding_threshold_bits_range)\r\n    current_combination = 0\r\n\r\n    # Loop over the ranges of n_bits and rounding_threshold_bits\r\n    for n_bits in n_bits_range:\r\n        for rounding_threshold_bits in rounding_threshold_bits_range:\r\n            current_combination += 1\r\n            print(f\"\\nProcessing combination {current_combination}/{total_combinations}\")\r\n            print(f\"n_bits: {n_bits}, rounding_threshold_bits: {rounding_threshold_bits}\")\r\n\r\n            q_module = compile_model(\r\n                resnet18, calib_images, n_bits, rounding_threshold_bits, fhe_mode\r\n            )\r\n\r\n            outputs = []\r\n            all_labels = []\r\n\r\n            print(\"Evaluating model...\")\r\n            for batch in processor.dataloader:\r\n                batch_images = batch[\"pixel_values\"].detach().numpy()\r\n                batch_outputs = q_module.forward(batch_images, fhe=fhe_mode)\r\n                outputs.append(batch_outputs)\r\n                all_labels.append(batch[\"labels\"].detach().numpy())\r\n\r\n            outputs = torch.from_numpy(np.concatenate(outputs))\r\n            all_labels = torch.from_numpy(np.concatenate(all_labels))\r\n\r\n            # Calculate and store accuracy\r\n            fhe_accuracy = processor.accuracy(outputs, all_labels)\r\n            accuracies[(n_bits, rounding_threshold_bits)] = fhe_accuracy\r\n            print(f\"Accuracy: {fhe_accuracy:.4f}\")\r\n\r\n    # Convert accuracies to a 2D array for plotting\r\n    accuracy_matrix = np.zeros((len(n_bits_range), len(rounding_threshold_bits_range)))\r\n    for i, n_bits in enumerate(n_bits_range):\r\n        for j, rounding_threshold_bits in enumerate(rounding_threshold_bits_range):\r\n            accuracy_matrix[i, j] = accuracies[(n_bits, rounding_threshold_bits)]\r\n\r\n    # Save the accuracy matrix to disk\r\n    np.save(\"accuracy_matrix.npy\", accuracy_matrix)\r\n\r\n    print(\"\\nGenerating plot...\")\r\n\r\n    # Plotting\r\n    fig, ax = plt.subplots(figsize=(10, 8))\r\n    cax = ax.matshow(accuracy_matrix, cmap=\"viridis\")\r\n    fig.colorbar(cax)\r\n\r\n    # Set ticks and labels\r\n    ax.set_xticks(range(len(rounding_threshold_bits_range)))\r\n    ax.set_xticklabels([str(x) for x in rounding_threshold_bits_range], rotation=45)\r\n    ax.set_yticks(range(len(n_bits_range)))\r\n    ax.set_yticklabels([str(x) for x in n_bits_range])\r\n    ax.set_xlabel(\"Rounding Threshold Bits\")\r\n    ax.set_ylabel(\"N Bits\")\r\n    ax.set_title(f\"Accuracy of FHE ({fhe_mode})\")\r\n\r\n    # Annotate each cell with the accuracy percentage\r\n    for i in range(len(n_bits_range)):\r\n        for j in range(len(rounding_threshold_bits_range)):\r\n            ax.text(j, i, f\"{accuracy_matrix[i, j]:.2f}\", va=\"center\", ha=\"center\", color=\"white\")\r\n\r\n    plt.tight_layout()\r\n    plt.savefig(\"accuracy_matrix.png\", dpi=300)\r\n    print(\"Plot saved as accuracy_matrix.png\")",
        "labels": [
            "PyTorch Call Method Misused"
        ]
    },
    {
        "code": "def train(\r\n    model: torch.nn.Module,\r\n    train_loader: DataLoader,\r\n    test_loader: DataLoader,\r\n    param: Dict,\r\n    step: int = 1,\r\n    device: str = \"cpu\",\r\n) -> torch.nn.Module:\r\n    \"\"\"Training the model.\r\n\r\n    Args:\r\n        model (nn.Module): A PyTorch or Brevitas network.\r\n        train_loader (DataLoader): The training set.\r\n        test_loader (DataLoader): The test set.\r\n        param (Dict): Set of hyper-parameters to use depending on whether\r\n            CIFAR-10 or CIFAR-100 is used.\r\n        step (int): Display the loss and accuracy every `epoch % step`.\r\n        device (str): Device type.\r\n    Returns:\r\n        nn.Module: the trained model.\r\n    \"\"\"\r\n\r\n    param[\"accuracy_test\"] = param.get(\"accuracy_test\", [])\r\n    param[\"accuracy_train\"] = param.get(\"accuracy_train\", [])\r\n    param[\"loss_test_history\"] = param.get(\"loss_test_history\", [])\r\n    param[\"loss_train_history\"] = param.get(\"loss_train_history\", [])\r\n    param[\"criterion\"] = param.get(\"criterion\", torch.nn.CrossEntropyLoss())\r\n\r\n    if param[\"seed\"]:\r\n\r\n        torch.manual_seed(param[\"seed\"])\r\n        random.seed(param[\"seed\"])\r\n\r\n    model = model.to(device)\r\n\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=param[\"lr\"])\r\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(\r\n        optimizer, milestones=param[\"milestones\"], gamma=param[\"gamma\"]\r\n    )\r\n\r\n    # Save the state_dict\r\n    dir = Path(\".\") / param[\"dir\"] / param[\"training\"]\r\n    dir.mkdir(parents=True, exist_ok=True)\r\n\r\n    # To avoid breaking up the tqdm bar\r\n    with tqdm(total=param[\"epochs\"], file=sys.stdout) as pbar:\r\n\r\n        for i in range(param[\"epochs\"]):\r\n            # Train the model\r\n            model.train()\r\n            loss_batch_train, accuracy_batch_train = [], []\r\n\r\n            for x, y in train_loader:\r\n                x, y = x.to(device), y.to(device)\r\n\r\n                optimizer.zero_grad()\r\n                yhat = model(x)\r\n                loss_train = param[\"criterion\"](yhat, y)\r\n                loss_train.backward()\r\n                optimizer.step()\r\n\r\n                loss_batch_train.append(loss_train.item())\r\n                accuracy_batch_train.extend((yhat.argmax(1) == y).cpu().float().tolist())\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            param[\"accuracy_train\"].append(np.mean(accuracy_batch_train))\r\n            param[\"loss_train_history\"].append(np.mean(loss_batch_train))\r\n\r\n            # Evaluation during training:\r\n            # Disable autograd engine (no backpropagation)\r\n            # To reduce memory usage and to speed up computations\r\n            with torch.no_grad():\r\n                # Notify batchnormalization & dropout layers to work in eval mode\r\n                model.eval()\r\n                loss_batch_test, accuracy_batch_test = [], []\r\n                for x, y in test_loader:\r\n                    x, y = x.to(device), y.to(device)\r\n                    yhat = model(x)\r\n                    loss_test = param[\"criterion\"](yhat, y)\r\n                    loss_batch_test.append(loss_test.item())\r\n                    accuracy_batch_test.extend((yhat.argmax(1) == y).cpu().float().tolist())\r\n\r\n                param[\"accuracy_test\"].append(np.mean(accuracy_batch_test))\r\n                param[\"loss_test_history\"].append(np.mean(loss_batch_test))\r\n\r\n            if i % step == 0:\r\n                pbar.write(\r\n                    f\"Epoch {i:2}: Train loss = {param['loss_train_history'][-1]:.4f} \"\r\n                    f\"VS Test loss = {param['loss_test_history'][-1]:.4f} - \"\r\n                    f\"Accuracy train: {param['accuracy_train'][-1]:.4f} \"\r\n                    f\"VS Accuracy test: {param['accuracy_test'][-1]:.4f}\"\r\n                )\r\n                pbar.update(step)\r\n\r\n    print(\"Save in:\", f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\")\r\n    torch.save(\r\n        model.state_dict(), f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\"\r\n    )\r\n\r\n    with open(f\"{dir}/{param['dataset_name']}_history.pkl\", \"wb\") as f:\r\n        pkl.dump(param, f)\r\n\r\n    torch.cuda.empty_cache()\r\n\r\n    return model",
        "labels": [
            "Chain Indexing"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    for estimator in [DecisionTreeClassifier(), SVC(gamma=\"scale\")]:\r\n        clf = CompatibleBaggingClassifier(\r\n            estimator=estimator,\r\n            n_estimators=100,\r\n            bootstrap=True,\r\n            oob_score=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n        test_score = clf.score(X_test, y_test)\r\n\r\n        assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n        # Test with few estimators\r\n        with pytest.warns(UserWarning):\r\n            with pytest.warns(RuntimeWarning):\r\n                CompatibleBaggingClassifier(\r\n                    estimator=estimator,\r\n                    n_estimators=1,\r\n                    bootstrap=True,\r\n                    oob_score=True,\r\n                    random_state=0,\r\n                ).fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    for estimator in [DecisionTreeClassifier(), SVC(gamma=\"scale\")]:\r\n        clf = OverBaggingClassifier(\r\n            estimator=estimator,\r\n            n_estimators=100,\r\n            bootstrap=True,\r\n            oob_score=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n        test_score = clf.score(X_test, y_test)\r\n\r\n        assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n        # Test with few estimators\r\n        with pytest.warns(UserWarning):\r\n            with pytest.warns(RuntimeWarning):\r\n                OverBaggingClassifier(\r\n                    estimator=estimator,\r\n                    n_estimators=1,\r\n                    bootstrap=True,\r\n                    oob_score=True,\r\n                    random_state=0,\r\n                ).fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    for estimator in [DecisionTreeClassifier(), SVC(gamma=\"scale\")]:\r\n        clf = SMOTEBaggingClassifier(\r\n            estimator=estimator,\r\n            n_estimators=100,\r\n            bootstrap=True,\r\n            oob_score=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n        test_score = clf.score(X_test, y_test)\r\n\r\n        assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n        # Test with few estimators\r\n        with pytest.warns(UserWarning):\r\n            with pytest.warns(RuntimeWarning):\r\n                SMOTEBaggingClassifier(\r\n                    estimator=estimator,\r\n                    n_estimators=1,\r\n                    bootstrap=True,\r\n                    oob_score=True,\r\n                    random_state=0,\r\n                ).fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bootstrap_samples():\r\n    # Test that bootstrapping samples generate non-perfect base estimators.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    estimator = DecisionTreeClassifier().fit(X_train, y_train)\r\n\r\n    # with bootstrap, trees are no longer perfect on the training set\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        ensemble = EasyEnsembleClassifier(\r\n            estimator=DecisionTreeClassifier(),\r\n            max_samples=1.0,\r\n            bootstrap=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n    assert ensemble.score(X_train, y_train) < estimator.score(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_consistency():\r\n    # Make sure OOB scores are identical when random_state, estimator, and\r\n    # training data are fixed and fitting is done twice\r\n    X, y = make_hastie_10_2(n_samples=200, random_state=1)\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        easyens = EasyEnsembleClassifier(\r\n            KNeighborsClassifier(),\r\n            max_samples=0.5,\r\n            max_features=0.5,\r\n            oob_score=True,\r\n            random_state=1,\r\n        )\r\n    assert easyens.fit(X, y).oob_score_ == easyens.fit(X, y).oob_score_",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_max_samples_consistency():\r\n    # Make sure validated max_samples and original max_samples are identical\r\n    # when valid integer max_samples supplied by user\r\n    max_samples = 100\r\n    X, y = make_hastie_10_2(n_samples=2 * max_samples, random_state=1)\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        easyens = EasyEnsembleClassifier(\r\n            KNeighborsClassifier(),\r\n            max_samples=max_samples,\r\n            max_features=0.5,\r\n            random_state=1,\r\n        )\r\n        easyens.fit(X, y)\r\n    assert easyens._max_samples == max_samples",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        for estimator in [DecisionTreeClassifier(), SVC(gamma=\"scale\")]:\r\n            clf = EasyEnsembleClassifier(\r\n                estimator=estimator,\r\n                n_estimators=100,\r\n                bootstrap=True,\r\n                oob_score=True,\r\n                random_state=0,\r\n            ).fit(X_train, y_train)\r\n\r\n            test_score = clf.score(X_test, y_test)\r\n\r\n            assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n            # Test with few estimators\r\n            with pytest.warns(UserWarning):\r\n                with pytest.warns(RuntimeWarning):\r\n                    EasyEnsembleClassifier(\r\n                        estimator=estimator,\r\n                        n_estimators=1,\r\n                        bootstrap=True,\r\n                        oob_score=True,\r\n                        random_state=0,\r\n                    ).fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_easy_ensemble_classifier(estimator, params):\r\n    # Check classification for various parameter settings.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    if estimator is None or type(estimator) == AdaBoostClassifier:\r\n        easyens = EasyEnsembleClassifier(\r\n            estimator=estimator, random_state=0, **params\r\n        ).fit(X_train, y_train)\r\n    else:\r\n        with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n            easyens = EasyEnsembleClassifier(\r\n                estimator=estimator, random_state=0, **params\r\n            ).fit(X_train, y_train)\r\n\r\n    easyens.predict(X_test)\r\n    easyens.predict_proba(X_test)\r\n    easyens.score(X_test, y_test)\r\n    if hasattr(estimator, \"decision_function\"):\r\n        easyens.decision_function(X_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_single_estimator():\r\n    # Check singleton ensembles.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        clf1 = EasyEnsembleClassifier(\r\n            estimator=KNeighborsClassifier(),\r\n            n_estimators=1,\r\n            bootstrap=False,\r\n            bootstrap_features=False,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n    clf2 = make_pipeline(\r\n        RandomUnderSampler(random_state=clf1.estimators_[0].steps[0][1].random_state),\r\n        KNeighborsClassifier(),\r\n    ).fit(X_train, y_train)\r\n\r\n    assert_array_equal(clf1.predict(X_test), clf2.predict(X_test))",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bootstrap_features():\r\n    # Test that bootstrapping features may generate duplicate features.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        ensemble = EasyEnsembleClassifier(\r\n            estimator=DecisionTreeClassifier(),\r\n            max_features=1.0,\r\n            bootstrap_features=False,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n    for features in ensemble.estimators_features_:\r\n        assert np.unique(features).shape[0] == X.shape[1]\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        ensemble = EasyEnsembleClassifier(\r\n            estimator=DecisionTreeClassifier(),\r\n            max_features=1.0,\r\n            bootstrap_features=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n    unique_features = [\r\n        np.unique(features).shape[0] for features in ensemble.estimators_features_\r\n    ]\r\n    assert np.median(unique_features) < X.shape[1]",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_probability():\r\n    # Predict probabilities.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\r\n        # Normal case\r\n        with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n            ensemble = EasyEnsembleClassifier(\r\n                estimator=DecisionTreeClassifier(), random_state=0\r\n            ).fit(X_train, y_train)\r\n\r\n        assert_array_almost_equal(\r\n            np.sum(ensemble.predict_proba(X_test), axis=1),\r\n            np.ones(len(X_test)),\r\n        )\r\n\r\n        assert_array_almost_equal(\r\n            ensemble.predict_proba(X_test),\r\n            np.exp(ensemble.predict_log_proba(X_test)),\r\n        )",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_bagging_with_pipeline():\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        estimator = EasyEnsembleClassifier(\r\n            make_pipeline(SelectKBest(k=1), DecisionTreeClassifier()),\r\n            max_features=2,\r\n        )\r\n        estimator.fit(X, y).predict(X)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_estimator_init(estimator, params):\r\n    \"\"\"Check classification for various parameter settings.\"\"\"\r\n    if (\r\n        params[\"k_bins\"] == 5\r\n        and type(estimator) is not DummyClassifier\r\n        and (not params[\"replacement\"] and not params[\"soft_resample_flag\"])\r\n    ):\r\n        with pytest.raises(RuntimeError, match=\"bin with insufficient number of data\"):\r\n            spe = SelfPacedEnsembleClassifier(\r\n                estimator=estimator, random_state=0, **params\r\n            ).fit(X_train, y_train)\r\n    else:\r\n        spe = SelfPacedEnsembleClassifier(\r\n            estimator=estimator, random_state=0, **params\r\n        ).fit(X_train, y_train)\r\n        spe.predict(X_test)\r\n        spe.predict_proba(X_test)\r\n        spe.score(X_test, y_test)\r\n        if hasattr(estimator, \"decision_function\"):\r\n            spe.decision_function(X_test)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_estimator():\r\n    # Check estimator and its default values.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    ensemble = EasyEnsembleClassifier(None, n_jobs=3, random_state=0).fit(\r\n        X_train, y_train\r\n    )\r\n\r\n    assert isinstance(ensemble.estimator_.steps[-1][1], AdaBoostClassifier)\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        ensemble = EasyEnsembleClassifier(\r\n            DecisionTreeClassifier(), n_jobs=3, random_state=0\r\n        ).fit(X_train, y_train)\r\n\r\n    assert isinstance(ensemble.estimator_.steps[-1][1], DecisionTreeClassifier)\r\n\r\n    with pytest.warns(UserWarning, match=\"You are trying to set\"):\r\n        ensemble = EasyEnsembleClassifier(\r\n            Perceptron(max_iter=1000, tol=1e-3), n_jobs=3, random_state=0\r\n        ).fit(X_train, y_train)\r\n\r\n    assert isinstance(ensemble.estimator_.steps[-1][1], Perceptron)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    },
    {
        "code": "def test_oob_score_classification():\r\n    # Check that oob prediction is a good estimation of the generalization\r\n    # error.\r\n    X, y = make_imbalance(\r\n        iris.data,\r\n        iris.target,\r\n        sampling_strategy={0: 20, 1: 25, 2: 50},\r\n        random_state=0,\r\n    )\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\n    for estimator in [DecisionTreeClassifier(), SVC(gamma=\"scale\")]:\r\n        clf = UnderBaggingClassifier(\r\n            estimator=estimator,\r\n            n_estimators=100,\r\n            bootstrap=True,\r\n            oob_score=True,\r\n            random_state=0,\r\n        ).fit(X_train, y_train)\r\n\r\n        test_score = clf.score(X_test, y_test)\r\n\r\n        assert abs(test_score - clf.oob_score_) < 0.1\r\n\r\n        # Test with few estimators\r\n        with pytest.warns(UserWarning):\r\n            with pytest.warns(RuntimeWarning):\r\n                UnderBaggingClassifier(\r\n                    estimator=estimator,\r\n                    n_estimators=1,\r\n                    bootstrap=True,\r\n                    oob_score=True,\r\n                    random_state=0,\r\n                ).fit(X_train, y_train)",
        "labels": [
            "Hyperparameter Not Explicitly Set"
        ]
    }
]