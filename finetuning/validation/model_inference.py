class ModelInference:
    """
    A class for handling inference with a machine learning model.

    Attributes:
        model: The pretrained machine learning model used for inference.
        tokenizer: The tokenizer associated with the model, used to preprocess
                   input data and decode model output.
        device (str): The device on which the inference will be performed
                      (e.g., 'cuda' for GPU, 'cpu' for CPU).
    """

    def __init__(self, model, tokenizer, device="cuda"):
        """
        Initializes the ModelInference class.

        Args:
            model: The pretrained model to use for inference.
            tokenizer: The tokenizer associated with the model.
            device (str): The device on which to
                          perform inference (default: 'cuda').
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def infer(self, user_message):
        """
        Generates predictions for a given user message.

        Args:
            user_message (str): The input message from the user.

        Returns:
            list[str]: A list of decoded responses generated by the model.

        """
        inputs = self.tokenizer.apply_chat_template(
            user_message,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(self.device)

        outputs = self.model.generate(
            input_ids=inputs,
            max_new_tokens=128,
            use_cache=True,
            temperature=1.5,
            min_p=0.1,
        )
        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
